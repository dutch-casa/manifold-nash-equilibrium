{"id": "0", "text": "1\nMultiscale Aggregated Hierarchical Attention\n(MAHA): A Game-Theoretic and\nOptimization-Driven Approach to Efficient\nContextual Modeling in Large Language Models\nCaner Erden\nAbstract—The quadratic computational complexity of Multi-\nHead Self-Attention (MHSA) remains a fundamental bottleneck\nin scaling Large Language Models (LLMs) for long-context tasks.\nWhile sparse and linearized attention mechanisms attempt to\nmitigate this, they often compromise the representation of global\ndependencies or fail to capture multiscale semantic granularity\neffectively. In this paper, we propose Multiscale Aggregated\nHierarchical Attention (MAHA), a novel architectural framework\nthat reformulates the attention mechanism through hierarchical\ndecomposition and mathematically rigorous aggregation. Unlike\nconventional approaches that treat token interactions at a single\nresolution, MAHA dynamically partitions the input sequence into\nhierarchical scales via learnable downsampling operators.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "1", "text": "While sparse and linearized attention mechanisms attempt to\nmitigate this, they often compromise the representation of global\ndependencies or fail to capture multiscale semantic granularity\neffectively. In this paper, we propose Multiscale Aggregated\nHierarchical Attention (MAHA), a novel architectural framework\nthat reformulates the attention mechanism through hierarchical\ndecomposition and mathematically rigorous aggregation. Unlike\nconventional approaches that treat token interactions at a single\nresolution, MAHA dynamically partitions the input sequence into\nhierarchical scales via learnable downsampling operators. The\ncore innovation lies in its aggregation strategy: we model the\nfusion of scale-specific attention matrices as a resource allocation\nproblem, solved via a convex optimization framework or a Nash\nequilibrium-based game-theoretic approach. This ensures a theo-\nretically optimal balance between local nuance and global context\nfidelity. Implemented within a hybrid dilated-convolutional trans-\nformer backbone, MAHA utilizes differentiable optimization\nlayers to enable end-to-end training.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "2", "text": "The\ncore innovation lies in its aggregation strategy: we model the\nfusion of scale-specific attention matrices as a resource allocation\nproblem, solved via a convex optimization framework or a Nash\nequilibrium-based game-theoretic approach. This ensures a theo-\nretically optimal balance between local nuance and global context\nfidelity. Implemented within a hybrid dilated-convolutional trans-\nformer backbone, MAHA utilizes differentiable optimization\nlayers to enable end-to-end training. Experimental evaluations\ndemonstrate that MAHA achieves superior scalability; empirical\nFLOPs analysis confirms an 81% reduction in computational cost\nat a sequence length of 4096 compared to standard attention. This\nwork bridges the gap between optimization theory and sequence\nmodeling, offering a scalable solution for next-generation LLMs.\nKeywords: Large Language Models, Hierarchical Attention,\nGame Theory, Convex Optimization, Nash Equilibrium, Efficient\nTransformers.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "3", "text": "Implemented within a hybrid dilated-convolutional trans-\nformer backbone, MAHA utilizes differentiable optimization\nlayers to enable end-to-end training. Experimental evaluations\ndemonstrate that MAHA achieves superior scalability; empirical\nFLOPs analysis confirms an 81% reduction in computational cost\nat a sequence length of 4096 compared to standard attention. This\nwork bridges the gap between optimization theory and sequence\nmodeling, offering a scalable solution for next-generation LLMs.\nKeywords: Large Language Models, Hierarchical Attention,\nGame Theory, Convex Optimization, Nash Equilibrium, Efficient\nTransformers.\nI. INTRODUCTION\nThe advent of transformer-based architectures has funda-\nmentally revolutionized natural language processing (NLP),\nestablishing the Multi-Head Self-Attention (MHSA) mecha-\nnism as the cornerstone of modern large language models\n(LLMs) [1].", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "4", "text": "This\nwork bridges the gap between optimization theory and sequence\nmodeling, offering a scalable solution for next-generation LLMs.\nKeywords: Large Language Models, Hierarchical Attention,\nGame Theory, Convex Optimization, Nash Equilibrium, Efficient\nTransformers.\nI. INTRODUCTION\nThe advent of transformer-based architectures has funda-\nmentally revolutionized natural language processing (NLP),\nestablishing the Multi-Head Self-Attention (MHSA) mecha-\nnism as the cornerstone of modern large language models\n(LLMs) [1]. Despite its efficacy, this mechanism confronts\ntwo critical challenges: (i) computational inefficiency arising\nfrom quadratic complexity (O(N 2)) with respect to sequence\nlength, and (ii) the inherent trade-off between capturing fine-\ngrained local patterns and coarse-grained global dependencies\nThis work was supported in part by Sakarya University of Applied Sciences.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "5", "text": "Despite its efficacy, this mechanism confronts\ntwo critical challenges: (i) computational inefficiency arising\nfrom quadratic complexity (O(N 2)) with respect to sequence\nlength, and (ii) the inherent trade-off between capturing fine-\ngrained local patterns and coarse-grained global dependencies\nThis work was supported in part by Sakarya University of Applied Sciences.\n(Corresponding author: Caner Erden)\nC. Erden is with the Department of Computer Engineering, Faculty of\nTechnology, Sakarya University of Applied Science, Sakarya, T¨urkiye (e-mail:\ncerden@subu.edu.tr; ORCID: 0000-0002-7311-862X).\nData\nAvailability:\nThe\nsource\ncode\nis\navailable\nat\nhttps://github.com/canererden/MAHA-Project\n(arXiv:\nhttps://arxiv.org/abs/2512.14925).\nsimultaneously. These limitations become increasingly pro-\nnounced as LLMs scale to process extended contexts and\nmodel complex linguistic structures [2].", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "6", "text": "Data\nAvailability:\nThe\nsource\ncode\nis\navailable\nat\nhttps://github.com/canererden/MAHA-Project\n(arXiv:\nhttps://arxiv.org/abs/2512.14925).\nsimultaneously. These limitations become increasingly pro-\nnounced as LLMs scale to process extended contexts and\nmodel complex linguistic structures [2].\nCurrent methodologies attempting to mitigate these chal-\nlenges typically rely on sparse attention patterns or hierarchical\nrepresentations. Sparse attention strategies alleviate computa-\ntional overhead by restricting token interactions to predefined\nor learned patterns; however, this often results in informa-\ntion loss and suboptimal context modeling, particularly for\nlong-range dependencies [3]. Conversely, hierarchical meth-\nods decompose the input into multiple levels of granularity\nbut frequently lack a principled mathematical framework for\nintegration. This often leads to ad-hoc aggregation schemes\nthat fail to preserve the full contextual richness of the input\nembedding space [4].", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "7", "text": "Sparse attention strategies alleviate computa-\ntional overhead by restricting token interactions to predefined\nor learned patterns; however, this often results in informa-\ntion loss and suboptimal context modeling, particularly for\nlong-range dependencies [3]. Conversely, hierarchical meth-\nods decompose the input into multiple levels of granularity\nbut frequently lack a principled mathematical framework for\nintegration. This often leads to ad-hoc aggregation schemes\nthat fail to preserve the full contextual richness of the input\nembedding space [4].\nTo bridge this gap, we introduce Multiscale Aggregated\nHierarchical Attention (MAHA), a novel framework that ad-\ndresses these limitations through a mathematically rigorous\napproach to multiscale attention computation and aggregation.\nMAHA dynamically partitions the input sequence into hier-\narchical scales, where each scale represents a distinct level\nof contextual abstraction. Distinguishing itself from prior hi-\nerarchical approaches, MAHA leverages convex optimization\n(CO) or game-theoretic equilibrium to synthesize these scales.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "8", "text": "To bridge this gap, we introduce Multiscale Aggregated\nHierarchical Attention (MAHA), a novel framework that ad-\ndresses these limitations through a mathematically rigorous\napproach to multiscale attention computation and aggregation.\nMAHA dynamically partitions the input sequence into hier-\narchical scales, where each scale represents a distinct level\nof contextual abstraction. Distinguishing itself from prior hi-\nerarchical approaches, MAHA leverages convex optimization\n(CO) or game-theoretic equilibrium to synthesize these scales.\nThis ensures that the aggregation process is not merely a\nweighted average but an optimization problem that balances\nefficiency and contextual awareness. Consequently, the pro-\nposed method provides a systematic mechanism to reconcile\nlocal nuances with global dependencies while maintaining\ncomputational tractability.\nThe primary contributions of this work are threefold:\n• Multiscale Decomposition: We introduce a robust de-\ncomposition strategy where the input sequence is pro-\ncessed across independent scales to isolate and capture\ndistinct levels of contextual granularity.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "9", "text": "This ensures that the aggregation process is not merely a\nweighted average but an optimization problem that balances\nefficiency and contextual awareness. Consequently, the pro-\nposed method provides a systematic mechanism to reconcile\nlocal nuances with global dependencies while maintaining\ncomputational tractability.\nThe primary contributions of this work are threefold:\n• Multiscale Decomposition: We introduce a robust de-\ncomposition strategy where the input sequence is pro-\ncessed across independent scales to isolate and capture\ndistinct levels of contextual granularity.\n• Optimization-Driven Aggregation: We propose a novel\naggregation mechanism governed by convex optimization\nand game-theoretic principles. This allows the model\nto determine the optimal trade-off between local and\nglobal context dynamically, rather than relying on static\nor heuristic fusion methods.\n• Computational Efficiency: MAHA significantly reduces\nthe quadratic complexity characteristic of standard atten-\narXiv:2512.14925v2  [cs.CL]  18 Dec 2025\n2\ntion mechanisms, enhancing scalability without compro-\nmising the model’s expressive power.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "10", "text": "• Optimization-Driven Aggregation: We propose a novel\naggregation mechanism governed by convex optimization\nand game-theoretic principles. This allows the model\nto determine the optimal trade-off between local and\nglobal context dynamically, rather than relying on static\nor heuristic fusion methods.\n• Computational Efficiency: MAHA significantly reduces\nthe quadratic complexity characteristic of standard atten-\narXiv:2512.14925v2  [cs.CL]  18 Dec 2025\n2\ntion mechanisms, enhancing scalability without compro-\nmising the model’s expressive power.\nMAHA is particularly pertinent to the evolution of LLMs,\nwhere the demand for efficient and scalable attention mech-\nanisms is paramount [5]. By integrating rigorous multiscale\nanalysis with optimization-based aggregation rules, MAHA\noffers a versatile solution adaptable to various transformer-\nbased architectures with minimal architectural overhead. The\nframework is designed for compatibility with existing LLM\ntraining pipelines, ensuring practicality for real-world deploy-\nment.\nII.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "11", "text": "MAHA is particularly pertinent to the evolution of LLMs,\nwhere the demand for efficient and scalable attention mech-\nanisms is paramount [5]. By integrating rigorous multiscale\nanalysis with optimization-based aggregation rules, MAHA\noffers a versatile solution adaptable to various transformer-\nbased architectures with minimal architectural overhead. The\nframework is designed for compatibility with existing LLM\ntraining pipelines, ensuring practicality for real-world deploy-\nment.\nII. RELATED WORK\nThe development of efficient attention mechanisms has be-\ncome a focal point in transformer-based architecture research,\nwith numerous approaches proposed to alleviate computa-\ntional bottlenecks and enhance contextual modeling capa-\nbilities. Existing literature can be broadly categorized into\nsparse attention methods, hierarchical attention frameworks,\nand optimization-driven aggregation techniques.\nA. Sparse Attention Mechanisms\nSparse attention mechanisms aim to reduce computational\noverhead by limiting token interactions to predefined or\nlearned patterns.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "12", "text": "The\nframework is designed for compatibility with existing LLM\ntraining pipelines, ensuring practicality for real-world deploy-\nment.\nII. RELATED WORK\nThe development of efficient attention mechanisms has be-\ncome a focal point in transformer-based architecture research,\nwith numerous approaches proposed to alleviate computa-\ntional bottlenecks and enhance contextual modeling capa-\nbilities. Existing literature can be broadly categorized into\nsparse attention methods, hierarchical attention frameworks,\nand optimization-driven aggregation techniques.\nA. Sparse Attention Mechanisms\nSparse attention mechanisms aim to reduce computational\noverhead by limiting token interactions to predefined or\nlearned patterns. For instance, [6] introduced a sliding window\nattention mechanism that restricts each token’s receptive field\nto its local neighborhood, significantly lowering memory re-\nquirements from O(n2) to O(n) for long sequences. Similarly,\n[7] proposed a hybrid approach combining local, global, and\nrandom attention patterns to approximate full self-attention\nwhile maintaining theoretical expressiveness.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "13", "text": "A. Sparse Attention Mechanisms\nSparse attention mechanisms aim to reduce computational\noverhead by limiting token interactions to predefined or\nlearned patterns. For instance, [6] introduced a sliding window\nattention mechanism that restricts each token’s receptive field\nto its local neighborhood, significantly lowering memory re-\nquirements from O(n2) to O(n) for long sequences. Similarly,\n[7] proposed a hybrid approach combining local, global, and\nrandom attention patterns to approximate full self-attention\nwhile maintaining theoretical expressiveness. However, these\nmethods often rely on heuristics to determine sparsity patterns,\nwhich may not adapt dynamically to diverse input structures or\ncapture long-range dependencies effectively without stacking\nmultiple layers.\nB. Hierarchical Attention Frameworks\nHierarchical approaches decompose input sequences into\nmultiple levels of granularity to capture both local syntactic\nfeatures and global semantic dependencies simultaneously.\nThe Hierarchical Attention Network (HAN) [3] processes\ndocuments at word and sentence levels, aggregating infor-\nmation through learned attention weights.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "14", "text": "However, these\nmethods often rely on heuristics to determine sparsity patterns,\nwhich may not adapt dynamically to diverse input structures or\ncapture long-range dependencies effectively without stacking\nmultiple layers.\nB. Hierarchical Attention Frameworks\nHierarchical approaches decompose input sequences into\nmultiple levels of granularity to capture both local syntactic\nfeatures and global semantic dependencies simultaneously.\nThe Hierarchical Attention Network (HAN) [3] processes\ndocuments at word and sentence levels, aggregating infor-\nmation through learned attention weights. More recently, [8]\nintroduced a hierarchical attention mechanism (hi-attention)\nthat integrates inter-layer information to improve sequence\nmodeling. While effective, these methods typically employ\nfixed or ad-hoc aggregation rules—such as weighted averag-\ning—which may not optimally balance the contributions from\ndifferent scales, leading to information dilution.\nC. Optimization-Driven and Game-Theoretic Aggregation\nOptimization techniques have been increasingly integrated\ninto neural architectures to enhance efficiency and robust-\nness.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "15", "text": "More recently, [8]\nintroduced a hierarchical attention mechanism (hi-attention)\nthat integrates inter-layer information to improve sequence\nmodeling. While effective, these methods typically employ\nfixed or ad-hoc aggregation rules—such as weighted averag-\ning—which may not optimally balance the contributions from\ndifferent scales, leading to information dilution.\nC. Optimization-Driven and Game-Theoretic Aggregation\nOptimization techniques have been increasingly integrated\ninto neural architectures to enhance efficiency and robust-\nness. For example, [9] utilized hierarchical decomposition\nto interpret intermediate CNN decisions, demonstrating the\npotential of optimization-based feature integration. In the\ncontext of sequence modeling, [10] explored multi-head self-\nattention with hierarchical aggregation but did not incorporate\nrigorous convex optimization or game-theoretic principles.\nGame theory, particularly the concept of Nash equilibrium, has\nbeen successfully employed in multi-agent systems to resolve\nconflicts [11]. Its application to attention mechanisms offers\na principled pathway to resolve conflicts between competing\nattention scales, a direction that remains largely unexplored in\ncurrent LLM architectures.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "16", "text": "For example, [9] utilized hierarchical decomposition\nto interpret intermediate CNN decisions, demonstrating the\npotential of optimization-based feature integration. In the\ncontext of sequence modeling, [10] explored multi-head self-\nattention with hierarchical aggregation but did not incorporate\nrigorous convex optimization or game-theoretic principles.\nGame theory, particularly the concept of Nash equilibrium, has\nbeen successfully employed in multi-agent systems to resolve\nconflicts [11]. Its application to attention mechanisms offers\na principled pathway to resolve conflicts between competing\nattention scales, a direction that remains largely unexplored in\ncurrent LLM architectures.\nD. Multiscale Analysis in Language Models\nMultiscale analysis is a staple in signal processing and com-\nputer vision [4], yet its direct application to language modeling\nremains limited. Recent work by [12] demonstrated the effec-\ntiveness of hierarchical decomposition in graph convolutional\nnetworks, suggesting potential benefits for attention mech-\nanisms. Similarly, [13] proposed hierarchical decomposition\nfor continual learning, highlighting the importance of scale-\nspecific feature extraction.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "17", "text": "D. Multiscale Analysis in Language Models\nMultiscale analysis is a staple in signal processing and com-\nputer vision [4], yet its direct application to language modeling\nremains limited. Recent work by [12] demonstrated the effec-\ntiveness of hierarchical decomposition in graph convolutional\nnetworks, suggesting potential benefits for attention mech-\nanisms. Similarly, [13] proposed hierarchical decomposition\nfor continual learning, highlighting the importance of scale-\nspecific feature extraction. These studies provide empirical\nevidence that processing information at varying resolutions can\nenhance representation learning.\nE. Integration of Optimization and Attention\nThe integration of differentiable optimization layers with\nattention mechanisms represents an emerging research frontier.\nWhile [12] applied hierarchical attention to fraud detection,\ntheir aggregation method lacked strong theoretical guarantees.\nIn contrast, the proposed MAHA framework distinguishes\nitself by unifying multiscale decomposition with rigorous\naggregation rules. Unlike sparse attention methods, MAHA\ndynamically adjusts the scale of token interactions without\nrelying on predefined patterns.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "18", "text": "These studies provide empirical\nevidence that processing information at varying resolutions can\nenhance representation learning.\nE. Integration of Optimization and Attention\nThe integration of differentiable optimization layers with\nattention mechanisms represents an emerging research frontier.\nWhile [12] applied hierarchical attention to fraud detection,\ntheir aggregation method lacked strong theoretical guarantees.\nIn contrast, the proposed MAHA framework distinguishes\nitself by unifying multiscale decomposition with rigorous\naggregation rules. Unlike sparse attention methods, MAHA\ndynamically adjusts the scale of token interactions without\nrelying on predefined patterns. Compared to existing hierar-\nchical approaches, it employs convex optimization or Nash\nequilibrium (NE) to optimally combine attention scores. This\ncombination enables MAHA to achieve superior computa-\ntional efficiency and contextual modeling, addressing the key\nlimitations of heuristic-based aggregation.\nIII. PRELIMINARIES AND BACKGROUND\nTo establish the theoretical foundation for MAHA, we\nbriefly review key concepts in attention mechanisms, multi-\nscale analysis, and game-theoretic optimization.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "19", "text": "Compared to existing hierar-\nchical approaches, it employs convex optimization or Nash\nequilibrium (NE) to optimally combine attention scores. This\ncombination enables MAHA to achieve superior computa-\ntional efficiency and contextual modeling, addressing the key\nlimitations of heuristic-based aggregation.\nIII. PRELIMINARIES AND BACKGROUND\nTo establish the theoretical foundation for MAHA, we\nbriefly review key concepts in attention mechanisms, multi-\nscale analysis, and game-theoretic optimization. These com-\nponents form the basis of our proposed framework.\nA. Attention Mechanisms in Transformers\nThe standard attention mechanism in transformers computes\npairwise interactions between all tokens in a sequence through\nscaled dot-product operations [1].", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "20", "text": "This\ncombination enables MAHA to achieve superior computa-\ntional efficiency and contextual modeling, addressing the key\nlimitations of heuristic-based aggregation.\nIII. PRELIMINARIES AND BACKGROUND\nTo establish the theoretical foundation for MAHA, we\nbriefly review key concepts in attention mechanisms, multi-\nscale analysis, and game-theoretic optimization. These com-\nponents form the basis of our proposed framework.\nA. Attention Mechanisms in Transformers\nThe standard attention mechanism in transformers computes\npairwise interactions between all tokens in a sequence through\nscaled dot-product operations [1]. Given an input sequence\nX ∈Rn×d, where n is the sequence length and d is the\nembedding dimension, the attention matrix A is computed as:\nA = softmax\n\u0012QKT\n√dk\n\u0013\n(1)\nwhere Q, K ∈Rn×dk are the query and key matrices,\nrespectively, and dk is the dimension of the keys.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "21", "text": "These com-\nponents form the basis of our proposed framework.\nA. Attention Mechanisms in Transformers\nThe standard attention mechanism in transformers computes\npairwise interactions between all tokens in a sequence through\nscaled dot-product operations [1]. Given an input sequence\nX ∈Rn×d, where n is the sequence length and d is the\nembedding dimension, the attention matrix A is computed as:\nA = softmax\n\u0012QKT\n√dk\n\u0013\n(1)\nwhere Q, K ∈Rn×dk are the query and key matrices,\nrespectively, and dk is the dimension of the keys. While\neffective, this operation exhibits quadratic complexity O(n2)\nin both computation and memory, rendering it impractical for\nvery long sequences [2].\n3\nB. Multiscale Signal Decomposition\nMultiscale analysis provides a rigorous framework for ex-\namining signals at varying levels of resolution. In NLP, this\ntranslates to capturing both local syntactic patterns (high\nfrequency) and global semantic structures (low frequency) [4].", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "22", "text": "While\neffective, this operation exhibits quadratic complexity O(n2)\nin both computation and memory, rendering it impractical for\nvery long sequences [2].\n3\nB. Multiscale Signal Decomposition\nMultiscale analysis provides a rigorous framework for ex-\namining signals at varying levels of resolution. In NLP, this\ntranslates to capturing both local syntactic patterns (high\nfrequency) and global semantic structures (low frequency) [4].\nInspired by wavelet transforms and pyramid decomposition\n[13], for a discrete signal representation x, a multiscale\ndecomposition can be expressed as:\nx =\nS\nX\ns=1\nDs(x) + R(x)\n(2)\nwhere Ds represents the detail component at scale s, and R\ndenotes the residual (coarse) component. This decomposition\nforms the structural basis for MAHA’s hierarchical processing\nlayers.\nC. Game-Theoretic Optimization\nGame theory provides mathematical tools for modeling\ninteractions between multiple decision-makers.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "23", "text": "Inspired by wavelet transforms and pyramid decomposition\n[13], for a discrete signal representation x, a multiscale\ndecomposition can be expressed as:\nx =\nS\nX\ns=1\nDs(x) + R(x)\n(2)\nwhere Ds represents the detail component at scale s, and R\ndenotes the residual (coarse) component. This decomposition\nforms the structural basis for MAHA’s hierarchical processing\nlayers.\nC. Game-Theoretic Optimization\nGame theory provides mathematical tools for modeling\ninteractions between multiple decision-makers. The concept\nof Nash equilibrium [14] is particularly relevant for MAHA’s\naggregation phase, where different attention scales can be\nmodeled as “players” competing for influence in the final\nrepresentation. Given a game with N players and strategy sets\nSi, a Nash equilibrium is a strategy profile s∗= (s∗\n1, . . .", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "24", "text": "This decomposition\nforms the structural basis for MAHA’s hierarchical processing\nlayers.\nC. Game-Theoretic Optimization\nGame theory provides mathematical tools for modeling\ninteractions between multiple decision-makers. The concept\nof Nash equilibrium [14] is particularly relevant for MAHA’s\naggregation phase, where different attention scales can be\nmodeled as “players” competing for influence in the final\nrepresentation. Given a game with N players and strategy sets\nSi, a Nash equilibrium is a strategy profile s∗= (s∗\n1, . . . , s∗\nN)\nsuch that for every player i:\nui(s∗\ni , s∗\n−i) ≥ui(si, s∗\n−i)\n∀si ∈Si\n(3)\nwhere ui is the utility function for player i and s∗\n−i denotes\nthe strategies of all other players. This equilibrium condition\nensures that no scale (player) can improve its contribution\nutility by unilaterally changing its attention weights, leading\nto a stable and optimal context representation.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "25", "text": ". . , s∗\nN)\nsuch that for every player i:\nui(s∗\ni , s∗\n−i) ≥ui(si, s∗\n−i)\n∀si ∈Si\n(3)\nwhere ui is the utility function for player i and s∗\n−i denotes\nthe strategies of all other players. This equilibrium condition\nensures that no scale (player) can improve its contribution\nutility by unilaterally changing its attention weights, leading\nto a stable and optimal context representation.\nD. Convex Optimization in Attention\nConvex optimization provides a principled method to com-\nbine multiple objectives under constraints. The general form\nof a convex optimization problem is defined as:\nmin\nx\nf(x)\nsubject to\ngi(x) ≤0,\nhj(x) = 0\n(4)\nwhere f is the convex objective function, gi are convex\ninequality constraints, and hj are affine equality constraints.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "26", "text": "This equilibrium condition\nensures that no scale (player) can improve its contribution\nutility by unilaterally changing its attention weights, leading\nto a stable and optimal context representation.\nD. Convex Optimization in Attention\nConvex optimization provides a principled method to com-\nbine multiple objectives under constraints. The general form\nof a convex optimization problem is defined as:\nmin\nx\nf(x)\nsubject to\ngi(x) ≤0,\nhj(x) = 0\n(4)\nwhere f is the convex objective function, gi are convex\ninequality constraints, and hj are affine equality constraints.\nIn MAHA, this framework is utilized to aggregate attention\nscores from different scales while enforcing constraints that\npreserve important linguistic properties, such as probability\ndistribution validity and sparsity.\nIV. THE MAHA FRAMEWORK\nThe MAHA framework introduces a systematic approach\nto sequence modeling by decomposing the input into multiple\nhierarchical scales and synthesizing them through mathemat-\nically rigorous aggregation rules.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "27", "text": "In MAHA, this framework is utilized to aggregate attention\nscores from different scales while enforcing constraints that\npreserve important linguistic properties, such as probability\ndistribution validity and sparsity.\nIV. THE MAHA FRAMEWORK\nThe MAHA framework introduces a systematic approach\nto sequence modeling by decomposing the input into multiple\nhierarchical scales and synthesizing them through mathemat-\nically rigorous aggregation rules. This section details the\nhierarchical decomposition strategy, scale-specific attention\ncomputation, and the optimization-driven mechanisms that\nInput Embedding\nDilated Convolution\n(Local Context Extraction)\nHierarchical Decomposition\nScale 0\nScale 1\nScale 2\nShared Value\nProjection\nShared Parameters\nAttention\nAttention\nAttention\nOptimization-Driven Aggregation\n(Convex / Nash Equilibrium)\nAdd & Norm\nResidual\nFeed Forward Network (FFN)\nAdd & Norm\nOutput\nFig. 1: Schematic overview of the MAHA architecture inte-\ngrated within a Transformer block.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "28", "text": "This section details the\nhierarchical decomposition strategy, scale-specific attention\ncomputation, and the optimization-driven mechanisms that\nInput Embedding\nDilated Convolution\n(Local Context Extraction)\nHierarchical Decomposition\nScale 0\nScale 1\nScale 2\nShared Value\nProjection\nShared Parameters\nAttention\nAttention\nAttention\nOptimization-Driven Aggregation\n(Convex / Nash Equilibrium)\nAdd & Norm\nResidual\nFeed Forward Network (FFN)\nAdd & Norm\nOutput\nFig. 1: Schematic overview of the MAHA architecture inte-\ngrated within a Transformer block. The input is decomposed\ninto multiple scales, processed via shared-value attention, and\naggregated using optimization or game-theoretic layers.\ngovern information fusion. As illustrated in Figure 1, MAHA\nis designed to replace the standard multi-head attention layer\nin transformer blocks while maintaining architectural compat-\nibility.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "29", "text": "1: Schematic overview of the MAHA architecture inte-\ngrated within a Transformer block. The input is decomposed\ninto multiple scales, processed via shared-value attention, and\naggregated using optimization or game-theoretic layers.\ngovern information fusion. As illustrated in Figure 1, MAHA\nis designed to replace the standard multi-head attention layer\nin transformer blocks while maintaining architectural compat-\nibility.\nA. Hierarchical Multiscale Decomposition with Learnable\nDownsampling\nLet X ∈Rn×d denote the input sequence, where n is the\nsequence length and d is the embedding dimension. MAHA\ndecomposes X into L hierarchical scales through a series of\nlearnable downsampling operations.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "30", "text": "govern information fusion. As illustrated in Figure 1, MAHA\nis designed to replace the standard multi-head attention layer\nin transformer blocks while maintaining architectural compat-\nibility.\nA. Hierarchical Multiscale Decomposition with Learnable\nDownsampling\nLet X ∈Rn×d denote the input sequence, where n is the\nsequence length and d is the embedding dimension. MAHA\ndecomposes X into L hierarchical scales through a series of\nlearnable downsampling operations. Each scale l is derived\nfrom the previous scale l −1 using a parameterized operator\nDl:\nXl = Dl(Xl−1),\nX0 = X\n(5)\nThe downsampling operator Dl is implemented via one of\ntwo mechanisms:\n1) Strided Convolution: Dl(X) = Conv1D(X, Ws\nl , sl),\nwhere Ws\nl is a learnable kernel and sl is the stride.\n2) Adaptive Pooling: Dl(X) = AdaptiveMaxPool(X, nl),\nwhich dynamically adjusts the pooling window to match\nthe target length nl.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "31", "text": "Each scale l is derived\nfrom the previous scale l −1 using a parameterized operator\nDl:\nXl = Dl(Xl−1),\nX0 = X\n(5)\nThe downsampling operator Dl is implemented via one of\ntwo mechanisms:\n1) Strided Convolution: Dl(X) = Conv1D(X, Ws\nl , sl),\nwhere Ws\nl is a learnable kernel and sl is the stride.\n2) Adaptive Pooling: Dl(X) = AdaptiveMaxPool(X, nl),\nwhich dynamically adjusts the pooling window to match\nthe target length nl.\nThe sequence lengths follow an exponential decay schedule\nnl = ⌊nl−1/r⌋, where r > 1 is a compression ratio hyper-\nparameter. This creates a pyramidal structure where higher\nscales capture increasingly coarse-grained semantic patterns\nwhile preserving essential features.\n4\nB. Multiscale Attention Computation\nAt each scale l, MAHA computes independent attention\nmatrices.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "32", "text": "2) Adaptive Pooling: Dl(X) = AdaptiveMaxPool(X, nl),\nwhich dynamically adjusts the pooling window to match\nthe target length nl.\nThe sequence lengths follow an exponential decay schedule\nnl = ⌊nl−1/r⌋, where r > 1 is a compression ratio hyper-\nparameter. This creates a pyramidal structure where higher\nscales capture increasingly coarse-grained semantic patterns\nwhile preserving essential features.\n4\nB. Multiscale Attention Computation\nAt each scale l, MAHA computes independent attention\nmatrices. A key innovation in MAHA is the decoupling of\nprojection parameters to enhance efficiency: while Query (Q)\nand Key (K) projections are scale-specific, the Value (V)\nprojection is shared across scales. Given the representation\nXl, the projections are defined as:\nQl = XlWQ\nl ,\nKl = XlWK\nl\n(6)\nwhere WQ\nl , WK\nl\n∈Rd×dk.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "33", "text": "4\nB. Multiscale Attention Computation\nAt each scale l, MAHA computes independent attention\nmatrices. A key innovation in MAHA is the decoupling of\nprojection parameters to enhance efficiency: while Query (Q)\nand Key (K) projections are scale-specific, the Value (V)\nprojection is shared across scales. Given the representation\nXl, the projections are defined as:\nQl = XlWQ\nl ,\nKl = XlWK\nl\n(6)\nwhere WQ\nl , WK\nl\n∈Rd×dk. The attention weights Al are\ncomputed via the scaled dot-product:\nAl = softmax\n\u0012QlKT\nl\n√dk\n\u0013\nUnlike standard transformers, MAHA employs a shared\nvalue projection: Vbase = XWV . The value matrix for scale\nl, denoted as Vl, is obtained by applying the corresponding\ndownsampling operator to the base values: Vl = Dl(Vbase).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "34", "text": "The attention weights Al are\ncomputed via the scaled dot-product:\nAl = softmax\n\u0012QlKT\nl\n√dk\n\u0013\nUnlike standard transformers, MAHA employs a shared\nvalue projection: Vbase = XWV . The value matrix for scale\nl, denoted as Vl, is obtained by applying the corresponding\ndownsampling operator to the base values: Vl = Dl(Vbase).\nThe scale-specific output Ol is then:\nOl = AlVl\nThis design reduces the parameter count significantly while\nensuring that the information flow remains consistent across\ngranularity levels.\nC. Aggregation of Multiscale Attention Outputs\nThe multiscale outputs {Ol} must be synthesized into\na unified representation O∗. MAHA proposes two rigorous\nstrategies:\nCO-Based Aggregation: We formulate aggregation as a\nconvex optimization problem.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "35", "text": "The scale-specific output Ol is then:\nOl = AlVl\nThis design reduces the parameter count significantly while\nensuring that the information flow remains consistent across\ngranularity levels.\nC. Aggregation of Multiscale Attention Outputs\nThe multiscale outputs {Ol} must be synthesized into\na unified representation O∗. MAHA proposes two rigorous\nstrategies:\nCO-Based Aggregation: We formulate aggregation as a\nconvex optimization problem. Let Ul denote an upsampling\noperator mapping Ol back to the original sequence length n.\nThe aggregated output is obtained by solving for the optimal\nmixing weights w:\nmin\nw\n\r\r\r\r\r\nL\nX\nl=0\nwlUl(Ol) −O∗\n\r\r\r\r\r\n2\nF\n+λ∥w∥1\ns.t.\nX\nwl = 1, wl ≥0\n(7)\nHere, λ controls sparsity, encouraging the model to select\nthe most informative scales.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "36", "text": "Let Ul denote an upsampling\noperator mapping Ol back to the original sequence length n.\nThe aggregated output is obtained by solving for the optimal\nmixing weights w:\nmin\nw\n\r\r\r\r\r\nL\nX\nl=0\nwlUl(Ol) −O∗\n\r\r\r\r\r\n2\nF\n+λ∥w∥1\ns.t.\nX\nwl = 1, wl ≥0\n(7)\nHere, λ controls sparsity, encouraging the model to select\nthe most informative scales.\nNash Equilibrium-Based Aggregation: Alternatively, ag-\ngregation is modeled as a non-cooperative game where each\nscale l competes to minimize its reconstruction error. The\nequilibrium weights w∗\nl satisfy:\nw∗\nl = arg min\nwl\n\r\rUl(Ol) −O∗(w∗\n−l)\n\r\r2\n2\n(8)\nThis ensures that no scale can unilaterally improve the\nrepresentation quality.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "37", "text": "Nash Equilibrium-Based Aggregation: Alternatively, ag-\ngregation is modeled as a non-cooperative game where each\nscale l competes to minimize its reconstruction error. The\nequilibrium weights w∗\nl satisfy:\nw∗\nl = arg min\nwl\n\r\rUl(Ol) −O∗(w∗\n−l)\n\r\r2\n2\n(8)\nThis ensures that no scale can unilaterally improve the\nrepresentation quality.\nD. Hybrid Dilated-Convolutional Transformer Design\nMAHA integrates dilated convolutions to capture local\ncontext prior to attention. The hybrid block consists of:\n• Dilated Convolution Blocks: For scale l, the output is\nCl = ReLU(DilatedConv(Xl)).\n• Cross-Scale Gating: Gl = σ(WgXl) ⊙Xl−1, where\nσ is the sigmoid function and ⊙denotes element-wise\nmultiplication.\n• Nearest-Neighbor Upsampling: Used to reconstruct the\nfull sequence efficiently.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "38", "text": "D. Hybrid Dilated-Convolutional Transformer Design\nMAHA integrates dilated convolutions to capture local\ncontext prior to attention. The hybrid block consists of:\n• Dilated Convolution Blocks: For scale l, the output is\nCl = ReLU(DilatedConv(Xl)).\n• Cross-Scale Gating: Gl = σ(WgXl) ⊙Xl−1, where\nσ is the sigmoid function and ⊙denotes element-wise\nmultiplication.\n• Nearest-Neighbor Upsampling: Used to reconstruct the\nfull sequence efficiently.\nE. Complexity Reduction through Hierarchical Sparsity\nThe total computational complexity of MAHA is governed\nby the hierarchical decomposition. For a sequence of length\nn, the complexity is defined as:\nΩ(n) =\nL\nX\nl=0\n\u0010 n\nrl\n\u00112\nd + O(n log n)\n(9)\nFor r = 2, the geometric series converges, yielding:\nO\n\u0012\nn2\nr2 −1\n\u0013\n(10)\nwhich is significantly lower than standard attention.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "39", "text": "• Nearest-Neighbor Upsampling: Used to reconstruct the\nfull sequence efficiently.\nE. Complexity Reduction through Hierarchical Sparsity\nThe total computational complexity of MAHA is governed\nby the hierarchical decomposition. For a sequence of length\nn, the complexity is defined as:\nΩ(n) =\nL\nX\nl=0\n\u0010 n\nrl\n\u00112\nd + O(n log n)\n(9)\nFor r = 2, the geometric series converges, yielding:\nO\n\u0012\nn2\nr2 −1\n\u0013\n(10)\nwhich is significantly lower than standard attention.\n• Scale-Specific Sparsity: Coarser scales have nl ≪n,\nreducing the cost quadratically.\n• Dynamic Weight Sparsity: The ℓ1-regularized weights\nwl prune uninformative scales during inference.\nComputational Complexity\n(FLOPs / Memory)\nSequence Length\nStandard Attention\nMAHA\nLong Context\nEfficiency Gap\n(Sub-quadratic)\nFig. 2: Computational complexity comparison.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "40", "text": "• Scale-Specific Sparsity: Coarser scales have nl ≪n,\nreducing the cost quadratically.\n• Dynamic Weight Sparsity: The ℓ1-regularized weights\nwl prune uninformative scales during inference.\nComputational Complexity\n(FLOPs / Memory)\nSequence Length\nStandard Attention\nMAHA\nLong Context\nEfficiency Gap\n(Sub-quadratic)\nFig. 2: Computational complexity comparison. MAHA demon-\nstrates near-linear scaling compared to the quadratic growth of\nstandard Self-Attention.\nV. EXPERIMENTS\nTo evaluate the empirical efficacy of MAHA, we conducted\nextensive experiments across diverse NLP tasks. Our evalua-\ntion framework focuses on three pivotal research questions:\n• RQ1: How does MAHA compare to state-of-the-art at-\ntention mechanisms in terms of computational efficiency\nand downstream task performance?\n• RQ2: What is the comparative impact of convex opti-\nmization versus game-theoretic aggregation strategies on\nmodel behavior?", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "41", "text": "MAHA demon-\nstrates near-linear scaling compared to the quadratic growth of\nstandard Self-Attention.\nV. EXPERIMENTS\nTo evaluate the empirical efficacy of MAHA, we conducted\nextensive experiments across diverse NLP tasks. Our evalua-\ntion framework focuses on three pivotal research questions:\n• RQ1: How does MAHA compare to state-of-the-art at-\ntention mechanisms in terms of computational efficiency\nand downstream task performance?\n• RQ2: What is the comparative impact of convex opti-\nmization versus game-theoretic aggregation strategies on\nmodel behavior?\n5\n• RQ3: How does the granularity of the hierarchical de-\ncomposition affect the trade-off between representational\naccuracy and computational cost?\nA. Experimental Setup\nWe evaluated MAHA on four benchmark datasets designed\nto stress-test different aspects of sequence modeling:\n• Text Classification: GLUE benchmark [15], focusing on\nMNLI and SST-2.\n• Long-Range Dependency Modeling: PG-19 dataset [16]\n(> 4k tokens).\n• Machine Translation: WMT14 English-German [17].", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "42", "text": "5\n• RQ3: How does the granularity of the hierarchical de-\ncomposition affect the trade-off between representational\naccuracy and computational cost?\nA. Experimental Setup\nWe evaluated MAHA on four benchmark datasets designed\nto stress-test different aspects of sequence modeling:\n• Text Classification: GLUE benchmark [15], focusing on\nMNLI and SST-2.\n• Long-Range Dependency Modeling: PG-19 dataset [16]\n(> 4k tokens).\n• Machine Translation: WMT14 English-German [17].\n• Question Answering: SQuAD v2.0 [18].\nFor comparative analysis, MAHA was benchmarked against\nfive widely adopted attention mechanisms: Standard Multi-\nHead Attention (MHA) [1], Longformer [6], BigBird [7],\nReformer [19], and Performer [20].\nImplementation Details:\n• Model Architecture: Transformer backbone with 12\nlayers, hidden dimensionality of 768, and 12 attention\nheads.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "43", "text": "• Long-Range Dependency Modeling: PG-19 dataset [16]\n(> 4k tokens).\n• Machine Translation: WMT14 English-German [17].\n• Question Answering: SQuAD v2.0 [18].\nFor comparative analysis, MAHA was benchmarked against\nfive widely adopted attention mechanisms: Standard Multi-\nHead Attention (MHA) [1], Longformer [6], BigBird [7],\nReformer [19], and Performer [20].\nImplementation Details:\n• Model Architecture: Transformer backbone with 12\nlayers, hidden dimensionality of 768, and 12 attention\nheads.\n• Training: Batch size 32 (classification), 16 (transla-\ntion/QA); LR 5 × 10−5 with warmup 10k steps.\n• Sequence Length: 512 (classification/QA), 4096 (PG-\n19).\n• MAHA Parameters: L = 4 scales (32, 64, 128, 256 to-\nkens); strided conv (kernel=3); aggregation regularization\nλ = 0.1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "44", "text": "Implementation Details:\n• Model Architecture: Transformer backbone with 12\nlayers, hidden dimensionality of 768, and 12 attention\nheads.\n• Training: Batch size 32 (classification), 16 (transla-\ntion/QA); LR 5 × 10−5 with warmup 10k steps.\n• Sequence Length: 512 (classification/QA), 4096 (PG-\n19).\n• MAHA Parameters: L = 4 scales (32, 64, 128, 256 to-\nkens); strided conv (kernel=3); aggregation regularization\nλ = 0.1.\nB. Main Results\nTable I summarizes the performance on benchmark datasets.\nMAHA achieves competitive accuracy with standard attention\nwhile outperforming sparse baselines on long-context tasks\n(PG-19).\nC. Computational Efficiency Analysis\nMAHA matches MHA performance (within 0.2%) while\nreducing memory by 56%. On PG-19, MAHA achieves lowest\nperplexity (23.1), outperforming sparse models.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "45", "text": "B. Main Results\nTable I summarizes the performance on benchmark datasets.\nMAHA achieves competitive accuracy with standard attention\nwhile outperforming sparse baselines on long-context tasks\n(PG-19).\nC. Computational Efficiency Analysis\nMAHA matches MHA performance (within 0.2%) while\nreducing memory by 56%. On PG-19, MAHA achieves lowest\nperplexity (23.1), outperforming sparse models. Throughput\nis highest (71 seq/s), making MAHA ideal for high-volume\ninference. We analyzed the theoretical complexity (FLOPs)\nrelative to sequence length. As illustrated in Figure 3, MAHA\ndemonstrates near-linear scaling compared to the quadratic\nbaseline of standard attention.\nAt N = 4096, MHA requires ≈16.8M FLOPs vs MAHA\n≈3.2M FLOPs (81% reduction). This efficiency stems from\nhierarchical compression avoiding full N × N attention.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "46", "text": "Throughput\nis highest (71 seq/s), making MAHA ideal for high-volume\ninference. We analyzed the theoretical complexity (FLOPs)\nrelative to sequence length. As illustrated in Figure 3, MAHA\ndemonstrates near-linear scaling compared to the quadratic\nbaseline of standard attention.\nAt N = 4096, MHA requires ≈16.8M FLOPs vs MAHA\n≈3.2M FLOPs (81% reduction). This efficiency stems from\nhierarchical compression avoiding full N × N attention. This\ngap widens exponentially as the sequence length increases,\nconfirming MAHA’s suitability for long-context applications.\nD. Ablation Studies\nWe evaluated the impact of aggregation methods and scale\nconfigurations.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "47", "text": "As illustrated in Figure 3, MAHA\ndemonstrates near-linear scaling compared to the quadratic\nbaseline of standard attention.\nAt N = 4096, MHA requires ≈16.8M FLOPs vs MAHA\n≈3.2M FLOPs (81% reduction). This efficiency stems from\nhierarchical compression avoiding full N × N attention. This\ngap widens exponentially as the sequence length increases,\nconfirming MAHA’s suitability for long-context applications.\nD. Ablation Studies\nWe evaluated the impact of aggregation methods and scale\nconfigurations.\n1K\n2K\n3K\n4K\n5K\n6K\n7K\n8K\nSequence Length (N)\n0\n10\n20\n30\n40\n50\n60\n70\n80\nAttention FLOPs (Millions)\n81% Reduction\n(13.6M FLOPs Gap)\n16.8M\n3.2M\nFigure 3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "48", "text": "This\ngap widens exponentially as the sequence length increases,\nconfirming MAHA’s suitability for long-context applications.\nD. Ablation Studies\nWe evaluated the impact of aggregation methods and scale\nconfigurations.\n1K\n2K\n3K\n4K\n5K\n6K\n7K\n8K\nSequence Length (N)\n0\n10\n20\n30\n40\n50\n60\n70\n80\nAttention FLOPs (Millions)\n81% Reduction\n(13.6M FLOPs Gap)\n16.8M\n3.2M\nFigure 3. Attention FLOPs vs Sequence Length\nStandard MHA (O(n2))\nMAHA (Ours, \nO(n))\nFig. 3: Attention FLOPs vs Sequence Length. MAHA exhibits\nan 81% reduction in FLOPs at N = 4096 compared to\nStandard MHA. The gap widens exponentially with longer\nsequences.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "49", "text": "Attention FLOPs vs Sequence Length\nStandard MHA (O(n2))\nMAHA (Ours, \nO(n))\nFig. 3: Attention FLOPs vs Sequence Length. MAHA exhibits\nan 81% reduction in FLOPs at N = 4096 compared to\nStandard MHA. The gap widens exponentially with longer\nsequences.\n1) Aggregation Strategy Comparison: To further analyze\nthe training dynamics, Figure 4 depicts the loss convergence\ncurves for both aggregation strategies. While both meth-\nods converge stably, the Nash Equilibrium (Orange) strategy\nachieves a marginally lower loss value in later epochs com-\npared to Convex Optimization (Blue).\nFig. 4: Training loss convergence comparison: Convex Opti-\nmization vs Nash Equilibrium.\nTable II shows that while Convex Optimization (CO) is\nfaster (1.0x), Nash Equilibrium (NE) provides robust perfor-\nmance at a slight cost (0.9x speed).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "50", "text": "While both meth-\nods converge stably, the Nash Equilibrium (Orange) strategy\nachieves a marginally lower loss value in later epochs com-\npared to Convex Optimization (Blue).\nFig. 4: Training loss convergence comparison: Convex Opti-\nmization vs Nash Equilibrium.\nTable II shows that while Convex Optimization (CO) is\nfaster (1.0x), Nash Equilibrium (NE) provides robust perfor-\nmance at a slight cost (0.9x speed).\n2) Scale Configuration Analysis: We analyzed how the\ndepth of the hierarchy (number of scales, L) affects model\nperformance. As illustrated in Figure 5, optimal results are\nobserved at L = 4, balancing granularity and context. Using\ntoo few scales (L = 2) results in insufficient detail (Acc:\n84.5%), while excessive downsampling (L = 6) introduces\nnoise (Acc: 84.8%).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "51", "text": "2) Scale Configuration Analysis: We analyzed how the\ndepth of the hierarchy (number of scales, L) affects model\nperformance. As illustrated in Figure 5, optimal results are\nobserved at L = 4, balancing granularity and context. Using\ntoo few scales (L = 2) results in insufficient detail (Acc:\n84.5%), while excessive downsampling (L = 6) introduces\nnoise (Acc: 84.8%).\nE. Qualitative Analysis\nTo interpret the internal representations learned by MAHA,\nwe visualized the attention weights across different hierarchi-\ncal scales. Figure 6 displays the heatmap of attention matrices.\nSeveral key observations can be drawn:\n1) Fine Scales (Scale 1): Exhibits a strong diagonal ten-\ndency, capturing local syntax like adjective-noun pairs.\n6\nTABLE I: Performance Comparison Across Tasks.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "52", "text": "Model\nMNLI\nSST-2\nPG-19\nWMT\nSQuAD\nMemory\n(Acc)\n(Acc)\n(PPL) ↓\n(BLEU)\n(F1)\n(GB) ↓\nStandard MHA\n86.2\n93.5\n24.3\n28.7\n88.4\n15.2\nLongformer\n85.7\n92.8\n23.8\n27.9\n87.6\n9.1\nBigBird\n85.9\n93.1\n23.5\n28.1\n87.9\n10.3\nReformer\n84.3\n91.7\n25.6\n26.4\n85.2\n7.8\nPerformer\n85.1\n92.4\n24.9\n27.3\n86.7\n8.5\nMAHA (Ours)\n86.0\n93.3\n23.1\n28.5\n88.2\n6.7\nNote: MAHA achieves lowest perplexity on PG-19 and significant memory reduction.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "53", "text": "TABLE II: Aggregation Method Impact on MNLI Task.\nMethod\nMNLI (Acc)\nMemory (GB)\nSpeed\nConvex Opt. (CO)\n86.0\n6.7\n1.0x\nNash Eq. (NE)\n85.8\n6.9\n0.9x\nMean Aggregation\n85.2\n7.2\n1.1x\n2\n3\n4\n5\n6\nNumber of Scales (L)\n84.0\n84.5\n85.0\n85.5\n86.0\n86.5\n87.0\nMNLI Accuracy (%)\nPeak Accuracy\n(L=4, Acc=86.0)\nLoss of\nGranularity\nNoise from\nDownsampling\nFig. 5: MNLI Accuracy vs Number of Hierarchical Scales (L).\nOptimal performance is at L = 4.\n2) Medium Scales (Scale 2): Shifts towards block-diagonal\nstructures, suggesting clause-level modeling.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "54", "text": "5: MNLI Accuracy vs Number of Hierarchical Scales (L).\nOptimal performance is at L = 4.\n2) Medium Scales (Scale 2): Shifts towards block-diagonal\nstructures, suggesting clause-level modeling.\n3) Coarse Scales (Scale 3): Attention becomes diffuse with\nvertical bands, tracking document-level themes regardless\nof distance.\nVI. DISCUSSION\nA. Scalability vs. Implementation Overhead\nOur experiments highlight a critical distinction between\nalgorithmic complexity and implementation overhead. While\nprototype implementations may exhibit initialization latency,\nthe growth rate is the decisive metric for Large Language Mod-\nels. Figure 3 confirms that MAHA’s computational cost grows\nlinearly (O(N)), whereas standard attention grows quadrat-\nically (O(N 2)). This implies that for very large sequences\n(e.g., N ≫4096), MAHA provides a decisive advantage in\nboth speed and memory.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "55", "text": "VI. DISCUSSION\nA. Scalability vs. Implementation Overhead\nOur experiments highlight a critical distinction between\nalgorithmic complexity and implementation overhead. While\nprototype implementations may exhibit initialization latency,\nthe growth rate is the decisive metric for Large Language Mod-\nels. Figure 3 confirms that MAHA’s computational cost grows\nlinearly (O(N)), whereas standard attention grows quadrat-\nically (O(N 2)). This implies that for very large sequences\n(e.g., N ≫4096), MAHA provides a decisive advantage in\nboth speed and memory.\nKey Position\nQuery Position\nScale 1: Fine-Grained\n(Local Syntax / Adj-Noun)\nKey Position\nScale 2: Medium-Grained\n(Clause-Level / Local Context)\nKey Position\nScale 3: Coarse-Grained\n(Document Themes / Global)\nFig. 6: Visualization of Learned Multiscale Attention Patterns.\nDarker regions indicate higher attention weights.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "56", "text": "This implies that for very large sequences\n(e.g., N ≫4096), MAHA provides a decisive advantage in\nboth speed and memory.\nKey Position\nQuery Position\nScale 1: Fine-Grained\n(Local Syntax / Adj-Noun)\nKey Position\nScale 2: Medium-Grained\n(Clause-Level / Local Context)\nKey Position\nScale 3: Coarse-Grained\n(Document Themes / Global)\nFig. 6: Visualization of Learned Multiscale Attention Patterns.\nDarker regions indicate higher attention weights.\nB. Limitations\nWhile MAHA demonstrates significant improvements in ef-\nficiency and modeling, certain limitations warrant discussion:\n• The framework involves additional hyperparameters (e.g.,\nnumber of scales L, compression ratio r) that may require\ndomain-specific tuning.\n• Although the Nash Equilibrium aggregation offers theo-\nretical guarantees, its iterative nature imposes a compu-\ntational overhead during training compared to the closed-\nform Convex Optimization (CO) solution.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "57", "text": "6: Visualization of Learned Multiscale Attention Patterns.\nDarker regions indicate higher attention weights.\nB. Limitations\nWhile MAHA demonstrates significant improvements in ef-\nficiency and modeling, certain limitations warrant discussion:\n• The framework involves additional hyperparameters (e.g.,\nnumber of scales L, compression ratio r) that may require\ndomain-specific tuning.\n• Although the Nash Equilibrium aggregation offers theo-\nretical guarantees, its iterative nature imposes a compu-\ntational overhead during training compared to the closed-\nform Convex Optimization (CO) solution.\n• The method assumes that linguistic information is inher-\nently hierarchical; this assumption may not fully capture\ncertain non-compositional semantic relationships or dis-\npersed references in highly unstructured text [21].\nC. Potential Application Scenarios\nThe versatility of MAHA extends beyond standard NLP:\n• Genomics: In genomic sequence analysis, where identi-\nfying long-range dependencies in megabase-scale DNA\nsequences is critical [22], MAHA’s multiscale attention\ncould enhance variant calling accuracy.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "58", "text": "• The method assumes that linguistic information is inher-\nently hierarchical; this assumption may not fully capture\ncertain non-compositional semantic relationships or dis-\npersed references in highly unstructured text [21].\nC. Potential Application Scenarios\nThe versatility of MAHA extends beyond standard NLP:\n• Genomics: In genomic sequence analysis, where identi-\nfying long-range dependencies in megabase-scale DNA\nsequences is critical [22], MAHA’s multiscale attention\ncould enhance variant calling accuracy.\n• Multimodal Learning: For video-text retrieval, the hi-\nerarchical scales align naturally with temporal video\nresolutions (frames, shots, scenes) [23], offering a unified\nattention mechanism for cross-modal alignment.\n• Federated Learning: The optimization-driven aggrega-\ntion is particularly relevant for federated settings where\nclients may operate on data of varying granularities or\nqualities [24].\nD. Ethical Considerations\nThe efficiency gains of MAHA present a dual-edged sword.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "59", "text": "• Multimodal Learning: For video-text retrieval, the hi-\nerarchical scales align naturally with temporal video\nresolutions (frames, shots, scenes) [23], offering a unified\nattention mechanism for cross-modal alignment.\n• Federated Learning: The optimization-driven aggrega-\ntion is particularly relevant for federated settings where\nclients may operate on data of varying granularities or\nqualities [24].\nD. Ethical Considerations\nThe efficiency gains of MAHA present a dual-edged sword.\nWhile significantly reducing the carbon footprint per train-\ning run [25], lower costs may paradoxically incentivize the\ntraining of even larger, redundant models (Jevons paradox).\nFurthermore, the hierarchical aggregation introduces inter-\npretability challenges; while individual scales are transparent,\nthe complex interplay of optimization weights may obscure the\nmodels decision-making path [26]. Future work must address\nthese transparency issues to ensure responsible deployment.\nVII.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "60", "text": "D. Ethical Considerations\nThe efficiency gains of MAHA present a dual-edged sword.\nWhile significantly reducing the carbon footprint per train-\ning run [25], lower costs may paradoxically incentivize the\ntraining of even larger, redundant models (Jevons paradox).\nFurthermore, the hierarchical aggregation introduces inter-\npretability challenges; while individual scales are transparent,\nthe complex interplay of optimization weights may obscure the\nmodels decision-making path [26]. Future work must address\nthese transparency issues to ensure responsible deployment.\nVII. CONCLUSION\nIn this paper, we introduced Multiscale Aggregated Hierar-\nchical Attention (MAHA), a novel framework that fundamen-\ntally rethinks attention mechanisms in LLMs through the lens\n7\nof multiscale analysis and optimization theory. By decompos-\ning sequences into hierarchical granularities and synthesizing\nthem via convex optimization or game-theoretic equilibrium,\nMAHA addresses the critical bottleneck of quadratic complex-\nity without compromising contextual fidelity.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "61", "text": "Future work must address\nthese transparency issues to ensure responsible deployment.\nVII. CONCLUSION\nIn this paper, we introduced Multiscale Aggregated Hierar-\nchical Attention (MAHA), a novel framework that fundamen-\ntally rethinks attention mechanisms in LLMs through the lens\n7\nof multiscale analysis and optimization theory. By decompos-\ning sequences into hierarchical granularities and synthesizing\nthem via convex optimization or game-theoretic equilibrium,\nMAHA addresses the critical bottleneck of quadratic complex-\nity without compromising contextual fidelity.\nOur\nextensive\nempirical\nevaluation\ndemonstrates\nthat\nMAHA achieves state-of-the-art performance on long-context\nmodeling (PG-19) and machine translation, while reducing\nmemory usage by up to 56% compared to standard transform-\ners. The proposed hybrid dilated-convolutional architecture\nserves as a drop-in replacement for existing attention layers,\nfacilitating seamless adoption.\nLooking forward, MAHA paves the way for scalable foun-\ndation models in resource-constrained environments.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "62", "text": "Our\nextensive\nempirical\nevaluation\ndemonstrates\nthat\nMAHA achieves state-of-the-art performance on long-context\nmodeling (PG-19) and machine translation, while reducing\nmemory usage by up to 56% compared to standard transform-\ners. The proposed hybrid dilated-convolutional architecture\nserves as a drop-in replacement for existing attention layers,\nfacilitating seamless adoption.\nLooking forward, MAHA paves the way for scalable foun-\ndation models in resource-constrained environments. We en-\nvision future research extending this rigorous aggregation\nparadigm to other modalities such as computer vision and\nspeech processing, where multiscale representation is equally\nparamount. Ultimately, this work underscores the potential\nof integrating mathematical optimization principles into deep\nlearning architectures to build more efficient, robust, and\ntheoretically grounded AI systems.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "63", "text": "The proposed hybrid dilated-convolutional architecture\nserves as a drop-in replacement for existing attention layers,\nfacilitating seamless adoption.\nLooking forward, MAHA paves the way for scalable foun-\ndation models in resource-constrained environments. We en-\nvision future research extending this rigorous aggregation\nparadigm to other modalities such as computer vision and\nspeech processing, where multiscale representation is equally\nparamount. Ultimately, this work underscores the potential\nof integrating mathematical optimization principles into deep\nlearning architectures to build more efficient, robust, and\ntheoretically grounded AI systems.\nDATA AVAILABILITY\nThe source code and pretrained models for MAHA\nare\npublicly\navailable\nat\nhttps://github.com/canererden/\nMAHA-Project with the permanent digital object identifier\nDOI: 10.5281/zenodo.17936753.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "64", "text": "Ultimately, this work underscores the potential\nof integrating mathematical optimization principles into deep\nlearning architectures to build more efficient, robust, and\ntheoretically grounded AI systems.\nDATA AVAILABILITY\nThe source code and pretrained models for MAHA\nare\npublicly\navailable\nat\nhttps://github.com/canererden/\nMAHA-Project with the permanent digital object identifier\nDOI: 10.5281/zenodo.17936753.\nREFERENCES\n[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances\nin Neural Information Processing Systems (NeurIPS), 2017, vol. 30.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "65", "text": "REFERENCES\n[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances\nin Neural Information Processing Systems (NeurIPS), 2017, vol. 30.\n[2] W. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang,\nJ. Zhang, and Z. Dong, “A survey of large language models,” 2023,\narXiv preprint arXiv:2303.18223.\n[3] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy, “Hierarchical\nattention networks for document classification,” in Proceedings of the\n2016 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies (NAACL-\nHLT), 2016, pp.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "66", "text": "[3] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy, “Hierarchical\nattention networks for document classification,” in Proceedings of the\n2016 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies (NAACL-\nHLT), 2016, pp. 1480–1489.\n[4] J. Starck, F. Murtagh, and J. Fadili, Sparse Image and Signal Processing:\nWavelets and Related Geometric Multiscale Analysis.\nCambridge\nUniversity Press, 2015.\n[5] H. Naveed, A. Khan, S. Qiu, M. Saqib, S. Anwar, M. Usman, N. Barnes,\nand A. Mian, “A comprehensive overview of large language models,”\nACM Transactions on Intelligent Systems and Technology, 2025.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "67", "text": "1480–1489.\n[4] J. Starck, F. Murtagh, and J. Fadili, Sparse Image and Signal Processing:\nWavelets and Related Geometric Multiscale Analysis.\nCambridge\nUniversity Press, 2015.\n[5] H. Naveed, A. Khan, S. Qiu, M. Saqib, S. Anwar, M. Usman, N. Barnes,\nand A. Mian, “A comprehensive overview of large language models,”\nACM Transactions on Intelligent Systems and Technology, 2025.\n[6] I. Beltagy, M. Peters, and A. Cohan, “Longformer: The long-document\ntransformer,” 2020, arXiv preprint arXiv:2004.05150.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "68", "text": "Cambridge\nUniversity Press, 2015.\n[5] H. Naveed, A. Khan, S. Qiu, M. Saqib, S. Anwar, M. Usman, N. Barnes,\nand A. Mian, “A comprehensive overview of large language models,”\nACM Transactions on Intelligent Systems and Technology, 2025.\n[6] I. Beltagy, M. Peters, and A. Cohan, “Longformer: The long-document\ntransformer,” 2020, arXiv preprint arXiv:2004.05150.\n[7] M. Zaheer, G. Guruganesh, K. Dubey, J. Ainslie, C. Alberti, S. Ontanon,\nP. Pham, A. Ravula, Q. Wang, and L. Yang, “Big bird: Transformers\nfor longer sequences,” in Advances in Neural Information Processing\nSystems (NeurIPS), 2020, vol. 33.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "69", "text": "[7] M. Zaheer, G. Guruganesh, K. Dubey, J. Ainslie, C. Alberti, S. Ontanon,\nP. Pham, A. Ravula, Q. Wang, and L. Yang, “Big bird: Transformers\nfor longer sequences,” in Advances in Neural Information Processing\nSystems (NeurIPS), 2020, vol. 33.\n[8] M. Cheng, P. Jiang, L. Han, L. Wang, and P. Torr, “Deeply explain\ncnn via hierarchical decomposition,” International Journal of Computer\nVision, vol. 131, 2023.\n[9] W. Jun, Z. Tianliang, Z. Jiahui, L. Tianyi, and W. Chunzhi, “Hierarchical\nmultiple self-attention mechanism for multi-modal analysis,” Multimedia\nSystems, 2023.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "70", "text": "33.\n[8] M. Cheng, P. Jiang, L. Han, L. Wang, and P. Torr, “Deeply explain\ncnn via hierarchical decomposition,” International Journal of Computer\nVision, vol. 131, 2023.\n[9] W. Jun, Z. Tianliang, Z. Jiahui, L. Tianyi, and W. Chunzhi, “Hierarchical\nmultiple self-attention mechanism for multi-modal analysis,” Multimedia\nSystems, 2023.\n[10] M. Zhu, A. Anwar, Z. Wan, J. Cho, C. Kamhoua, and M. Singh,\n“A survey of defensive deception: Approaches using game theory and\nmachine learning,” IEEE Communications Surveys & Tutorials, vol. 23,\nno. 4, pp. 2460–2493, 2021.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "71", "text": "[10] M. Zhu, A. Anwar, Z. Wan, J. Cho, C. Kamhoua, and M. Singh,\n“A survey of defensive deception: Approaches using game theory and\nmachine learning,” IEEE Communications Surveys & Tutorials, vol. 23,\nno. 4, pp. 2460–2493, 2021.\n[11] J. Lee, M. Lee, D. Lee, and S. Lee, “Hierarchically decomposed\ngraph convolutional networks for skeleton-based action recognition,” in\nProceedings of the IEEE/CVF International Conference on Computer\nVision (ICCV), 2023.\n[12] J. Lu, K. Lin, R. Chen, M. Lin, X. Chen, and P. Lu, “Health insur-\nance fraud detection by using an attributed heterogeneous information\nnetwork with a hierarchical attention mechanism,” BMC Medical Infor-\nmatics and Decision Making, vol. 23, 2023.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "72", "text": "[12] J. Lu, K. Lin, R. Chen, M. Lin, X. Chen, and P. Lu, “Health insur-\nance fraud detection by using an attributed heterogeneous information\nnetwork with a hierarchical attention mechanism,” BMC Medical Infor-\nmatics and Decision Making, vol. 23, 2023.\n[13] M. Farge, “Wavelet transforms and their applications to turbulence,”\nAnnual Review of Fluid Mechanics, vol. 24, no. 1, pp. 395–457, 1992.\n[14] J. Nash, “Non-cooperative games,” in The Foundations of Price Theory,\nreprint ed.\nTaylor & Francis, 2024, vol. 4.\n[15] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman, “Glue:\nA multi-task benchmark and analysis platform for natural language\nunderstanding,” in Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), 2018.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "73", "text": "24, no. 1, pp. 395–457, 1992.\n[14] J. Nash, “Non-cooperative games,” in The Foundations of Price Theory,\nreprint ed.\nTaylor & Francis, 2024, vol. 4.\n[15] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman, “Glue:\nA multi-task benchmark and analysis platform for natural language\nunderstanding,” in Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), 2018.\n[16] S. Sun, K. Krishna, A. Mattarella-Micke, and M. Iyyer, “Do long-range\nlanguage models actually use long-range context?” 2021, arXiv preprint\narXiv:2109.09115.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "74", "text": "[16] S. Sun, K. Krishna, A. Mattarella-Micke, and M. Iyyer, “Do long-range\nlanguage models actually use long-range context?” 2021, arXiv preprint\narXiv:2109.09115.\n[17] O. Bojar, R. Chatterjee, C. Federmann, Y. Graham, B. Haddow,\nM. Huck, A. Yepes, P. Koehn, V. Logacheva, and C. Monz, “Findings\nof the 2016 conference on machine translation,” in Proceedings of the\nFirst Conference on Machine Translation (WMT), 2016, pp. 131–198.\n[18] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “Squad: 100,000+\nquestions for machine comprehension of text,” 2016, arXiv preprint\narXiv:1606.05250.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "75", "text": "131–198.\n[18] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “Squad: 100,000+\nquestions for machine comprehension of text,” 2016, arXiv preprint\narXiv:1606.05250.\n[19] N. Kitaev, L. Kaiser, and A. Levskaya, “Reformer: The efficient trans-\nformer,” 2020, arXiv preprint arXiv:2001.04451.\n[20] K. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sar-\nlos, P. Hawkins, J. Davis, A. Mohiuddin, and L. Kaiser, “Rethinking\nattention with performers,” 2020, arXiv preprint arXiv:2009.14794.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "76", "text": "[20] K. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sar-\nlos, P. Hawkins, J. Davis, A. Mohiuddin, and L. Kaiser, “Rethinking\nattention with performers,” 2020, arXiv preprint arXiv:2009.14794.\n[21] W. Rapaport, “Syntactic semantics: Foundations of computational\nnatural-language understanding,” in Thinking Computers and Virtual\nPersons.\nAcademic Press, 1994.\n[22] S. Choi and M. Lee, “Transformer architecture and attention mechanisms\nin genome data analysis: A comprehensive review,” Biology, vol. 12,\nno. 1, 2023.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "77", "text": "[21] W. Rapaport, “Syntactic semantics: Foundations of computational\nnatural-language understanding,” in Thinking Computers and Virtual\nPersons.\nAcademic Press, 1994.\n[22] S. Choi and M. Lee, “Transformer architecture and attention mechanisms\nin genome data analysis: A comprehensive review,” Biology, vol. 12,\nno. 1, 2023.\n[23] S. Liu, H. Fan, S. Qian, Y. Chen, W. Ding, and Z. Wang, “Hit: Hier-\narchical transformer with momentum contrast for video-text retrieval,”\nin Proceedings of the IEEE/CVF International Conference on Computer\nVision (ICCV), 2021.\n[24] Y. Chen, Y. Ning, Z. Chai, and H. Rangwala, “Federated multi-task\nlearning with hierarchical attention for sensor data analytics,” in 2020\nInternational Joint Conference on Neural Networks (IJCNN), 2020.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "78", "text": "[23] S. Liu, H. Fan, S. Qian, Y. Chen, W. Ding, and Z. Wang, “Hit: Hier-\narchical transformer with momentum contrast for video-text retrieval,”\nin Proceedings of the IEEE/CVF International Conference on Computer\nVision (ICCV), 2021.\n[24] Y. Chen, Y. Ning, Z. Chai, and H. Rangwala, “Federated multi-task\nlearning with hierarchical attention for sensor data analytics,” in 2020\nInternational Joint Conference on Neural Networks (IJCNN), 2020.\n[25] E. Strubell, A. Ganesh, and A. McCallum, “Energy and policy consid-\nerations for deep learning in nlp,” in Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics (ACL), 2019.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "79", "text": "[24] Y. Chen, Y. Ning, Z. Chai, and H. Rangwala, “Federated multi-task\nlearning with hierarchical attention for sensor data analytics,” in 2020\nInternational Joint Conference on Neural Networks (IJCNN), 2020.\n[25] E. Strubell, A. Ganesh, and A. McCallum, “Energy and policy consid-\nerations for deep learning in nlp,” in Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics (ACL), 2019.\n[26] M. Danilevsky, S. Dhanorkar, Y. Li, L. Popa, K. Qian, and C. Li,\n“Explainability for natural language processing,” in Proceedings of\nthe 27th ACM SIGKDD Conference on Knowledge Discovery & Data\nMining, 2021.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "80", "text": "Closed-form Continuous-time Neural Networks\nRamin Hasani 1⋆,∗, Mathias Lechner 2⋆, Alexander Amini 1,\nLucas Liebenwein 1, Aaron Ray 1,\nMax Tschaikowski 3, Gerald Teschl 4, Daniela Rus 1\n1Massachusetts Institute of Technology (MIT), Cambridge, USA\n2Institute of Science and Technology Austria (IST Austria), Austria\n3Aalborg University, Denmark\n4University of Vienna (Uni Wien), Austria\n⋆These authors contributed equally to the paper\n∗To whom correspondence should be addressed; E-mail: rhasani@mit.edu.\nContinuous-time neural processes are performant sequential decision-\nmakers that are built by differential equations (DE). However, their ex-\npressive power when they are deployed on computers is bottlenecked\nby numerical DE solvers. This limitation has signiﬁcantly slowed down\nscaling and understanding of numerous natural physical phenomena such\nas the dynamics of nervous systems.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "81", "text": "Continuous-time neural processes are performant sequential decision-\nmakers that are built by differential equations (DE). However, their ex-\npressive power when they are deployed on computers is bottlenecked\nby numerical DE solvers. This limitation has signiﬁcantly slowed down\nscaling and understanding of numerous natural physical phenomena such\nas the dynamics of nervous systems. Ideally we would circumvent this\nbottleneck by solving the given dynamical system in closed-form. This\nis known to be intractable in general. Here, we show it is possible to\nclosely approximate the interaction between neurons and synapses – the\nbuilding blocks of natural and artiﬁcial neural networks – constructed\nby liquid time-constant networks (LTCs) (1) efﬁciently in closed-form.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "82", "text": "This limitation has signiﬁcantly slowed down\nscaling and understanding of numerous natural physical phenomena such\nas the dynamics of nervous systems. Ideally we would circumvent this\nbottleneck by solving the given dynamical system in closed-form. This\nis known to be intractable in general. Here, we show it is possible to\nclosely approximate the interaction between neurons and synapses – the\nbuilding blocks of natural and artiﬁcial neural networks – constructed\nby liquid time-constant networks (LTCs) (1) efﬁciently in closed-form.\nTo this end, we compute a tightly-bounded approximation of the solu-\n1\narXiv:2106.13898v2  [cs.LG]  2 Mar 2022\ntion of an integral appearing in LTCs’ dynamics, that has had no known\nclosed-form solution so far.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "83", "text": "Here, we show it is possible to\nclosely approximate the interaction between neurons and synapses – the\nbuilding blocks of natural and artiﬁcial neural networks – constructed\nby liquid time-constant networks (LTCs) (1) efﬁciently in closed-form.\nTo this end, we compute a tightly-bounded approximation of the solu-\n1\narXiv:2106.13898v2  [cs.LG]  2 Mar 2022\ntion of an integral appearing in LTCs’ dynamics, that has had no known\nclosed-form solution so far. This closed-form solution substantially im-\npacts the design of continuous-time and continuous-depth neural mod-\nels; for instance, since time appears explicitly in closed-form, the formu-\nlation relaxes the need for complex numerical solvers. Consequently, we\nobtain models that are between one and ﬁve orders of magnitude faster in\ntraining and inference compared to differential equation-based counter-\nparts.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "84", "text": "This closed-form solution substantially im-\npacts the design of continuous-time and continuous-depth neural mod-\nels; for instance, since time appears explicitly in closed-form, the formu-\nlation relaxes the need for complex numerical solvers. Consequently, we\nobtain models that are between one and ﬁve orders of magnitude faster in\ntraining and inference compared to differential equation-based counter-\nparts. More importantly, in contrast to ODE-based continuous networks,\nclosed-form networks can scale remarkably well compared to other deep\nlearning instances. Lastly, as these models are derived from liquid net-\nworks, they show remarkable performance in time series modeling, com-\npared to advanced recurrent models.\nOne Sentence Summary: We ﬁnd an approximate closed-form solution for the inter-\naction of neurons and synapses and build a strong artiﬁcial neural network model out\nof it.\nMain Text:\nContinuous neural network architectures built by ordinary differential equations (ODEs)\n(2) opened a new paradigm for obtaining expressive and performant neural models.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "85", "text": "Lastly, as these models are derived from liquid net-\nworks, they show remarkable performance in time series modeling, com-\npared to advanced recurrent models.\nOne Sentence Summary: We ﬁnd an approximate closed-form solution for the inter-\naction of neurons and synapses and build a strong artiﬁcial neural network model out\nof it.\nMain Text:\nContinuous neural network architectures built by ordinary differential equations (ODEs)\n(2) opened a new paradigm for obtaining expressive and performant neural models.\nThese models transform the depth dimension of static neural networks and the time di-\nmension of recurrent neural networks into a continuous vector ﬁeld, enabling param-\neter sharing, adaptive computations, and function approximation for non-uniformly\nsampled data.\nThese continuous-depth (time) models have shown promise in density estimation\napplications (3–6), as well as modeling sequential and irregularly-sampled data (1,7–9).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "86", "text": "Main Text:\nContinuous neural network architectures built by ordinary differential equations (ODEs)\n(2) opened a new paradigm for obtaining expressive and performant neural models.\nThese models transform the depth dimension of static neural networks and the time di-\nmension of recurrent neural networks into a continuous vector ﬁeld, enabling param-\neter sharing, adaptive computations, and function approximation for non-uniformly\nsampled data.\nThese continuous-depth (time) models have shown promise in density estimation\napplications (3–6), as well as modeling sequential and irregularly-sampled data (1,7–9).\nWhile ODE-based neural networks with careful memory and gradient propaga-\n2\ntion design (9) perform competitively with advanced discretized recurrent models on\nrelatively small benchmarks, their training and inference are slow due to the use of ad-\nvanced numerical DE solvers (10).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "87", "text": "These continuous-depth (time) models have shown promise in density estimation\napplications (3–6), as well as modeling sequential and irregularly-sampled data (1,7–9).\nWhile ODE-based neural networks with careful memory and gradient propaga-\n2\ntion design (9) perform competitively with advanced discretized recurrent models on\nrelatively small benchmarks, their training and inference are slow due to the use of ad-\nvanced numerical DE solvers (10). This becomes even more troublesome as the com-\nplexity of the data, task and state-space increases (i.e., requiring more precision) (11),\nfor instance, in open-world problems such as medical data processing, self-driving\ncars, ﬁnancial time-series, and physics simulations.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "88", "text": "While ODE-based neural networks with careful memory and gradient propaga-\n2\ntion design (9) perform competitively with advanced discretized recurrent models on\nrelatively small benchmarks, their training and inference are slow due to the use of ad-\nvanced numerical DE solvers (10). This becomes even more troublesome as the com-\nplexity of the data, task and state-space increases (i.e., requiring more precision) (11),\nfor instance, in open-world problems such as medical data processing, self-driving\ncars, ﬁnancial time-series, and physics simulations.\nThe research community has developed solutions for resolving this computational\noverhead and for facilitating the training of neural ODEs, for instance, by relaxing the\nstiffness of a ﬂow by state augmentation techniques (4,12), reformulating the forward-\npass as a root-ﬁnding problem (13), using regularization schemes (14–16), or improv-\ning the inference time of the network (17).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "89", "text": "The research community has developed solutions for resolving this computational\noverhead and for facilitating the training of neural ODEs, for instance, by relaxing the\nstiffness of a ﬂow by state augmentation techniques (4,12), reformulating the forward-\npass as a root-ﬁnding problem (13), using regularization schemes (14–16), or improv-\ning the inference time of the network (17).\nIn this paper, we take a step back and propose a fundamental solution: we de-\nrive a closed-form continuous-depth model that has the rich modeling capabilities\nof ODE-based models and does not require any solver to model data (see Figure 1).\nThe proposed continuous neural networks yield signiﬁcantly faster training and infer-\nence speeds while being as expressive as their ODE-based counterparts. We provide\na derivation for the approximate closed-form solution to a class of continuous neural\nnetworks that explicitly models time.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "90", "text": "In this paper, we take a step back and propose a fundamental solution: we de-\nrive a closed-form continuous-depth model that has the rich modeling capabilities\nof ODE-based models and does not require any solver to model data (see Figure 1).\nThe proposed continuous neural networks yield signiﬁcantly faster training and infer-\nence speeds while being as expressive as their ODE-based counterparts. We provide\na derivation for the approximate closed-form solution to a class of continuous neural\nnetworks that explicitly models time. We demonstrate how this transformation can be\nformulated into a novel neural model and scaled to create ﬂexible, highly performant\nand fast neural architectures on challenging sequential datasets.\nDeriving an Approximate Closed-form Solution for Neural Interactions. Two neu-\nrons interact with each other through synapses as shown in Figure 1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "91", "text": "Deriving an Approximate Closed-form Solution for Neural Interactions. Two neu-\nrons interact with each other through synapses as shown in Figure 1. There are three\nprincipal mechanisms for information propagation in natural brains that are abstracted\naway in the current building blocks of deep learning systems: 1) neural dynamics are\ntypically continuous processes described by differential equations (c.f., dynamics of\n3\nSynapses\nPostsynaptic Neuron\nPresynaptic Stimuli\n𝑰(𝑡)\n𝑆𝑡= 𝑓𝑰𝑡\n(𝐴−𝒙𝑡)\n𝑥(𝑡)\n𝑑𝒙(𝑡)\n𝑑𝑡\n= −𝒙𝑡\n𝜏\n+ 𝑆(𝑡)\nthis is a liquid time-\nconstant differential \nequation instance\nwe solve this in \nclosed-form\n𝒙𝑡=\n(𝒙0 −𝐴) 𝑒! \"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "92", "text": "#$% & '\n' 𝑓(−𝐼𝑡) + 𝐴\n𝒙𝑡\n𝐴\n𝑓.\n𝜏\nPostsynaptic neuron’s potential\nSynaptic reversal potential\nSynaptic release nonlinearity\nPostsynaptic neuron’s time-constant\nFig. 1: Neural and Synapse Dynamics. A postsynaptic neuron receives stimuli I(t), through\na nonlinear conductance-based synapse model. The dynamics of the membrane potential of\nthis postsynaptic neuron is given by the differential equation presented in the middle. This\nequation is a fundamental building block of liquid time-constant networks (LTCs) (1), for which\nthere is no known closed-form expression. Here, we provided an approximate solution for this\nequation which shows the interaction of nonlinear synapses with a postsynaptic neurons, in\nclosed-form.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "93", "text": "1: Neural and Synapse Dynamics. A postsynaptic neuron receives stimuli I(t), through\na nonlinear conductance-based synapse model. The dynamics of the membrane potential of\nthis postsynaptic neuron is given by the differential equation presented in the middle. This\nequation is a fundamental building block of liquid time-constant networks (LTCs) (1), for which\nthere is no known closed-form expression. Here, we provided an approximate solution for this\nequation which shows the interaction of nonlinear synapses with a postsynaptic neurons, in\nclosed-form.\nx(t) in Figure 1), 2) synaptic release is much more than scalar weights; it involves a\nnonlinear transmission of neurotransmitters, the probability of activation of receptors,\nand the concentration of available neurotransmitters, among other nonlinearities (c.f.,\nS(t) in Figure 1), and 3) the propagation of information between neurons is induced\nby feedback and memory apparatuses (c.f.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "94", "text": "Here, we provided an approximate solution for this\nequation which shows the interaction of nonlinear synapses with a postsynaptic neurons, in\nclosed-form.\nx(t) in Figure 1), 2) synaptic release is much more than scalar weights; it involves a\nnonlinear transmission of neurotransmitters, the probability of activation of receptors,\nand the concentration of available neurotransmitters, among other nonlinearities (c.f.,\nS(t) in Figure 1), and 3) the propagation of information between neurons is induced\nby feedback and memory apparatuses (c.f. I(t) stimulates x(t) through a nonlinear\nsynapse S(t) which also has a multiplicative difference of potential to the postsynaptic\nneuron accounting for a negative feedback mechanism). Liquid time-constant (LTC)\nnetworks (1), which are expressive continuous-depth models obtained by a bilinear\napproximation (18) of neural ODE formulation (2) are designed based on these mecha-\nnisms.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "95", "text": "I(t) stimulates x(t) through a nonlinear\nsynapse S(t) which also has a multiplicative difference of potential to the postsynaptic\nneuron accounting for a negative feedback mechanism). Liquid time-constant (LTC)\nnetworks (1), which are expressive continuous-depth models obtained by a bilinear\napproximation (18) of neural ODE formulation (2) are designed based on these mecha-\nnisms. Correspondingly, we take their ODE semantics and approximate a closed-form\nsolution for the scalar case of a postsynaptic neuron receiving an input stimuli from a\npresynaptic source through a nonlinear synapse.\nTo this end, we apply the theory of linear ODEs (19) to analytically solve the dy-\nnamics of an LTC differential equation shown in Figure 1. We then simplify the so-\n4\nTable 1: Time Complexity of the process to compute K solver’s steps. ϵ is step-size, ˜ϵ is the\nmax step-size and δ << 0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "96", "text": "To this end, we apply the theory of linear ODEs (19) to analytically solve the dy-\nnamics of an LTC differential equation shown in Figure 1. We then simplify the so-\n4\nTable 1: Time Complexity of the process to compute K solver’s steps. ϵ is step-size, ˜ϵ is the\nmax step-size and δ << 0.\n˜K is time steps for closed-form continuous depth models (CfCs)\nwhich is equivalent to K. Table is reproduced and taken from (17).\nMethod\nComplexity Local Error\np-th order solver\nO(K · p)\nO(ϵp+1)\nadaptive–step solver\n−\nO(˜ϵ p+1)\nEuler hypersolver\nO(K)\nO(δϵ2)\np-th order hypersolver O(K · p)\nO(δϵp+1)\nCfC (Ours)\nO( ˜K)\nnot relevant\nlution to the point where there is one integral left to solve.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "97", "text": "Method\nComplexity Local Error\np-th order solver\nO(K · p)\nO(ϵp+1)\nadaptive–step solver\n−\nO(˜ϵ p+1)\nEuler hypersolver\nO(K)\nO(δϵ2)\np-th order hypersolver O(K · p)\nO(δϵp+1)\nCfC (Ours)\nO( ˜K)\nnot relevant\nlution to the point where there is one integral left to solve. This integral compart-\nment, R t\n0 f (I(s))ds in which f is a positive, continuous, monotonically increasing, and\nbounded nonlinearity, is challenging to solve in closed-form; since it has dependencies\non an input signal I(s) that is arbitrarily deﬁned (such as a real-world sensory read-\nouts). To approach this problem, we discretize I(s) into piecewise constant segments\nand obtain the discrete approximation of the integral in terms of sum of piecewise con-\nstant compartments over intervals.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "98", "text": "This integral compart-\nment, R t\n0 f (I(s))ds in which f is a positive, continuous, monotonically increasing, and\nbounded nonlinearity, is challenging to solve in closed-form; since it has dependencies\non an input signal I(s) that is arbitrarily deﬁned (such as a real-world sensory read-\nouts). To approach this problem, we discretize I(s) into piecewise constant segments\nand obtain the discrete approximation of the integral in terms of sum of piecewise con-\nstant compartments over intervals. This piecewise constant approximation inspired\nus to introduce an approximate closed-form solution for the integral R t\n0 f (I(s))ds that\nis provably tight when the integral appears as the exponent of an exponential decay,\nwhich is the case for LTCs. We theoretically justify how this closed-form solution rep-\nresents LTCs’ ODE semantics and is as expressive (see Figure 1).\nExplicit Time Dependency.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "99", "text": "To approach this problem, we discretize I(s) into piecewise constant segments\nand obtain the discrete approximation of the integral in terms of sum of piecewise con-\nstant compartments over intervals. This piecewise constant approximation inspired\nus to introduce an approximate closed-form solution for the integral R t\n0 f (I(s))ds that\nis provably tight when the integral appears as the exponent of an exponential decay,\nwhich is the case for LTCs. We theoretically justify how this closed-form solution rep-\nresents LTCs’ ODE semantics and is as expressive (see Figure 1).\nExplicit Time Dependency. We then dissect the properties of the obtained closed-\nform solution and design a new class of neural network models we call Closed-form\nContinuous-depth networks (CfC). CfCs have an explicit time dependency in their for-\nmulation that does not require an ODE solver to obtain their temporal rollouts. Thus,\nthey maximize the trade-off between accuracy and efﬁciency of solvers (See Table 1).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "100", "text": "We theoretically justify how this closed-form solution rep-\nresents LTCs’ ODE semantics and is as expressive (see Figure 1).\nExplicit Time Dependency. We then dissect the properties of the obtained closed-\nform solution and design a new class of neural network models we call Closed-form\nContinuous-depth networks (CfC). CfCs have an explicit time dependency in their for-\nmulation that does not require an ODE solver to obtain their temporal rollouts. Thus,\nthey maximize the trade-off between accuracy and efﬁciency of solvers (See Table 1).\nCfCs perform computations at least one order of magnitude faster training and inference\ntime compared to their ODE-based counterparts, without loss of accuracy.\n5\nTable 2: Sequence and time-step prediction complexity. n is the sequence length, k the num-\nber of hidden units, and p = order of the ODE-solver.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "101", "text": "CfCs have an explicit time dependency in their for-\nmulation that does not require an ODE solver to obtain their temporal rollouts. Thus,\nthey maximize the trade-off between accuracy and efﬁciency of solvers (See Table 1).\nCfCs perform computations at least one order of magnitude faster training and inference\ntime compared to their ODE-based counterparts, without loss of accuracy.\n5\nTable 2: Sequence and time-step prediction complexity. n is the sequence length, k the num-\nber of hidden units, and p = order of the ODE-solver.\nModel\nSequence\nTime-step\nprediction\nprediction\nRNN\nO(nk)\nO(k)\nODE-RNN\nO(nkp)\nO(kp)\nTransformer\nO(n2k)\nO(nk)\nCfC\nO(nk)\nO(k)\nSequence and Time-step Prediction Efﬁciency. CfCs perform per-time-step and per-\nsequence predictions by establishing a continuous ﬂow similar to ODE-based models.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "102", "text": "n is the sequence length, k the num-\nber of hidden units, and p = order of the ODE-solver.\nModel\nSequence\nTime-step\nprediction\nprediction\nRNN\nO(nk)\nO(k)\nODE-RNN\nO(nkp)\nO(kp)\nTransformer\nO(n2k)\nO(nk)\nCfC\nO(nk)\nO(k)\nSequence and Time-step Prediction Efﬁciency. CfCs perform per-time-step and per-\nsequence predictions by establishing a continuous ﬂow similar to ODE-based models.\nHowever, as they do not require ODE-solvers, their complexity is at least one order of\nmagnitude less than ODE based models. Consider having a performant gated recur-\nrent model (20) with the abilities to create expressive continuous ﬂows (2) and adapt-\nable dynamics (1). Table 2 compares the time complexity of CfCs to that of standard\nRNNs, ODE-RNNs and Transformers.\nCfCs: Flexible Deep Models for Sequential Tasks.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "103", "text": "However, as they do not require ODE-solvers, their complexity is at least one order of\nmagnitude less than ODE based models. Consider having a performant gated recur-\nrent model (20) with the abilities to create expressive continuous ﬂows (2) and adapt-\nable dynamics (1). Table 2 compares the time complexity of CfCs to that of standard\nRNNs, ODE-RNNs and Transformers.\nCfCs: Flexible Deep Models for Sequential Tasks. CfCs are equipped with novel gat-\ning mechanisms that explicitly control their memory. CfCs are as expressive as their\nODE-based peers and can be supplied with mixed memory architectures (9) to avoid\ngradient issues in sequential data processing applications. Beyond accuracy and per-\nformance metrics, our results indicate that when considering accuracy-per-compute\ntime, CfCs exhibit over 150× improvement.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "104", "text": "Table 2 compares the time complexity of CfCs to that of standard\nRNNs, ODE-RNNs and Transformers.\nCfCs: Flexible Deep Models for Sequential Tasks. CfCs are equipped with novel gat-\ning mechanisms that explicitly control their memory. CfCs are as expressive as their\nODE-based peers and can be supplied with mixed memory architectures (9) to avoid\ngradient issues in sequential data processing applications. Beyond accuracy and per-\nformance metrics, our results indicate that when considering accuracy-per-compute\ntime, CfCs exhibit over 150× improvement. We perform a diverse set of advanced\ntime series modeling experiments and present the performance and speed gain achiev-\nable by using CfCs in tasks with long-term dependencies, irregular data, and modeling\nphysical dynamics, among others.\n6\nDeriving a Closed-form Solution\nIn this section, we derive an approximate closed-form solution for liquid time-constant\n(LTC) networks, an expressive subclass of time-continuous models.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "105", "text": "Beyond accuracy and per-\nformance metrics, our results indicate that when considering accuracy-per-compute\ntime, CfCs exhibit over 150× improvement. We perform a diverse set of advanced\ntime series modeling experiments and present the performance and speed gain achiev-\nable by using CfCs in tasks with long-term dependencies, irregular data, and modeling\nphysical dynamics, among others.\n6\nDeriving a Closed-form Solution\nIn this section, we derive an approximate closed-form solution for liquid time-constant\n(LTC) networks, an expressive subclass of time-continuous models. We discuss how\nthe scalar closed-form expression derived from a small LTC system can inspire the\ndesign of CfC models.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "106", "text": "6\nDeriving a Closed-form Solution\nIn this section, we derive an approximate closed-form solution for liquid time-constant\n(LTC) networks, an expressive subclass of time-continuous models. We discuss how\nthe scalar closed-form expression derived from a small LTC system can inspire the\ndesign of CfC models.\nThe hidden state of an LTC network is determined by the solution of the initial-\nvalue problem (IVP) given below (1):\ndx\ndt = −(wτ + f (x, I, θ))x(t) + A f (x, I, θ),\n(1)\nwhere x(t) deﬁnes the hidden states, I(t) is the input to the system, wτ is a time-\nconstant parameter vector, A is a bias vector, and f is a neural network parametrized\nby θ.\nTheorem 1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "107", "text": "The hidden state of an LTC network is determined by the solution of the initial-\nvalue problem (IVP) given below (1):\ndx\ndt = −(wτ + f (x, I, θ))x(t) + A f (x, I, θ),\n(1)\nwhere x(t) deﬁnes the hidden states, I(t) is the input to the system, wτ is a time-\nconstant parameter vector, A is a bias vector, and f is a neural network parametrized\nby θ.\nTheorem 1. Given an LTC system determined by the IVP (1), constructed by one cell, receiv-\ning a single dimensional time-series input I with no self connections, the following expression\nis an approximation of its closed-form solution:\nx(t) = (x0 −A)e−[wτ+ f (I(t),θ)]t f (−I(t), θ) + A\n(2)\nProof.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "108", "text": "Given an LTC system determined by the IVP (1), constructed by one cell, receiv-\ning a single dimensional time-series input I with no self connections, the following expression\nis an approximation of its closed-form solution:\nx(t) = (x0 −A)e−[wτ+ f (I(t),θ)]t f (−I(t), θ) + A\n(2)\nProof. In the single-dimensional case, the IVP (1) becomes linear in x as follows:\nd\ndtx(t) = −\n\u0002\nwτ + f (I(t))\n\u0003 · x(t) + A f (I(t))\n(3)\nTherefore, we can use the theory of linear ODEs to obtain an integral closed-form solu-\ntion (19, Section 1.10) consisting of two nested integrals. The inner integral can be elim-\ninated by means of integration by substitution (21). With this, the remaining integral\n7\nexpression can be solved in the case of piecewise constant inputs and approximated in\nthe case of general inputs.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "109", "text": "The inner integral can be elim-\ninated by means of integration by substitution (21). With this, the remaining integral\n7\nexpression can be solved in the case of piecewise constant inputs and approximated in\nthe case of general inputs. The three steps of the proof are outlined below.\nIntegral closed-form solution of LTC. We consider the ODE semantics of a single neu-\nron that receives some arbitrary continuous input signal I and has a positive, bounded,\ncontinuous, and monotonically increasing nonlinearity f:\nd\ndtx(t) = −\n\u0002\nwτ + f (I(t))\n\u0003 · x(t) + A ·\n\u0002\nwτ + f (I(t))\n\u0003\nAssumption. We assumed a second constant value wτ in the above representation of a\nsingle LTC neuron. This is done to introduce symmetry on the structure of the ODE,\nhence being able to apply the theory of linear ODEs for solving the equation analyti-\ncally.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "110", "text": "We assumed a second constant value wτ in the above representation of a\nsingle LTC neuron. This is done to introduce symmetry on the structure of the ODE,\nhence being able to apply the theory of linear ODEs for solving the equation analyti-\ncally.\nBy applying linear ODE systems theory (19, Section 1.10), we obtain:\nx(t) = e−R t\n0 [wτ+ f (I(s))]ds · x(0)+\nZ t\n0 e−R t\ns [wτ+ f (I(v))]dv · A · (wτ + f (I(s)))ds\n(4)\nTo resolve the double integral in the equation above, we deﬁne\nu(s) :=\nZ t\ns [wτ + f (I(v))]dv,\nand observe that d\ndsu(s) = −(wτ + f (I(s))).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "111", "text": "Hence, integration by substitution allows\nus to rewrite (4) into:\nx(t) = e−R t\n0 [wτ+ f (I(s))]ds · x(0) −A\nZ u(t)\nu(0) e−udu\n= x(0)e−R t\n0 [wτ+ f (I(s))]ds + A[e−u]u(t)\nu(0)\n= x(0)e−R t\n0 [wτ+ f (I(s))]ds + A\n\u00001 −e−R t\n0 [wτ+ f (I(s))]ds\u0001\n= (x(0) −A)e−wτte−R t\n0 f (I(s))ds + A\n(5)\n8\nAnalytical LTC solution for piecewise constant inputs. The derivation of a useful\nclosed-form expression of x requires us to solve the integral expression R t\n0 f (I(s))ds for\nany t ≥0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "112", "text": "The derivation of a useful\nclosed-form expression of x requires us to solve the integral expression R t\n0 f (I(s))ds for\nany t ≥0. Fortunately, the integral R t\n0 f (I(s))ds enjoys a simple closed-form expression\nfor piecewise constant inputs I. Speciﬁcally, assume that we are given a sequence of\ntime points:\n0 = τ0 < τ1 < τ2 < . . . < τn−1 < τn = ∞,\nsuch that τ1, . . . , τn−1 ∈R and I(t) = γi for all t ∈[τi; τi+1) with 0 ≤i ≤n −1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "113", "text": "Fortunately, the integral R t\n0 f (I(s))ds enjoys a simple closed-form expression\nfor piecewise constant inputs I. Speciﬁcally, assume that we are given a sequence of\ntime points:\n0 = τ0 < τ1 < τ2 < . . . < τn−1 < τn = ∞,\nsuch that τ1, . . . , τn−1 ∈R and I(t) = γi for all t ∈[τi; τi+1) with 0 ≤i ≤n −1. Then,\nit holds that\nZ t\n0 f (I(s))ds = f (γk)(t −τk) +\nk−1\n∑\ni=0\nf (γi)(τi+1 −τi),\n(6)\nwhen τk ≤t < τk+1 for some 0 ≤k ≤n −1 (as usual, one deﬁnes ∑−1\ni=0 := 0).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "114", "text": "Then,\nit holds that\nZ t\n0 f (I(s))ds = f (γk)(t −τk) +\nk−1\n∑\ni=0\nf (γi)(τi+1 −τi),\n(6)\nwhen τk ≤t < τk+1 for some 0 ≤k ≤n −1 (as usual, one deﬁnes ∑−1\ni=0 := 0). With\nthis, we have:\nx(t) = (x(0) −A)e−wτte−f (γk)(t−τk)−∑k−1\ni=0 f (γi)(τi+1−τi) + A,\n(7)\nwhen τk ≤t < τk+1 for some 0 ≤k ≤n −1. While any continuous input can be ap-\nproximated arbitrarily well by a piecewise constant input (21), a tight approximation\nmay require a large number of discretization points τ1, . . . , τn.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "115", "text": "While any continuous input can be ap-\nproximated arbitrarily well by a piecewise constant input (21), a tight approximation\nmay require a large number of discretization points τ1, . . . , τn. We address this next.\nAnalytical LTC approximation for general inputs. Inspired by Eq. 6, the next result\nprovides an analytical approximation of x(t).\nLemma 1. For any Lipschitz continuous, positive, monotonically increasing, and bounded f\nand continuous input signal I(t), we approximate x(t) in (5) as follows:\n˜x(t) = (x(0) −A)e−\n\u0002\nwτt+ f (I(t))t\n\u0003\nf (−I(t)) + A\n(8)\nThen, |x(t) −˜x(t)| ≤|x(0) −A|e−wτt for all t ≥0. Writing c = x(0) −A for convenience,\nwe can obtain the following sharpness results, additionally:\n9\n1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "116", "text": "Writing c = x(0) −A for convenience,\nwe can obtain the following sharpness results, additionally:\n9\n1. For any t ≥0, we have sup{ 1\nc(x(t) −˜x(t)) | I : [0; t] →R} = e−wτt.\n2. For any t ≥0, we have inf{ 1\nc(x(t) −˜x(t)) | I : [0; t] →R} = e−wτt(e−t −1).\nAbove, the supremum and inﬁmum are meant to be taken across all continuous input signals.\nThese statements settle the question about the worst-case errors of the approximation. The ﬁrst\nstatement implies in particular that our bound is sharp.\nThe full proof is given in Methods. Lemma 1 demonstrates that the integral solution we\nobtained shown in Equation 5 is tightly close to the approximate closed-form solution\nwe proposed in Equation 8.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "117", "text": "Above, the supremum and inﬁmum are meant to be taken across all continuous input signals.\nThese statements settle the question about the worst-case errors of the approximation. The ﬁrst\nstatement implies in particular that our bound is sharp.\nThe full proof is given in Methods. Lemma 1 demonstrates that the integral solution we\nobtained shown in Equation 5 is tightly close to the approximate closed-form solution\nwe proposed in Equation 8. Note that as wτ is positively deﬁned, the derived bound\nbetween Equations 5 and 8 ensures an exponentially decaying error as time goes by.\nTherefore, we have the statement of the theorem.\nAn Instantiation of LTCs and their approximate closed-form expressions. Figure 2\nshows a liquid network with two neurons and ﬁve synaptic connections. The network\nreceives an input signal I(t). Figure 2 further derives the differential equation expres-\nsion of the network along with its closed-form approximate solution.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "118", "text": "Note that as wτ is positively deﬁned, the derived bound\nbetween Equations 5 and 8 ensures an exponentially decaying error as time goes by.\nTherefore, we have the statement of the theorem.\nAn Instantiation of LTCs and their approximate closed-form expressions. Figure 2\nshows a liquid network with two neurons and ﬁve synaptic connections. The network\nreceives an input signal I(t). Figure 2 further derives the differential equation expres-\nsion of the network along with its closed-form approximate solution.\nIn general, it is possible to compile a trained LTC network into its closed-form ver-\nsion. This compilation allows us to speed up inference time of ODE-based networks\nas the closed-form variant does not require complex ODE solvers to compute outputs.\nAlgorithm 1 provides the instructions on how to transfer a trained LTC network into\nits closed form variant.\nTightness of the Closed-form Solution in Practice\nFigure 3 shows an LTC-based network trained for autonomous driving (22).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "119", "text": "Figure 2 further derives the differential equation expres-\nsion of the network along with its closed-form approximate solution.\nIn general, it is possible to compile a trained LTC network into its closed-form ver-\nsion. This compilation allows us to speed up inference time of ODE-based networks\nas the closed-form variant does not require complex ODE solvers to compute outputs.\nAlgorithm 1 provides the instructions on how to transfer a trained LTC network into\nits closed form variant.\nTightness of the Closed-form Solution in Practice\nFigure 3 shows an LTC-based network trained for autonomous driving (22). The ﬁgure\nfurther illustrates how close the proposed solution ﬁts the actual dynamics exhibited\nfrom a single neuron ODE given the same parametrization.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "120", "text": "The ﬁgure\nfurther illustrates how close the proposed solution ﬁts the actual dynamics exhibited\nfrom a single neuron ODE given the same parametrization.\n10\nAlgorithm 1 Translate a trained LTC network into its closed-form variant\nInputs: LTC inputs I(N×T)(t), LTC neurons activity x(H×T)(t), and their initial states\nx(H×1)(0), Synapses adjacency matrix W[(N+H)∗(N+H)]\nAdj\nLTC’s ODE Solver, Solver’s step ∆t,\ntime-instance vectors of inputs, t(1×T)\nI(t)\ntime-instance of LTC neurons tx(t)\n∇time might be sampled irregularly\nLTC neurons’ parameter τ(H×1)\nLTC network synaptic parameters { σ(N×H), µ(N×H), A(N×H)}\nOutputs: LTC’s closed-form approximation of hidden state neurons, ˆx(N×T)(t)\nxpre(t) = WAdj × [I0 . . . IN,\nx0 . . .", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "121", "text": ". . IN,\nx0 . . . xH]\n∇all presynaptic signals to nodes\nfor ith neuron in neurons 1 to H do\nfor j in Synapses to ith neuron do\nˆxi += (x0 −Aij)e\nh\n−tx(t)⊙\n\u0010\n1/τi+\n1\n1+e\n(−σij(xpreij −µij))\n\u0011\n)\ni\n⊙\n1\n1 + e(σij(xpreij−µij)) + Aij\nend for\nend for\nreturn ˆx(t)\nWe took a trained Neural Circuit Policy (NCP) (22), which consists of a perception\nmodule and a liquid time-constant (LTC) based network (1) that possess 19 neurons\nand 253 synapses. The network was trained to autonomously steer a self-driving ve-\nhicle. We used recorded real-world test-runs of the vehicle for a lane-keeping task,\ngoverned by this network.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "122", "text": "The network was trained to autonomously steer a self-driving ve-\nhicle. We used recorded real-world test-runs of the vehicle for a lane-keeping task,\ngoverned by this network. The records included the inputs, outputs as well as all LTC\nneurons’ activities and parameters. To perform a sanity check whether our proposed\nclosed-form solution for LTC neurons is good enough in practice as well as the theory,\nwe plugged in the parameters of individual neurons and synapses of the differential\nequations into the closed-form solution (Similar to the representations shown in Fig-\nure 2b and 2c) and emulated the structure of the ODE-based LTC networks. We then\nvisualized the output neuron’s dynamics of the ODE (in blue) and of the closed-form\n11\n𝑆!", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "123", "text": "We then\nvisualized the output neuron’s dynamics of the ODE (in blue) and of the closed-form\n11\n𝑆!\"(𝑡)\n𝑆!#(𝑡)\n𝑑𝒙𝟏(𝑡)\n𝑑𝑡\n= −𝒙𝟏𝑡\n𝜏\"\n+ 𝑓#\" 𝑰𝑡\n𝐴#\" −𝒙𝟏𝑡\n+ 𝑓$\" 𝒙𝟐𝑡\n(𝐴$\" −𝒙𝟏𝑡)\n𝒙𝟏𝑡= 𝒙𝟏0 −𝐴!\" 𝑒\n% \"\n&!'(\"! ) *\n* 𝑓!\" −𝐼𝑡\n+ 𝐴!\" + 𝒙𝟏0 −𝐴#\" 𝑒\n% \"\n&!'(#! 𝒙𝟐*\n* 𝑓#\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "124", "text": "𝑒\n% \"\n&!'(\"! ) *\n* 𝑓!\" −𝐼𝑡\n+ 𝐴!\" + 𝒙𝟏0 −𝐴#\" 𝑒\n% \"\n&!'(#! 𝒙𝟐*\n* 𝑓#\" −𝒙𝟐𝑡\n+ 𝐴#\"\n𝑑𝒙𝟐(𝑡)\n𝑑𝑡\n= −𝒙𝟐𝑡\n𝜏$\n+ 𝑓#$ 𝑰𝑡\n𝐴#$ −𝒙𝟐𝑡\n+ 𝑓\"$ 𝒙𝟏𝑡\n𝐴\"$ −𝒙𝟐𝑡\n+ 𝑓$$ 𝒙𝟐𝑡\n(𝐴$$ −𝒙𝟐𝑡)\n𝒙𝟐𝑡=", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "125", "text": "−𝒙𝟐𝑡\n𝜏$\n+ 𝑓#$ 𝑰𝑡\n𝐴#$ −𝒙𝟐𝑡\n+ 𝑓\"$ 𝒙𝟏𝑡\n𝐴\"$ −𝒙𝟐𝑡\n+ 𝑓$$ 𝒙𝟐𝑡\n(𝐴$$ −𝒙𝟐𝑡)\n𝒙𝟐𝑡= 𝒙𝟐𝟎−𝐴!# 𝑒\n% \"\n&#'(\"# ) *\n* 𝑓!# −𝐼𝑡\n+ 𝐴!# + 𝒙𝟐𝟎−𝐴\"# 𝑒\n% \"\n&#'(!# 𝒙𝟏*\n* 𝑓\"# −𝒙𝟐𝑡\n+ 𝐴\"# + 𝒙𝟐𝟎−𝐴##", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "126", "text": "𝒙𝟐𝟎−𝐴!# 𝑒\n% \"\n&#'(\"# ) *\n* 𝑓!# −𝐼𝑡\n+ 𝐴!# + 𝒙𝟐𝟎−𝐴\"# 𝑒\n% \"\n&#'(!# 𝒙𝟏*\n* 𝑓\"# −𝒙𝟐𝑡\n+ 𝐴\"# + 𝒙𝟐𝟎−𝐴## 𝑒\n% \"\n&#'(## 𝒙𝟐*\n* 𝑓## −𝒙𝟐𝑡\n+ 𝐴##\n𝑆\"#(𝑡)\n𝑆!\"(𝑡)\n𝑆#\"(𝑡)\n𝑰(𝑡)\n𝑆!", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "127", "text": "\"(𝑡)\n𝑆!#(𝑡)\n𝑆#\"(𝑡)\n𝑆\"#(𝑡)\n𝑆##(𝑡)\n𝑆##(𝑡)\n𝑆#\"(𝑡)\n𝑆!#(𝑡)\n𝑆\"#(𝑡)\n𝑆##(𝑡)\na. LTC network with 2 neurons\nb. LTC differential equations\nc. Approximate closed-form solution of LTCs \nInput\nLegend\n𝑥𝑖(𝑡)\npotential of neuron i\n𝑆𝑖𝑗𝑡\nsynapse between node i and j\n𝜏𝑖\ntime-constant of neuron i\n𝐴/0 synaptic reversal potential for nodes i and j\n𝑓/0\nnonlinearity of a synapse between i and j\n𝑡\ntime\nFig. 2: Instantiation of LTCs in ODE and closed-form representations.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "128", "text": "2: Instantiation of LTCs in ODE and closed-form representations. a) A sample LTC\nnetwork with two nodes and ﬁve synapses. b) the ODE representation of this two-neuron\nsystem. c) the approximate closed-form representation of the network.\nsolution (in red). As illustrated in Figure 3, we observed that the behavior of the ODE\nis captured with a mean-squared error of 0.006 by the closed-form solution. This ex-\nperiment is an empirical evidence for the tightness results presented in our theory.\nHence, the closed-form solution contains the main properties of liquid networks in ap-\nproximating dynamics. We next show how to design a novel neural network instance\ninspired by this closed-form solution, that has well-behaved gradient properties and\napproximation capabilities.\nDesign a Closed-form Continuous-depth Model Inspired\nby the Solution\nLeveraging the scalar closed-form solution expressed by Eq. (2), we can now distill\nthis model into a neural network that can be trained at scale.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "129", "text": "This ex-\nperiment is an empirical evidence for the tightness results presented in our theory.\nHence, the closed-form solution contains the main properties of liquid networks in ap-\nproximating dynamics. We next show how to design a novel neural network instance\ninspired by this closed-form solution, that has well-behaved gradient properties and\napproximation capabilities.\nDesign a Closed-form Continuous-depth Model Inspired\nby the Solution\nLeveraging the scalar closed-form solution expressed by Eq. (2), we can now distill\nthis model into a neural network that can be trained at scale. The solution provid-\ning a grounded theoretical basis for solving scalar continuous-time dynamics and it is\nimportant to translate this theory into a practical neural network model which can be\n12\n𝑑𝑥\n𝑑𝑡= −𝑤!", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "130", "text": "The solution provid-\ning a grounded theoretical basis for solving scalar continuous-time dynamics and it is\nimportant to translate this theory into a practical neural network model which can be\n12\n𝑑𝑥\n𝑑𝑡= −𝑤! + 𝑓𝑥, 𝐼\n𝑥𝑡+ 𝐴𝑓𝑥, 𝐼\n𝑥𝑡= 𝑥0 −𝐴𝑒\" #!$% &,( ) 𝑓(−𝑥, −𝐼) + 𝐴\nPerception module\nliquid time-constant (LTC) module\ninput stream\ndynamics of each node\nTime (s)\nOutput neuron dynamics\nODE\nCfC\nclosed-form \nsolution of LTC\nLTC\nOutputs\nInputs  𝐼𝑡\nneuron’s state 𝑥(𝑡)\nNonlinearity 𝑓(. )\nParameters 𝑤\", 𝐴\nFig.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "131", "text": "Parameters 𝑤\", 𝐴\nFig. 3: Tightness of the closed-form solution in practice. We approximate a closed-form so-\nlution for LTCs (1) while largely preserving the trajectories of their equivalent ODE systems.\nWe develop our solution into closed-form continuous-depth (CfC) models that are at least 100x\nfaster than neural ODEs at both training and inference on complex time-series prediction tasks.\nintegrated into larger representation learning systems. Doing so requires careful atten-\ntion to potential gradient and expressivity issues that can arise during optimization,\nwhich we will outline in this section.\nFormally, the hidden states, x(t)(D×1) with D hidden units at each time step t, can be\nexplicitly obtained by:\nx(t) = B ⊙e−[wτ+ f (x,I;θ)]t ⊙f (−x, −I; θ) + A,\n(9)\nwhere B(D) collapses (x0 −A) of Eq. 2 into a parameter vector.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "132", "text": "Formally, the hidden states, x(t)(D×1) with D hidden units at each time step t, can be\nexplicitly obtained by:\nx(t) = B ⊙e−[wτ+ f (x,I;θ)]t ⊙f (−x, −I; θ) + A,\n(9)\nwhere B(D) collapses (x0 −A) of Eq. 2 into a parameter vector. A(D) and w(D)\nτ\nare\nsystem’s parameter vectors, as well, I(t)(m×1) is an m-dimensional input at each time\nstep t, f is a neural network parametrized by θ = {W(m×D)\nIx\n, W(D×D)\nxx\n, b(D)\nx\n}, and ⊙is\n13\nneural network heads\nBackbone \nneural \nnetwork\n𝑓\n𝑔\nℎ\n+\n𝜎(. )", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "133", "text": "2 into a parameter vector. A(D) and w(D)\nτ\nare\nsystem’s parameter vectors, as well, I(t)(m×1) is an m-dimensional input at each time\nstep t, f is a neural network parametrized by θ = {W(m×D)\nIx\n, W(D×D)\nxx\n, b(D)\nx\n}, and ⊙is\n13\nneural network heads\nBackbone \nneural \nnetwork\n𝑓\n𝑔\nℎ\n+\n𝜎(. )\n1 −𝜎\n𝑦(𝑡)\n𝐭= 𝑡!, 𝑡\", … , 𝑡#$%, 𝑡#\ntime vector\nClosed-form Continuous-depth (CfC)\nHadamard\nsigmoid\n-1\n𝑡\n𝑡!\n𝑡\"\n𝑡#$%\n𝑡#\n𝐼(𝑡)\n𝐼\"\n𝐼#\n𝐼$\nInput batch\nFig.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "134", "text": "1 −𝜎\n𝑦(𝑡)\n𝐭= 𝑡!, 𝑡\", … , 𝑡#$%, 𝑡#\ntime vector\nClosed-form Continuous-depth (CfC)\nHadamard\nsigmoid\n-1\n𝑡\n𝑡!\n𝑡\"\n𝑡#$%\n𝑡#\n𝐼(𝑡)\n𝐼\"\n𝐼#\n𝐼$\nInput batch\nFig. 4: Closed-form Continuous-depth neural architecture. A baclbone neural network layer\ndelivers the input signals into three head networks g, f and h. f acts as a liquid time-constant\nfor the sigmoidal time-gates of the network. g and h construct the nonlinearieties of the overall\nCfC network.\nthe Hadamard (element-wise) product.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "135", "text": "𝑡\"\n𝑡#$%\n𝑡#\n𝐼(𝑡)\n𝐼\"\n𝐼#\n𝐼$\nInput batch\nFig. 4: Closed-form Continuous-depth neural architecture. A baclbone neural network layer\ndelivers the input signals into three head networks g, f and h. f acts as a liquid time-constant\nfor the sigmoidal time-gates of the network. g and h construct the nonlinearieties of the overall\nCfC network.\nthe Hadamard (element-wise) product. While the neural network presented in 9 can be\nproven to be a universal approximator as it is an approximation of an ODE system (1,2),\nin its current form, it has trainability issues which we point out and resolve shortly:\nResolving the gradient issues. The exponential term in Eq.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "136", "text": "g and h construct the nonlinearieties of the overall\nCfC network.\nthe Hadamard (element-wise) product. While the neural network presented in 9 can be\nproven to be a universal approximator as it is an approximation of an ODE system (1,2),\nin its current form, it has trainability issues which we point out and resolve shortly:\nResolving the gradient issues. The exponential term in Eq. 9 derives the system’s\nﬁrst part (exponentially fast) to 0 and the entire hidden state to A. This issue becomes\nmore apparent when there are recurrent connections and causes vanishing gradient\nfactors when trained by gradient descent (23). To reduce the effect, we replace the\nexponential decay term with a reversed sigmoidal nonlinearity σ(.). This nonlinearity\nis approximately 1 at t = 0 and approaches 0 in the limit t →∞. However, unlike the\nexponential decay, its transition happens much smoother, yielding a better condition\non the loss surface.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "137", "text": "To reduce the effect, we replace the\nexponential decay term with a reversed sigmoidal nonlinearity σ(.). This nonlinearity\nis approximately 1 at t = 0 and approaches 0 in the limit t →∞. However, unlike the\nexponential decay, its transition happens much smoother, yielding a better condition\non the loss surface.\nReplacing biases by learnable instances. Next, we consider the bias parameter B to be\npart of the trainable parameters of the neural network f (−x, −I; θ) and choose to use\na new network instance instead of f (presented in the exponential decay factor). We\nalso replace A with another neural network instance, h(.) to enhance the ﬂexibility of\nthe model. To obtain a more general network architecture, we allow the nonlinearity\nf (−x, −I; θ) present in Eq. 9 have both shared (backbone) and independent, (g(.)),\n14\nnetwork compartments.\nGating balance.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "138", "text": "We\nalso replace A with another neural network instance, h(.) to enhance the ﬂexibility of\nthe model. To obtain a more general network architecture, we allow the nonlinearity\nf (−x, −I; θ) present in Eq. 9 have both shared (backbone) and independent, (g(.)),\n14\nnetwork compartments.\nGating balance. The time-decaying sigmoidal term can play a gating role if we addi-\ntionally multiply h(.), with (1 −σ(.)). This way, the time-decaying sigmoid function\nstands for a gating mechanism that interpolates between the two limits of t →−∞and\nt →∞of the ODE trajectory.\nBackbone. Instead of learning all three neural network instances f, g and h separately,\nwe have them share the ﬁrst few layers in the form of a backbone that branches out\ninto these three functions. As a result, the backbone allows our model to learn shared\nrepresentations, thereby speeding up and stabilizing the learning process.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "139", "text": "), with (1 −σ(.)). This way, the time-decaying sigmoid function\nstands for a gating mechanism that interpolates between the two limits of t →−∞and\nt →∞of the ODE trajectory.\nBackbone. Instead of learning all three neural network instances f, g and h separately,\nwe have them share the ﬁrst few layers in the form of a backbone that branches out\ninto these three functions. As a result, the backbone allows our model to learn shared\nrepresentations, thereby speeding up and stabilizing the learning process. More im-\nportantly, this architectural prior enables two simultaneous beneﬁts: 1) Through the\nshared backbone a coupling between time-constant of the system and its state nonlin-\nearity get established that exploits causal representation learning evident in a liquid\nneural network (1, 24). 2) through separate head network layers, the system has the\nability to explore temporal and structural dependencies independently of each other.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "140", "text": "As a result, the backbone allows our model to learn shared\nrepresentations, thereby speeding up and stabilizing the learning process. More im-\nportantly, this architectural prior enables two simultaneous beneﬁts: 1) Through the\nshared backbone a coupling between time-constant of the system and its state nonlin-\nearity get established that exploits causal representation learning evident in a liquid\nneural network (1, 24). 2) through separate head network layers, the system has the\nability to explore temporal and structural dependencies independently of each other.\nThese modiﬁcations result in the closed-form continuous-depth (CfC) neural network\nmodel:\nx(t) = σ(−f (x, I; θ f ) t)\n|\n{z\n}\ntime-continuous gating\n⊙g(x, I; θg)+\n(10)\n\u0002\n1 −σ(−[ f (x, I; θ f )] t)\n\u0003\n|\n{z\n}\ntime-continuous gating\n⊙h(x, I; θh).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "141", "text": "These modiﬁcations result in the closed-form continuous-depth (CfC) neural network\nmodel:\nx(t) = σ(−f (x, I; θ f ) t)\n|\n{z\n}\ntime-continuous gating\n⊙g(x, I; θg)+\n(10)\n\u0002\n1 −σ(−[ f (x, I; θ f )] t)\n\u0003\n|\n{z\n}\ntime-continuous gating\n⊙h(x, I; θh).\nThe CfC architecture is illustrated in Figure 4. The neural network instances could\nbe selected arbitrarily. The time complexity of the algorithm is equivalent to that of\ndiscretized recurrent networks (25), which is at least one order of magnitude faster\nthan ODE-based networks.\n15\nHow do you deal with time, t? CfCs are continuous-depth models that can set their\ntemporal behavior based on the task-under-test.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "142", "text": "The CfC architecture is illustrated in Figure 4. The neural network instances could\nbe selected arbitrarily. The time complexity of the algorithm is equivalent to that of\ndiscretized recurrent networks (25), which is at least one order of magnitude faster\nthan ODE-based networks.\n15\nHow do you deal with time, t? CfCs are continuous-depth models that can set their\ntemporal behavior based on the task-under-test. For time-variant datasets (e.g., irreg-\nularly sampled time series, event-based data, and sparse data), t for each incoming\nsample is set based on its time-stamp or order. For sequential applications where the\ntime of the occurrence of a sample does not matter, t is sampled batch-length-times with\nequidistant intervals within two hyperparameters a and b.\nExperiments with CfCs\nWe now assess the performance of CfCs in a series of sequential data processing tasks\ncompared to advanced, recurrent models.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "143", "text": "For time-variant datasets (e.g., irreg-\nularly sampled time series, event-based data, and sparse data), t for each incoming\nsample is set based on its time-stamp or order. For sequential applications where the\ntime of the occurrence of a sample does not matter, t is sampled batch-length-times with\nequidistant intervals within two hyperparameters a and b.\nExperiments with CfCs\nWe now assess the performance of CfCs in a series of sequential data processing tasks\ncompared to advanced, recurrent models. We ﬁrst evaluate how CfCs compare to\nLTC-based neural circuit policies (NCPs) (22) in real-world autonomous lane keeping\ntasks. We then approach solving conventional sequential data modeling tasks (e.g., bit-\nstream prediction, sentiment analysis on text data, medical time-series prediction, and\nrobot kinematics modeling), and compare CfC variants to an extensive set of advanced\nrecurrent neural network baselines.\nCfC Network Variants.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "144", "text": "We ﬁrst evaluate how CfCs compare to\nLTC-based neural circuit policies (NCPs) (22) in real-world autonomous lane keeping\ntasks. We then approach solving conventional sequential data modeling tasks (e.g., bit-\nstream prediction, sentiment analysis on text data, medical time-series prediction, and\nrobot kinematics modeling), and compare CfC variants to an extensive set of advanced\nrecurrent neural network baselines.\nCfC Network Variants. To evaluate how the proposed modiﬁcations we applied to\nthe closed-form solution network described by Eq. 9, we test four variants of the CfC\narchitecture: 1) Closed-form solution network (Cf-S) obtained by Eq. 9, 2) CfC without\nthe second gating mechanism (CfC-noGate). This variant does not have the 1 −σ\ninstance shown in Figure 4. 3) Closed-form Continuous-depth model (CfC) expressed\nby Eq. 10.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "145", "text": "CfC Network Variants. To evaluate how the proposed modiﬁcations we applied to\nthe closed-form solution network described by Eq. 9, we test four variants of the CfC\narchitecture: 1) Closed-form solution network (Cf-S) obtained by Eq. 9, 2) CfC without\nthe second gating mechanism (CfC-noGate). This variant does not have the 1 −σ\ninstance shown in Figure 4. 3) Closed-form Continuous-depth model (CfC) expressed\nby Eq. 10. 4) CfC wrapped inside a mixed-memory architecture (i.e., CfC deﬁnes the\nmemory state of an RNN for instance an LSTM). We call this variant CfC-mmRNN.\nEach of these four proposed variants leverage our proposed solution, and thus are at\nleast one order of magnitude faster than continuous-time ODE models.\nHow well CfCs perform in autonomous driving compared to NCPs and other mod-\n16\nels?", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "146", "text": "3) Closed-form Continuous-depth model (CfC) expressed\nby Eq. 10. 4) CfC wrapped inside a mixed-memory architecture (i.e., CfC deﬁnes the\nmemory state of an RNN for instance an LSTM). We call this variant CfC-mmRNN.\nEach of these four proposed variants leverage our proposed solution, and thus are at\nleast one order of magnitude faster than continuous-time ODE models.\nHow well CfCs perform in autonomous driving compared to NCPs and other mod-\n16\nels? In this experiment, our objective is to evaluate how robustly CfCs learn to perform\nautonomous navigation as opposed to its ODE-based counterparts LTC networks. The\ntask is to map incoming pixel observations to steering curvature commands. We start\noff by training neural network architectures that possess a convolutional head stacked\nwith the choice of RNN. The RNN compartment of networks are replaced by LSTM\nnetworks, NCPs (22), Cf-S, CfC-NoGate, and CfC-mmRNN.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "147", "text": "In this experiment, our objective is to evaluate how robustly CfCs learn to perform\nautonomous navigation as opposed to its ODE-based counterparts LTC networks. The\ntask is to map incoming pixel observations to steering curvature commands. We start\noff by training neural network architectures that possess a convolutional head stacked\nwith the choice of RNN. The RNN compartment of networks are replaced by LSTM\nnetworks, NCPs (22), Cf-S, CfC-NoGate, and CfC-mmRNN. We also trained a fully\nconvolutional neural network for the sake of proper comparison.\nOur training pipeline followed an imitation learning approach with paired pixel-\ncontrol data, from a 30Hz BlackFly PGE-23S3C RGB camera, collected by a human\nexpert driver across a variety of rural driving environments, including times of day,\nweather conditions, and season of the year. The original 3-hour dataset was further\naugmented to include off-orientation recovery data using a privileged controller (26)\nand a data-driven view synthesizer (27).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "148", "text": "We also trained a fully\nconvolutional neural network for the sake of proper comparison.\nOur training pipeline followed an imitation learning approach with paired pixel-\ncontrol data, from a 30Hz BlackFly PGE-23S3C RGB camera, collected by a human\nexpert driver across a variety of rural driving environments, including times of day,\nweather conditions, and season of the year. The original 3-hour dataset was further\naugmented to include off-orientation recovery data using a privileged controller (26)\nand a data-driven view synthesizer (27). The privileged controller enabled training all\nnetworks using guided policy learning (28). After training, all networks were trans-\nferred on-board our full-scale autonomous vehicle (Lexus RX450H, retroﬁtted with\ndrive-by-wire capability). The vehicle was consistently started at the center of the\nlane, initialized with each trained model, and was run to completion at the end of\nthe road.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "149", "text": "The original 3-hour dataset was further\naugmented to include off-orientation recovery data using a privileged controller (26)\nand a data-driven view synthesizer (27). The privileged controller enabled training all\nnetworks using guided policy learning (28). After training, all networks were trans-\nferred on-board our full-scale autonomous vehicle (Lexus RX450H, retroﬁtted with\ndrive-by-wire capability). The vehicle was consistently started at the center of the\nlane, initialized with each trained model, and was run to completion at the end of\nthe road. If the model exited the bounds of the lane a human safety driver intervened\nand restarted the model from the center of the road at the intervention location. All\nmodels were tested with and without noise added to the sensory inputs to evaluate\nrobustness.\nThe testing environment consisted of 1km of private test road with unlabeled lane-\nmarkers and we observed that all trained networks were able to successfully complete\nthe lane-keeping task at a constant velocity of 30 km/hr. Fig.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "150", "text": "If the model exited the bounds of the lane a human safety driver intervened\nand restarted the model from the center of the road at the intervention location. All\nmodels were tested with and without noise added to the sensory inputs to evaluate\nrobustness.\nThe testing environment consisted of 1km of private test road with unlabeled lane-\nmarkers and we observed that all trained networks were able to successfully complete\nthe lane-keeping task at a constant velocity of 30 km/hr. Fig. 5 provides an insight\ninto how these networks come with driving decisions. To this end, we computed the\n17\nTable 3: Lane-keeping models’ parameter count.\nCfC and NCP networks perform lane-\nkeeping in unseen scenarios with a compact representation.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "151", "text": "The testing environment consisted of 1km of private test road with unlabeled lane-\nmarkers and we observed that all trained networks were able to successfully complete\nthe lane-keeping task at a constant velocity of 30 km/hr. Fig. 5 provides an insight\ninto how these networks come with driving decisions. To this end, we computed the\n17\nTable 3: Lane-keeping models’ parameter count.\nCfC and NCP networks perform lane-\nkeeping in unseen scenarios with a compact representation.\nModes\nTotal Parameter Count\nRNN Parameter Count\n(CNN head + RNN)\nCNN\n2,124,901\n-\nLSTM\n259,733\n33089\nNCP\n233,139\n6495\nCf-S\n227,728\n1084\nCfC\n230,828\n4184\nCfC-NoGate\n230,828\n4184\nCfC-mmRNN\n235,052\n8408\nattention of each network while driving, by using the visual-backprop algorithm (29).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "152", "text": "Modes\nTotal Parameter Count\nRNN Parameter Count\n(CNN head + RNN)\nCNN\n2,124,901\n-\nLSTM\n259,733\n33089\nNCP\n233,139\n6495\nCf-S\n227,728\n1084\nCfC\n230,828\n4184\nCfC-NoGate\n230,828\n4184\nCfC-mmRNN\n235,052\n8408\nattention of each network while driving, by using the visual-backprop algorithm (29).\nWe observe that CfCs similar to NCPs demonstrate a consistent attention pattern in\neach subtask, while maintaining their attention proﬁle under heavy noise depicted in\nFig. 5c. Similar to NCPs, CfCs are very parameter efﬁcient. They performed the end-\nto-end autonomous lane keeping task with around 4k trainable parameters in their\nrecurrent neural network component.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "153", "text": "We observe that CfCs similar to NCPs demonstrate a consistent attention pattern in\neach subtask, while maintaining their attention proﬁle under heavy noise depicted in\nFig. 5c. Similar to NCPs, CfCs are very parameter efﬁcient. They performed the end-\nto-end autonomous lane keeping task with around 4k trainable parameters in their\nrecurrent neural network component.\nIn the following, we design sequence data processing pipelines where we exten-\nsively test CfCs’ effectiveness in learning spatiotemporal dynamics, compared to a\nlarge range of advanced recurrent models.\nBaselines. We compare CfCs to a diverse set of advanced algorithms developed for\nsequence modeling by both discretized and continuous mechanisms.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "154", "text": "5c. Similar to NCPs, CfCs are very parameter efﬁcient. They performed the end-\nto-end autonomous lane keeping task with around 4k trainable parameters in their\nrecurrent neural network component.\nIn the following, we design sequence data processing pipelines where we exten-\nsively test CfCs’ effectiveness in learning spatiotemporal dynamics, compared to a\nlarge range of advanced recurrent models.\nBaselines. We compare CfCs to a diverse set of advanced algorithms developed for\nsequence modeling by both discretized and continuous mechanisms. Examples in-\nclude some variations of classical autoregressive RNNs, such as an RNN with concate-\nnated ∆t (RNN-∆t), a recurrent model with moving average on missing values (RNN-\nimpute), RNN Decay (7), long short-term memory (LSTMs) (20), and gated recurrent\nunits (GRUs) (30).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "155", "text": "Baselines. We compare CfCs to a diverse set of advanced algorithms developed for\nsequence modeling by both discretized and continuous mechanisms. Examples in-\nclude some variations of classical autoregressive RNNs, such as an RNN with concate-\nnated ∆t (RNN-∆t), a recurrent model with moving average on missing values (RNN-\nimpute), RNN Decay (7), long short-term memory (LSTMs) (20), and gated recurrent\nunits (GRUs) (30). We also report results for a variety of encoder-decoder ODE-RNN\nbased models, such as RNN-VAE, Latent variable models with RNNs, and with ODEs,\nall from (7).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "156", "text": "We also report results for a variety of encoder-decoder ODE-RNN\nbased models, such as RNN-VAE, Latent variable models with RNNs, and with ODEs,\nall from (7).\n18\nInput                       CNN                      LSTM                      NCP                        CfC\nCfC-mmRNN\nTime\nsaliency map\n0\n1\nTest in \nSummer\nTest in \nWinter\nTime\nsaliency map\n0\n1\nsaliency map\n0\n1\nInput                       CNN                      LSTM                      NCP                        CfC\nCfC-mmRNN\nInput                       CNN                      LSTM                      NCP                        CfC\nCfC-mmRNN\nTest Under \nNoise\nTime\na\nb\nc\nFig. 5: Attention Proﬁle of networks. Trained networks receive unseen inputs (ﬁrst column\nin each tab) and generate acceleration and steering commands. We use the Visual-Backprop\nalgorithm (29) to compute the saliency maps of the convolutional part of each network.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "157", "text": "5: Attention Proﬁle of networks. Trained networks receive unseen inputs (ﬁrst column\nin each tab) and generate acceleration and steering commands. We use the Visual-Backprop\nalgorithm (29) to compute the saliency maps of the convolutional part of each network. a)\nresults for networks tested on data collected in summer. b) results for networks tested on data\ncollected in winter. c) results for inputs corrupted by a zero-mean Gaussian noise with variance,\nσ2 = 0.35.\nFurthermore, we include models such as interpolation prediction networks (IP-\nNet) (31), Set functions for time-series (SeFT) (32), CT-RNNs (33), CT-GRU (34), CT-\nLSTM (35), GRU-D (36), Phased-LSTM (37), bi-directional RNNs (38).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "158", "text": "b) results for networks tested on data\ncollected in winter. c) results for inputs corrupted by a zero-mean Gaussian noise with variance,\nσ2 = 0.35.\nFurthermore, we include models such as interpolation prediction networks (IP-\nNet) (31), Set functions for time-series (SeFT) (32), CT-RNNs (33), CT-GRU (34), CT-\nLSTM (35), GRU-D (36), Phased-LSTM (37), bi-directional RNNs (38).\nFinally, we\nbenchmarked CfCs against competitive recent RNN architectures with the premise of\ntackling long-term dependencies, such as Legandre Memory Units (LMU) (39), high-\norder polynomial projection operators (Hippo) (40), orthogonal recurrent models (ex-\npRNNs) (41), mixed memory RNNs such as (ODE-LSTMs) (9), coupled oscillatory\n19\nRNNs (coRNN) (42), and Lipschitz RNNs (43).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "159", "text": "Finally, we\nbenchmarked CfCs against competitive recent RNN architectures with the premise of\ntackling long-term dependencies, such as Legandre Memory Units (LMU) (39), high-\norder polynomial projection operators (Hippo) (40), orthogonal recurrent models (ex-\npRNNs) (41), mixed memory RNNs such as (ODE-LSTMs) (9), coupled oscillatory\n19\nRNNs (coRNN) (42), and Lipschitz RNNs (43).\nRegularly and Irregularly-Sampled Bit-Stream XOR\nThe bit-stream XOR dataset (9) considers classifying bit-streams implementing an XOR\nfunction in time, i.e., each item in the sequence contributes equally to the correct out-\nput. The bit-streams are provided in densely sampled and event-based sampled for-\nmat. The densely sampled version simply represents an incoming bit as an input event.\nThe event sampled version transmits only bit-changes to the network, i.e., multiple\nequal bit is packed into a single input event.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "160", "text": "Regularly and Irregularly-Sampled Bit-Stream XOR\nThe bit-stream XOR dataset (9) considers classifying bit-streams implementing an XOR\nfunction in time, i.e., each item in the sequence contributes equally to the correct out-\nput. The bit-streams are provided in densely sampled and event-based sampled for-\nmat. The densely sampled version simply represents an incoming bit as an input event.\nThe event sampled version transmits only bit-changes to the network, i.e., multiple\nequal bit is packed into a single input event. Consequently, the densely sampled vari-\nant is a regular sequence classiﬁcation problem, whereas the event-based encoding\nvariant represents an irregularly sampled sequence classiﬁcation problem.\nTable 4 compares the performance of many RNN baselines.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "161", "text": "The bit-streams are provided in densely sampled and event-based sampled for-\nmat. The densely sampled version simply represents an incoming bit as an input event.\nThe event sampled version transmits only bit-changes to the network, i.e., multiple\nequal bit is packed into a single input event. Consequently, the densely sampled vari-\nant is a regular sequence classiﬁcation problem, whereas the event-based encoding\nvariant represents an irregularly sampled sequence classiﬁcation problem.\nTable 4 compares the performance of many RNN baselines. Many architectures\nsuch as Augmented LSTM, CT-GRU, GRU-D, ODE-LSTM, coRNN, and Lipschitz RNN,\nand all variants of CfC can successfully solve the task with 100% accuracy when the\nbit-stream samples are equidistant from each other. However, when the bit-stream\nsamples arrive at non-uniform distances, only architectures that are immune to the\nvanishing gradient in irregularly sampled data can solve the task.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "162", "text": "Table 4 compares the performance of many RNN baselines. Many architectures\nsuch as Augmented LSTM, CT-GRU, GRU-D, ODE-LSTM, coRNN, and Lipschitz RNN,\nand all variants of CfC can successfully solve the task with 100% accuracy when the\nbit-stream samples are equidistant from each other. However, when the bit-stream\nsamples arrive at non-uniform distances, only architectures that are immune to the\nvanishing gradient in irregularly sampled data can solve the task. These include GRU-\nD, ODE-LSTM and CfCs, and CfC-mmRNNs. ODE-based RNNs cannot solve the\nevent-based encoding tasks regardless of their choice of solvers, as they have vanish-\ning/exploding gradient issues (9). The hyperparameter details of this experiment is\nprovided in Table S1.\nPhysioNet Challenge\nThe PhysioNet Challenge 2012 dataset considers the prediction of the mortality of 8000\npatients admitted to the intensive care unit (ICU).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "163", "text": "These include GRU-\nD, ODE-LSTM and CfCs, and CfC-mmRNNs. ODE-based RNNs cannot solve the\nevent-based encoding tasks regardless of their choice of solvers, as they have vanish-\ning/exploding gradient issues (9). The hyperparameter details of this experiment is\nprovided in Table S1.\nPhysioNet Challenge\nThe PhysioNet Challenge 2012 dataset considers the prediction of the mortality of 8000\npatients admitted to the intensive care unit (ICU). The features represent time series\n20\nTable 4: Bit-stream XOR sequence classiﬁcation. The performance values for all baseline\nmodels are reproduced from (9). Numbers present mean ± standard deviations, n=5\nModel\nEquidistant\nEvent-based\nTime Per epoch\nODE-based?\nencoding\n(irregular) encoding\n(min)\n†Augmented LSTM (20)\n100.00% ± 0.00\n89.71% ± 3.48\n0.62\nNo\n†CT-GRU (34)\n100.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "164", "text": "The features represent time series\n20\nTable 4: Bit-stream XOR sequence classiﬁcation. The performance values for all baseline\nmodels are reproduced from (9). Numbers present mean ± standard deviations, n=5\nModel\nEquidistant\nEvent-based\nTime Per epoch\nODE-based?\nencoding\n(irregular) encoding\n(min)\n†Augmented LSTM (20)\n100.00% ± 0.00\n89.71% ± 3.48\n0.62\nNo\n†CT-GRU (34)\n100.00% ± 0.00\n61.36% ± 4.87\n0.80\nNo\n†RNN Decay (7)\n60.28% ± 19.87\n75.53% ± 5.28\n0.90\nNo\n†Bi-directional RNN (38)\n100.00% ± 0.00\n90.17% ± 0.69\n1.82\nNo\n†GRU-D (36)\n100.00% ± 0.00\n97.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "165", "text": "62\nNo\n†CT-GRU (34)\n100.00% ± 0.00\n61.36% ± 4.87\n0.80\nNo\n†RNN Decay (7)\n60.28% ± 19.87\n75.53% ± 5.28\n0.90\nNo\n†Bi-directional RNN (38)\n100.00% ± 0.00\n90.17% ± 0.69\n1.82\nNo\n†GRU-D (36)\n100.00% ± 0.00\n97.90% ± 1.71\n0.58\nNo\n†PhasedLSTM (37)\n50.99% ± 0.76\n80.29% ± 0.99\n1.22\nNo\n†CT-LSTM (35)\n97.73% ± 0.08\n95.09% ± 0.30\n0.86\nNo\ncoRNN (42)\n100.00% ± 0.00\n52.89% ± 1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "166", "text": "82\nNo\n†GRU-D (36)\n100.00% ± 0.00\n97.90% ± 1.71\n0.58\nNo\n†PhasedLSTM (37)\n50.99% ± 0.76\n80.29% ± 0.99\n1.22\nNo\n†CT-LSTM (35)\n97.73% ± 0.08\n95.09% ± 0.30\n0.86\nNo\ncoRNN (42)\n100.00% ± 0.00\n52.89% ± 1.25\n0.57\nNo\nLipschitz RNN (43)\n100.00% ± 0.00\n52.84% ± 3.25\n0.63\nNo\n†ODE-RNN (7)\n50.47% ± 0.06\n51.21% ± 0.37\n4.11\nYes\n†CT-RNN (33)\n50.42% ± 0.12\n50.79% ± 0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "167", "text": "00% ± 0.00\n52.89% ± 1.25\n0.57\nNo\nLipschitz RNN (43)\n100.00% ± 0.00\n52.84% ± 3.25\n0.63\nNo\n†ODE-RNN (7)\n50.47% ± 0.06\n51.21% ± 0.37\n4.11\nYes\n†CT-RNN (33)\n50.42% ± 0.12\n50.79% ± 0.34\n4.83\nYes\n†GRU-ODE (7)\n50.41% ± 0.40\n52.52% ± 0.35\n1.55\nYes\n†ODE-LSTM (9)\n100.00% ± 0.00\n98.89% ± 0.26\n1.18\nYes\nLTC (1)\n100.00% ± 0.00\n49.11% ± 0.00\n2.67\nYes\nCf-S (ours)\n100.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "168", "text": "12\n50.79% ± 0.34\n4.83\nYes\n†GRU-ODE (7)\n50.41% ± 0.40\n52.52% ± 0.35\n1.55\nYes\n†ODE-LSTM (9)\n100.00% ± 0.00\n98.89% ± 0.26\n1.18\nYes\nLTC (1)\n100.00% ± 0.00\n49.11% ± 0.00\n2.67\nYes\nCf-S (ours)\n100.00% ± 0.00\n85.42% ± 2.84\n0.36\nNo\nCfC-noGate (ours)\n100.00% ± 0.00\n96.29% ± 1.61\n0.78\nNo\nCfC (ours)\n100.00% ± 0.00\n99.42% ± 0.42\n0.75\nNo\nCfC-mmRNN (ours)\n100.00% ± 0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "169", "text": "00\n2.67\nYes\nCf-S (ours)\n100.00% ± 0.00\n85.42% ± 2.84\n0.36\nNo\nCfC-noGate (ours)\n100.00% ± 0.00\n96.29% ± 1.61\n0.78\nNo\nCfC (ours)\n100.00% ± 0.00\n99.42% ± 0.42\n0.75\nNo\nCfC-mmRNN (ours)\n100.00% ± 0.00\n99.72% ± 0.08\n1.26\nNo\nNote: The performance of models marked by † are reported from (9).\nof medical measurements of the ﬁrst 48 hours after admission. The data is irregularly\nsampled in time, and over features, i.e., only a subset of the 37 possible features is given\nat each time point.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "170", "text": "00% ± 0.00\n99.42% ± 0.42\n0.75\nNo\nCfC-mmRNN (ours)\n100.00% ± 0.00\n99.72% ± 0.08\n1.26\nNo\nNote: The performance of models marked by † are reported from (9).\nof medical measurements of the ﬁrst 48 hours after admission. The data is irregularly\nsampled in time, and over features, i.e., only a subset of the 37 possible features is given\nat each time point. We perform the same test-train split and preprocessing as (7), and\nreport the area under the curve (AUC) on the test set as metric in Table 5. We observe\nthat CfCs perform competitively to other baselines while performing 160 times faster\ntraining time compared to ODE-RNNs and 220 times compared to continuous latent\nmodels. CfCs are also, on average, three times faster than advanced discretized gated\nrecurrent models.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "171", "text": "We perform the same test-train split and preprocessing as (7), and\nreport the area under the curve (AUC) on the test set as metric in Table 5. We observe\nthat CfCs perform competitively to other baselines while performing 160 times faster\ntraining time compared to ODE-RNNs and 220 times compared to continuous latent\nmodels. CfCs are also, on average, three times faster than advanced discretized gated\nrecurrent models. The hyperparameter details of this experiment is provided in Table\nS2.\n21\nTable 5: PhysioNet. The experiment is performed without any pretraining or pretrained word-\nembeddings. Thus, we excluded advanced attention-based models (44,45) such as Transform-\ners (46) and RNN structures that use pretraining. Numbers present mean ± standard devia-\ntions, n=5\nModel\nAUC Score (%)\ntime per epoch (min)\n†RNN-Impute (7)\n0.764 ± 0.016\n0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "172", "text": "The hyperparameter details of this experiment is provided in Table\nS2.\n21\nTable 5: PhysioNet. The experiment is performed without any pretraining or pretrained word-\nembeddings. Thus, we excluded advanced attention-based models (44,45) such as Transform-\ners (46) and RNN structures that use pretraining. Numbers present mean ± standard devia-\ntions, n=5\nModel\nAUC Score (%)\ntime per epoch (min)\n†RNN-Impute (7)\n0.764 ± 0.016\n0.5\n†RNN-delta-t (7)\n0.787 ± 0.014\n0.5\n†RNN-Decay (7)\n0.807 ± 0.003\n0.7\n†GRU-D (36)\n0.818 ± 0.008\n0.7\n†Phased-LSTM (37)\n0.836 ± 0.003\n0.3\n∗IP-Nets (31)\n0.819 ± 0.006\n1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "173", "text": "764 ± 0.016\n0.5\n†RNN-delta-t (7)\n0.787 ± 0.014\n0.5\n†RNN-Decay (7)\n0.807 ± 0.003\n0.7\n†GRU-D (36)\n0.818 ± 0.008\n0.7\n†Phased-LSTM (37)\n0.836 ± 0.003\n0.3\n∗IP-Nets (31)\n0.819 ± 0.006\n1.3\n∗SeFT (32)\n0.795 ± 0.015\n0.5\n†RNN-VAE (7)\n0.515 ± 0.040\n2.0\n†ODE-RNN (7)\n0.833 ± 0.009\n16.5\n†Latent-ODE-RNN (7)\n0.781 ± 0.018\n6.7\n†Latent-ODE-ODE (7)\n0.829 ± 0.004\n22.0\nLTC (1)\n0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "174", "text": "819 ± 0.006\n1.3\n∗SeFT (32)\n0.795 ± 0.015\n0.5\n†RNN-VAE (7)\n0.515 ± 0.040\n2.0\n†ODE-RNN (7)\n0.833 ± 0.009\n16.5\n†Latent-ODE-RNN (7)\n0.781 ± 0.018\n6.7\n†Latent-ODE-ODE (7)\n0.829 ± 0.004\n22.0\nLTC (1)\n0.6477 ± 0.010\n0.5\nCf-S (ours)\n0.643 ± 0.018\n0.1\nCfC-noGate (ours)\n0.840 ± 0.003\n0.1\nCfC (ours)\n0.839 ± 0.002\n0.1\nCfC-mmRNN (ours)\n0.834 +- 0.006\n0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "175", "text": "018\n6.7\n†Latent-ODE-ODE (7)\n0.829 ± 0.004\n22.0\nLTC (1)\n0.6477 ± 0.010\n0.5\nCf-S (ours)\n0.643 ± 0.018\n0.1\nCfC-noGate (ours)\n0.840 ± 0.003\n0.1\nCfC (ours)\n0.839 ± 0.002\n0.1\nCfC-mmRNN (ours)\n0.834 +- 0.006\n0.2\nNote: The performance of the models marked by † are reported from (7) and the ones with ∗from (44).\nSentiment Analysis - IMDB\nThe IMDB sentiment analysis dataset (47) consists of 25,000 training and 25,000 test\nsentences. Each sentence corresponds to either positive or negative sentiment.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "176", "text": "840 ± 0.003\n0.1\nCfC (ours)\n0.839 ± 0.002\n0.1\nCfC-mmRNN (ours)\n0.834 +- 0.006\n0.2\nNote: The performance of the models marked by † are reported from (7) and the ones with ∗from (44).\nSentiment Analysis - IMDB\nThe IMDB sentiment analysis dataset (47) consists of 25,000 training and 25,000 test\nsentences. Each sentence corresponds to either positive or negative sentiment. We tok-\nenize the sentences in a word-by-word fashion with a vocabulary consisting of 20,000\nmost frequently occurring words in the dataset. We map each token to a vector using a\ntrainable word embedding. The word embedding is initialized randomly. No pretrain-\ning of the network or the word embedding is performed. Table 6 represents how CfCs\nequipped with mixed memory instances outperform advanced RNN benchmarks.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "177", "text": "Each sentence corresponds to either positive or negative sentiment. We tok-\nenize the sentences in a word-by-word fashion with a vocabulary consisting of 20,000\nmost frequently occurring words in the dataset. We map each token to a vector using a\ntrainable word embedding. The word embedding is initialized randomly. No pretrain-\ning of the network or the word embedding is performed. Table 6 represents how CfCs\nequipped with mixed memory instances outperform advanced RNN benchmarks. The\nhyperparameter details of this experiment is provided in Table S3.\n22\nTable 6: Results on the IMDB datasets. The experiment is performed without any pretraining\nor pretrained word-embeddings. Thus, we excluded advanced attention-based models (44,45)\nsuch as Transformers (46) and RNN structures that use pretraining. Numbers present mean ±\nstandard deviations, n=5\nModel\nTest accuracy (%)\n†HiPPO-LagT (40)\n88.0 ± 0.2\n†HiPPO-LegS (40)\n88.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "178", "text": "The\nhyperparameter details of this experiment is provided in Table S3.\n22\nTable 6: Results on the IMDB datasets. The experiment is performed without any pretraining\nor pretrained word-embeddings. Thus, we excluded advanced attention-based models (44,45)\nsuch as Transformers (46) and RNN structures that use pretraining. Numbers present mean ±\nstandard deviations, n=5\nModel\nTest accuracy (%)\n†HiPPO-LagT (40)\n88.0 ± 0.2\n†HiPPO-LegS (40)\n88.0 ± 0.2\n†LMU (39)\n87.7 ± 0.1\n†LSTM (20)\n87.3 ± 0.4\n†GRU (30)\n86.2 ± n/a\n∗ReLU GRU (48)\n84.8 ± n/a\n∗Skip LSTM (49)\n86.6 ± n/a\n†expRNN (41)\n84.3 ± 0.3\n†Vanilla RNN (49)\n67.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "179", "text": "0 ± 0.2\n†HiPPO-LegS (40)\n88.0 ± 0.2\n†LMU (39)\n87.7 ± 0.1\n†LSTM (20)\n87.3 ± 0.4\n†GRU (30)\n86.2 ± n/a\n∗ReLU GRU (48)\n84.8 ± n/a\n∗Skip LSTM (49)\n86.6 ± n/a\n†expRNN (41)\n84.3 ± 0.3\n†Vanilla RNN (49)\n67.4 ± 7.7\n∗coRNN (42)\n86.7 ± 0.3\nLTC (1)\n61.8 ± 6.1\nCf-S (ours)\n81.7 ± 0.5\nCfC-noGate (ours)\n87.5 ± 0.1\nCfC (ours)\n85.9 ± 0.9\nCfC-mmRNN (ours)\n88.3 ± 0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "180", "text": "3 ± 0.3\n†Vanilla RNN (49)\n67.4 ± 7.7\n∗coRNN (42)\n86.7 ± 0.3\nLTC (1)\n61.8 ± 6.1\nCf-S (ours)\n81.7 ± 0.5\nCfC-noGate (ours)\n87.5 ± 0.1\nCfC (ours)\n85.9 ± 0.9\nCfC-mmRNN (ours)\n88.3 ± 0.1\nNote: The performance of the models marked by † are reported from (40), and ∗are reported\nfrom (42). The n/a standard deviation denotes that the original report of these experiments did\nnot provide the statistics of their analysis.\nPhysical Dynamics Modeling\nThe Walker2D dataset consists of kinematic simulations of the MuJoCo physics en-\ngine (50) on the Walker2d-v2 OpenAI gym (51) environment using four different\nstochastic policies.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "181", "text": "9 ± 0.9\nCfC-mmRNN (ours)\n88.3 ± 0.1\nNote: The performance of the models marked by † are reported from (40), and ∗are reported\nfrom (42). The n/a standard deviation denotes that the original report of these experiments did\nnot provide the statistics of their analysis.\nPhysical Dynamics Modeling\nThe Walker2D dataset consists of kinematic simulations of the MuJoCo physics en-\ngine (50) on the Walker2d-v2 OpenAI gym (51) environment using four different\nstochastic policies. The objective is to predict the physics state of the next time step.\nThe training and testing sequences are provided at irregularly-sampled intervals. We\nreport the squared error on the test set as a metric. As shown in Table 7, CfCs outper-\nform the other baselines by a large margin rooting for their strong capability to model\nirregularly sampled physical dynamics with missing phases. It is worth mentioning\n23\nTable 7: Per time-step regression.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "182", "text": "The objective is to predict the physics state of the next time step.\nThe training and testing sequences are provided at irregularly-sampled intervals. We\nreport the squared error on the test set as a metric. As shown in Table 7, CfCs outper-\nform the other baselines by a large margin rooting for their strong capability to model\nirregularly sampled physical dynamics with missing phases. It is worth mentioning\n23\nTable 7: Per time-step regression. Modeling the physical dynamics of a Walker agent in simu-\nlation. Numbers present mean ± standard deviations. n = 5\nModel\nSquare-error\nTime per epoch (min)\n†ODE-RNN (7)\n1.904 ± 0.061\n0.79\n†CT-RNN (33)\n1.198 ± 0.004\n0.91\n†Augmented LSTM (20)\n1.065 ± 0.006\n0.10\n†CT-GRU (34)\n1.172 ± 0.011\n0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "183", "text": "Modeling the physical dynamics of a Walker agent in simu-\nlation. Numbers present mean ± standard deviations. n = 5\nModel\nSquare-error\nTime per epoch (min)\n†ODE-RNN (7)\n1.904 ± 0.061\n0.79\n†CT-RNN (33)\n1.198 ± 0.004\n0.91\n†Augmented LSTM (20)\n1.065 ± 0.006\n0.10\n†CT-GRU (34)\n1.172 ± 0.011\n0.18\n†RNN-Decay (7)\n1.406 ± 0.005\n0.16\n†Bi-directional RNN (38)\n1.071 ± 0.009\n0.39\n†GRU-D (36)\n1.090 ± 0.034\n0.11\n†PhasedLSTM (37)\n1.063 ± 0.010\n0.25\n†GRU-ODE (7)\n1.051 ± 0.018\n0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "184", "text": "10\n†CT-GRU (34)\n1.172 ± 0.011\n0.18\n†RNN-Decay (7)\n1.406 ± 0.005\n0.16\n†Bi-directional RNN (38)\n1.071 ± 0.009\n0.39\n†GRU-D (36)\n1.090 ± 0.034\n0.11\n†PhasedLSTM (37)\n1.063 ± 0.010\n0.25\n†GRU-ODE (7)\n1.051 ± 0.018\n0.56\n†CT-LSTM (35)\n1.014 ± 0.014\n0.31\n†ODE-LSTM (9)\n0.883 ± 0.014\n0.29\ncoRNN (42)\n3.241 ± 0.215\n0.18\nLipschitz RNN (43)\n1.781 ± 0.013\n0.17\nLTC (1)\n0.662 ± 0.013\n0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "185", "text": "010\n0.25\n†GRU-ODE (7)\n1.051 ± 0.018\n0.56\n†CT-LSTM (35)\n1.014 ± 0.014\n0.31\n†ODE-LSTM (9)\n0.883 ± 0.014\n0.29\ncoRNN (42)\n3.241 ± 0.215\n0.18\nLipschitz RNN (43)\n1.781 ± 0.013\n0.17\nLTC (1)\n0.662 ± 0.013\n0.78\nTransformer (46)\n0.761 ± 0.032\n0.8\nCf-S (ours)\n0.948 ± 0.009\n0.12\nCfC-noGate (ours)\n0.650 ± 0.008\n0.21\nCfC (ours)\n0.643 ± 0.006\n0.08\nCfC-mmRNN (ours)\n0.617 ± 0.006\n0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "186", "text": "781 ± 0.013\n0.17\nLTC (1)\n0.662 ± 0.013\n0.78\nTransformer (46)\n0.761 ± 0.032\n0.8\nCf-S (ours)\n0.948 ± 0.009\n0.12\nCfC-noGate (ours)\n0.650 ± 0.008\n0.21\nCfC (ours)\n0.643 ± 0.006\n0.08\nCfC-mmRNN (ours)\n0.617 ± 0.006\n0.34\nNote: The performance of the models marked by † are reported from (9).\nthat on this task, CfCs even outperform Transformers by a considerable 18% margin.\nThe hyperparameter details of this experiment is provided in Table S4.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "187", "text": "948 ± 0.009\n0.12\nCfC-noGate (ours)\n0.650 ± 0.008\n0.21\nCfC (ours)\n0.643 ± 0.006\n0.08\nCfC-mmRNN (ours)\n0.617 ± 0.006\n0.34\nNote: The performance of the models marked by † are reported from (9).\nthat on this task, CfCs even outperform Transformers by a considerable 18% margin.\nThe hyperparameter details of this experiment is provided in Table S4.\nScope, Discussions and Conclusions\nWe introduced a closed-form continuous-time neural model build from an approxi-\nmate close-form solution of liquid time-constant networks that possesses the strong\nmodeling capabilities of ODE-based networks while being signiﬁcantly faster, more ac-\ncurate, and stable. These closed-form continuous-depth models achieve this by explicit\ntime-dependent gating mechanisms and having a liquid time-constant modulated by\n24\nneural networks.\nContinuous-Depth Models.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "188", "text": "The hyperparameter details of this experiment is provided in Table S4.\nScope, Discussions and Conclusions\nWe introduced a closed-form continuous-time neural model build from an approxi-\nmate close-form solution of liquid time-constant networks that possesses the strong\nmodeling capabilities of ODE-based networks while being signiﬁcantly faster, more ac-\ncurate, and stable. These closed-form continuous-depth models achieve this by explicit\ntime-dependent gating mechanisms and having a liquid time-constant modulated by\n24\nneural networks.\nContinuous-Depth Models. Machine learning, control theory and dynamical systems\nmerge at models with continuous-time dynamics (52–56). In a seminal work, Chen\net. al. 2018 (2) revived the class of continuous-time neural networks (33, 57), with\nneural ODEs. These continuous-depth models give rise to vector ﬁeld representations\nand a set of functions which were not possible to generate before with discrete neural\nnetworks.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "189", "text": "Continuous-Depth Models. Machine learning, control theory and dynamical systems\nmerge at models with continuous-time dynamics (52–56). In a seminal work, Chen\net. al. 2018 (2) revived the class of continuous-time neural networks (33, 57), with\nneural ODEs. These continuous-depth models give rise to vector ﬁeld representations\nand a set of functions which were not possible to generate before with discrete neural\nnetworks. These capabilities enabled ﬂexible density estimation (3–5, 58, 59), as well\nas performant modeling of sequential and irregularly-sampled data (1,7–9,43). In this\npaper, we showed how to relax the need for an ODE-solver to realize an expressive\ncontinuous-time neural network model for challenging time-series problems.\nImproving Neural ODEs. ODE-based neural networks are as good as their ODE-\nsolvers.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "190", "text": "These capabilities enabled ﬂexible density estimation (3–5, 58, 59), as well\nas performant modeling of sequential and irregularly-sampled data (1,7–9,43). In this\npaper, we showed how to relax the need for an ODE-solver to realize an expressive\ncontinuous-time neural network model for challenging time-series problems.\nImproving Neural ODEs. ODE-based neural networks are as good as their ODE-\nsolvers. As the complexity or the dimensionality of the modeling task increases, ODE-\nbased networks demand a more advanced solver that signiﬁcantly impacts their efﬁ-\nciency (17), stability (13,15,60–62) and performance (1). A large body of research went\ninto improving the computational overhead of these solvers, for example, by designing\nhypersolvers (17), deploying augmentation methods (4, 12), pruning (6) and by regu-\nlarizing the continuous ﬂows (14–16).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "191", "text": "As the complexity or the dimensionality of the modeling task increases, ODE-\nbased networks demand a more advanced solver that signiﬁcantly impacts their efﬁ-\nciency (17), stability (13,15,60–62) and performance (1). A large body of research went\ninto improving the computational overhead of these solvers, for example, by designing\nhypersolvers (17), deploying augmentation methods (4, 12), pruning (6) and by regu-\nlarizing the continuous ﬂows (14–16). To enhance the performance of an ODE-based\nmodel, especially in time series modeling tasks (63), solutions provided for stabilizing\ntheir gradient propagation (9, 43, 64). In this work, we showed that CfCs improve the\nscalability, efﬁciency, and performance of continuous-depth neural models.\nNow that we have a closed-form system, where does it make sense to use ODE-based\nnetworks?", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "192", "text": "To enhance the performance of an ODE-based\nmodel, especially in time series modeling tasks (63), solutions provided for stabilizing\ntheir gradient propagation (9, 43, 64). In this work, we showed that CfCs improve the\nscalability, efﬁciency, and performance of continuous-depth neural models.\nNow that we have a closed-form system, where does it make sense to use ODE-based\nnetworks? For large-scale time-series prediction tasks, and where closed-loop perfor-\nmance matters (24) CfCs should be the method of choice.This is because, they capture\nthe ﬂexible, continuous-time nature of ODE-based networks while presenting large\ngains in performance and scalability. On the other hand, implicit ODE-based mod-\n25\nels can still be signiﬁcantly beneﬁcial in solving continuously deﬁned physics prob-\nlems and control tasks.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "193", "text": "For large-scale time-series prediction tasks, and where closed-loop perfor-\nmance matters (24) CfCs should be the method of choice.This is because, they capture\nthe ﬂexible, continuous-time nature of ODE-based networks while presenting large\ngains in performance and scalability. On the other hand, implicit ODE-based mod-\n25\nels can still be signiﬁcantly beneﬁcial in solving continuously deﬁned physics prob-\nlems and control tasks. Moreover, for generative modeling, continuous normalizing\nﬂows built by ODEs are the suitable choice of model as they ensure invertibility un-\nlike CfCs (2). This is because differential equations guarantee invertibility (i.e., under\nuniqueness conditions (6), one can run them backwards in time). CfCs only approxi-\nmate ODEs and therefore they no longer necessarily form a bijection (65).\nWhat are the limitations of CfCs? CfCs might express vanishing gradient problems.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "194", "text": "Moreover, for generative modeling, continuous normalizing\nﬂows built by ODEs are the suitable choice of model as they ensure invertibility un-\nlike CfCs (2). This is because differential equations guarantee invertibility (i.e., under\nuniqueness conditions (6), one can run them backwards in time). CfCs only approxi-\nmate ODEs and therefore they no longer necessarily form a bijection (65).\nWhat are the limitations of CfCs? CfCs might express vanishing gradient problems.\nTo avoid this, for tasks that require long-term dependencies, it is better to use them\ntogether with mixed memory networks (9) (See CfC-mmRNN). Moreover, we specu-\nlate that inferring causality from ODE-based networks might be more straightforward\nthan a closed-form solution (24). It would also be beneﬁcial to assess if verifying a\ncontinuous neural ﬂow (66) is more tractable by an ODE representation of the system\nor their closed form.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "195", "text": "What are the limitations of CfCs? CfCs might express vanishing gradient problems.\nTo avoid this, for tasks that require long-term dependencies, it is better to use them\ntogether with mixed memory networks (9) (See CfC-mmRNN). Moreover, we specu-\nlate that inferring causality from ODE-based networks might be more straightforward\nthan a closed-form solution (24). It would also be beneﬁcial to assess if verifying a\ncontinuous neural ﬂow (66) is more tractable by an ODE representation of the system\nor their closed form.\nIn what application scenarios shall we use CfCs? For problems such as language\nmodeling where a signiﬁcant amount of sequential data and substantial compute re-\nsources are available, Transformers (46) are the right choice.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "196", "text": "Moreover, we specu-\nlate that inferring causality from ODE-based networks might be more straightforward\nthan a closed-form solution (24). It would also be beneﬁcial to assess if verifying a\ncontinuous neural ﬂow (66) is more tractable by an ODE representation of the system\nor their closed form.\nIn what application scenarios shall we use CfCs? For problems such as language\nmodeling where a signiﬁcant amount of sequential data and substantial compute re-\nsources are available, Transformers (46) are the right choice. In contrast, we use CfCs\nwhen: 1) data has limitations and irregularities (e.g., medical data, ﬁnancial time-\nseries, robotics (67) and closed loop control and robotics, and multi-agent autonomous\nsystems in supervised and reinforcement learning schemes (68)), 2) training and infer-\nence efﬁciency of a model is important (e.g., embedded applications (69–71)), and 3)\nwhen interpretability matters (72).\nReferences\n1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "197", "text": "In contrast, we use CfCs\nwhen: 1) data has limitations and irregularities (e.g., medical data, ﬁnancial time-\nseries, robotics (67) and closed loop control and robotics, and multi-agent autonomous\nsystems in supervised and reinforcement learning schemes (68)), 2) training and infer-\nence efﬁciency of a model is important (e.g., embedded applications (69–71)), and 3)\nwhen interpretability matters (72).\nReferences\n1. Hasani, R., Lechner, M., Amini, A., Rus, D. & Grosu, R.\nLiquid time-constant\nnetworks. Proceedings of the AAAI Conference on Artiﬁcial Intelligence 35, 7657–7666\n26\n(2021).\n2. Chen, T. Q., Rubanova, Y., Bettencourt, J. & Duvenaud, D. K. Neural ordinary\ndifferential equations. In Advances in neural information processing systems, 6571–\n6583 (2018).\n3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "198", "text": "Hasani, R., Lechner, M., Amini, A., Rus, D. & Grosu, R.\nLiquid time-constant\nnetworks. Proceedings of the AAAI Conference on Artiﬁcial Intelligence 35, 7657–7666\n26\n(2021).\n2. Chen, T. Q., Rubanova, Y., Bettencourt, J. & Duvenaud, D. K. Neural ordinary\ndifferential equations. In Advances in neural information processing systems, 6571–\n6583 (2018).\n3. Grathwohl, W., Chen, R. T., Bettencourt, J., Sutskever, I. & Duvenaud, D. Ffjord:\nFree-form continuous dynamics for scalable reversible generative models. arXiv\npreprint arXiv:1810.01367 (2018).\n4. Dupont, E., Doucet, A. & Teh, Y. W. Augmented neural odes.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "199", "text": "In Advances in neural information processing systems, 6571–\n6583 (2018).\n3. Grathwohl, W., Chen, R. T., Bettencourt, J., Sutskever, I. & Duvenaud, D. Ffjord:\nFree-form continuous dynamics for scalable reversible generative models. arXiv\npreprint arXiv:1810.01367 (2018).\n4. Dupont, E., Doucet, A. & Teh, Y. W. Augmented neural odes. In Advances in Neural\nInformation Processing Systems, 3134–3144 (2019).\n5. Yang, G. et al. Pointﬂow: 3d point cloud generation with continuous normalizing\nﬂows. In Proceedings of the IEEE/CVF International Conference on Computer Vision,\n4541–4550 (2019).\n6. Liebenwein, L., Hasani, R., Amini, A.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "200", "text": "4. Dupont, E., Doucet, A. & Teh, Y. W. Augmented neural odes. In Advances in Neural\nInformation Processing Systems, 3134–3144 (2019).\n5. Yang, G. et al. Pointﬂow: 3d point cloud generation with continuous normalizing\nﬂows. In Proceedings of the IEEE/CVF International Conference on Computer Vision,\n4541–4550 (2019).\n6. Liebenwein, L., Hasani, R., Amini, A. & Daniela, R.\nSparse ﬂows: Pruning\ncontinuous-depth models. arXiv preprint arXiv:2106.12718 (2021).\n7. Rubanova, Y., Chen, R. T. & Duvenaud, D. Latent odes for irregularly-sampled\ntime series. arXiv preprint arXiv:1907.03907 (2019).\n8.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "201", "text": "6. Liebenwein, L., Hasani, R., Amini, A. & Daniela, R.\nSparse ﬂows: Pruning\ncontinuous-depth models. arXiv preprint arXiv:2106.12718 (2021).\n7. Rubanova, Y., Chen, R. T. & Duvenaud, D. Latent odes for irregularly-sampled\ntime series. arXiv preprint arXiv:1907.03907 (2019).\n8. Gholami, A., Keutzer, K. & Biros, G. Anode: Unconditionally accurate memory-\nefﬁcient gradients for neural odes. arXiv preprint arXiv:1902.10298 (2019).\n9. Lechner, M. & Hasani, R. Learning long-term dependencies in irregularly-sampled\ntime series. arXiv preprint arXiv:2006.04418 (2020).\n10. Prince, P. J.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "202", "text": "8. Gholami, A., Keutzer, K. & Biros, G. Anode: Unconditionally accurate memory-\nefﬁcient gradients for neural odes. arXiv preprint arXiv:1902.10298 (2019).\n9. Lechner, M. & Hasani, R. Learning long-term dependencies in irregularly-sampled\ntime series. arXiv preprint arXiv:2006.04418 (2020).\n10. Prince, P. J. & Dormand, J. R. High order embedded runge-kutta formulae. Journal\nof computational and applied mathematics 7, 67–75 (1981).\n27\n11. Raissi, M., Perdikaris, P. & Karniadakis, G. E. Physics-informed neural networks:\nA deep learning framework for solving forward and inverse problems involving\nnonlinear partial differential equations. Journal of Computational Physics 378, 686–\n707 (2019).\n12.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "203", "text": "10. Prince, P. J. & Dormand, J. R. High order embedded runge-kutta formulae. Journal\nof computational and applied mathematics 7, 67–75 (1981).\n27\n11. Raissi, M., Perdikaris, P. & Karniadakis, G. E. Physics-informed neural networks:\nA deep learning framework for solving forward and inverse problems involving\nnonlinear partial differential equations. Journal of Computational Physics 378, 686–\n707 (2019).\n12. Massaroli, S., Poli, M., Park, J., Yamashita, A. & Asma, H. Dissecting neural odes.\nIn 34th Conference on Neural Information Processing Systems, NeurIPS 2020 (The Neu-\nral Information Processing Systems, 2020).\n13. Bai, S., Kolter, J. Z. & Koltun, V. Deep equilibrium models. Advances in Neural\nInformation Processing Systems 32, 690–701 (2019).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "204", "text": "12. Massaroli, S., Poli, M., Park, J., Yamashita, A. & Asma, H. Dissecting neural odes.\nIn 34th Conference on Neural Information Processing Systems, NeurIPS 2020 (The Neu-\nral Information Processing Systems, 2020).\n13. Bai, S., Kolter, J. Z. & Koltun, V. Deep equilibrium models. Advances in Neural\nInformation Processing Systems 32, 690–701 (2019).\n14. Finlay, C., Jacobsen, J.-H., Nurbekyan, L. & Oberman, A. M. How to train your\nneural ode. arXiv preprint arXiv:2002.02798 (2020).\n15. Massaroli, S. et al. Stable neural ﬂows. arXiv preprint arXiv:2003.08063 (2020).\n16. Kidger, P., Chen, R. T.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "205", "text": "14. Finlay, C., Jacobsen, J.-H., Nurbekyan, L. & Oberman, A. M. How to train your\nneural ode. arXiv preprint arXiv:2002.02798 (2020).\n15. Massaroli, S. et al. Stable neural ﬂows. arXiv preprint arXiv:2003.08063 (2020).\n16. Kidger, P., Chen, R. T. & Lyons, T. ” hey, that’s not an ode”: Faster ode adjoints\nwith 12 lines of code. arXiv preprint arXiv:2009.09457 (2020).\n17. Poli, M. et al. Hypersolvers: Toward fast continuous-depth models. Advances in\nNeural Information Processing Systems 33 (2020).\n18. Friston, K. J., Harrison, L. & Penny, W. Dynamic causal modelling.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "206", "text": "16. Kidger, P., Chen, R. T. & Lyons, T. ” hey, that’s not an ode”: Faster ode adjoints\nwith 12 lines of code. arXiv preprint arXiv:2009.09457 (2020).\n17. Poli, M. et al. Hypersolvers: Toward fast continuous-depth models. Advances in\nNeural Information Processing Systems 33 (2020).\n18. Friston, K. J., Harrison, L. & Penny, W. Dynamic causal modelling. Neuroimage 19,\n1273–1302 (2003).\n19. Perko, L. Differential Equations and Dynamical Systems (Springer-Verlag, Berlin, Hei-\ndelberg, 1991).\n20. Hochreiter, S. & Schmidhuber, J. Long short-term memory. Neural computation 9,\n1735–1780 (1997).\n28\n21.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "207", "text": "18. Friston, K. J., Harrison, L. & Penny, W. Dynamic causal modelling. Neuroimage 19,\n1273–1302 (2003).\n19. Perko, L. Differential Equations and Dynamical Systems (Springer-Verlag, Berlin, Hei-\ndelberg, 1991).\n20. Hochreiter, S. & Schmidhuber, J. Long short-term memory. Neural computation 9,\n1735–1780 (1997).\n28\n21. Rudin, W. Principles of mathematical analysis (McGraw-Hill New York, 1976), 3d ed.\nedn.\n22. Lechner, M. et al.\nNeural circuit policies enabling auditable autonomy.\nNature\nMachine Intelligence 2, 642–652 (2020).\n23. Hochreiter, S. Untersuchungen zu dynamischen neuronalen netzen. Diploma, Tech-\nnische Universit¨at M¨unchen 91 (1991).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "208", "text": "28\n21. Rudin, W. Principles of mathematical analysis (McGraw-Hill New York, 1976), 3d ed.\nedn.\n22. Lechner, M. et al.\nNeural circuit policies enabling auditable autonomy.\nNature\nMachine Intelligence 2, 642–652 (2020).\n23. Hochreiter, S. Untersuchungen zu dynamischen neuronalen netzen. Diploma, Tech-\nnische Universit¨at M¨unchen 91 (1991).\n24. Vorbach, C., Hasani, R., Amini, A., Lechner, M. & Rus, D. Causal navigation by\ncontinuous-time neural networks. arXiv preprint arXiv:2106.08314 (2021).\n25. Hasani, R. et al. Response characterization for auditing cell dynamics in long short-\nterm memory networks. In 2019 International Joint Conference on Neural Networks\n(IJCNN), 1–8 (IEEE, 2019).\n26.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "209", "text": "24. Vorbach, C., Hasani, R., Amini, A., Lechner, M. & Rus, D. Causal navigation by\ncontinuous-time neural networks. arXiv preprint arXiv:2106.08314 (2021).\n25. Hasani, R. et al. Response characterization for auditing cell dynamics in long short-\nterm memory networks. In 2019 International Joint Conference on Neural Networks\n(IJCNN), 1–8 (IEEE, 2019).\n26. Amini, A. et al. Vista 2.0: An open, data-driven simulator for multimodal sens-\ning and policy learning for autonomous vehicles. arXiv preprint arXiv:2111.12083\n(2021).\n27. Amini, A. et al. Learning robust control policies for end-to-end autonomous driv-\ning from data-driven simulation. IEEE Robotics and Automation Letters 5, 1143–1150\n(2020).\n28. Levine, S.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "210", "text": "26. Amini, A. et al. Vista 2.0: An open, data-driven simulator for multimodal sens-\ning and policy learning for autonomous vehicles. arXiv preprint arXiv:2111.12083\n(2021).\n27. Amini, A. et al. Learning robust control policies for end-to-end autonomous driv-\ning from data-driven simulation. IEEE Robotics and Automation Letters 5, 1143–1150\n(2020).\n28. Levine, S. & Koltun, V. Guided policy search. In International conference on machine\nlearning, 1–9 (PMLR, 2013).\n29. Bojarski, M. et al. Visualbackprop: Efﬁcient visualization of cnns for autonomous\ndriving. In IEEE International Conference on Robotics and Automation (ICRA), 1–8\n(2018).\n29\n30. Chung, J., Gulcehre, C., Cho, K.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "211", "text": "28. Levine, S. & Koltun, V. Guided policy search. In International conference on machine\nlearning, 1–9 (PMLR, 2013).\n29. Bojarski, M. et al. Visualbackprop: Efﬁcient visualization of cnns for autonomous\ndriving. In IEEE International Conference on Robotics and Automation (ICRA), 1–8\n(2018).\n29\n30. Chung, J., Gulcehre, C., Cho, K. & Bengio, Y. Empirical evaluation of gated recur-\nrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555 (2014).\n31. Shukla, S. N. & Marlin, B. Interpolation-prediction networks for irregularly sam-\npled time series. In International Conference on Learning Representations (2018).\n32. Horn, M., Moor, M., Bock, C., Rieck, B.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "212", "text": "Chung, J., Gulcehre, C., Cho, K. & Bengio, Y. Empirical evaluation of gated recur-\nrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555 (2014).\n31. Shukla, S. N. & Marlin, B. Interpolation-prediction networks for irregularly sam-\npled time series. In International Conference on Learning Representations (2018).\n32. Horn, M., Moor, M., Bock, C., Rieck, B. & Borgwardt, K. Set functions for time\nseries. In International Conference on Machine Learning, 4353–4363 (PMLR, 2020).\n33. Funahashi, K.-i. & Nakamura, Y. Approximation of dynamical systems by contin-\nuous time recurrent neural networks. Neural networks 6, 801–806 (1993).\n34. Mozer, M. C., Kazakov, D.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "213", "text": "32. Horn, M., Moor, M., Bock, C., Rieck, B. & Borgwardt, K. Set functions for time\nseries. In International Conference on Machine Learning, 4353–4363 (PMLR, 2020).\n33. Funahashi, K.-i. & Nakamura, Y. Approximation of dynamical systems by contin-\nuous time recurrent neural networks. Neural networks 6, 801–806 (1993).\n34. Mozer, M. C., Kazakov, D. & Lindsey, R. V. Discrete event, continuous time rnns.\narXiv preprint arXiv:1710.04110 (2017).\n35. Mei, H. & Eisner, J. The neural hawkes process: a neurally self-modulating mul-\ntivariate point process. In Proceedings of the 31st International Conference on Neural\nInformation Processing Systems, 6757–6767 (2017).\n36.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "214", "text": "34. Mozer, M. C., Kazakov, D. & Lindsey, R. V. Discrete event, continuous time rnns.\narXiv preprint arXiv:1710.04110 (2017).\n35. Mei, H. & Eisner, J. The neural hawkes process: a neurally self-modulating mul-\ntivariate point process. In Proceedings of the 31st International Conference on Neural\nInformation Processing Systems, 6757–6767 (2017).\n36. Che, Z., Purushotham, S., Cho, K., Sontag, D. & Liu, Y. Recurrent neural networks\nfor multivariate time series with missing values. Scientiﬁc reports 8, 1–12 (2018).\n37. Neil, D., Pfeiffer, M. & Liu, S.-C.\nPhased lstm: accelerating recurrent network\ntraining for long or event-based sequences.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "215", "text": "In Proceedings of the 31st International Conference on Neural\nInformation Processing Systems, 6757–6767 (2017).\n36. Che, Z., Purushotham, S., Cho, K., Sontag, D. & Liu, Y. Recurrent neural networks\nfor multivariate time series with missing values. Scientiﬁc reports 8, 1–12 (2018).\n37. Neil, D., Pfeiffer, M. & Liu, S.-C.\nPhased lstm: accelerating recurrent network\ntraining for long or event-based sequences. In Proceedings of the 30th International\nConference on Neural Information Processing Systems, 3889–3897 (2016).\n38. Schuster, M. & Paliwal, K. K. Bidirectional recurrent neural networks. IEEE trans-\nactions on Signal Processing 45, 2673–2681 (1997).\n30\n39. Voelker, A. R., Kaji´c, I.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "216", "text": "Neil, D., Pfeiffer, M. & Liu, S.-C.\nPhased lstm: accelerating recurrent network\ntraining for long or event-based sequences. In Proceedings of the 30th International\nConference on Neural Information Processing Systems, 3889–3897 (2016).\n38. Schuster, M. & Paliwal, K. K. Bidirectional recurrent neural networks. IEEE trans-\nactions on Signal Processing 45, 2673–2681 (1997).\n30\n39. Voelker, A. R., Kaji´c, I. & Eliasmith, C.\nLegendre memory units: Continuous-\ntime representation in recurrent neural networks. NeurIPS Reproducability Challenge\n(2019).\n40. Gu, A., Dao, T., Ermon, S., Rudra, A. & R´e, C. Hippo: Recurrent memory with\noptimal polynomial projections. arXiv preprint arXiv:2008.07669 (2020).\n41.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "217", "text": "30\n39. Voelker, A. R., Kaji´c, I. & Eliasmith, C.\nLegendre memory units: Continuous-\ntime representation in recurrent neural networks. NeurIPS Reproducability Challenge\n(2019).\n40. Gu, A., Dao, T., Ermon, S., Rudra, A. & R´e, C. Hippo: Recurrent memory with\noptimal polynomial projections. arXiv preprint arXiv:2008.07669 (2020).\n41. Lezcano-Casado, M. & Martınez-Rubio, D. Cheap orthogonal constraints in neu-\nral networks: A simple parametrization of the orthogonal and unitary group. In\nInternational Conference on Machine Learning, 3794–3803 (PMLR, 2019).\n42. Rusch, T. K.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "218", "text": "& R´e, C. Hippo: Recurrent memory with\noptimal polynomial projections. arXiv preprint arXiv:2008.07669 (2020).\n41. Lezcano-Casado, M. & Martınez-Rubio, D. Cheap orthogonal constraints in neu-\nral networks: A simple parametrization of the orthogonal and unitary group. In\nInternational Conference on Machine Learning, 3794–3803 (PMLR, 2019).\n42. Rusch, T. K. & Mishra, S. Coupled oscillatory recurrent neural network (co{rnn}):\nAn accurate and (gradient) stable architecture for learning long time dependen-\ncies. In International Conference on Learning Representations (2021). URL https:\n//openreview.net/forum?id=F3s69XzWOia.\n43. Erichson, N. B., Azencot, O., Queiruga, A., Hodgkinson, L.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "219", "text": "42. Rusch, T. K. & Mishra, S. Coupled oscillatory recurrent neural network (co{rnn}):\nAn accurate and (gradient) stable architecture for learning long time dependen-\ncies. In International Conference on Learning Representations (2021). URL https:\n//openreview.net/forum?id=F3s69XzWOia.\n43. Erichson, N. B., Azencot, O., Queiruga, A., Hodgkinson, L. & Mahoney, M. W.\nLipschitz recurrent neural networks. In International Conference on Learning Repre-\nsentations (2021). URL https://openreview.net/forum?id=-N7PBXqOUJZ.\n44. Shukla, S. N. & Marlin, B. M. Multi-time attention networks for irregularly sam-\npled time series. arXiv preprint arXiv:2101.10318 (2021).\n45. Xiong, Y. et al.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "220", "text": "& Mahoney, M. W.\nLipschitz recurrent neural networks. In International Conference on Learning Repre-\nsentations (2021). URL https://openreview.net/forum?id=-N7PBXqOUJZ.\n44. Shukla, S. N. & Marlin, B. M. Multi-time attention networks for irregularly sam-\npled time series. arXiv preprint arXiv:2101.10318 (2021).\n45. Xiong, Y. et al. Nystr¨omformer: A nystr¨om-based algorithm for approximating\nself-attention. CoRR abs/2102.03902 (2021).\n46. Vaswani, A. et al. Attention is all you need. In Advances in neural information pro-\ncessing systems, 5998–6008 (2017).\n31\n47. Maas, A. et al. Learning word vectors for sentiment analysis.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "221", "text": "45. Xiong, Y. et al. Nystr¨omformer: A nystr¨om-based algorithm for approximating\nself-attention. CoRR abs/2102.03902 (2021).\n46. Vaswani, A. et al. Attention is all you need. In Advances in neural information pro-\ncessing systems, 5998–6008 (2017).\n31\n47. Maas, A. et al. Learning word vectors for sentiment analysis. In Proceedings of the\n49th annual meeting of the association for computational linguistics: Human language\ntechnologies, 142–150 (2011).\n48. Dey, R. & Salem, F. M. Gate-variants of gated recurrent unit (gru) neural networks.\nIn 2017 IEEE 60th international midwest symposium on circuits and systems (MWS-\nCAS), 1597–1600 (IEEE, 2017).\n49.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "222", "text": "31\n47. Maas, A. et al. Learning word vectors for sentiment analysis. In Proceedings of the\n49th annual meeting of the association for computational linguistics: Human language\ntechnologies, 142–150 (2011).\n48. Dey, R. & Salem, F. M. Gate-variants of gated recurrent unit (gru) neural networks.\nIn 2017 IEEE 60th international midwest symposium on circuits and systems (MWS-\nCAS), 1597–1600 (IEEE, 2017).\n49. Campos, V., Jou, B., Gir´o-i Nieto, X., Torres, J. & Chang, S.-F. Skip rnn: Learning\nto skip state updates in recurrent neural networks. arXiv preprint arXiv:1708.06834\n(2017).\n50. Todorov, E., Erez, T. & Tassa, Y. Mujoco: A physics engine for model-based control.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "223", "text": "49. Campos, V., Jou, B., Gir´o-i Nieto, X., Torres, J. & Chang, S.-F. Skip rnn: Learning\nto skip state updates in recurrent neural networks. arXiv preprint arXiv:1708.06834\n(2017).\n50. Todorov, E., Erez, T. & Tassa, Y. Mujoco: A physics engine for model-based control.\nIn 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, 5026–5033\n(IEEE, 2012).\n51. Brockman, G. et al. Openai gym. arXiv preprint arXiv:1606.01540 (2016).\n52. Zhang, H., Wang, Z. & Liu, D. A comprehensive review of stability analysis of\ncontinuous-time recurrent neural networks. IEEE Transactions on Neural Networks\nand Learning Systems 25, 1229–1262 (2014).\n53.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "224", "text": "In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, 5026–5033\n(IEEE, 2012).\n51. Brockman, G. et al. Openai gym. arXiv preprint arXiv:1606.01540 (2016).\n52. Zhang, H., Wang, Z. & Liu, D. A comprehensive review of stability analysis of\ncontinuous-time recurrent neural networks. IEEE Transactions on Neural Networks\nand Learning Systems 25, 1229–1262 (2014).\n53. Weinan, E. A proposal on machine learning via dynamical systems. Communica-\ntions in Mathematics and Statistics 5, 1–11 (2017).\n54. Lu, Z., Pu, H., Wang, F., Hu, Z. & Wang, L. The expressive power of neural net-\nworks: A view from the width. arXiv preprint arXiv:1709.02540 (2017).\n55.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "225", "text": "IEEE Transactions on Neural Networks\nand Learning Systems 25, 1229–1262 (2014).\n53. Weinan, E. A proposal on machine learning via dynamical systems. Communica-\ntions in Mathematics and Statistics 5, 1–11 (2017).\n54. Lu, Z., Pu, H., Wang, F., Hu, Z. & Wang, L. The expressive power of neural net-\nworks: A view from the width. arXiv preprint arXiv:1709.02540 (2017).\n55. Li, Q., Chen, L., Tai, C. et al. Maximum principle based algorithms for deep learn-\ning. arXiv preprint arXiv:1710.09513 (2017).\n32\n56. Lechner, M., Hasani, R., Zimmer, M., Henzinger, T. A. & Grosu, R.\nDesigning\nworm-inspired neural networks for interpretable robotic control.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "226", "text": "arXiv preprint arXiv:1709.02540 (2017).\n55. Li, Q., Chen, L., Tai, C. et al. Maximum principle based algorithms for deep learn-\ning. arXiv preprint arXiv:1710.09513 (2017).\n32\n56. Lechner, M., Hasani, R., Zimmer, M., Henzinger, T. A. & Grosu, R.\nDesigning\nworm-inspired neural networks for interpretable robotic control. In International\nConference on Robotics and Automation (ICRA), 87–94 (2019).\n57. Cohen, M. A. & Grossberg, S. Absolute stability of global pattern formation and\nparallel memory storage by competitive neural networks. IEEE transactions on sys-\ntems, man, and cybernetics 815–826 (1983).\n58. Mathieu, E. & Nickel, M.\nRiemannian continuous normalizing ﬂows.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "227", "text": "& Grosu, R.\nDesigning\nworm-inspired neural networks for interpretable robotic control. In International\nConference on Robotics and Automation (ICRA), 87–94 (2019).\n57. Cohen, M. A. & Grossberg, S. Absolute stability of global pattern formation and\nparallel memory storage by competitive neural networks. IEEE transactions on sys-\ntems, man, and cybernetics 815–826 (1983).\n58. Mathieu, E. & Nickel, M.\nRiemannian continuous normalizing ﬂows.\nIn\nLarochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F. & Lin, H. (eds.) Advances\nin Neural Information Processing Systems, vol. 33, 2503–2515 (Curran Associates,\nInc., 2020).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "228", "text": "IEEE transactions on sys-\ntems, man, and cybernetics 815–826 (1983).\n58. Mathieu, E. & Nickel, M.\nRiemannian continuous normalizing ﬂows.\nIn\nLarochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F. & Lin, H. (eds.) Advances\nin Neural Information Processing Systems, vol. 33, 2503–2515 (Curran Associates,\nInc., 2020). URL https://proceedings.neurips.cc/paper/2020/file/\n1aa3d9c6ce672447e1e5d0f1b5207e85-Paper.pdf.\n59. Hodgkinson, L., van der Heide, C., Roosta, F. & Mahoney, M. W. Stochastic nor-\nmalizing ﬂows. arXiv preprint arXiv:2002.09547 (2020).\n60.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "229", "text": "33, 2503–2515 (Curran Associates,\nInc., 2020). URL https://proceedings.neurips.cc/paper/2020/file/\n1aa3d9c6ce672447e1e5d0f1b5207e85-Paper.pdf.\n59. Hodgkinson, L., van der Heide, C., Roosta, F. & Mahoney, M. W. Stochastic nor-\nmalizing ﬂows. arXiv preprint arXiv:2002.09547 (2020).\n60. Haber, E., Lensink, K., Treister, E. & Ruthotto, L. Imexnet a forward stable deep\nneural network. In International Conference on Machine Learning, 2525–2534 (PMLR,\n2019).\n61. Chang, B., Chen, M., Haber, E. & Chi, E. H.\nAntisymmetricrnn: A dynamical\nsystem view on recurrent neural networks.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "230", "text": "arXiv preprint arXiv:2002.09547 (2020).\n60. Haber, E., Lensink, K., Treister, E. & Ruthotto, L. Imexnet a forward stable deep\nneural network. In International Conference on Machine Learning, 2525–2534 (PMLR,\n2019).\n61. Chang, B., Chen, M., Haber, E. & Chi, E. H.\nAntisymmetricrnn: A dynamical\nsystem view on recurrent neural networks. arXiv preprint arXiv:1902.09689 (2019).\n62. Lechner, M., Hasani, R., Rus, D. & Grosu, R. Gershgorin loss stabilizes the recurrent\nneural network compartment of an end-to-end robot learning scheme. In 2020 IEEE\nInternational Conference on Robotics and Automation (ICRA), 5446–5452 (IEEE, 2020).\n33\n63.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "231", "text": "& Chi, E. H.\nAntisymmetricrnn: A dynamical\nsystem view on recurrent neural networks. arXiv preprint arXiv:1902.09689 (2019).\n62. Lechner, M., Hasani, R., Rus, D. & Grosu, R. Gershgorin loss stabilizes the recurrent\nneural network compartment of an end-to-end robot learning scheme. In 2020 IEEE\nInternational Conference on Robotics and Automation (ICRA), 5446–5452 (IEEE, 2020).\n33\n63. Gleeson, P., Lung, D., Grosu, R., Hasani, R. & Larson, S. D. c302: a multiscale\nframework for modelling the nervous system of caenorhabditis elegans. Philosoph-\nical Transactions of the Royal Society B: Biological Sciences 373, 20170379 (2018).\n64. Li, X., Wong, T.-K. L., Chen, R. T.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "232", "text": "33\n63. Gleeson, P., Lung, D., Grosu, R., Hasani, R. & Larson, S. D. c302: a multiscale\nframework for modelling the nervous system of caenorhabditis elegans. Philosoph-\nical Transactions of the Royal Society B: Biological Sciences 373, 20170379 (2018).\n64. Li, X., Wong, T.-K. L., Chen, R. T. & Duvenaud, D. Scalable gradients for stochastic\ndifferential equations. In International Conference on Artiﬁcial Intelligence and Statis-\ntics, 3870–3882 (PMLR, 2020).\n65. Rezende, D. & Mohamed, S.\nVariational inference with normalizing ﬂows.\nIn\nInternational conference on machine learning, 1530–1538 (PMLR, 2015).\n66. Grunbacher, S. et al. On the veriﬁcation of neural odes with stochastic guarantees.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "233", "text": "& Duvenaud, D. Scalable gradients for stochastic\ndifferential equations. In International Conference on Artiﬁcial Intelligence and Statis-\ntics, 3870–3882 (PMLR, 2020).\n65. Rezende, D. & Mohamed, S.\nVariational inference with normalizing ﬂows.\nIn\nInternational conference on machine learning, 1530–1538 (PMLR, 2015).\n66. Grunbacher, S. et al. On the veriﬁcation of neural odes with stochastic guarantees.\nProceedings of the AAAI Conference on Artiﬁcial Intelligence 35, 11525–11535 (2021).\n67. Lechner, M., Hasani, R., Grosu, R., Rus, D. & Henzinger, T. A. Adversarial training\nis not ready for robot learning. arXiv preprint arXiv:2103.08187 (2021).\n68. Brunnbauer, A. et al.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "234", "text": "On the veriﬁcation of neural odes with stochastic guarantees.\nProceedings of the AAAI Conference on Artiﬁcial Intelligence 35, 11525–11535 (2021).\n67. Lechner, M., Hasani, R., Grosu, R., Rus, D. & Henzinger, T. A. Adversarial training\nis not ready for robot learning. arXiv preprint arXiv:2103.08187 (2021).\n68. Brunnbauer, A. et al. Model-based versus model-free deep reinforcement learning\nfor autonomous racing cars. arXiv preprint arXiv:2103.04909 (2021).\n69. Hasani, R. M., Haerle, D. & Grosu, R. Efﬁcient modeling of complex analog inte-\ngrated circuits using neural networks. In 2016 12th Conference on Ph. D. Research in\nMicroelectronics and Electronics (PRIME), 1–4 (IEEE, 2016).\n70.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "235", "text": "68. Brunnbauer, A. et al. Model-based versus model-free deep reinforcement learning\nfor autonomous racing cars. arXiv preprint arXiv:2103.04909 (2021).\n69. Hasani, R. M., Haerle, D. & Grosu, R. Efﬁcient modeling of complex analog inte-\ngrated circuits using neural networks. In 2016 12th Conference on Ph. D. Research in\nMicroelectronics and Electronics (PRIME), 1–4 (IEEE, 2016).\n70. Wang, G., Ledwoch, A., Hasani, R. M., Grosu, R. & Brintrup, A. A generative neu-\nral network model for the quality prediction of work in progress products. Applied\nSoft Computing 85, 105683 (2019).\n71. DelPreto, J. et al. Plug-and-play supervisory control using muscle and brain signals\nfor real-time gesture and error detection.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "236", "text": "D. Research in\nMicroelectronics and Electronics (PRIME), 1–4 (IEEE, 2016).\n70. Wang, G., Ledwoch, A., Hasani, R. M., Grosu, R. & Brintrup, A. A generative neu-\nral network model for the quality prediction of work in progress products. Applied\nSoft Computing 85, 105683 (2019).\n71. DelPreto, J. et al. Plug-and-play supervisory control using muscle and brain signals\nfor real-time gesture and error detection. Autonomous Robots 44, 1303–1322 (2020).\n34\n72. Hasani, R. Interpretable Recurrent Neural Networks in Continuous-time Control Envi-\nronments. PhD dissertation, Technische Universit¨at Wien (2020).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "237", "text": "Applied\nSoft Computing 85, 105683 (2019).\n71. DelPreto, J. et al. Plug-and-play supervisory control using muscle and brain signals\nfor real-time gesture and error detection. Autonomous Robots 44, 1303–1322 (2020).\n34\n72. Hasani, R. Interpretable Recurrent Neural Networks in Continuous-time Control Envi-\nronments. PhD dissertation, Technische Universit¨at Wien (2020).\nAcknowledgments\nAuthors would like to thank Tsun-Hsuan Wang, Patrick Kao, Makram Chahine, Wei\nXiao, Xiao Li, Lianhao Yin, and Yutong Ben for useful suggestions and testing out\nCfC models for conﬁrmation of results across other domains. Funding: R.H. and D.R.\nare partially supported by Boeing and MIT. M.L. is supported in part by the Austrian\nScience Fund (FWF) under grant Z211-N23 (Wittgenstein Award). A.A.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "238", "text": "Acknowledgments\nAuthors would like to thank Tsun-Hsuan Wang, Patrick Kao, Makram Chahine, Wei\nXiao, Xiao Li, Lianhao Yin, and Yutong Ben for useful suggestions and testing out\nCfC models for conﬁrmation of results across other domains. Funding: R.H. and D.R.\nare partially supported by Boeing and MIT. M.L. is supported in part by the Austrian\nScience Fund (FWF) under grant Z211-N23 (Wittgenstein Award). A.A. is supported\nby the National Science Foundation (NSF) Graduate Research Fellowship Program.\nM.T. is supported by the Poul Due Jensen Foundation, grant 883901. This research\nwas partially sponsored by the United States Air Force Research Laboratory and the\nUnited States Air Force Artiﬁcial Intelligence Accelerator and was accomplished un-\nder Cooperative Agreement Number FA8750-19-2-1000.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "239", "text": "M.L. is supported in part by the Austrian\nScience Fund (FWF) under grant Z211-N23 (Wittgenstein Award). A.A. is supported\nby the National Science Foundation (NSF) Graduate Research Fellowship Program.\nM.T. is supported by the Poul Due Jensen Foundation, grant 883901. This research\nwas partially sponsored by the United States Air Force Research Laboratory and the\nUnited States Air Force Artiﬁcial Intelligence Accelerator and was accomplished un-\nder Cooperative Agreement Number FA8750-19-2-1000. The views and conclusions\ncontained in this document are those of the authors and should not be interpreted\nas representing the ofﬁcial policies, either expressed or implied, of the United States\nAir Force or the U.S. Government. The U.S. Government is authorized to reproduce\nand distribute reprints for Government purposes notwithstanding any copyright no-\ntation herein.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "240", "text": "This research\nwas partially sponsored by the United States Air Force Research Laboratory and the\nUnited States Air Force Artiﬁcial Intelligence Accelerator and was accomplished un-\nder Cooperative Agreement Number FA8750-19-2-1000. The views and conclusions\ncontained in this document are those of the authors and should not be interpreted\nas representing the ofﬁcial policies, either expressed or implied, of the United States\nAir Force or the U.S. Government. The U.S. Government is authorized to reproduce\nand distribute reprints for Government purposes notwithstanding any copyright no-\ntation herein. This work was further supported by The Boeing Company and the\nOfﬁce of Naval Research (ONR) Grant N00014-18-1-2830. Data and materials avail-\nability: All data, code, and materials used in the analysis are openly available at\nhttps://github.com/raminmh/CfC under Apache 2.0 License, for purposes of re-\nproducing and extending the analysis.\n35\nList of Supplementary materials\nMaterials and Methods.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "241", "text": "Government is authorized to reproduce\nand distribute reprints for Government purposes notwithstanding any copyright no-\ntation herein. This work was further supported by The Boeing Company and the\nOfﬁce of Naval Research (ONR) Grant N00014-18-1-2830. Data and materials avail-\nability: All data, code, and materials used in the analysis are openly available at\nhttps://github.com/raminmh/CfC under Apache 2.0 License, for purposes of re-\nproducing and extending the analysis.\n35\nList of Supplementary materials\nMaterials and Methods.\nTables S1 to S4.\n36\nSupplementary Materials\nHere, we provide all supplementary materials used in our analysis.\nMaterials and Methods\nIn this section, we provide the full proof for Lemma 1.\nProof of Lemma 1\nProof.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "242", "text": "35\nList of Supplementary materials\nMaterials and Methods.\nTables S1 to S4.\n36\nSupplementary Materials\nHere, we provide all supplementary materials used in our analysis.\nMaterials and Methods\nIn this section, we provide the full proof for Lemma 1.\nProof of Lemma 1\nProof. We start by noting that\nx(t) −˜x(t) = c[e−wτt−R t\n0 f (I(s))ds −e−wτt−f (I(t))t f (−I(t))]\n= ce−wτt[e−R t\n0 f (I(s))ds −e−f (I(t))t f (−I(t))]\nSince 0 ≤f ≤1, we conclude e−R t\n0 f (I(s))ds ∈[0; 1] and e−f (I(t))t f (−I(t)) ∈[0; 1]. This\nshows that |x(t) −˜x(t)| ≤|c|e−wτt.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "243", "text": "This\nshows that |x(t) −˜x(t)| ≤|c|e−wτt. To see the sharpness results, pick some arbitrary\nsmall ε > 0 and a sufﬁciently large C > 0 such that f (−C) ≤ε and 1 −ε ≤f (C). With\nthis, for any 0 < δ < t, we consider the piecewise constant input signal I such that\nI(s) = −C for s ∈[0; t −δ] and I(s) = C for s ∈(t −δ; t].", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "244", "text": "To see the sharpness results, pick some arbitrary\nsmall ε > 0 and a sufﬁciently large C > 0 such that f (−C) ≤ε and 1 −ε ≤f (C). With\nthis, for any 0 < δ < t, we consider the piecewise constant input signal I such that\nI(s) = −C for s ∈[0; t −δ] and I(s) = C for s ∈(t −δ; t]. Then, it can be noted that\ne−R t\n0 f (I(s))ds −e−f (I(t))t f (−I(t)) ≥\ne−εt−δ·1 −e−(1−ε)·tε →1,\nwhen ε, δ →0\nStatement 1) follows by noting that there exists a family of continuous signals In :\n[0; t] →R such that |In(·)| ≤C for all n ≥1 and In →I pointwise as n →∞.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "245", "text": "This is\nbecause\nlim\nn→∞\n\f\f\f\nZ t\n0 f (I(s))ds −\nZ t\n0 f (In(s))ds\n\f\f\f ≤\nlim\nn→∞\nZ t\n0 | f (I(s)) −f (In(s))|ds ≤lim\nn→∞L\nZ t\n0 |I(s) −In(s)|ds\n= 0\n37\nwhere L is the Lipschitz constant of f and the last identity is due to dominated conver-\ngence theorem (21). To see 2), we ﬁrst note that the negation of the signal −I provides\nus with\ne−R t\n0 f (−I(s))ds −e−f (−I(t))t f (I(t)) ≤\ne−(1−ε)(t−δ)−δ·0 −e−ε·t(1 −ε) →e−t −1,\nif ε, δ →0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "246", "text": "To see 2), we ﬁrst note that the negation of the signal −I provides\nus with\ne−R t\n0 f (−I(s))ds −e−f (−I(t))t f (I(t)) ≤\ne−(1−ε)(t−δ)−δ·0 −e−ε·t(1 −ε) →e−t −1,\nif ε, δ →0. The fact that the left-hand side of the last inequality must be at least e−t −1\nfollows by observing that e−t ≤e−R t\n0 f (I′(s))ds and e−f (I′′(t))t f (−I′′(t)) ≤1 for any\nI′, I′′ : [0; t] →R.\n38\nTable S1: Bit-Stream XOR experiments.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "247", "text": "The fact that the left-hand side of the last inequality must be at least e−t −1\nfollows by observing that e−t ≤e−R t\n0 f (I′(s))ds and e−f (I′′(t))t f (−I′′(t)) ≤1 for any\nI′, I′′ : [0; t] →R.\n38\nTable S1: Bit-Stream XOR experiments. Hyperparameters\nParameter\nValue\nCf-S\nCfC\nCfC-noGate\nCfC-mmRNN\nclipnorm\n5\n1\n10\n10\noptimizer\nAdam\nRMSProp\nRMSprop\nRMSprop\nbatch size\n256\n128\n128\n128\nHidden size\n64\n192\n128\n64\nepochs\n200\n200\n200\n200\nbase lr\n0.005\n0.05\n0.005\n0.005\ndecay lr\n0.9\n0.7\n0.95\n0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "248", "text": "Hyperparameters\nParameter\nValue\nCf-S\nCfC\nCfC-noGate\nCfC-mmRNN\nclipnorm\n5\n1\n10\n10\noptimizer\nAdam\nRMSProp\nRMSprop\nRMSprop\nbatch size\n256\n128\n128\n128\nHidden size\n64\n192\n128\n64\nepochs\n200\n200\n200\n200\nbase lr\n0.005\n0.05\n0.005\n0.005\ndecay lr\n0.9\n0.7\n0.95\n0.95\nbackbone activation\nSiLU\nReLU\nSiLU\nReLU\nbackbone dr\n0.0\n0.0\n0.3\n0.0\nforget bias\n1.2\n1.2\n4.7\n0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "249", "text": "005\n0.05\n0.005\n0.005\ndecay lr\n0.9\n0.7\n0.95\n0.95\nbackbone activation\nSiLU\nReLU\nSiLU\nReLU\nbackbone dr\n0.0\n0.0\n0.3\n0.0\nforget bias\n1.2\n1.2\n4.7\n0.6\nbackbone units\n64\n128\n192\n128\nbackbone layers\n1\n1\n1\n1\nweight decay\n3e-05\n3e-06\n5e-06\n2e-06\nTable S2: Physionet experiments. Hyperparameters\nParameter\nValue\nCf-S\nCfC\nCfC-noGate\nCfC-mmRNN\nepochs\n116\n57\n58\n65\nclass weight\n18.25\n11.69\n7.73\n5.91\nclipnorm\n0\n0\n0\n0\nHidden size\n64\n256\n64\n64\nbase lr\n0.003\n0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "250", "text": "Hyperparameters\nParameter\nValue\nCf-S\nCfC\nCfC-noGate\nCfC-mmRNN\nepochs\n116\n57\n58\n65\nclass weight\n18.25\n11.69\n7.73\n5.91\nclipnorm\n0\n0\n0\n0\nHidden size\n64\n256\n64\n64\nbase lr\n0.003\n0.002\n0.003\n0.001\ndecay lr\n0.72\n0.9\n0.73\n0.9\nbackbone activation\nTanh\nSiLU\nReLU\nLeCun Tanh\nbackbone units\n64\n64\n192\n64\nbackbone dr\n0.1\n0.2\n0.0\n0.3\nbackbone layers\n3\n2\n2\n2\nweight decay\n5e-05\n4e-06\n5e-05\n4e-06\noptimizer\nAdamW\nAdamW\nAdamW\nAdamW\ninit\n0.53\n0.50\n0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "251", "text": "72\n0.9\n0.73\n0.9\nbackbone activation\nTanh\nSiLU\nReLU\nLeCun Tanh\nbackbone units\n64\n64\n192\n64\nbackbone dr\n0.1\n0.2\n0.0\n0.3\nbackbone layers\n3\n2\n2\n2\nweight decay\n5e-05\n4e-06\n5e-05\n4e-06\noptimizer\nAdamW\nAdamW\nAdamW\nAdamW\ninit\n0.53\n0.50\n0.55\n0.6\nbatch size\n128\n128\n128\n128\n39\nTable S3: IMDB experiments.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "252", "text": "1\n0.2\n0.0\n0.3\nbackbone layers\n3\n2\n2\n2\nweight decay\n5e-05\n4e-06\n5e-05\n4e-06\noptimizer\nAdamW\nAdamW\nAdamW\nAdamW\ninit\n0.53\n0.50\n0.55\n0.6\nbatch size\n128\n128\n128\n128\n39\nTable S3: IMDB experiments. Hyperparameters\nParameter\nValue\nCf-S\nCfC\nCfC-noGate\nCfC-mmRNN\nclipnorm\n1\n10\n5\n10\noptimizer\nAdam\nRMSProp\nRMSprop\nRMSprop\nbatch size\n128\n128\n128\n128\nHidden size\n320\n192\n224\n64\nembed dim\n64\n192\n192\n32\nembed dr\n0.0\n0.0\n0.2\n0.3\nepochs\n27\n47\n37\n20\nbase lr\n0.0005\n0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "253", "text": "Hyperparameters\nParameter\nValue\nCf-S\nCfC\nCfC-noGate\nCfC-mmRNN\nclipnorm\n1\n10\n5\n10\noptimizer\nAdam\nRMSProp\nRMSprop\nRMSprop\nbatch size\n128\n128\n128\n128\nHidden size\n320\n192\n224\n64\nembed dim\n64\n192\n192\n32\nembed dr\n0.0\n0.0\n0.2\n0.3\nepochs\n27\n47\n37\n20\nbase lr\n0.0005\n0.0005\n0.0005\n0.0005\ndecay lr\n0.8\n0.7\n0.8\n0.8\nbackbone activation\nRelu\nSiLU\nSiLU\nLeCun Tanh\nbackbone dr\n0.0\n0.0\n0.1\n0.0\nbackbone units\n64\n64\n128\n64\nbackbone layers\n1\n2\n1\n1\nweight decay\n0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "254", "text": "2\n0.3\nepochs\n27\n47\n37\n20\nbase lr\n0.0005\n0.0005\n0.0005\n0.0005\ndecay lr\n0.8\n0.7\n0.8\n0.8\nbackbone activation\nRelu\nSiLU\nSiLU\nLeCun Tanh\nbackbone dr\n0.0\n0.0\n0.1\n0.0\nbackbone units\n64\n64\n128\n64\nbackbone layers\n1\n2\n1\n1\nweight decay\n0.00048\n3.6e-05\n2.7e-05\n0.00029\nTable S4: Walker2D experiments.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "255", "text": "Hyperparameters\nParameter\nValue\nCf-S\nCfC\nCfC-noGate\nCfC-mmRNN\nclipnorm\n10\n1\n1\n10\noptimizer\nAdam\nAdam\nAdam\nAdam\nbatch size\n128\n256\n128\n128\nHidden size\n256\n64\n256\n128\nepochs\n50\n50\n50\n50\nbase lr\n0.006\n0.02\n0.008\n0.005\ndecay lr\n0.95\n0.95\n0.95\n0.95\nbackbone activation\nSiLU\nSiLU\nLeCun Tanh\nLeCun Tanh\nbackbone dr\n0.0\n0.1\n0.1\n0.2\nforget bias\n5.0\n1.6\n2.8\n2.1\nbackbone units\n192\n256\n128\n128\nbackbone layers\n1\n1\n1\n2\nweight decay\n1e-06\n1e-06\n3e-05\n6e-06\n40", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "256", "text": "mHC: Manifold-Constrained Hyper-Connections\nZhenda Xie*†, Yixuan Wei*, Huanqi Cao*,\nChenggang Zhao, Chengqi Deng, Jiashi Li, Damai Dai, Huazuo Gao, Jiang Chang,\nLiang Zhao, Shangyan Zhou, Zhean Xu, Zhengyan Zhang, Wangding Zeng,\nShengding Hu, Yuqing Wang, Jingyang Yuan, Lean Wang, Wenfeng Liang\nDeepSeek-AI\nAbstract\nRecently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous resid-\nual connection paradigm established over the past decade by expanding the residual stream\nwidth and diversifying connectivity patterns. While yielding substantial performance gains,\nthis diversification fundamentally compromises the identity mapping property intrinsic to\nthe residual connection, which causes severe training instability and restricted scalability, and\nadditionally incurs notable memory access overhead.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "257", "text": "While yielding substantial performance gains,\nthis diversification fundamentally compromises the identity mapping property intrinsic to\nthe residual connection, which causes severe training instability and restricted scalability, and\nadditionally incurs notable memory access overhead. To address these challenges, we pro-\npose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects\nthe residual connection space of HC onto a specific manifold to restore the identity mapping\nproperty, while incorporating rigorous infrastructure optimization to ensure efficiency. Em-\npirical experiments demonstrate that mHC is effective for training at scale, offering tangible\nperformance improvements and superior scalability. We anticipate that mHC, as a flexible and\npractical extension of HC, will contribute to a deeper understanding of topological architecture\ndesign and suggest promising directions for the evolution of foundational models.\n(a) Residual Connection\n(b) Hyper-Connections (HC)\n(c) Manifold-Constrained HC (mHC)\nLayer ℱ\nx!\nx!\"#\nRes Mapping\nℋ!\n$%&\nPre Mapping\nℋ!\n'$%\nPost Mapping\nℋ!\n'(&)\nLayer ℱ\nx!", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "258", "text": "We anticipate that mHC, as a flexible and\npractical extension of HC, will contribute to a deeper understanding of topological architecture\ndesign and suggest promising directions for the evolution of foundational models.\n(a) Residual Connection\n(b) Hyper-Connections (HC)\n(c) Manifold-Constrained HC (mHC)\nLayer ℱ\nx!\nx!\"#\nRes Mapping\nℋ!\n$%&\nPre Mapping\nℋ!\n'$%\nPost Mapping\nℋ!\n'(&)\nLayer ℱ\nx!\"#\nh!\n$%&\nx!\nh!\n'(&)\nh!\n*+\nh!\n(,)\nRes Mapping\n𝒫ℳ!\"#(ℋ!\n$%&)\nPre Mapping\n𝒫ℳ$!\"(ℋ!\n'$%)\nPost Mapping\n𝒫ℳ$%#&(ℋ!\n'(&))\nLayer ℱ\nx!\"#\nh!\n$%&\nx!\nh!\n'(&)\nh!\n*+\nh!\n(,)\nFigure 1 | Illustrations of Residual Connection Paradigms.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "259", "text": "'(&)\nLayer ℱ\nx!\"#\nh!\n$%&\nx!\nh!\n'(&)\nh!\n*+\nh!\n(,)\nRes Mapping\n𝒫ℳ!\"#(ℋ!\n$%&)\nPre Mapping\n𝒫ℳ$!\"(ℋ!\n'$%)\nPost Mapping\n𝒫ℳ$%#&(ℋ!\n'(&))\nLayer ℱ\nx!\"#\nh!\n$%&\nx!\nh!\n'(&)\nh!\n*+\nh!\n(,)\nFigure 1 | Illustrations of Residual Connection Paradigms. This figure compares the structural\ndesign of (a) standard Residual Connection, (b) Hyper-Connections (HC), and (c) our proposed\nManifold-Constrained Hyper-Connections (mHC). Unlike the unconstrained HC, mHC focuses\non optimizing the residual connection space by projecting the matrices onto a constrained\nmanifold to ensure stability.\n*Core contributors.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "260", "text": "'(&))\nLayer ℱ\nx!\"#\nh!\n$%&\nx!\nh!\n'(&)\nh!\n*+\nh!\n(,)\nFigure 1 | Illustrations of Residual Connection Paradigms. This figure compares the structural\ndesign of (a) standard Residual Connection, (b) Hyper-Connections (HC), and (c) our proposed\nManifold-Constrained Hyper-Connections (mHC). Unlike the unconstrained HC, mHC focuses\non optimizing the residual connection space by projecting the matrices onto a constrained\nmanifold to ensure stability.\n*Core contributors. †Corresponding author: xie.zhenda@deepseek.com\narXiv:2512.24880v1  [cs.CL]  31 Dec 2025\nContents\n1\nIntroduction\n3\n2\nRelated Works\n4\n2.1\nMicro Design\n. . . . . . . . . . . . . . . . . . . . . .", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "261", "text": "*Core contributors. †Corresponding author: xie.zhenda@deepseek.com\narXiv:2512.24880v1  [cs.CL]  31 Dec 2025\nContents\n1\nIntroduction\n3\n2\nRelated Works\n4\n2.1\nMicro Design\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2.2\nMacro Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "262", "text": ". . . . . . . . . . . . . . . . . . . . . .\n4\n2.2\nMacro Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n3\nPreliminary\n5\n3.1\nNumerical Instability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n3.2\nSystem Overhead . . .", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "263", "text": ". . . . . . . . . . . . .\n5\n3\nPreliminary\n5\n3.1\nNumerical Instability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n3.2\nSystem Overhead . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n4\nMethod\n8\n4.1\nManifold-Constrained Hyper-Connections\n. . . . . . . .", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "264", "text": ". . . . .\n6\n3.2\nSystem Overhead . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n4\nMethod\n8\n4.1\nManifold-Constrained Hyper-Connections\n. . . . . . . . . . . . . . . . . . . . . .\n8\n4.2\nParameterization and Manifold Projection . . . . . . . . . . . . . . . . . . . . . . .\n9\n4.3\nEfficient Infrastructure Design . . . . .", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "265", "text": ". . . . . . . . . . . . . . . . . . . . .\n8\n4.2\nParameterization and Manifold Projection . . . . . . . . . . . . . . . . . . . . . . .\n9\n4.3\nEfficient Infrastructure Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n4.3.1\nKernel Fusion . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "266", "text": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n4.3.1\nKernel Fusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n4.3.2\nRecomputing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "267", "text": ". . . . . . . . . . . . . . . . . . . . . . . .\n9\n4.3.2\nRecomputing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n4.3.3\nOverlapping Communication in DualPipe\n. . . . . . . . . . . . . . . . . .\n11\n5\nExperiments\n12\n5.1\nExperimental Setup . . . . . . . . . . . . . . . . . . .", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "268", "text": ". . . . . . . . . . . . .\n10\n4.3.3\nOverlapping Communication in DualPipe\n. . . . . . . . . . . . . . . . . .\n11\n5\nExperiments\n12\n5.1\nExperimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n5.2\nMain Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "269", "text": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n5.2\nMain Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n5.3\nScaling Experiments\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "270", "text": ". . . . . . . . . . . . . . . . . . . . . . . . .\n12\n5.3\nScaling Experiments\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n5.4\nStability Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "271", "text": ". . . . . . . . . . . . . . . . . . . . . . .\n13\n5.4\nStability Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n6\nConclusion and Outlook\n15\nA Appendix\n19\nA.1 Detailed Model Specifications and Hyper-parameters. . . . . . . . . . . . . . . . .\n19\n2\n1. Introduction\nDeep neural network architectures have undergone rapid evolution since the introduction of\nResNets (He et al., 2016a). As illustrated in Fig.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "272", "text": ". . . . . . . . . . . . . .\n14\n6\nConclusion and Outlook\n15\nA Appendix\n19\nA.1 Detailed Model Specifications and Hyper-parameters. . . . . . . . . . . . . . . . .\n19\n2\n1. Introduction\nDeep neural network architectures have undergone rapid evolution since the introduction of\nResNets (He et al., 2016a). As illustrated in Fig. 1(a), the structure of a single-layer can be\nformulated as follows:\nx𝑙+1 = x𝑙+ F (x𝑙, W𝑙),\n(1)\nwhere x𝑙and x𝑙+1 denote the 𝐶-dimensional input and output of the 𝑙-th layer, respectively,\nand F represents the residual function.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "273", "text": ". .\n19\n2\n1. Introduction\nDeep neural network architectures have undergone rapid evolution since the introduction of\nResNets (He et al., 2016a). As illustrated in Fig. 1(a), the structure of a single-layer can be\nformulated as follows:\nx𝑙+1 = x𝑙+ F (x𝑙, W𝑙),\n(1)\nwhere x𝑙and x𝑙+1 denote the 𝐶-dimensional input and output of the 𝑙-th layer, respectively,\nand F represents the residual function. Although the residual function F has evolved over\nthe past decade to include various operations such as convolution, attention mechanisms, and\nfeed forward networks, the paradigm of the residual connection has maintained its original\nform.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "274", "text": "1(a), the structure of a single-layer can be\nformulated as follows:\nx𝑙+1 = x𝑙+ F (x𝑙, W𝑙),\n(1)\nwhere x𝑙and x𝑙+1 denote the 𝐶-dimensional input and output of the 𝑙-th layer, respectively,\nand F represents the residual function. Although the residual function F has evolved over\nthe past decade to include various operations such as convolution, attention mechanisms, and\nfeed forward networks, the paradigm of the residual connection has maintained its original\nform. Accompanying the progression of Transformer (Vaswani et al., 2017) architecture, this\nparadigm has currently established itself as a fundamental design element in large language\nmodels (LLMs) (Brown et al., 2020; Liu et al., 2024b; Touvron et al., 2023).\nThis success is primarily attributed to the concise form of the residual connection.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "275", "text": "Accompanying the progression of Transformer (Vaswani et al., 2017) architecture, this\nparadigm has currently established itself as a fundamental design element in large language\nmodels (LLMs) (Brown et al., 2020; Liu et al., 2024b; Touvron et al., 2023).\nThis success is primarily attributed to the concise form of the residual connection. More\nimportantly, early research (He et al., 2016b) revealed that the identity mapping property of the\nresidual connection maintains stability and efficiency during large-scale training. By recursively\nextending the residual connection across multiple layers, Eq. (1) yields:\nx𝐿= x𝑙+\n𝐿−1\n∑︁\n𝑖=𝑙\nF (x𝑖, W𝑖),\n(2)\nwhere 𝐿and 𝑙correspond to deeper and shallower layers, respectively.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "276", "text": "More\nimportantly, early research (He et al., 2016b) revealed that the identity mapping property of the\nresidual connection maintains stability and efficiency during large-scale training. By recursively\nextending the residual connection across multiple layers, Eq. (1) yields:\nx𝐿= x𝑙+\n𝐿−1\n∑︁\n𝑖=𝑙\nF (x𝑖, W𝑖),\n(2)\nwhere 𝐿and 𝑙correspond to deeper and shallower layers, respectively. The term identity\nmapping refers to the component x𝑙itself, which emphasizes the property that the signal from\nthe shallower layer maps directly to the deeper layer without any modification.\nRecently, studies exemplified by Hyper-Connections (HC) (Zhu et al., 2024) have introduced\na new dimension to the residual connection and empirically demonstrated its performance\npotential. The single-layer architecture of HC is illustrated in Fig. 1(b).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "277", "text": "The term identity\nmapping refers to the component x𝑙itself, which emphasizes the property that the signal from\nthe shallower layer maps directly to the deeper layer without any modification.\nRecently, studies exemplified by Hyper-Connections (HC) (Zhu et al., 2024) have introduced\na new dimension to the residual connection and empirically demonstrated its performance\npotential. The single-layer architecture of HC is illustrated in Fig. 1(b). By expanding the width of\nthe residual stream and enhancing connection complexity, HC significantly increases topological\ncomplexity without altering the computational overhead of individual units regarding FLOPs.\nFormally, single-layer propagation in HC is defined as:\nx𝑙+1 = Hres\n𝑙\nx𝑙+ Hpost ⊤\n𝑙\nF (Hpre\n𝑙\nx𝑙, W𝑙),\n(3)\nwhere x𝑙and x𝑙+1 denote the input and output of the 𝑙-th layer, respectively.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "278", "text": "By expanding the width of\nthe residual stream and enhancing connection complexity, HC significantly increases topological\ncomplexity without altering the computational overhead of individual units regarding FLOPs.\nFormally, single-layer propagation in HC is defined as:\nx𝑙+1 = Hres\n𝑙\nx𝑙+ Hpost ⊤\n𝑙\nF (Hpre\n𝑙\nx𝑙, W𝑙),\n(3)\nwhere x𝑙and x𝑙+1 denote the input and output of the 𝑙-th layer, respectively. Unlike the formu-\nlation in Eq. (1), the feature dimension of x𝑙and x𝑙+1 is expanded from 𝐶to 𝑛× 𝐶, where 𝑛is\nthe expansion rate. The term Hres\n𝑙\n∈R𝑛×𝑛represents a learnable mapping that mixes features\nwithin the residual stream.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "279", "text": "Unlike the formu-\nlation in Eq. (1), the feature dimension of x𝑙and x𝑙+1 is expanded from 𝐶to 𝑛× 𝐶, where 𝑛is\nthe expansion rate. The term Hres\n𝑙\n∈R𝑛×𝑛represents a learnable mapping that mixes features\nwithin the residual stream. Also as a learnable mapping, Hpre\n𝑙\n∈R1×𝑛aggregates features from\nthe 𝑛𝐶-dim stream into a 𝐶-dim layer input, and conversely, Hpost\n𝑙\n∈R1×𝑛maps the layer output\nback onto the stream.\nHowever, as the training scale increases, HC introduces potential risks of instability. The\nprimary concern is that the unconstrained nature of HC compromises the identity mapping\nproperty when the architecture extends across multiple layers. In architectures comprising\nmultiple parallel streams, an ideal identity mapping serves as a conservation mechanism.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "280", "text": "However, as the training scale increases, HC introduces potential risks of instability. The\nprimary concern is that the unconstrained nature of HC compromises the identity mapping\nproperty when the architecture extends across multiple layers. In architectures comprising\nmultiple parallel streams, an ideal identity mapping serves as a conservation mechanism. It\nensures that the average signal intensity across streams remains invariant during both forward\nand backward propagation. Recursively extending HC to multiple layers via Eq. (3) yields:\nx𝐿=\n 𝐿−𝑙\nÖ\n𝑖=1\nHres\n𝐿−𝑖\n!", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "281", "text": "In architectures comprising\nmultiple parallel streams, an ideal identity mapping serves as a conservation mechanism. It\nensures that the average signal intensity across streams remains invariant during both forward\nand backward propagation. Recursively extending HC to multiple layers via Eq. (3) yields:\nx𝐿=\n 𝐿−𝑙\nÖ\n𝑖=1\nHres\n𝐿−𝑖\n!\nx𝑙+\n𝐿−1\n∑︁\n𝑖=𝑙\n©­\n«\n𝐿−1−𝑖\nÖ\n𝑗=1\nHres\n𝐿−𝑗\nª®\n¬\nHpost ⊤\n𝑖\nF (Hpre\n𝑖\nx𝑖, W𝑖),\n(4)\n3\nwhere 𝐿and 𝑙represent a deeper layer and a shallower layer, respectively. In contrast to Eq.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "282", "text": "x𝑙+\n𝐿−1\n∑︁\n𝑖=𝑙\n©­\n«\n𝐿−1−𝑖\nÖ\n𝑗=1\nHres\n𝐿−𝑗\nª®\n¬\nHpost ⊤\n𝑖\nF (Hpre\n𝑖\nx𝑖, W𝑖),\n(4)\n3\nwhere 𝐿and 𝑙represent a deeper layer and a shallower layer, respectively. In contrast to Eq. (2),\nthe composite mapping Î𝐿−𝑙\n𝑖=1 Hres\n𝐿−𝑖in HC fails to preserve the global mean of the features. This\ndiscrepancy leads to unbounded signal amplification or attenuation, resulting in instability\nduring large-scale training.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "283", "text": "In contrast to Eq. (2),\nthe composite mapping Î𝐿−𝑙\n𝑖=1 Hres\n𝐿−𝑖in HC fails to preserve the global mean of the features. This\ndiscrepancy leads to unbounded signal amplification or attenuation, resulting in instability\nduring large-scale training. A further consideration is that, while HC preserves computational\nefficiency in terms of FLOPs, the hardware efficiency concerning memory access costs for the\nwidened residual stream remains unaddressed in the original design. These factors collectively\nrestrict the practical scalability of HC and hinder its application in large-scale training.\nTo address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC),\nas shown in Fig. 1(c), a general framework that projects the residual connection space of HC\nonto a specific manifold to restore the identity mapping property, while incorporating rigorous\ninfrastructure optimization to ensure efficiency.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "284", "text": "A further consideration is that, while HC preserves computational\nefficiency in terms of FLOPs, the hardware efficiency concerning memory access costs for the\nwidened residual stream remains unaddressed in the original design. These factors collectively\nrestrict the practical scalability of HC and hinder its application in large-scale training.\nTo address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC),\nas shown in Fig. 1(c), a general framework that projects the residual connection space of HC\nonto a specific manifold to restore the identity mapping property, while incorporating rigorous\ninfrastructure optimization to ensure efficiency. Specifically, mHC utilizes the Sinkhorn-Knopp\nalgorithm (Sinkhorn and Knopp, 1967) to entropically project Hres\n𝑙\nonto the Birkhoff polytope.\nThis operation effectively constrains the residual connection matrices within the manifold\nthat is constituted by doubly stochastic matrices.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "285", "text": "1(c), a general framework that projects the residual connection space of HC\nonto a specific manifold to restore the identity mapping property, while incorporating rigorous\ninfrastructure optimization to ensure efficiency. Specifically, mHC utilizes the Sinkhorn-Knopp\nalgorithm (Sinkhorn and Knopp, 1967) to entropically project Hres\n𝑙\nonto the Birkhoff polytope.\nThis operation effectively constrains the residual connection matrices within the manifold\nthat is constituted by doubly stochastic matrices. Since the row and column sums of these\nmatrices equal to 1, the operation Hres\n𝑙\nx𝑙functions as a convex combination of the input features.\nThis characteristic facilitates a well-conditioned signal propagation where the feature mean\nis conserved, and the signal norm is strictly regularized, effectively mitigating the risk of\nvanishing or exploding signals.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "286", "text": "This operation effectively constrains the residual connection matrices within the manifold\nthat is constituted by doubly stochastic matrices. Since the row and column sums of these\nmatrices equal to 1, the operation Hres\n𝑙\nx𝑙functions as a convex combination of the input features.\nThis characteristic facilitates a well-conditioned signal propagation where the feature mean\nis conserved, and the signal norm is strictly regularized, effectively mitigating the risk of\nvanishing or exploding signals. Furthermore, due to the closure of matrix multiplication for\ndoubly stochastic matrices, the composite mapping Î𝐿−𝑙\n𝑖=1 Hres\n𝐿−𝑖retains this conservation property.\nConsequently, mHC effectively maintains the stability of identity mappings between arbitrary\ndepths. To ensure efficiency, we employ kernel fusion and develop mixed precision kernels\nutilizing TileLang (Wang et al., 2025).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "287", "text": "Furthermore, due to the closure of matrix multiplication for\ndoubly stochastic matrices, the composite mapping Î𝐿−𝑙\n𝑖=1 Hres\n𝐿−𝑖retains this conservation property.\nConsequently, mHC effectively maintains the stability of identity mappings between arbitrary\ndepths. To ensure efficiency, we employ kernel fusion and develop mixed precision kernels\nutilizing TileLang (Wang et al., 2025). Furthermore, we mitigate the memory footprint through\nselective recomputing and carefully overlap communication within the DualPipe schedule (Liu\net al., 2024b).\nExtensive experiments on language model pretraining demonstrate that mHC exhibits\nexceptional stability and scalability while maintaining the performance advantages of HC. In-\nhouse large-scale training indicates that mHC supports training at scale and introduces only a\n6.7% additional time overhead when expansion rate 𝑛= 4.\n2. Related Works\nArchitectural advancements in deep learning can be primarily classified into micro-design and\nmacro-design.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "288", "text": "Furthermore, we mitigate the memory footprint through\nselective recomputing and carefully overlap communication within the DualPipe schedule (Liu\net al., 2024b).\nExtensive experiments on language model pretraining demonstrate that mHC exhibits\nexceptional stability and scalability while maintaining the performance advantages of HC. In-\nhouse large-scale training indicates that mHC supports training at scale and introduces only a\n6.7% additional time overhead when expansion rate 𝑛= 4.\n2. Related Works\nArchitectural advancements in deep learning can be primarily classified into micro-design and\nmacro-design. Micro-design concerns the internal architecture of computational blocks, specifying\nhow features are processed across spatial, temporal, and channel dimensions. In contrast,\nmacro-design establishes the inter-block topological structure, thereby dictating how feature\nrepresentations are propagated, routed, and merged across distinct layers.\n2.1. Micro Design\nDriven by parameter sharing and translation invariance, convolution initially dominated the pro-\ncessing of structured signals.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "289", "text": "2. Related Works\nArchitectural advancements in deep learning can be primarily classified into micro-design and\nmacro-design. Micro-design concerns the internal architecture of computational blocks, specifying\nhow features are processed across spatial, temporal, and channel dimensions. In contrast,\nmacro-design establishes the inter-block topological structure, thereby dictating how feature\nrepresentations are propagated, routed, and merged across distinct layers.\n2.1. Micro Design\nDriven by parameter sharing and translation invariance, convolution initially dominated the pro-\ncessing of structured signals. While subsequent variations such as depthwise separable (Chollet,\n2017) and grouped convolutions (Xie et al., 2017) optimized efficiency, the advent of Trans-\nformers (Vaswani et al., 2017) established Attention and Feed-Forward Networks (FFNs) as\nthe fundamental building blocks of modern architecture. Attention mechanisms facilitate\nglobal information propagation, while FFNs enhance the representational capacity of individual\nfeatures.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "290", "text": "2.1. Micro Design\nDriven by parameter sharing and translation invariance, convolution initially dominated the pro-\ncessing of structured signals. While subsequent variations such as depthwise separable (Chollet,\n2017) and grouped convolutions (Xie et al., 2017) optimized efficiency, the advent of Trans-\nformers (Vaswani et al., 2017) established Attention and Feed-Forward Networks (FFNs) as\nthe fundamental building blocks of modern architecture. Attention mechanisms facilitate\nglobal information propagation, while FFNs enhance the representational capacity of individual\nfeatures. To balance performance with the computational demands of LLMs, attention mecha-\nnisms have evolved towards efficient variants such as Multi-Query Attention (MQA) (Shazeer,\n2019), Grouped-Query Attention (GQA) (Ainslie et al., 2023), and Multi-Head Latent Attention\n4\n(MLA) (Liu et al., 2024a).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "291", "text": "Attention mechanisms facilitate\nglobal information propagation, while FFNs enhance the representational capacity of individual\nfeatures. To balance performance with the computational demands of LLMs, attention mecha-\nnisms have evolved towards efficient variants such as Multi-Query Attention (MQA) (Shazeer,\n2019), Grouped-Query Attention (GQA) (Ainslie et al., 2023), and Multi-Head Latent Attention\n4\n(MLA) (Liu et al., 2024a). Simultaneously, FFNs have been generalized into sparse computing\nparadigms via Mixture-of-Experts (MoE) (Fedus et al., 2022; Lepikhin et al., 2020; Shazeer et al.,\n2017), allowing for massive parameter scaling without proportional computational costs.\n2.2. Macro Design\nMacro-design governs the global topology of the network (Srivastava et al., 2015).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "292", "text": "Simultaneously, FFNs have been generalized into sparse computing\nparadigms via Mixture-of-Experts (MoE) (Fedus et al., 2022; Lepikhin et al., 2020; Shazeer et al.,\n2017), allowing for massive parameter scaling without proportional computational costs.\n2.2. Macro Design\nMacro-design governs the global topology of the network (Srivastava et al., 2015). Following\nResNet (He et al., 2016a), architectures such as DenseNet (Huang et al., 2017) and Fractal-\nNet (Larsson et al., 2016) aimed to enhance performance by increasing topological complexity\nthrough dense connectivity and multi-path structures, respectively. Deep Layer Aggregation\n(DLA) (Yu et al., 2018) further extended this paradigm by recursively aggregating features across\nvarious depths and resolutions.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "293", "text": "Macro Design\nMacro-design governs the global topology of the network (Srivastava et al., 2015). Following\nResNet (He et al., 2016a), architectures such as DenseNet (Huang et al., 2017) and Fractal-\nNet (Larsson et al., 2016) aimed to enhance performance by increasing topological complexity\nthrough dense connectivity and multi-path structures, respectively. Deep Layer Aggregation\n(DLA) (Yu et al., 2018) further extended this paradigm by recursively aggregating features across\nvarious depths and resolutions.\nMore recently, the focus of macro-design has shifted toward expanding the width of the\nresidual stream (Chai et al., 2020; Fang et al., 2023; Heddes et al., 2025; Mak and Flanigan,\n2025; Menghani et al., 2025; Pagliardini et al., 2024; Xiao et al., 2025; Xie et al., 2023; Zhu et al.,\n2024).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "294", "text": "More recently, the focus of macro-design has shifted toward expanding the width of the\nresidual stream (Chai et al., 2020; Fang et al., 2023; Heddes et al., 2025; Mak and Flanigan,\n2025; Menghani et al., 2025; Pagliardini et al., 2024; Xiao et al., 2025; Xie et al., 2023; Zhu et al.,\n2024). Hyper-Connections (HC) (Zhu et al., 2024) introduced learnable matrices to modulate\nconnection strengths among features at varying depths, while the Residual Matrix Transformer\n(RMT) (Mak and Flanigan, 2025) replaced the standard residual stream with an outer-product\nmemory matrix to facilitate feature storage. Similarly, MUDDFormer (Xiao et al., 2025) employs\nmultiway dynamic dense connections to optimize cross-layer information flow. Despite their\npotential, these approaches compromise the inherent identity mapping property of the residual\nconnection, thereby introducing instability and hindering scalability.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "295", "text": "Similarly, MUDDFormer (Xiao et al., 2025) employs\nmultiway dynamic dense connections to optimize cross-layer information flow. Despite their\npotential, these approaches compromise the inherent identity mapping property of the residual\nconnection, thereby introducing instability and hindering scalability. Furthermore, they incur\nsignificant memory access overhead due to expanded feature widths. Building upon HC,\nthe proposed mHC restricts the residual connection space onto a specific manifold to restore\nthe identity mapping property, while also incorporating rigorous infrastructure optimizations\nto ensure efficiency. This approach enhances stability and scalability while maintaining the\ntopological benefits of expanded connections.\n3. Preliminary\nWe first establish the notation used in this work. In the HC formulation, the input to the 𝑙-th layer,\nx𝑙∈R1×𝐶, is expanded by a factor of 𝑛to construct a hidden matrix x𝑙= (x⊤\n𝑙,0, . . .", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "296", "text": "This approach enhances stability and scalability while maintaining the\ntopological benefits of expanded connections.\n3. Preliminary\nWe first establish the notation used in this work. In the HC formulation, the input to the 𝑙-th layer,\nx𝑙∈R1×𝐶, is expanded by a factor of 𝑛to construct a hidden matrix x𝑙= (x⊤\n𝑙,0, . . . , x⊤\n𝑙,𝑛−1)⊤∈R𝑛×𝐶\nwhich can be viewed as 𝑛-stream residual. This operation effectively broadens the width of\nthe residual stream. To govern the read-out, write-in, and updating processes of this stream,\nHC introduces three learnable linear mappings—Hpre\n𝑙\n, Hpost\n𝑙\n∈R1×𝑛, and Hres\n𝑙\n∈R𝑛×𝑛.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "297", "text": ". . , x⊤\n𝑙,𝑛−1)⊤∈R𝑛×𝐶\nwhich can be viewed as 𝑛-stream residual. This operation effectively broadens the width of\nthe residual stream. To govern the read-out, write-in, and updating processes of this stream,\nHC introduces three learnable linear mappings—Hpre\n𝑙\n, Hpost\n𝑙\n∈R1×𝑛, and Hres\n𝑙\n∈R𝑛×𝑛. These\nmappings modify the standard residual connection shown in Eq. (1), resulting in the formulation\ngiven in Eq. (3).\nIn the HC formulation, learnable mappings are composed of two parts of coefficients: the\ninput-dependent one and the global one, referred to as dynamic mappings and static mappings,\nrespectively. Formally,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "298", "text": "Formally, HC computes the coefficients as follows:\n\n\n˜x𝑙= RMSNorm(x𝑙)\nHpre\n𝑙\n= 𝛼pre\n𝑙\n· tanh(𝜃pre\n𝑙\n˜x⊤\n𝑙) + bpre\n𝑙\nHpost\n𝑙\n= 𝛼post\n𝑙\n· tanh(𝜃post\n𝑙\n˜x⊤\n𝑙) + bpost\n𝑙\nHres\n𝑙\n= 𝛼res\n𝑙\n· tanh(𝜃res\n𝑙\n˜x⊤\n𝑙) + bres\n𝑙,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "299", "text": "(5)\nwhere RMSNorm(·) (Zhang and Sennrich, 2019) is applied to the last dimension, and the scalars\n𝛼pre\n𝑙\n, 𝛼post\n𝑙\nand 𝛼res\n𝑙\n∈R are learnable gating factors initialized to small values. The dynamic\n5\nmappings are derived via linear projections parameterized by 𝜃pre\n𝑙\n, 𝜃post\n𝑙\n∈R1×𝐶and 𝜃res\n𝑙\n∈R𝑛×𝐶,\nwhile the static mappings are represented by learnable biases bpre\n𝑙\n, bpost\n𝑙\n∈R1×𝑛and bres\n𝑙\n∈R𝑛×𝑛.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "300", "text": "The dynamic\n5\nmappings are derived via linear projections parameterized by 𝜃pre\n𝑙\n, 𝜃post\n𝑙\n∈R1×𝐶and 𝜃res\n𝑙\n∈R𝑛×𝐶,\nwhile the static mappings are represented by learnable biases bpre\n𝑙\n, bpost\n𝑙\n∈R1×𝑛and bres\n𝑙\n∈R𝑛×𝑛.\nIt is worth noting that the introduction of these mappings—Hpre\n𝑙\n, Hpost\n𝑙\n, and Hres\n𝑙\n—incurs\nnegligible computational overhead, as the typical expansion rate 𝑛, e.g. 4, is much smaller than\nthe input dimension 𝐶. With this design, HC effectively decouples the information capacity\nof the residual stream from the layer’s input dimension, which is strongly correlated with the\nmodel’s computational complexity (FLOPs).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "301", "text": "It is worth noting that the introduction of these mappings—Hpre\n𝑙\n, Hpost\n𝑙\n, and Hres\n𝑙\n—incurs\nnegligible computational overhead, as the typical expansion rate 𝑛, e.g. 4, is much smaller than\nthe input dimension 𝐶. With this design, HC effectively decouples the information capacity\nof the residual stream from the layer’s input dimension, which is strongly correlated with the\nmodel’s computational complexity (FLOPs). Consequently, HC offers a new avenue for scaling\nby adjusting the residual stream width, complementing the traditional scaling dimensions of\nmodel FLOPs and training data size discussed in pre-training scaling laws (Hoffmann et al.,\n2022).\nAlthough HC necessitates three mappings to manage the dimensional mismatch between\nthe residual stream and the layer input, preliminary experiments presented in Tab. 1 indicate\nthat the residual mapping Hres\n𝑙\nyields the most significant performance gain. This finding\nunderscores the critical importance of effective information exchange within the residual stream.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "302", "text": "Consequently, HC offers a new avenue for scaling\nby adjusting the residual stream width, complementing the traditional scaling dimensions of\nmodel FLOPs and training data size discussed in pre-training scaling laws (Hoffmann et al.,\n2022).\nAlthough HC necessitates three mappings to manage the dimensional mismatch between\nthe residual stream and the layer input, preliminary experiments presented in Tab. 1 indicate\nthat the residual mapping Hres\n𝑙\nyields the most significant performance gain. This finding\nunderscores the critical importance of effective information exchange within the residual stream.\nTable 1 | Ablation Study of HC Components. When a specific mapping (Hpre\n𝑙\n, Hpost\n𝑙\n, or Hres\n𝑙\n) is\ndisabled, we employ a fixed mapping to maintain dimensional consistency: uniform weights of\n1/𝑛for Hpre\n𝑙\n, uniform weights of ones for Hpost\n𝑙\n, and the identity matrix for Hres\n𝑙\n.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "303", "text": "This finding\nunderscores the critical importance of effective information exchange within the residual stream.\nTable 1 | Ablation Study of HC Components. When a specific mapping (Hpre\n𝑙\n, Hpost\n𝑙\n, or Hres\n𝑙\n) is\ndisabled, we employ a fixed mapping to maintain dimensional consistency: uniform weights of\n1/𝑛for Hpre\n𝑙\n, uniform weights of ones for Hpost\n𝑙\n, and the identity matrix for Hres\n𝑙\n.\nHres\n𝑙\nHpre\n𝑙\nHpost\n𝑙\nAbsolute Loss Gap\n0.0\n✓\n−0.022\n✓\n✓\n−0.025\n✓\n✓\n✓\n−0.027\n3.1. Numerical Instability\nWhile the residual mapping Hres\n𝑙\nis instrumental for performance, its sequential application\nposes a significant risk to numerical stability. As detailed in Eq.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "304", "text": "Hres\n𝑙\nHpre\n𝑙\nHpost\n𝑙\nAbsolute Loss Gap\n0.0\n✓\n−0.022\n✓\n✓\n−0.025\n✓\n✓\n✓\n−0.027\n3.1. Numerical Instability\nWhile the residual mapping Hres\n𝑙\nis instrumental for performance, its sequential application\nposes a significant risk to numerical stability. As detailed in Eq. (4), when HC is extended across\nmultiple layers, the effective signal propagation from layer 𝑙to 𝐿is governed by the composite\nmapping Î𝐿−𝑙\n𝑖=1 Hres\n𝐿−𝑖. Since the learnable mapping Hres\n𝑙\nis unconstrained, this composite mapping\ninevitably deviates from the identity mapping. Consequently, the signal magnitude is prone to\nexplosion or vanishing during both the forward pass and backpropagation.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "305", "text": "As detailed in Eq. (4), when HC is extended across\nmultiple layers, the effective signal propagation from layer 𝑙to 𝐿is governed by the composite\nmapping Î𝐿−𝑙\n𝑖=1 Hres\n𝐿−𝑖. Since the learnable mapping Hres\n𝑙\nis unconstrained, this composite mapping\ninevitably deviates from the identity mapping. Consequently, the signal magnitude is prone to\nexplosion or vanishing during both the forward pass and backpropagation. This phenomenon\nundermines the fundamental premise of residual learning, which relies on unimpeded signal\nflow, thereby destabilizing the training process in deeper or larger-scale models.\nEmpirical evidence supports this analysis. We observe unstable loss behavior in large-scale\nexperiments, as illustrated in Fig. 2. Taking mHC as the baseline, HC exhibits an unexpected\nloss surge around the 12k step, which is highly correlated with the instability in the gradient\nnorm.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "306", "text": "Consequently, the signal magnitude is prone to\nexplosion or vanishing during both the forward pass and backpropagation. This phenomenon\nundermines the fundamental premise of residual learning, which relies on unimpeded signal\nflow, thereby destabilizing the training process in deeper or larger-scale models.\nEmpirical evidence supports this analysis. We observe unstable loss behavior in large-scale\nexperiments, as illustrated in Fig. 2. Taking mHC as the baseline, HC exhibits an unexpected\nloss surge around the 12k step, which is highly correlated with the instability in the gradient\nnorm. Furthermore, the analysis on Hres\n𝑙\nvalidates the mechanism of this instability. To quantify\nhow the composite mapping Î𝐿−𝑙\n𝑖=1 Hres\n𝐿−𝑖amplifies signals along the residual stream, we utilize\ntwo metrics. The first, based on the maximum absolute value of the row sums of the composite\nmapping, captures the worst-case expansion in the forward pass.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "307", "text": "Furthermore, the analysis on Hres\n𝑙\nvalidates the mechanism of this instability. To quantify\nhow the composite mapping Î𝐿−𝑙\n𝑖=1 Hres\n𝐿−𝑖amplifies signals along the residual stream, we utilize\ntwo metrics. The first, based on the maximum absolute value of the row sums of the composite\nmapping, captures the worst-case expansion in the forward pass. The second, based on the\nmaximum absolute column sum, corresponds to the backward pass. We refer to these metrics\nas the Amax Gain Magnitude of the composite mapping. As shown in Fig. 3 (b), the Amax Gain\nMagnitude yields extreme values with peaks of 3000, a stark divergence from 1 that confirms\nthe presence of exploding residual streams.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "308", "text": "The first, based on the maximum absolute value of the row sums of the composite\nmapping, captures the worst-case expansion in the forward pass. The second, based on the\nmaximum absolute column sum, corresponds to the backward pass. We refer to these metrics\nas the Amax Gain Magnitude of the composite mapping. As shown in Fig. 3 (b), the Amax Gain\nMagnitude yields extreme values with peaks of 3000, a stark divergence from 1 that confirms\nthe presence of exploding residual streams.\n6\n0\n10000\n20000\n30000\n40000\n50000\nSteps\n-0.002\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\nAbsolute Loss Gap\n(a) Absolute Training Loss Gap vs.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "309", "text": "We refer to these metrics\nas the Amax Gain Magnitude of the composite mapping. As shown in Fig. 3 (b), the Amax Gain\nMagnitude yields extreme values with peaks of 3000, a stark divergence from 1 that confirms\nthe presence of exploding residual streams.\n6\n0\n10000\n20000\n30000\n40000\n50000\nSteps\n-0.002\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\nAbsolute Loss Gap\n(a) Absolute Training Loss Gap vs.  Training Steps\nmHC\nHC\n0\n10000\n20000\n30000\n40000\n50000\nSteps\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nGrad Norm\n(b) Gradient Norm vs.  Training Steps\nmHC\nHC\nFigure 2 | Training Instability of Hyper-Connections (HC).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "310", "text": "Training Steps\nmHC\nHC\n0\n10000\n20000\n30000\n40000\n50000\nSteps\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nGrad Norm\n(b) Gradient Norm vs.  Training Steps\nmHC\nHC\nFigure 2 | Training Instability of Hyper-Connections (HC). This figure illustrates (a) the absolute\nloss gap of HC relative to mHC, and (b) the comparisons of gradient norms. All results are based\non 27B models.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "311", "text": "Training Steps\nmHC\nHC\nFigure 2 | Training Instability of Hyper-Connections (HC). This figure illustrates (a) the absolute\nloss gap of HC relative to mHC, and (b) the comparisons of gradient norms. All results are based\non 27B models.\n0\n10\n20\n30\n40\n50\n60\nLayer Index l\n100\n101\nAmax Gain Magnitude\n(a) Single-Layer Mapping\nHres\nl  Forward Signal Gain\nHres\nl  Backward Gradient Gain\n0\n10\n20\n30\n40\n50\n60\nLayer Index l\n101\n102\n103\n104\n105\nAmax Gain Magnitude\n(b) Composite Mapping\nY\nl\ni = 1Hres\nl + 1 −i Forward Signal Gain\nY\n61 −l\ni = 1 Hres\n61 −i Backward Gradient Gain\nFigure 3 | Propagation Instability of Hyper-Connections (HC).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "312", "text": "This figure illustrates the\npropagation dynamics of (a) the single-layer mapping Hres\n𝑙\nand (b) the composite mapping\nÎ𝐿−𝑙\n𝑖=1 Hres\n𝐿−𝑖within the 27B model. The layer index 𝑙(x-axis) unrolls each standard Transformer\nblock into two independent layers (Attention and FFN). The Amax Gain Magnitude (y-axis) is\ncalculated as the maximum absolute row sum (for the forward signal) and column sum (for the\nbackward gradient), averaged over all tokens in a selected sequence.\n3.2. System Overhead\nWhile the computational complexity of HC remains manageable due to the linearity of the\nadditional mappings, the system-level overhead prevents a non-negligible challenge. Specifically,\nmemory access (I/O) costs often constitute one of the primary bottlenecks in modern model\narchitectures, which is widely referred to as the “memory wall” (Dao et al., 2022).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "313", "text": "3.2. System Overhead\nWhile the computational complexity of HC remains manageable due to the linearity of the\nadditional mappings, the system-level overhead prevents a non-negligible challenge. Specifically,\nmemory access (I/O) costs often constitute one of the primary bottlenecks in modern model\narchitectures, which is widely referred to as the “memory wall” (Dao et al., 2022). This bottleneck\nis frequently overlooked in architectural design, yet it decisively impacts runtime efficiency.\nFocusing on the widely adopted pre-norm Transformer (Vaswani et al., 2017) architecture,\nwe analyze the I/O patterns inherent to HC. Tab. 2 summarizes the per token memory access\noverhead in a single residual layer introduced by the 𝑛-stream residual design. The analysis\nreveals that HC increases the memory access cost by a factor approximately proportional to 𝑛.\nThis excessive I/O demand significantly degrades training throughput without the mitigation of\nfused kernels.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "314", "text": "This bottleneck\nis frequently overlooked in architectural design, yet it decisively impacts runtime efficiency.\nFocusing on the widely adopted pre-norm Transformer (Vaswani et al., 2017) architecture,\nwe analyze the I/O patterns inherent to HC. Tab. 2 summarizes the per token memory access\noverhead in a single residual layer introduced by the 𝑛-stream residual design. The analysis\nreveals that HC increases the memory access cost by a factor approximately proportional to 𝑛.\nThis excessive I/O demand significantly degrades training throughput without the mitigation of\nfused kernels. Besides, since Hpre\n𝑙\n, Hpost\n𝑙\n, and Hres\n𝑙\ninvolve learnable parameters, their interme-\ndiate activations are required for backpropagation. This results in a substantial increase in the\nGPU memory footprint, often necessitating gradient checkpointing to maintain feasible memory\nusage.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "315", "text": "The analysis\nreveals that HC increases the memory access cost by a factor approximately proportional to 𝑛.\nThis excessive I/O demand significantly degrades training throughput without the mitigation of\nfused kernels. Besides, since Hpre\n𝑙\n, Hpost\n𝑙\n, and Hres\n𝑙\ninvolve learnable parameters, their interme-\ndiate activations are required for backpropagation. This results in a substantial increase in the\nGPU memory footprint, often necessitating gradient checkpointing to maintain feasible memory\nusage. Furthermore, HC requires 𝑛-fold more communication cost in pipeline parallelism (Qi\net al., 2024), leading to larger bubbles and decreasing the training throughput.\n7\nTable 2 | Comparison of Memory Access Costs Per Token. This analysis accounts for the\noverhead introduced by the residual stream maintenance in the forward pass, excluding the\ninternal I/O of the layer function F .", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "316", "text": "Method\nOperation\nRead (Elements)\nWrite (Elements)\nResidual\nConnection\nResidual Merge\n2𝐶\n𝐶\nTotal I/O\n2C\nC\nHyper-\nConnections\nCalculate Hpre\n𝑙\n, Hpost\n𝑙\n, Hres\n𝑙\n𝑛𝐶\n𝑛2 + 2𝑛\nHpre\n𝑙\n𝑛𝐶+ 𝑛\n𝐶\nHpost\n𝑙\n𝐶+ 𝑛\n𝑛𝐶\nHres\n𝑙\n𝑛𝐶+ 𝑛2\n𝑛𝐶\nResidual Merge\n2𝑛𝐶\n𝑛𝐶\nTotal I/O\n(5n + 1)C + n2 + 2n\n(3n + 1)C + n2 + 2n\n4. Method\n4.1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "317", "text": "Method\n4.1. Manifold-Constrained Hyper-Connections\nDrawing inspiration from the identity mapping principle (He et al., 2016b), the core premise\nof mHC is to constrain the residual mapping Hres\n𝑙\nonto a specific manifold. While the original\nidentity mapping ensures stability by enforcing Hres\n𝑙\n= I, it fundamentally precludes information\nexchange within the residual stream, which is critical for maximizing the potential of multi-\nstream architectures. Therefore, we propose projecting the residual mapping onto a manifold\nthat simultaneously maintains the stability of signal propagation across layers and facilitates\nmutual interaction among residual streams to preserve the model’s expressivity. To this end,\nwe restrict Hres\n𝑙\nto be a doubly stochastic matrix, which has non-negative entries where both\nthe rows and columns sum to 1. Formally, let Mres denote the manifold of doubly stochastic\nmatrices (also known as the Birkhoff polytope).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "318", "text": "To this end,\nwe restrict Hres\n𝑙\nto be a doubly stochastic matrix, which has non-negative entries where both\nthe rows and columns sum to 1. Formally, let Mres denote the manifold of doubly stochastic\nmatrices (also known as the Birkhoff polytope). We constrain Hres\n𝑙\nto PMres(Hres\n𝑙\n), defined as:\nPMres(Hres\n𝑙\n) ≔\n\b\nHres\n𝑙\n∈R𝑛×𝑛| Hres\n𝑙\n1𝑛= 1𝑛, 1⊤\n𝑛Hres\n𝑙\n= 1⊤\n𝑛, Hres\n𝑙\n⩾0\n\t\n,\n(6)\nwhere 1𝑛represents the 𝑛-dimensional vector of all ones.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "319", "text": "It is worth noting that when 𝑛= 1, the doubly stochastic condition degenerates to the scalar\n1, thereby recovering the original identity mapping. The choice of double stochasticity confers\nseveral rigorous theoretical properties beneficial for large-scale model training:\n1. Norm Preservation: The spectral norm of a doubly stochastic matrix is bounded by 1\n(i.e., ∥Hres\n𝑙\n∥2 ≤1). This implies that the learnable mapping is non-expansive, effectively\nmitigating the gradient explosion problem.\n2. Compositional Closure: The set of doubly stochastic matrices is closed under matrix\nmultiplication. This ensures that the composite residual mapping across multiple layers,\nÎ𝐿−𝑙\n𝑖=1 Hres\n𝐿−𝑖, remains doubly stochastic, thereby preserving stability throughout the entire\ndepth of the model.\n3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "320", "text": "This implies that the learnable mapping is non-expansive, effectively\nmitigating the gradient explosion problem.\n2. Compositional Closure: The set of doubly stochastic matrices is closed under matrix\nmultiplication. This ensures that the composite residual mapping across multiple layers,\nÎ𝐿−𝑙\n𝑖=1 Hres\n𝐿−𝑖, remains doubly stochastic, thereby preserving stability throughout the entire\ndepth of the model.\n3. Geometric Interpretation via the Birkhoff Polytope: The set Mres forms the Birkhoff\npolytope, which is the convex hull of the set of permutation matrices. This provides a\nclear geometric interpretation: the residual mapping acts as a convex combination of\npermutations. Mathematically, the repeated application of such matrices tends to increase\n8\nthe mixing of information across streams monotonically, effectively functioning as a robust\nfeature fusion mechanism.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "321", "text": "3. Geometric Interpretation via the Birkhoff Polytope: The set Mres forms the Birkhoff\npolytope, which is the convex hull of the set of permutation matrices. This provides a\nclear geometric interpretation: the residual mapping acts as a convex combination of\npermutations. Mathematically, the repeated application of such matrices tends to increase\n8\nthe mixing of information across streams monotonically, effectively functioning as a robust\nfeature fusion mechanism.\nAdditionally, we impose non-negativity constraints on the input mappings Hpre\n𝑙\nand output\nmappings Hpost\n𝑙\n. This constrain prevents signal cancellation arising from the composition of\npositive and negative coefficients, which can also be considered as a special manifold projection.\n4.2. Parameterization and Manifold Projection\nIn this section, we detail the calculation process of Hpre\n𝑙\n, Hpost\n𝑙\n, and Hres\n𝑙\nin mHC.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "322", "text": "Additionally, we impose non-negativity constraints on the input mappings Hpre\n𝑙\nand output\nmappings Hpost\n𝑙\n. This constrain prevents signal cancellation arising from the composition of\npositive and negative coefficients, which can also be considered as a special manifold projection.\n4.2. Parameterization and Manifold Projection\nIn this section, we detail the calculation process of Hpre\n𝑙\n, Hpost\n𝑙\n, and Hres\n𝑙\nin mHC. Given the\ninput hidden matrix x𝑙∈R𝑛×𝐶at the 𝑙-th layer, we first flatten it into a vector ®x𝑙= vec(x𝑙) ∈R1×𝑛𝐶\nto preserve full context information. Then,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "323", "text": "Then, we follow the original HC formulation to get the\ndynamic mappings and the static mappings as follows:\n\n\n®x′\n𝑙= RMSNorm(®x𝑙)\n˜Hpre\n𝑙\n= 𝛼pre\n𝑙\n· (®x′\n𝑙𝜑pre\n𝑙\n) + bpre\n𝑙\n˜Hpost\n𝑙\n= 𝛼post\n𝑙\n· (®x′\n𝑙𝜑post\n𝑙\n) + bpost\n𝑙\n˜Hres\n𝑙\n= 𝛼res\n𝑙\n· mat(®x′\n𝑙𝜑res\n𝑙) + bres\n𝑙,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "324", "text": "(7)\nwhere 𝜑pre\n𝑙\n, 𝜑post\n𝑙\n∈R𝑛𝐶×𝑛and 𝜑res\n𝑙\n∈R𝑛𝐶×𝑛2 are linear projections for dynamic mappings and\nmat(·) is a reshape function from R1×𝑛2 to R𝑛×𝑛.\nThen, the final constrained mappings are obtained via:\n\n\nHpre\n𝑙\n= 𝜎( ˜Hpre\n𝑙\n)\nHpost\n𝑙\n= 2𝜎( ˜Hpost\n𝑙\n)\nHres\n𝑙\n= Sinkhorn-Knopp( ˜Hres\n𝑙\n),\n(8)\nwhere 𝜎(·) denotes the Sigmoid function.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "325", "text": "The Sinkhorn-Knopp(·) operator firstly makes all\nelements to be positive via an exponent operator and then conducts iterative normalization\nprocess that alternately rescales rows and columns to sum to 1. Specifically, given a positive\nmatrix M(0) = exp( ˜Hres\n𝑙\n) as the start point, the normalization iteration proceeds as:\nM(𝑡) = T𝑟\n\u0010\nT𝑐(M(𝑡−1))\n\u0011\n,\n(9)\nwhere T𝑟and T𝑐denote row and column normalization, respectively. This process converges to a\ndoubly stochastic matrix Hres\n𝑙\n= M(𝑡max) as 𝑡max →∞. We choose 𝑡max = 20 as a practical value in\nour experiments.\n4.3. Efficient Infrastructure Design\nIn this section, we detail the infrastructure design tailored for mHC.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "326", "text": "This process converges to a\ndoubly stochastic matrix Hres\n𝑙\n= M(𝑡max) as 𝑡max →∞. We choose 𝑡max = 20 as a practical value in\nour experiments.\n4.3. Efficient Infrastructure Design\nIn this section, we detail the infrastructure design tailored for mHC. Through rigorous optimiza-\ntion, we implement mHC (with 𝑛= 4) in large-scale models with a marginal training overhead\nof only 6.7%.\n4.3.1. Kernel Fusion\nObserving that RMSNorm in mHC imposes significant latency when operating on the high-\ndimensional hidden state ®x𝑙∈R1×𝑛𝐶, we reorder the dividing-by-norm operation to follow the\n9\nmatrix multiplication. This optimization maintains mathematical equivalence while improving\nefficiency.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "327", "text": "Through rigorous optimiza-\ntion, we implement mHC (with 𝑛= 4) in large-scale models with a marginal training overhead\nof only 6.7%.\n4.3.1. Kernel Fusion\nObserving that RMSNorm in mHC imposes significant latency when operating on the high-\ndimensional hidden state ®x𝑙∈R1×𝑛𝐶, we reorder the dividing-by-norm operation to follow the\n9\nmatrix multiplication. This optimization maintains mathematical equivalence while improving\nefficiency. Furthermore, we employ mixed-precision strategies to maximize numerical accuracy\nwithout compromising speed, and fuse multiple operations with shared memory access into\nunified compute kernels to reduce memory bandwidth bottlenecks. Based on the inputs and\nparameters detailed in Eq. (10) to (13), we implement three specialized mHC kernels to compute\nHpre\n𝑙\n, Hpost\n𝑙\n, and Hres\n𝑙\n.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "328", "text": "This optimization maintains mathematical equivalence while improving\nefficiency. Furthermore, we employ mixed-precision strategies to maximize numerical accuracy\nwithout compromising speed, and fuse multiple operations with shared memory access into\nunified compute kernels to reduce memory bandwidth bottlenecks. Based on the inputs and\nparameters detailed in Eq. (10) to (13), we implement three specialized mHC kernels to compute\nHpre\n𝑙\n, Hpost\n𝑙\n, and Hres\n𝑙\n. In these kernels, the biases and linear projections are consolidated into b𝑙\nand 𝜑𝑙, and the RMSNorm weight is also absorbed in 𝜑𝑙.\n• Eq. (14) to (15): We develop a unified kernel that fuses two scans on ®x𝑙, leveraging ma-\ntrix multiplication units to maximize memory bandwidth utilization. The backward\npass—comprising two matrix multiplications—is similarly consolidated into a single ker-\nnel, eliminating redundant reloading of ®x𝑙.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "329", "text": "In these kernels, the biases and linear projections are consolidated into b𝑙\nand 𝜑𝑙, and the RMSNorm weight is also absorbed in 𝜑𝑙.\n• Eq. (14) to (15): We develop a unified kernel that fuses two scans on ®x𝑙, leveraging ma-\ntrix multiplication units to maximize memory bandwidth utilization. The backward\npass—comprising two matrix multiplications—is similarly consolidated into a single ker-\nnel, eliminating redundant reloading of ®x𝑙. Both kernels feature a finely tuned pipeline\n(load, cast, compute, store) to efficiently handle mixed-precision processing.\n• Eq. (16) to (18): These lightweight operations on small coefficients are opportunistically\nfused into a single kernel, significantly reducing kernel launch overhead.\n• Eq. (19): We implement the Sinkhorn-Knopp iteration within a single kernel. For the\nbackward pass, we derive a custom backward kernel that recomputes the intermediate\nresults on-chip and traverses the entire iteration.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "330", "text": "Both kernels feature a finely tuned pipeline\n(load, cast, compute, store) to efficiently handle mixed-precision processing.\n• Eq. (16) to (18): These lightweight operations on small coefficients are opportunistically\nfused into a single kernel, significantly reducing kernel launch overhead.\n• Eq. (19): We implement the Sinkhorn-Knopp iteration within a single kernel. For the\nbackward pass, we derive a custom backward kernel that recomputes the intermediate\nresults on-chip and traverses the entire iteration.\n𝜑𝑙: tfloat32\n[𝑛𝐶, 𝑛2 + 2𝑛]\n(10)\n®x𝑙: bfloat16\n[1, 𝑛𝐶]\n(11)\n𝛼pre\n𝑙\n, 𝛼post\n𝑙\n, 𝛼res\n𝑙\n: float32\nScalars\n(12)\nb𝑙: float32\n[1,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "331", "text": "𝜑𝑙: tfloat32\n[𝑛𝐶, 𝑛2 + 2𝑛]\n(10)\n®x𝑙: bfloat16\n[1, 𝑛𝐶]\n(11)\n𝛼pre\n𝑙\n, 𝛼post\n𝑙\n, 𝛼res\n𝑙\n: float32\nScalars\n(12)\nb𝑙: float32\n[1, 𝑛2 + 2𝑛]\n(13)\nh ˜˜Hpre\n𝑙\n, ˜˜Hpost\n𝑙\n, ˜˜Hres\n𝑙\ni\n: float32\n= ®x𝑙𝜑𝑙\n(14)\n𝑟: float32\n=\n\r\r®x𝑙\n\r\r\n2 /\n√\n𝑛𝐶\n(15)\nh\n˜Hpre\n𝑙\n,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "332", "text": "𝑛2 + 2𝑛]\n(13)\nh ˜˜Hpre\n𝑙\n, ˜˜Hpost\n𝑙\n, ˜˜Hres\n𝑙\ni\n: float32\n= ®x𝑙𝜑𝑙\n(14)\n𝑟: float32\n=\n\r\r®x𝑙\n\r\r\n2 /\n√\n𝑛𝐶\n(15)\nh\n˜Hpre\n𝑙\n, ˜Hpost\n𝑙\n, ˜Hres\n𝑙\ni\n: float32\n= 1/𝑟\nh\n𝛼pre\n𝑙\n˜˜Hpre\n𝑙\n, 𝛼post\n𝑙\n˜˜Hpost\n𝑙\n,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "333", "text": "˜Hpost\n𝑙\n, ˜Hres\n𝑙\ni\n: float32\n= 1/𝑟\nh\n𝛼pre\n𝑙\n˜˜Hpre\n𝑙\n, 𝛼post\n𝑙\n˜˜Hpost\n𝑙\n, 𝛼res\n𝑙\n˜˜Hres\n𝑙\ni\n+ b𝑙\n(16)\nHpre\n𝑙\n: float32\n= 𝜎\n\u0010\n˜Hpre\n𝑙\n\u0011\n(17)\nHpost\n𝑙\n: float32\n= 2𝜎\n\u0010\n˜Hpost\n𝑙\n\u0011\n(18)\nHres\n𝑙\n: float32\n= Sinkhorn-Knopp \u0000 ˜Hres\n𝑙\n\u0001\n(19)\nUsing the coefficients derived from the aforementioned kernels,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "334", "text": "we introduce two addi-\ntional kernels to apply these mappings: one for Fpre ≔Hpre\n𝑙\nx𝑙and another for Fpost,res ≔\nHres\n𝑙\nx𝑙+ Hpost ⊤\n𝑙\nF (·, ·). Through fusing the application of Hpost\n𝑙\nand Hres\n𝑙\nwith residual merging,\nwe reduce the number of elements read from (3𝑛+ 1)𝐶to (𝑛+ 1)𝐶and the number of elements\nwritten from 3𝑛𝐶to 𝑛𝐶for this kernel. We efficiently implement the majority of kernels (ex-\ncluding Eq. (14) to (15)) using TileLang (Wang et al., 2025). This framework streamlines the\nimplementation of kernels with complex calculation process and allows us to fully utilize the\nmemory bandwidth with minimal engineering effort.\n4.3.2.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "335", "text": "We efficiently implement the majority of kernels (ex-\ncluding Eq. (14) to (15)) using TileLang (Wang et al., 2025). This framework streamlines the\nimplementation of kernels with complex calculation process and allows us to fully utilize the\nmemory bandwidth with minimal engineering effort.\n4.3.2. Recomputing\nThe 𝑛-stream residual design introduces substantial memory overhead during training. To\nmitigate this, we discard the intermediate activations of the mHC kernels after the forward pass\nand recompute them on-the-fly in the backward pass, through re-executing the mHC kernels\n10\nwithout the heavy layer function F . Consequently, for a block of 𝐿𝑟consecutive layers, we need\nonly store the input x𝑙0 to the first layer. Excluding lightweight coefficients while accounting\nfor the pre-norm with in F , Tab. 3 summarizes the intermediate activations preserved for the\nbackward pass.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "336", "text": "To\nmitigate this, we discard the intermediate activations of the mHC kernels after the forward pass\nand recompute them on-the-fly in the backward pass, through re-executing the mHC kernels\n10\nwithout the heavy layer function F . Consequently, for a block of 𝐿𝑟consecutive layers, we need\nonly store the input x𝑙0 to the first layer. Excluding lightweight coefficients while accounting\nfor the pre-norm with in F , Tab. 3 summarizes the intermediate activations preserved for the\nbackward pass.\nTable 3 | Stored and Recomputed Intermediate Activations We list per token activation pre-\nserved for the backward pass and the transient activation recomputed in 𝐿𝑟consecutive layers.\nLayer 𝑙0 represents the first layer in 𝐿𝑟layers and layer 𝑙is in [𝑙0, 𝑙0 + 𝐿𝑟−1].", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "337", "text": "Layer 𝑙0 represents the first layer in 𝐿𝑟layers and layer 𝑙is in [𝑙0, 𝑙0 + 𝐿𝑟−1].\nActivations\nx𝑙0\nF (Hpre\n𝑙\nx𝑙, W𝑙)\nx𝑙\nHpre\n𝑙\nx𝑙\nRMSNorm(Hpre\n𝑙\nx𝑙)\nSize (Elements)\n𝑛𝐶\n𝐶\n𝑛𝐶\n𝐶\n𝐶\nStored Method\nEvery 𝐿𝑟layers\nEvery layer\nTransient inside 𝐿𝑟layers\nSince mHC kernels recomputation is performed for blocks of 𝐿𝑟consecutive layers, given\na total of 𝐿layers, we must persistently store the first layer input x𝑙0 for all ⌈𝐿\n𝐿𝑟⌉blocks for the\nbackward pass.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "338", "text": "In addition to this resident memory, the recomputation process introduces a\ntransient memory overhead of (𝑛+ 2)𝐶× 𝐿𝑟elements for the active block, which determines the\npeak memory usage during backpropagation. Consequently, we determine the optimal block\nsize 𝐿∗\n𝑟by minimizing the total memory footprint corresponded to 𝐿𝑟:\n𝐿∗\n𝑟= arg min\n𝐿𝑟\n\u0014\n𝑛𝐶×\n\u0018 𝐿\n𝐿𝑟\n\u0019\n+ (𝑛+ 2)𝐶× 𝐿𝑟\n\u0015\n≈\n√︂\n𝑛𝐿\n𝑛+ 2.\n(20)\nFurthermore, pipeline parallelism in large-scale training imposes a constraint: recomputation\nblocks must not cross pipeline stage boundaries.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "339", "text": "(20)\nFurthermore, pipeline parallelism in large-scale training imposes a constraint: recomputation\nblocks must not cross pipeline stage boundaries. Observing that the theoretical optimum 𝐿∗\n𝑟\ntypically aligns with the number of layers per pipeline stage, we choose to synchronize the\nrecomputation boundaries with the pipeline stages.\n4.3.3. Overlapping Communication in DualPipe\nIn large-scale training, pipeline parallelism is the standard practice for mitigating parameter and\ngradient memory footprints. Specifically, we adopt the DualPipe schedule (Liu et al., 2024b),\nwhich effectively overlaps scale-out interconnected communication traffic, such as those in\nexpert and pipeline parallelism. However, compared to the single-stream design, the proposed\n𝑛-stream residual in mHC incurs substantial communication latency across pipeline stages.\nFurthermore, at stage boundaries, the recomputation of mHC kernels for all 𝐿𝑟layers introduces\nnon-negligible computational overhead. To address these bottlenecks, we extend the DualPipe\nschedule (see Fig.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "340", "text": "Specifically, we adopt the DualPipe schedule (Liu et al., 2024b),\nwhich effectively overlaps scale-out interconnected communication traffic, such as those in\nexpert and pipeline parallelism. However, compared to the single-stream design, the proposed\n𝑛-stream residual in mHC incurs substantial communication latency across pipeline stages.\nFurthermore, at stage boundaries, the recomputation of mHC kernels for all 𝐿𝑟layers introduces\nnon-negligible computational overhead. To address these bottlenecks, we extend the DualPipe\nschedule (see Fig. 4) to facilitate improved overlapping of communication and computation at\npipeline stage boundaries.\nNotably, to prevent blocking the communication stream, we execute the Fpost,res kernels\nof MLP (i.e. FFN) layers on a dedicated high-priority compute stream. We further refrain\nfrom employing persistent kernels for long-running operations in attention layers, thereby\npreventing extended stalls.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "341", "text": "To address these bottlenecks, we extend the DualPipe\nschedule (see Fig. 4) to facilitate improved overlapping of communication and computation at\npipeline stage boundaries.\nNotably, to prevent blocking the communication stream, we execute the Fpost,res kernels\nof MLP (i.e. FFN) layers on a dedicated high-priority compute stream. We further refrain\nfrom employing persistent kernels for long-running operations in attention layers, thereby\npreventing extended stalls. This design enables the preemption of overlapped attention com-\nputations, allowing for flexible scheduling while maintaining high utilization of the compute\ndevice’s processing units. Furthermore, the recomputation process is decoupled from pipeline\ncommunication dependencies, as the initial activation of each stage x𝑙0 is already cached locally.\n11\nMLP (B)\nDISPATCH (B)\nDISPATCH (F)\nMLP (W)\nMLP (F)\nATTN (B)\nCOMBINE (F)\nCOMBINE (B)\nPP Send Recv (F)\nPP Send Recv (B)\nℱ!", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "342", "text": "This design enables the preemption of overlapped attention com-\nputations, allowing for flexible scheduling while maintaining high utilization of the compute\ndevice’s processing units. Furthermore, the recomputation process is decoupled from pipeline\ncommunication dependencies, as the initial activation of each stage x𝑙0 is already cached locally.\n11\nMLP (B)\nDISPATCH (B)\nDISPATCH (F)\nMLP (W)\nMLP (F)\nATTN (B)\nCOMBINE (F)\nCOMBINE (B)\nPP Send Recv (F)\nPP Send Recv (B)\nℱ!\"#$, '(#\n)\n(F)\nℱ!\"#$, '(#\n)\n(B)\nℱ!\"#$, '(#\n*\n(B)\nℱ!'(\n) (B)\nATTN (W)\nATTN (F)\nWhole Stage \nRecompute (B)\nℱ!'(\n* (B)\nℱ!'(\n* (F)\nℱ!\"#$, '(#\n*\n(F)\nℱ!", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "343", "text": "\"#$, '(#\n)\n(F)\nℱ!\"#$, '(#\n)\n(B)\nℱ!\"#$, '(#\n*\n(B)\nℱ!'(\n) (B)\nATTN (W)\nATTN (F)\nWhole Stage \nRecompute (B)\nℱ!'(\n* (B)\nℱ!'(\n* (F)\nℱ!\"#$, '(#\n*\n(F)\nℱ!'(\n) (F)\nNormal Compute Stream\nCommunication Stream\nHigh Priority Compute Stream\nFigure 4 | Communication-Computation Overlapping for mHC. We extend the DualPipe\nschedule to handle the overhead introduced by mHC. Lengths of each block are illustrative only\nand do not represent actual duration. (F), (B), (W) refers to forward pass, backward pass, weight\ngradient computation, respectively. F A and F M represents kernels corresponded to Attention\nand MLP, respectively.\n5. Experiments\n5.1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "344", "text": "'(\n) (F)\nNormal Compute Stream\nCommunication Stream\nHigh Priority Compute Stream\nFigure 4 | Communication-Computation Overlapping for mHC. We extend the DualPipe\nschedule to handle the overhead introduced by mHC. Lengths of each block are illustrative only\nand do not represent actual duration. (F), (B), (W) refers to forward pass, backward pass, weight\ngradient computation, respectively. F A and F M represents kernels corresponded to Attention\nand MLP, respectively.\n5. Experiments\n5.1. Experimental Setup\nWe validate the proposed method via language model pre-training, conducting a comparative\nanalysis between the baseline, HC, and our proposed mHC. Utilizing MoE architectures inspired\nby DeepSeek-V3 (Liu et al., 2024b), we train four distinct model variants to cover different\nevaluation regimes. Specifically, the expansion rate 𝑛for both HC and mHC is set to 4.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "345", "text": "F A and F M represents kernels corresponded to Attention\nand MLP, respectively.\n5. Experiments\n5.1. Experimental Setup\nWe validate the proposed method via language model pre-training, conducting a comparative\nanalysis between the baseline, HC, and our proposed mHC. Utilizing MoE architectures inspired\nby DeepSeek-V3 (Liu et al., 2024b), we train four distinct model variants to cover different\nevaluation regimes. Specifically, the expansion rate 𝑛for both HC and mHC is set to 4. Our\nprimary focus is a 27B model trained with a dataset size proportional to its parameters, which\nserves as the subject for our system-level main results. Expanding on this, we analyze the\ncompute scaling behavior by incorporating smaller 3B and 9B models trained with proportional\ndata, which allows us to observe performance trends across varying compute. Additionally,\nto specifically investigate the token scaling behavior, we train a separate 3B model on a fixed\ncorpus of 1 trillion tokens.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "346", "text": "Our\nprimary focus is a 27B model trained with a dataset size proportional to its parameters, which\nserves as the subject for our system-level main results. Expanding on this, we analyze the\ncompute scaling behavior by incorporating smaller 3B and 9B models trained with proportional\ndata, which allows us to observe performance trends across varying compute. Additionally,\nto specifically investigate the token scaling behavior, we train a separate 3B model on a fixed\ncorpus of 1 trillion tokens. Detailed model configurations and training hyper-parameters are\nprovided in Appendix A.1.\n5.2. Main Results\n10000\n20000\n30000\n40000\n50000\nSteps\n-0.06\n-0.04\n-0.02\n0.00\nAbsolute Loss Gap\n(a) Absolute Training Loss Gap vs.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "347", "text": "Additionally,\nto specifically investigate the token scaling behavior, we train a separate 3B model on a fixed\ncorpus of 1 trillion tokens. Detailed model configurations and training hyper-parameters are\nprovided in Appendix A.1.\n5.2. Main Results\n10000\n20000\n30000\n40000\n50000\nSteps\n-0.06\n-0.04\n-0.02\n0.00\nAbsolute Loss Gap\n(a) Absolute Training Loss Gap vs.  Training Steps\nBaseline\nHC\nmHC\n10000\n20000\n30000\n40000\n50000\nSteps\n0.00\n0.05\n0.10\n0.15\n0.20\nGrad Norm\n(b) Gradient Norm vs.  Training Steps\nBaseline\nHC\nmHC\nFigure 5 | Training Stability of Manifold-Constrained Hyper-Connections (mHC). This figure\nillustrates (a) the absolute training loss gap of mHC and HC relative to the baseline, and (b)\nthe gradient norm of the three methods.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "348", "text": "Training Steps\nBaseline\nHC\nmHC\n10000\n20000\n30000\n40000\n50000\nSteps\n0.00\n0.05\n0.10\n0.15\n0.20\nGrad Norm\n(b) Gradient Norm vs.  Training Steps\nBaseline\nHC\nmHC\nFigure 5 | Training Stability of Manifold-Constrained Hyper-Connections (mHC). This figure\nillustrates (a) the absolute training loss gap of mHC and HC relative to the baseline, and (b)\nthe gradient norm of the three methods. All experiments utilize the 27B model. The results\ndemonstrate that mHC exhibits improved stability in terms of both loss and gradient norm.\nWe begin by examining the training stability and convergence of the 27B models. As\nillustrated in Fig. 5 (a), mHC effectively mitigates the training instability observed in HC,\nachieving a final loss reduction of 0.021 compared to the baseline. This improved stability is\nfurther corroborated by the gradient norm analysis in Fig.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "349", "text": "All experiments utilize the 27B model. The results\ndemonstrate that mHC exhibits improved stability in terms of both loss and gradient norm.\nWe begin by examining the training stability and convergence of the 27B models. As\nillustrated in Fig. 5 (a), mHC effectively mitigates the training instability observed in HC,\nachieving a final loss reduction of 0.021 compared to the baseline. This improved stability is\nfurther corroborated by the gradient norm analysis in Fig. 5 (b), where mHC exhibits significantly\nbetter behavior than HC, maintaining a stable profile comparable to the baseline.\n12\nTable 4 | System-level Benchmark Results for 27B Models. This table compares the zero-\nshot and few-shot performance of the Baseline, HC, and mHC across 8 diverse downstream\nbenchmarks. mHC consistently outperforms the Baseline and surpasses HC on the majority of\nbenchmarks, demonstrating its effectiveness in large-scale pre-training.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "350", "text": "This improved stability is\nfurther corroborated by the gradient norm analysis in Fig. 5 (b), where mHC exhibits significantly\nbetter behavior than HC, maintaining a stable profile comparable to the baseline.\n12\nTable 4 | System-level Benchmark Results for 27B Models. This table compares the zero-\nshot and few-shot performance of the Baseline, HC, and mHC across 8 diverse downstream\nbenchmarks. mHC consistently outperforms the Baseline and surpasses HC on the majority of\nbenchmarks, demonstrating its effectiveness in large-scale pre-training.\nBenchmark\nBBH\nDROP\nGSM8K\nHellaSwag\nMATH\nMMLU\nPIQA\nTriviaQA\n(Metric)\n(EM)\n(F1)\n(EM)\n(Acc.)\n(EM)\n(Acc.)\n(Acc.)", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "351", "text": "mHC consistently outperforms the Baseline and surpasses HC on the majority of\nbenchmarks, demonstrating its effectiveness in large-scale pre-training.\nBenchmark\nBBH\nDROP\nGSM8K\nHellaSwag\nMATH\nMMLU\nPIQA\nTriviaQA\n(Metric)\n(EM)\n(F1)\n(EM)\n(Acc.)\n(EM)\n(Acc.)\n(Acc.)\n(EM)\n# Shots\n3-shot\n3-shot\n8-shot\n10-shot\n4-shot\n5-shot\n0-shot\n5-shot\n27B Baseline\n43.8\n47.0\n46.7\n73.7\n22.0\n59.0\n78.5\n54.3\n27B w/ HC\n48.9\n51.6\n53.2\n74.3\n26.4\n63.0\n79.9\n56.3\n27B w/ mHC\n51.0\n53.9\n53.8\n74.7\n26.0\n63.4\n80.5\n57.6\nTab.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "352", "text": "4 presents the downstream performance across a diverse set of benchmarks (Bisk et al.,\n2020; Cobbe et al., 2021; Hendrycks et al., 2020, 2021; Joshi et al., 2017; Zellers et al., 2019). mHC\nyields comprehensive improvements, consistently outperforming the baseline and surpassing\nHC on the majority of tasks. Notably, compared to HC, mHC further enhances the model’s\nreasoning capabilities, delivering performance gains of 2.1% on BBH (Suzgun et al., 2022) and\n2.3% on DROP (Dua et al., 2019).\n5.3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "353", "text": "5.3. Scaling Experiments\n1021\n1022\nFLOPs\n-0.04\n-0.03\n-0.02\n-0.01\n0.00\n0.01\n0.02\nAbsolute Loss Gap\nBaseline\nmHC\n1021\n1022\nFLOPs\n98.0%\n99.0%\n100.0%\n101.0%\nRelative Loss Ratio\nBaseline\nmHC\n2\n4\nFLOPs\n×1021\n-0.03\n-0.02\n-0.01\n0.00\n0.01\nAbsolute Loss Gap\nBaseline\nmHC\n2\n4\nFLOPs\n×1021\n98.0%\n99.0%\n100.0%\n101.0%\nRelative Loss Ratio\nBaseline\nmHC\n(a) Compute Scaling Curve\n(b) Token Scaling Curve\nFigure 6 | Scaling properties of mHC compared to the Baseline. (a) Compute Scaling Curve.\nSolid lines depict the performance gap across different compute budgets.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "354", "text": "(a) Compute Scaling Curve.\nSolid lines depict the performance gap across different compute budgets. Each point represents\na specific compute-optimal configuration of model size and dataset size, scaling from 3B and 9B\nto 27B parameters. (b) Token Scaling Curve. Trajectory of the 3B model during training. Each\npoint represents the model’s performance at different training tokens. Detailed architectures\nand training configurations are provided in Appendix A.1.\nTo assess the scalability of our approach, we report the relative loss improvement of mHC\nagainst the baseline across different scales. In Fig. 6 (a), we plot the compute scaling curve\nspanning 3B, 9B, and 27B parameters. The trajectory indicates that the performance advantage is\nrobustly maintained even at higher computational budgets, showing only marginal attenuation.\nFurthermore, we examine the within-run dynamics in Fig. 6 (b), which presents the token\nscaling curve for the 3B model. Collectively, these findings validate the effectiveness of mHC\nin large-scale scenarios.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "355", "text": "In Fig. 6 (a), we plot the compute scaling curve\nspanning 3B, 9B, and 27B parameters. The trajectory indicates that the performance advantage is\nrobustly maintained even at higher computational budgets, showing only marginal attenuation.\nFurthermore, we examine the within-run dynamics in Fig. 6 (b), which presents the token\nscaling curve for the 3B model. Collectively, these findings validate the effectiveness of mHC\nin large-scale scenarios. This conclusion is further corroborated by our in-house large-scale\ntraining experiments.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "356", "text": "Collectively, these findings validate the effectiveness of mHC\nin large-scale scenarios. This conclusion is further corroborated by our in-house large-scale\ntraining experiments.\n13\n0\n10\n20\n30\n40\n50\n60\nLayer Index l\n0.0\n0.5\n1.0\n1.5\n2.0\nAmax Gain Magnitude\n(a) Single-Layer Mapping\nPMres(Hres\nl ) Forward Signal Gain\nPMres(Hres\nl ) Backward Gradient Gain\n0\n10\n20\n30\n40\n50\n60\nLayer Index l\n0.0\n0.5\n1.0\n1.5\n2.0\nAmax Gain Magnitude\n(b) Composite Mapping\nY\nl\ni = 1PMres(Hres\nl + 1 −i) Forward Signal Gain\nY\n61 −l\ni = 1 PMres(Hres\n61 −i) Backward Gradient Gain\nFigure 7 | Propagation Stability of Manifold-Constrained Hyper-Connections (mHC).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "357", "text": "This\nfigure illustrates the propagation dynamics of (a) the single-layer mapping PMres(Hres\n𝑙\n) and (b)\nthe composite mapping Î𝐿−𝑙\n𝑖=1 PMres(Hres\n𝐿−𝑖) within the 27B model. The results demonstrate that\nmHC significantly enhances propagation stability compared to HC.\n-6.81 -6.81 -6.81 -6.81\n18.73\n-15.29\n-14.79\n-15.88\n5.43\n4.43\n4.43\n4.43\n-4.07 -3.07 -4.07 -4.07\n-3.95 -3.95 -2.95 -3.95\n-4.22 -4.22 -4.22 -3.22\nHC\nHres\n1\n0.83\n0.73\n0.66\n0.75\n0.84\n0.67\n0.49\n0.96\n0.94 -0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "358", "text": "29\n-14.79\n-15.88\n5.43\n4.43\n4.43\n4.43\n-4.07 -3.07 -4.07 -4.07\n-3.95 -3.95 -2.95 -3.95\n-4.22 -4.22 -4.22 -3.22\nHC\nHres\n1\n0.83\n0.73\n0.66\n0.75\n0.84\n0.67\n0.49\n0.96\n0.94 -0.07 -0.05 0.02\n-0.08 0.89 -0.07 -0.07\n-0.10 -0.14 0.81 -0.07\n0.06\n0.05 -0.03 0.87\nHres\n30\n-11.97 -6.86 -10.23-11.89\n-21.64\n-20.22\n22.50\n-21.59\n-5.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "359", "text": "75\n0.84\n0.67\n0.49\n0.96\n0.94 -0.07 -0.05 0.02\n-0.08 0.89 -0.07 -0.07\n-0.10 -0.14 0.81 -0.07\n0.06\n0.05 -0.03 0.87\nHres\n30\n-11.97 -6.86 -10.23-11.89\n-21.64\n-20.22\n22.50\n-21.59\n-5.58 -3.74 -5.71 -6.60\n-6.06 -2.27 -5.33 -6.57\n6.08\n3.12\n6.53\n6.77\n-6.41 -3.97 -5.72 -5.49\nHres\n60\n1.22\n1.04\n1.06\n1.02\n-1.35\n6.47\n0.03\n-0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "360", "text": "89\n-21.64\n-20.22\n22.50\n-21.59\n-5.58 -3.74 -5.71 -6.60\n-6.06 -2.27 -5.33 -6.57\n6.08\n3.12\n6.53\n6.77\n-6.41 -3.97 -5.72 -5.49\nHres\n60\n1.22\n1.04\n1.06\n1.02\n-1.35\n6.47\n0.03\n-0.81\n-0.38 -0.33 -0.34 -0.31\n1.81\n1.56\n1.58\n1.51\n0.01 -0.00 0.01\n0.01\n-0.23 -0.19 -0.20 -0.19\nY\n30\ni = 1Hres\n31 −i\n-135.4-133.4-489.0273.3\n-251.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "361", "text": "04\n1.06\n1.02\n-1.35\n6.47\n0.03\n-0.81\n-0.38 -0.33 -0.34 -0.31\n1.81\n1.56\n1.58\n1.51\n0.01 -0.00 0.01\n0.01\n-0.23 -0.19 -0.20 -0.19\nY\n30\ni = 1Hres\n31 −i\n-135.4-133.4-489.0273.3\n-251.4\n-243.0\n264.6\n-254.8\n-69.9 -68.3 -255.3142.1\n-69.1 -66.1 -247.4139.6\n74.8\n72.7 268.9-151.8\n-71.2 -71.8 -255.2143.3\nY\n30\ni = 1Hres\n61 −i\n-259.2-219.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "362", "text": "4-133.4-489.0273.3\n-251.4\n-243.0\n264.6\n-254.8\n-69.9 -68.3 -255.3142.1\n-69.1 -66.1 -247.4139.6\n74.8\n72.7 268.9-151.8\n-71.2 -71.8 -255.2143.3\nY\n30\ni = 1Hres\n61 −i\n-259.2-219.1-228.2-221.0\n-475.3\n-462.8\n509.1\n-498.5\n-132.8-112.2-117.0-113.3\n-129.3-109.3-113.9-110.3\n142.3 120.2 125.3 121.3\n-139.3-117.8-122.6-118.7\nY\n60\ni = 1Hres\n61 −i\n0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "363", "text": "2-219.1-228.2-221.0\n-475.3\n-462.8\n509.1\n-498.5\n-132.8-112.2-117.0-113.3\n-129.3-109.3-113.9-110.3\n142.3 120.2 125.3 121.3\n-139.3-117.8-122.6-118.7\nY\n60\ni = 1Hres\n61 −i\n0.98\n1.00\n0.98\n1.04\n1.00\n1.00\n1.00\n1.00\n0.67\n0.09\n0.03\n0.22\n0.26\n0.48\n0.26\n0.00\n0.03\n0.24\n0.00\n0.73\n0.03\n0.20\n0.69\n0.09\nmHC\nPMres(Hres\n1 )\n0.96\n1.02\n1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "364", "text": "98\n1.00\n0.98\n1.04\n1.00\n1.00\n1.00\n1.00\n0.67\n0.09\n0.03\n0.22\n0.26\n0.48\n0.26\n0.00\n0.03\n0.24\n0.00\n0.73\n0.03\n0.20\n0.69\n0.09\nmHC\nPMres(Hres\n1 )\n0.96\n1.02\n1.04\n0.99\n1.00\n1.00\n1.00\n1.00\n0.96\n0.01\n0.00\n0.04\n0.00\n0.97\n0.03\n0.00\n0.00\n0.00\n1.00\n0.00\n0.00\n0.04\n0.01\n0.95\nPMres(Hres\n30 )\n1.00\n1.01\n0.99\n1.00\n1.00\n1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "365", "text": "96\n1.02\n1.04\n0.99\n1.00\n1.00\n1.00\n1.00\n0.96\n0.01\n0.00\n0.04\n0.00\n0.97\n0.03\n0.00\n0.00\n0.00\n1.00\n0.00\n0.00\n0.04\n0.01\n0.95\nPMres(Hres\n30 )\n1.00\n1.01\n0.99\n1.00\n1.00\n1.00\n1.00\n1.00\n0.92\n0.06\n0.01\n0.01\n0.05\n0.81\n0.01\n0.13\n0.00\n0.01\n0.97\n0.02\n0.03\n0.13\n0.00\n0.84\nPMres(Hres\n60 )\n0.90\n1.06\n0.93\n1.11\n1.00\n1.00\n1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "366", "text": "00\n1.01\n0.99\n1.00\n1.00\n1.00\n1.00\n1.00\n0.92\n0.06\n0.01\n0.01\n0.05\n0.81\n0.01\n0.13\n0.00\n0.01\n0.97\n0.02\n0.03\n0.13\n0.00\n0.84\nPMres(Hres\n60 )\n0.90\n1.06\n0.93\n1.11\n1.00\n1.00\n1.00\n1.00\n0.30\n0.25\n0.22\n0.24\n0.24\n0.25\n0.25\n0.26\n0.20\n0.24\n0.28\n0.28\n0.17\n0.32\n0.18\n0.34\nY\n30\ni = 1PMres(Hres\n31 −i)\n0.41\n1.50\n1.50\n0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "367", "text": "90\n1.06\n0.93\n1.11\n1.00\n1.00\n1.00\n1.00\n0.30\n0.25\n0.22\n0.24\n0.24\n0.25\n0.25\n0.26\n0.20\n0.24\n0.28\n0.28\n0.17\n0.32\n0.18\n0.34\nY\n30\ni = 1PMres(Hres\n31 −i)\n0.41\n1.50\n1.50\n0.60\n1.00\n1.00\n1.00\n1.00\n0.35\n0.28\n0.17\n0.20\n0.03\n0.62\n0.29\n0.07\n0.01\n0.17\n0.80\n0.02\n0.02\n0.42\n0.25\n0.31\nY\n30\ni = 1PMres(Hres\n61 −i)\n0.88\n1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "368", "text": "41\n1.50\n1.50\n0.60\n1.00\n1.00\n1.00\n1.00\n0.35\n0.28\n0.17\n0.20\n0.03\n0.62\n0.29\n0.07\n0.01\n0.17\n0.80\n0.02\n0.02\n0.42\n0.25\n0.31\nY\n30\ni = 1PMres(Hres\n61 −i)\n0.88\n1.03\n1.00\n1.11\n1.00\n1.01\n1.01\n1.00\n0.24\n0.26\n0.23\n0.27\n0.23\n0.25\n0.26\n0.27\n0.21\n0.25\n0.27\n0.28\n0.21\n0.27\n0.24\n0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "369", "text": "02\n0.42\n0.25\n0.31\nY\n30\ni = 1PMres(Hres\n61 −i)\n0.88\n1.03\n1.00\n1.11\n1.00\n1.01\n1.01\n1.00\n0.24\n0.26\n0.23\n0.27\n0.23\n0.25\n0.26\n0.27\n0.21\n0.25\n0.27\n0.28\n0.21\n0.27\n0.24\n0.29\nY\n60\ni = 1PMres(Hres\n61 −i)\nFigure 8 | Visualizations of Learnable Mappings. This figure displays representative single-\nlayer and composite mappings for HC (first row) and mHC (second row). Each matrix is\ncomputed by averaging over all tokens within a selected sequence. The labels annotated along\nthe y-axis and x-axis indicate the forward signal gain (row sum) and the backward gradient gain\n(column sum), respectively.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "370", "text": "25\n0.27\n0.28\n0.21\n0.27\n0.24\n0.29\nY\n60\ni = 1PMres(Hres\n61 −i)\nFigure 8 | Visualizations of Learnable Mappings. This figure displays representative single-\nlayer and composite mappings for HC (first row) and mHC (second row). Each matrix is\ncomputed by averaging over all tokens within a selected sequence. The labels annotated along\nthe y-axis and x-axis indicate the forward signal gain (row sum) and the backward gradient gain\n(column sum), respectively.\n5.4. Stability Analysis\nSimilar to Fig. 3, Fig. 7 illustrates the propagation stability of mHC. Ideally, the single-layer\nmapping satisfies the doubly stochastic constraint, implying that both the forward signal gain\nand the backward gradient gain should equal to 1. However, practice implementations utilizing\nthe Sinkhorn-Knopp algorithm must limit the number of iterations to achieve computational\nefficiency.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "371", "text": "The labels annotated along\nthe y-axis and x-axis indicate the forward signal gain (row sum) and the backward gradient gain\n(column sum), respectively.\n5.4. Stability Analysis\nSimilar to Fig. 3, Fig. 7 illustrates the propagation stability of mHC. Ideally, the single-layer\nmapping satisfies the doubly stochastic constraint, implying that both the forward signal gain\nand the backward gradient gain should equal to 1. However, practice implementations utilizing\nthe Sinkhorn-Knopp algorithm must limit the number of iterations to achieve computational\nefficiency. In our settings, we use 20 iterations to obtain an approximate solution. Consequently,\nas shown in Fig. 7(a), the backward gradient gain deviates slightly from 1. In the composite case\nshown in Fig. 7(b), the deviation increases but remains bounded, reaching a maximum value\nof approximately 1.6. Notably, compared to the maximum gain magnitude of nearly 3000 in\nHC, mHC significantly reduces it by three orders of magnitude.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "372", "text": "In our settings, we use 20 iterations to obtain an approximate solution. Consequently,\nas shown in Fig. 7(a), the backward gradient gain deviates slightly from 1. In the composite case\nshown in Fig. 7(b), the deviation increases but remains bounded, reaching a maximum value\nof approximately 1.6. Notably, compared to the maximum gain magnitude of nearly 3000 in\nHC, mHC significantly reduces it by three orders of magnitude. These results demonstrate that\nmHC significantly enhances propagation stability compared to HC, ensuring stable forward\nsignal and backward gradient flows. Additionally, Fig. 8 displays representative mappings. We\nobserve that for HC, when the maximum gain is large, other values also tend to be significant,\nwhich indicates general instability across all propagation paths. In contrast, mHC consistently\nyields stable results.\n14\n6.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "373", "text": "Notably, compared to the maximum gain magnitude of nearly 3000 in\nHC, mHC significantly reduces it by three orders of magnitude. These results demonstrate that\nmHC significantly enhances propagation stability compared to HC, ensuring stable forward\nsignal and backward gradient flows. Additionally, Fig. 8 displays representative mappings. We\nobserve that for HC, when the maximum gain is large, other values also tend to be significant,\nwhich indicates general instability across all propagation paths. In contrast, mHC consistently\nyields stable results.\n14\n6. Conclusion and Outlook\nIn this paper, we identify that while expanding the width of residual stream and diversifying\nconnections yields performance gains as proposed in Hyper-Connections (HC), the uncon-\nstrained nature of these connections leads to signal divergence. This disruption compromises\nthe conservation of signal energy across layers, inducing training instability and hindering the\nscalability of deep networks. To address these challenges, we introduce Manifold-Constrained\nHyper-Connections (mHC), a generalized framework that projects the residual connection space\nonto a specific manifold.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "374", "text": "In contrast, mHC consistently\nyields stable results.\n14\n6. Conclusion and Outlook\nIn this paper, we identify that while expanding the width of residual stream and diversifying\nconnections yields performance gains as proposed in Hyper-Connections (HC), the uncon-\nstrained nature of these connections leads to signal divergence. This disruption compromises\nthe conservation of signal energy across layers, inducing training instability and hindering the\nscalability of deep networks. To address these challenges, we introduce Manifold-Constrained\nHyper-Connections (mHC), a generalized framework that projects the residual connection space\nonto a specific manifold. By employing the Sinkhorn-Knopp algorithm to enforce a doubly\nstochastic constraint on residual mappings, mHC transforms signal propagation into a convex\ncombination of features. Empirical results confirm that mHC effectively restores the identity\nmapping property, enabling stable large-scale training with superior scalability compared to\nconventional HC. Crucially, through efficient infrastructure-level optimizations, mHC delivers\nthese improvements with negligible computational overhead.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "375", "text": "To address these challenges, we introduce Manifold-Constrained\nHyper-Connections (mHC), a generalized framework that projects the residual connection space\nonto a specific manifold. By employing the Sinkhorn-Knopp algorithm to enforce a doubly\nstochastic constraint on residual mappings, mHC transforms signal propagation into a convex\ncombination of features. Empirical results confirm that mHC effectively restores the identity\nmapping property, enabling stable large-scale training with superior scalability compared to\nconventional HC. Crucially, through efficient infrastructure-level optimizations, mHC delivers\nthese improvements with negligible computational overhead.\nAs a generalized extension of the HC paradigm, mHC opens several promising avenues for\nfuture research. Although this work utilizes doubly stochastic matrices to ensure stability, the\nframework accommodates the exploration of diverse manifold constraints tailored to specific\nlearning objectives. We anticipate that further investigation into distinct geometric constraints\ncould yield novel methods that better optimize the trade-off between plasticity and stability.\nFurthermore, we hope mHC rejuvenates community interest in macro-architecture design.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "376", "text": "Crucially, through efficient infrastructure-level optimizations, mHC delivers\nthese improvements with negligible computational overhead.\nAs a generalized extension of the HC paradigm, mHC opens several promising avenues for\nfuture research. Although this work utilizes doubly stochastic matrices to ensure stability, the\nframework accommodates the exploration of diverse manifold constraints tailored to specific\nlearning objectives. We anticipate that further investigation into distinct geometric constraints\ncould yield novel methods that better optimize the trade-off between plasticity and stability.\nFurthermore, we hope mHC rejuvenates community interest in macro-architecture design.\nBy deepening the understanding of how topological structures influence optimization and\nrepresentation learning, mHC will help address current limitations and potentially illuminate\nnew pathways for the evolution of next-generation foundational architectures.\nReferences\nJ. Ainslie, J. Lee-Thorp, M. De Jong, Y. Zemlyanskiy, F. Lebrón, and S. Sanghai. Gqa: Training\ngeneralized multi-query transformer models from multi-head checkpoints. arXiv preprint\narXiv:2305.13245, 2023.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "377", "text": "By deepening the understanding of how topological structures influence optimization and\nrepresentation learning, mHC will help address current limitations and potentially illuminate\nnew pathways for the evolution of next-generation foundational architectures.\nReferences\nJ. Ainslie, J. Lee-Thorp, M. De Jong, Y. Zemlyanskiy, F. Lebrón, and S. Sanghai. Gqa: Training\ngeneralized multi-query transformer models from multi-head checkpoints. arXiv preprint\narXiv:2305.13245, 2023.\nY. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi. PIQA: reasoning about physical commonsense\nin natural language.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "378", "text": "References\nJ. Ainslie, J. Lee-Thorp, M. De Jong, Y. Zemlyanskiy, F. Lebrón, and S. Sanghai. Gqa: Training\ngeneralized multi-query transformer models from multi-head checkpoints. arXiv preprint\narXiv:2305.13245, 2023.\nY. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi. PIQA: reasoning about physical commonsense\nin natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI\n2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI\n2020, New York, NY, USA, February 7-12, 2020, pages 7432–7439. AAAI Press, 2020. doi:\n10.1609/aaai.v34i05.6239.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "379", "text": "PIQA: reasoning about physical commonsense\nin natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI\n2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI\n2020, New York, NY, USA, February 7-12, 2020, pages 7432–7439. AAAI Press, 2020. doi:\n10.1609/aaai.v34i05.6239. URL https://doi.org/10.1609/aaai.v34i05.6239.\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural\ninformation processing systems, 33:1877–1901, 2020.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "380", "text": "URL https://doi.org/10.1609/aaai.v34i05.6239.\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural\ninformation processing systems, 33:1877–1901, 2020.\nY. Chai, S. Jin, and X. Hou. Highway transformer: Self-gating enhanced self-attentive networks.\nIn D. Jurafsky, J. Chai, N. Schluter, and J. Tetreault, editors, Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics, pages 6887–6900, Online, July\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.616.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "381", "text": "Y. Chai, S. Jin, and X. Hou. Highway transformer: Self-gating enhanced self-attentive networks.\nIn D. Jurafsky, J. Chai, N. Schluter, and J. Tetreault, editors, Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics, pages 6887–6900, Online, July\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.616. URL\nhttps://aclanthology.org/2020.acl-main.616/.\nF. Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition, pages 1251–1258, 2017.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "382", "text": "Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.616. URL\nhttps://aclanthology.org/2020.acl-main.616/.\nF. Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition, pages 1251–1258, 2017.\n15\nK. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek,\nJ. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint\narXiv:2110.14168, 2021.\nT. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. Ré. FlashAttention: Fast and memory-efficient\nexact attention with IO-awareness.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "383", "text": "15\nK. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek,\nJ. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint\narXiv:2110.14168, 2021.\nT. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. Ré. FlashAttention: Fast and memory-efficient\nexact attention with IO-awareness. In Advances in Neural Information Processing Systems\n(NeurIPS), 2022.\nD. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner. DROP: A reading compre-\nhension benchmark requiring discrete reasoning over paragraphs.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "384", "text": "arXiv preprint\narXiv:2110.14168, 2021.\nT. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. Ré. FlashAttention: Fast and memory-efficient\nexact attention with IO-awareness. In Advances in Neural Information Processing Systems\n(NeurIPS), 2022.\nD. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner. DROP: A reading compre-\nhension benchmark requiring discrete reasoning over paragraphs. In J. Burstein, C. Doran, and\nT. Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, NAACL-HLT\n2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 2368–\n2378.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "385", "text": "DROP: A reading compre-\nhension benchmark requiring discrete reasoning over paragraphs. In J. Burstein, C. Doran, and\nT. Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, NAACL-HLT\n2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 2368–\n2378. Association for Computational Linguistics, 2019. doi: 10.18653/V1/N19-1246. URL\nhttps://doi.org/10.18653/v1/n19-1246.\nY. Fang, Y. CAI, J. Chen, J. Zhao, G. Tian, and G. Li. Cross-layer retrospective retrieving via layer\nattention. In The Eleventh International Conference on Learning Representations, 2023. URL\nhttps://openreview.net/forum?id=pvgEL1yS3Ql.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "386", "text": "Association for Computational Linguistics, 2019. doi: 10.18653/V1/N19-1246. URL\nhttps://doi.org/10.18653/v1/n19-1246.\nY. Fang, Y. CAI, J. Chen, J. Zhao, G. Tian, and G. Li. Cross-layer retrospective retrieving via layer\nattention. In The Eleventh International Conference on Learning Representations, 2023. URL\nhttps://openreview.net/forum?id=pvgEL1yS3Ql.\nW. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models\nwith simple and efficient sparsity. Journal of Machine Learning Research, 23(120):1–39, 2022.\nK. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings\nof the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016a.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "387", "text": "URL\nhttps://openreview.net/forum?id=pvgEL1yS3Ql.\nW. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models\nwith simple and efficient sparsity. Journal of Machine Learning Research, 23(120):1–39, 2022.\nK. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings\nof the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016a.\nK. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In European\nconference on computer vision, pages 630–645. Springer, 2016b.\nM. Heddes, A. Javanmard, K. Axiotis, G. Fu, M. Bateni, and V. Mirrokni. Deepcrossattention:\nSupercharging transformer residual connections.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "388", "text": "In Proceedings\nof the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016a.\nK. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In European\nconference on computer vision, pages 630–645. Springer, 2016b.\nM. Heddes, A. Javanmard, K. Axiotis, G. Fu, M. Bateni, and V. Mirrokni. Deepcrossattention:\nSupercharging transformer residual connections. In Forty-second International Conference\non Machine Learning, 2025. URL https://openreview.net/forum?id=j3JBfFnGYh.\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring\nmassive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "389", "text": "Deepcrossattention:\nSupercharging transformer residual connections. In Forty-second International Conference\non Machine Learning, 2025. URL https://openreview.net/forum?id=j3JBfFnGYh.\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring\nmassive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\nD. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Mea-\nsuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874,\n2021.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "390", "text": "Measuring\nmassive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\nD. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Mea-\nsuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874,\n2021.\nJ. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas,\nL. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche,\nB. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, O. Vinyals, J. Rae, and L. Sifre.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "391", "text": "J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas,\nL. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche,\nB. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, O. Vinyals, J. Rae, and L. Sifre.\nAn empirical analysis of compute-optimal large language model training. In S. Koyejo,\nS. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural\nInformation Processing Systems, volume 35, pages 30016–30030. Curran Associates, Inc., 2022.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "392", "text": "An empirical analysis of compute-optimal large language model training. In S. Koyejo,\nS. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural\nInformation Processing Systems, volume 35, pages 30016–30030. Curran Associates, Inc., 2022.\nURL https://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faf\nf6f588870935f114ebe04a3e5-Paper-Conference.pdf.\nG. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional\nnetworks. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 4700–4708, 2017.\n16\nM. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. TriviaQA: A large scale distantly supervised chal-\nlenge dataset for reading comprehension.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "393", "text": "G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional\nnetworks. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 4700–4708, 2017.\n16\nM. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. TriviaQA: A large scale distantly supervised chal-\nlenge dataset for reading comprehension. In R. Barzilay and M.-Y. Kan, editors, Proceedings of\nthe 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 1601–1611, Vancouver, Canada, July 2017. Association for Computational\nLinguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147.\nG. Larsson, M. Maire, and G. Shakhnarovich.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "394", "text": "In R. Barzilay and M.-Y. Kan, editors, Proceedings of\nthe 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 1601–1611, Vancouver, Canada, July 2017. Association for Computational\nLinguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147.\nG. Larsson, M. Maire, and G. Shakhnarovich. Fractalnet: Ultra-deep neural networks without\nresiduals. arXiv preprint arXiv:1605.07648, 2016.\nD. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen.\nGshard: Scaling giant models with conditional computation and automatic sharding.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "395", "text": "URL https://aclanthology.org/P17-1147.\nG. Larsson, M. Maire, and G. Shakhnarovich. Fractalnet: Ultra-deep neural networks without\nresiduals. arXiv preprint arXiv:1605.07648, 2016.\nD. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen.\nGshard: Scaling giant models with conditional computation and automatic sharding. arXiv\npreprint arXiv:2006.16668, 2020.\nA. Liu, B. Feng, B. Wang, B. Wang, B. Liu, C. Zhao, C. Dengr, C. Ruan, D. Dai, D. Guo, et al.\nDeepseek-v2: A strong, economical, and efficient mixture-of-experts language model.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "396", "text": "Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv\npreprint arXiv:2006.16668, 2020.\nA. Liu, B. Feng, B. Wang, B. Wang, B. Liu, C. Zhao, C. Dengr, C. Ruan, D. Dai, D. Guo, et al.\nDeepseek-v2: A strong, economical, and efficient mixture-of-experts language model. arXiv\npreprint arXiv:2405.04434, 2024a.\nA. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, et al.\nDeepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024b.\nI. Loshchilov and F. Hutter.\nDecoupled weight decay regularization.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "397", "text": "arXiv\npreprint arXiv:2405.04434, 2024a.\nA. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, et al.\nDeepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024b.\nI. Loshchilov and F. Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017.\nB. Mak and J. Flanigan. Residual matrix transformers: Scaling the size of the residual stream.\narXiv preprint arXiv:2506.22696, 2025.\nG. Menghani, R. Kumar, and S. Kumar.\nLAurel: Learned augmented residual layer.\nIn\nForty-second International Conference on Machine Learning, 2025.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "398", "text": "I. Loshchilov and F. Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017.\nB. Mak and J. Flanigan. Residual matrix transformers: Scaling the size of the residual stream.\narXiv preprint arXiv:2506.22696, 2025.\nG. Menghani, R. Kumar, and S. Kumar.\nLAurel: Learned augmented residual layer.\nIn\nForty-second International Conference on Machine Learning, 2025. URL https://open\nreview.net/forum?id=rUDRWP9WvZ.\nM. Pagliardini, A. Mohtashami, F. Fleuret, and M. Jaggi. Denseformer: Enhancing information\nflow in transformers via depth weighted averaging. In The Thirty-eighth Annual Conference\non Neural Information Processing Systems, 2024. URL https://openreview.net/forum\n?id=kMnoh7CXrq.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "399", "text": "LAurel: Learned augmented residual layer.\nIn\nForty-second International Conference on Machine Learning, 2025. URL https://open\nreview.net/forum?id=rUDRWP9WvZ.\nM. Pagliardini, A. Mohtashami, F. Fleuret, and M. Jaggi. Denseformer: Enhancing information\nflow in transformers via depth weighted averaging. In The Thirty-eighth Annual Conference\non Neural Information Processing Systems, 2024. URL https://openreview.net/forum\n?id=kMnoh7CXrq.\nP. Qi, X. Wan, G. Huang, and M. Lin. Zero bubble (almost) pipeline parallelism. In The Twelfth\nInternational Conference on Learning Representations, 2024. URL https://openreview\n.net/forum?id=tuzTN0eIO5.\nN. Shazeer.\nFast transformer decoding: One write-head is all you need.\narXiv preprint\narXiv:1911.02150, 2019.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "400", "text": "URL https://openreview.net/forum\n?id=kMnoh7CXrq.\nP. Qi, X. Wan, G. Huang, and M. Lin. Zero bubble (almost) pipeline parallelism. In The Twelfth\nInternational Conference on Learning Representations, 2024. URL https://openreview\n.net/forum?id=tuzTN0eIO5.\nN. Shazeer.\nFast transformer decoding: One write-head is all you need.\narXiv preprint\narXiv:1911.02150, 2019.\nN. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean.\nOutra-\ngeously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint\narXiv:1701.06538, 2017.\nR. Sinkhorn and P. Knopp. Concerning nonnegative matrices and doubly stochastic matrices.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "401", "text": "arXiv preprint\narXiv:1911.02150, 2019.\nN. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean.\nOutra-\ngeously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint\narXiv:1701.06538, 2017.\nR. Sinkhorn and P. Knopp. Concerning nonnegative matrices and doubly stochastic matrices.\nPacific Journal of Mathematics, 21(2):343–348, 1967.\nR. K. Srivastava, K. Greff, and J. Schmidhuber. Training very deep networks. In C. Cortes,\nN. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information\nProcessing Systems, volume 28. Curran Associates, Inc., 2015. URL https://proceedings.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "402", "text": "R. Sinkhorn and P. Knopp. Concerning nonnegative matrices and doubly stochastic matrices.\nPacific Journal of Mathematics, 21(2):343–348, 1967.\nR. K. Srivastava, K. Greff, and J. Schmidhuber. Training very deep networks. In C. Cortes,\nN. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information\nProcessing Systems, volume 28. Curran Associates, Inc., 2015. URL https://proceedings.\nneurips.cc/paper_files/paper/2015/file/215a71a12769b056c3c32e7299f1c5e\nd-Paper.pdf.\n17\nJ. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu. Roformer: Enhanced transformer with rotary\nposition embedding. Neurocomputing, 568:127063, 2024.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "403", "text": "Curran Associates, Inc., 2015. URL https://proceedings.\nneurips.cc/paper_files/paper/2015/file/215a71a12769b056c3c32e7299f1c5e\nd-Paper.pdf.\n17\nJ. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu. Roformer: Enhanced transformer with rotary\nposition embedding. Neurocomputing, 568:127063, 2024.\nM. Suzgun, N. Scales, N. Schärli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le,\nE. H. Chi, D. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve\nthem. arXiv preprint arXiv:2210.09261, 2022.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "404", "text": "Roformer: Enhanced transformer with rotary\nposition embedding. Neurocomputing, 568:127063, 2024.\nM. Suzgun, N. Scales, N. Schärli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le,\nE. H. Chi, D. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve\nthem. arXiv preprint arXiv:2210.09261, 2022.\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal,\nE. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv\npreprint arXiv:2302.13971, 2023.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "405", "text": "arXiv preprint arXiv:2210.09261, 2022.\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal,\nE. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv\npreprint arXiv:2302.13971, 2023.\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polo-\nsukhin. Attention is all you need. Advances in neural information processing systems, 30,\n2017.\nL. Wang, H. Gao, C. Zhao, X. Sun, and D. Dai. Auxiliary-loss-free load balancing strategy for\nmixture-of-experts.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "406", "text": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polo-\nsukhin. Attention is all you need. Advances in neural information processing systems, 30,\n2017.\nL. Wang, H. Gao, C. Zhao, X. Sun, and D. Dai. Auxiliary-loss-free load balancing strategy for\nmixture-of-experts. arXiv preprint arXiv:2408.15664, 2024.\nL. Wang, Y. Cheng, Y. Shi, Z. Tang, Z. Mo, W. Xie, L. Ma, Y. Xia, J. Xue, F. Yang, et al. Tilelang: A\ncomposable tiled programming model for ai systems. arXiv preprint arXiv:2504.17577, 2025.\nD. Xiao, Q. Meng, S. Li, and X. Yuan.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "407", "text": "arXiv preprint arXiv:2408.15664, 2024.\nL. Wang, Y. Cheng, Y. Shi, Z. Tang, Z. Mo, W. Xie, L. Ma, Y. Xia, J. Xue, F. Yang, et al. Tilelang: A\ncomposable tiled programming model for ai systems. arXiv preprint arXiv:2504.17577, 2025.\nD. Xiao, Q. Meng, S. Li, and X. Yuan. Muddformer: Breaking residual bottlenecks in transformers\nvia multiway dynamic dense connections. arXiv preprint arXiv:2502.12170, 2025.\nS. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He. Aggregated residual transformations for deep\nneural networks. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 1492–1500, 2017.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "408", "text": "D. Xiao, Q. Meng, S. Li, and X. Yuan. Muddformer: Breaking residual bottlenecks in transformers\nvia multiway dynamic dense connections. arXiv preprint arXiv:2502.12170, 2025.\nS. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He. Aggregated residual transformations for deep\nneural networks. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 1492–1500, 2017.\nS. Xie, H. Zhang, J. Guo, X. Tan, J. Bian, H. H. Awadalla, A. Menezes, T. Qin, and R. Yan. Residual:\nTransformer with dual residual connections, 2023. URL https://arxiv.org/abs/2304.1\n4802.\nF. Yu, D. Wang, E. Shelhamer, and T. Darrell. Deep layer aggregation.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "409", "text": "S. Xie, H. Zhang, J. Guo, X. Tan, J. Bian, H. H. Awadalla, A. Menezes, T. Qin, and R. Yan. Residual:\nTransformer with dual residual connections, 2023. URL https://arxiv.org/abs/2304.1\n4802.\nF. Yu, D. Wang, E. Shelhamer, and T. Darrell. Deep layer aggregation. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 2403–2412, 2018.\nR. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can a machine really finish\nyour sentence?", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "410", "text": "Residual:\nTransformer with dual residual connections, 2023. URL https://arxiv.org/abs/2304.1\n4802.\nF. Yu, D. Wang, E. Shelhamer, and T. Darrell. Deep layer aggregation. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 2403–2412, 2018.\nR. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can a machine really finish\nyour sentence? In A. Korhonen, D. R. Traum, and L. Màrquez, editors, Proceedings of the 57th\nConference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers, pages 4791–4800. Association for Computational\nLinguistics, 2019. doi: 10.18653/v1/p19-1472.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "411", "text": "HellaSwag: Can a machine really finish\nyour sentence? In A. Korhonen, D. R. Traum, and L. Màrquez, editors, Proceedings of the 57th\nConference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers, pages 4791–4800. Association for Computational\nLinguistics, 2019. doi: 10.18653/v1/p19-1472. URL https://doi.org/10.18653/v1/p1\n9-1472.\nB. Zhang and R. Sennrich.\nRoot mean square layer normalization.\nAdvances in neural\ninformation processing systems, 32, 2019.\nD. Zhu, H. Huang, Z. Huang, Y. Zeng, Y. Mao, B. Wu, Q. Min, and X. Zhou. Hyper-connections.\narXiv preprint arXiv:2409.19606, 2024.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "412", "text": "URL https://doi.org/10.18653/v1/p1\n9-1472.\nB. Zhang and R. Sennrich.\nRoot mean square layer normalization.\nAdvances in neural\ninformation processing systems, 32, 2019.\nD. Zhu, H. Huang, Z. Huang, Y. Zeng, Y. Mao, B. Wu, Q. Min, and X. Zhou. Hyper-connections.\narXiv preprint arXiv:2409.19606, 2024.\n18\nA. Appendix\nA.1. Detailed Model Specifications and Hyper-parameters.\nTable 5 | Detailed Model Specifications and Hyper-parameters. This table presents the architec-\ntural configurations for the 3B, 9B, and 27B models based on the DeepSeek-V3 (Liu et al., 2024b)\narchitecture. It outlines the specific hyper-parameters for mHC and HC, including the residual\nstream expansion and Sinkhorn-Knopp settings, alongside the optimization and training proto-\ncols used in the experiments.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "413", "text": "18\nA. Appendix\nA.1. Detailed Model Specifications and Hyper-parameters.\nTable 5 | Detailed Model Specifications and Hyper-parameters. This table presents the architec-\ntural configurations for the 3B, 9B, and 27B models based on the DeepSeek-V3 (Liu et al., 2024b)\narchitecture. It outlines the specific hyper-parameters for mHC and HC, including the residual\nstream expansion and Sinkhorn-Knopp settings, alongside the optimization and training proto-\ncols used in the experiments.\nAttribute\n3B\n9B\n27B\n3B\n1T Tokens\nVocab Params\n331M\n496M\n662M\n331M\nActive Params\n612M\n1.66B\n4.14B\n612M\nTotal Params\n2.97B\n9.18B\n27.0B\n2.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "414", "text": "It outlines the specific hyper-parameters for mHC and HC, including the residual\nstream expansion and Sinkhorn-Knopp settings, alongside the optimization and training proto-\ncols used in the experiments.\nAttribute\n3B\n9B\n27B\n3B\n1T Tokens\nVocab Params\n331M\n496M\n662M\n331M\nActive Params\n612M\n1.66B\n4.14B\n612M\nTotal Params\n2.97B\n9.18B\n27.0B\n2.97B\nLayers\n12\n18\n30\n12\nLeading Dense Layers\n1\n1\nRouted Experts\n64\n64\n72\n64\nActive Experts\n6\n6\nShared Experts\n2\n2\nDimension\n1280\n1920\n2560\n1280\nFFN Dimension\n896\n1280\n1536\n896\nLoad Balancing Method\nLoss-Free (Wang et al.,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "415", "text": "66B\n4.14B\n612M\nTotal Params\n2.97B\n9.18B\n27.0B\n2.97B\nLayers\n12\n18\n30\n12\nLeading Dense Layers\n1\n1\nRouted Experts\n64\n64\n72\n64\nActive Experts\n6\n6\nShared Experts\n2\n2\nDimension\n1280\n1920\n2560\n1280\nFFN Dimension\n896\n1280\n1536\n896\nLoad Balancing Method\nLoss-Free (Wang et al., 2024)\nLoss-Free\nAttention Heads\n16\n24\n32\n16\nAttention Dimension\n128\n128\nAttention Variant\nMLA (Liu et al., 2024a)\nMLA\nKV Rank\n512\n512\nPosition Embedding\nRoPE (Su et al., 2024)\nRoPE\nRoPE Dimension\n64\n64\nRoPE 𝜃\n10000\n10000\nLayer Norm Type\nRMSNorm (Zhang and Sennrich,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "416", "text": ", 2024)\nLoss-Free\nAttention Heads\n16\n24\n32\n16\nAttention Dimension\n128\n128\nAttention Variant\nMLA (Liu et al., 2024a)\nMLA\nKV Rank\n512\n512\nPosition Embedding\nRoPE (Su et al., 2024)\nRoPE\nRoPE Dimension\n64\n64\nRoPE 𝜃\n10000\n10000\nLayer Norm Type\nRMSNorm (Zhang and Sennrich, 2019)\nRMSNorm\nLayer Norm 𝜀\n1e-20\n1e-20\nmHC/HC Expansion Rate 𝑛\n4\n4\nmHC/HC Gating Factor Init 𝛼\n0.01\n0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "417", "text": ", 2024a)\nMLA\nKV Rank\n512\n512\nPosition Embedding\nRoPE (Su et al., 2024)\nRoPE\nRoPE Dimension\n64\n64\nRoPE 𝜃\n10000\n10000\nLayer Norm Type\nRMSNorm (Zhang and Sennrich, 2019)\nRMSNorm\nLayer Norm 𝜀\n1e-20\n1e-20\nmHC/HC Expansion Rate 𝑛\n4\n4\nmHC/HC Gating Factor Init 𝛼\n0.01\n0.01\nmHC Sinkhorn-Knopp 𝑡max\n20\n20\nSequence Length\n4096\n4096\nVocab Size\n129280\n129280\nBatch Size\n320\n512\n1280\n2560\nTraining Steps\n30000\n50000\n50000\n100000\nTraining Tokens\n39.3B\n105B\n262B\n1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "418", "text": "01\n0.01\nmHC Sinkhorn-Knopp 𝑡max\n20\n20\nSequence Length\n4096\n4096\nVocab Size\n129280\n129280\nBatch Size\n320\n512\n1280\n2560\nTraining Steps\n30000\n50000\n50000\n100000\nTraining Tokens\n39.3B\n105B\n262B\n1.05T\nWarmup Steps\n2000\n2000\nOptimizer\nAdamW (Loshchilov and Hutter, 2017)\nAdamW\nAdamW Betas\n(0.9, 0.95)\n(0.9, 0.95)\nAdamW 𝜀\n1e-20\n1e-20\nBase Learning Rate\n8.6e-4\n5.9e-4\n4.0e-4\n9.0e-4\nLr Scheduler\nStep\nStep\nLr Decay Step Ratio\n[0.8 ×, 0.9 ×]\n[0.8 ×, 0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "419", "text": "2017)\nAdamW\nAdamW Betas\n(0.9, 0.95)\n(0.9, 0.95)\nAdamW 𝜀\n1e-20\n1e-20\nBase Learning Rate\n8.6e-4\n5.9e-4\n4.0e-4\n9.0e-4\nLr Scheduler\nStep\nStep\nLr Decay Step Ratio\n[0.8 ×, 0.9 ×]\n[0.8 ×, 0.9 ×]\nLr Decay Rate\n[0.316, 0.1]\n[0.316, 0.1]\nWeight Decay\n0.1\n0.1\n19", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "420", "text": "Key Position\nQuery Position\nScale 1: Fine-Grained\n(Local Syntax / Adj-Noun)\nKey Position\nScale 2: Medium-Grained\n(Clause-Level / Local Context)\nKey Position\nScale 3: Coarse-Grained\n(Document Themes / Global)", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/Figure5_Attention.pdf", "file_name": ""}}
{"id": "421", "text": "2\n3\n4\n5\n6\nNumber of Scales (L)\n84.0\n84.5\n85.0\n85.5\n86.0\n86.5\n87.0\nMNLI Accuracy (%)\nPeak Accuracy\n(L=4, Acc=86.0)\nLoss of\nGranularity\nNoise from\nDownsampling", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/Figure4_Scales.pdf", "file_name": ""}}
{"id": "422", "text": "1K\n2K\n3K\n4K\n5K\n6K\n7K\n8K\nSequence Length (N)\n0\n10\n20\n30\n40\n50\n60\n70\n80\nAttention FLOPs (Millions)\n81% Reduction\n(13.6M FLOPs Gap)\n16.8M\n3.2M\nFigure 3. Attention FLOPs vs Sequence Length\nStandard MHA (O(n2))\nMAHA (Ours, \nO(n))", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/Figure3_Attention_FLOPs.pdf", "file_name": ""}}
{"id": "423", "text": "# Deep-Equilibrium Engine: JAX/TPU Implementation Plan\r\n\r\n## Overview\r\n\r\nGreenfield JAX/Equinox implementation of a \"Deep-Equilibrium\" language model combining:\r\n- **mHC**: Manifold-Constrained Hyper-Connections (Birkhoff polytope via Sinkhorn-Knopp)\r\n- **MAHA**: Nash Equilibrium Multiscale Hierarchical Attention\r\n\r\n**Target**: TPU v5p, 1-3B parameters,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "424", "text": "1-3B parameters, autoregressive language modeling\r\n\r\n---\r\n\r\n## Design Decisions\r\n\r\n| Decision | Choice | Rationale |\r\n|----------|--------|-----------|\r\n| RoPE | Per-scale adjusted | Position `i` at scale `l` maps to `floor(i * r^l)` |\r\n| mHC scope | Both branches | Full signal conservation on attention AND FFN |\r\n| Sequence alignment | Assert failure | Enforce `max_seq_len % (r^(L-1)) == 0` at data load |\r\n| Package manager | uv | User requirement |\r\n| Type checker | ty | User requirement |\r\n\r\n---\r\n\r\n## Dependencies\r\n\r\n```toml\r\n[project]\r\nname = \"nash-mhc\"\r\nversion = \"0.1.0\"\r\nrequires-python = \">=3.11\"\r\ndependencies = [\r\n    # Core JAX ecosystem\r\n    \"jax[tpu]>=0.4.30\",\r\n    \"equinox>=0.11.0\",\r\n    \"flax>=0.8.0\",", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "425", "text": "1.0\"\r\nrequires-python = \">=3.11\"\r\ndependencies = [\r\n    # Core JAX ecosystem\r\n    \"jax[tpu]>=0.4.30\",\r\n    \"equinox>=0.11.0\",\r\n    \"flax>=0.8.0\",\r\n    \"optax>=0.2.0\",\r\n    \"lineax>=0.0.5\",\r\n\r\n    # Type safety\r\n    \"jaxtyping>=0.2.28\",\r\n    \"beartype>=0.18.0\",\r\n\r\n    # Data pipeline\r\n    \"datasets>=2.16.0\",\r\n    \"grain>=0.2.0\",\r\n]\r\n\r\n[dependency-groups]\r\ndev = [\r\n    \"pytest>=8.0\",\r\n    \"hypothesis>=6.100\",\r\n]\r\n```\r\n\r\n---\r\n\r\n## Project Structure\r\n\r\n```\r\nnash_mhc/\r\n├── pyproject.toml\r\n├── ty.toml\r\n├── PLAN.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "426", "text": "5\",\r\n\r\n    # Type safety\r\n    \"jaxtyping>=0.2.28\",\r\n    \"beartype>=0.18.0\",\r\n\r\n    # Data pipeline\r\n    \"datasets>=2.16.0\",\r\n    \"grain>=0.2.0\",\r\n]\r\n\r\n[dependency-groups]\r\ndev = [\r\n    \"pytest>=8.0\",\r\n    \"hypothesis>=6.100\",\r\n]\r\n```\r\n\r\n---\r\n\r\n## Project Structure\r\n\r\n```\r\nnash_mhc/\r\n├── pyproject.toml\r\n├── ty.toml\r\n├── PLAN.md                     # This file\r\n├── src/nash_mhc/\r\n│   ├── __init__.py\r\n│   ├── py.typed\r\n│   │\r\n│   ├── types/                  # Phase 1: Type system\r\n│   │   ├── __init__.py\r\n│   │   ├── arrays.py           # Semantic newtypes (DoublyStochastic, SimplexWeights)\r\n│   │   ├── configs.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "427", "text": "]\r\n```\r\n\r\n---\r\n\r\n## Project Structure\r\n\r\n```\r\nnash_mhc/\r\n├── pyproject.toml\r\n├── ty.toml\r\n├── PLAN.md                     # This file\r\n├── src/nash_mhc/\r\n│   ├── __init__.py\r\n│   ├── py.typed\r\n│   │\r\n│   ├── types/                  # Phase 1: Type system\r\n│   │   ├── __init__.py\r\n│   │   ├── arrays.py           # Semantic newtypes (DoublyStochastic, SimplexWeights)\r\n│   │   ├── configs.py          # Frozen ModelConfig, TrainingConfig\r\n│   │   ├── shapes.py           # Compile-time shape tracking\r\n│   │   └── invariants.py       # Runtime assertion helpers\r\n│   │\r\n│   ├── primitives/             # Phase 2: Hardware-aligned ops\r\n│   │   ├── __init__.py\r\n│   │   ├── sinkhorn.py         # Sinkhorn-Knopp via lax.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "428", "text": "py\r\n│   │   ├── arrays.py           # Semantic newtypes (DoublyStochastic, SimplexWeights)\r\n│   │   ├── configs.py          # Frozen ModelConfig, TrainingConfig\r\n│   │   ├── shapes.py           # Compile-time shape tracking\r\n│   │   └── invariants.py       # Runtime assertion helpers\r\n│   │\r\n│   ├── primitives/             # Phase 2: Hardware-aligned ops\r\n│   │   ├── __init__.py\r\n│   │   ├── sinkhorn.py         # Sinkhorn-Knopp via lax.scan + Lineax\r\n│   │   ├── nash_solver.py      # Best-response via lax.fori_loop\r\n│   │   ├── strided_conv.py     # TPU-aligned Conv1d\r\n│   │   ├── upsample.py         # Manual nearest-neighbor\r\n│   │   └── rope.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "429", "text": "py       # Runtime assertion helpers\r\n│   │\r\n│   ├── primitives/             # Phase 2: Hardware-aligned ops\r\n│   │   ├── __init__.py\r\n│   │   ├── sinkhorn.py         # Sinkhorn-Knopp via lax.scan + Lineax\r\n│   │   ├── nash_solver.py      # Best-response via lax.fori_loop\r\n│   │   ├── strided_conv.py     # TPU-aligned Conv1d\r\n│   │   ├── upsample.py         # Manual nearest-neighbor\r\n│   │   └── rope.py             # Per-scale rotary position encoding\r\n│   │\r\n│   ├── layers/                 # Phase 3: Equinox modules\r\n│   │   ├── __init__.py\r\n│   │   ├── mhc.py              # ManifoldHyperConnection\r\n│   │   ├── decomposition.py    # HierarchicalDecomposition\r\n│   │   ├── attention.py        # MultiscaleAttention (shared V)\r\n│   │   ├── aggregation.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "430", "text": "py         # Manual nearest-neighbor\r\n│   │   └── rope.py             # Per-scale rotary position encoding\r\n│   │\r\n│   ├── layers/                 # Phase 3: Equinox modules\r\n│   │   ├── __init__.py\r\n│   │   ├── mhc.py              # ManifoldHyperConnection\r\n│   │   ├── decomposition.py    # HierarchicalDecomposition\r\n│   │   ├── attention.py        # MultiscaleAttention (shared V)\r\n│   │   ├── aggregation.py      # OptimizationAggregation (Nash/Convex)\r\n│   │   └── ffn.py              # SwiGLU feed-forward\r\n│   │\r\n│   ├── blocks/                 # Phase 4: Composed blocks\r\n│   │   ├── __init__.py\r\n│   │   └── decoder_block.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "431", "text": "py              # ManifoldHyperConnection\r\n│   │   ├── decomposition.py    # HierarchicalDecomposition\r\n│   │   ├── attention.py        # MultiscaleAttention (shared V)\r\n│   │   ├── aggregation.py      # OptimizationAggregation (Nash/Convex)\r\n│   │   └── ffn.py              # SwiGLU feed-forward\r\n│   │\r\n│   ├── blocks/                 # Phase 4: Composed blocks\r\n│   │   ├── __init__.py\r\n│   │   └── decoder_block.py    # MAHADecoderBlock with mHC on both branches\r\n│   │\r\n│   ├── models/                 # Phase 5: Full architecture\r\n│   │   ├── __init__.py\r\n│   │   ├── backbone.py         # MAHALanguageModel\r\n│   │   └── lm_head.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "432", "text": "py              # SwiGLU feed-forward\r\n│   │\r\n│   ├── blocks/                 # Phase 4: Composed blocks\r\n│   │   ├── __init__.py\r\n│   │   └── decoder_block.py    # MAHADecoderBlock with mHC on both branches\r\n│   │\r\n│   ├── models/                 # Phase 5: Full architecture\r\n│   │   ├── __init__.py\r\n│   │   ├── backbone.py         # MAHALanguageModel\r\n│   │   └── lm_head.py          # Tied embeddings option\r\n│   │\r\n│   ├── sharding/               # Phase 6: TPU distribution\r\n│   │   ├── __init__.py\r\n│   │   ├── mesh.py             # Device mesh (dp, fsdp, tp)\r\n│   │   ├── specs.py            # PartitionSpec definitions\r\n│   │   └── checkpoint.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "433", "text": "py\r\n│   │   ├── backbone.py         # MAHALanguageModel\r\n│   │   └── lm_head.py          # Tied embeddings option\r\n│   │\r\n│   ├── sharding/               # Phase 6: TPU distribution\r\n│   │   ├── __init__.py\r\n│   │   ├── mesh.py             # Device mesh (dp, fsdp, tp)\r\n│   │   ├── specs.py            # PartitionSpec definitions\r\n│   │   └── checkpoint.py       # Orbax distributed checkpointing\r\n│   │\r\n│   ├── data/                   # Phase 7: Data pipeline\r\n│   │   ├── __init__.py\r\n│   │   ├── loader.py           # Grain-based deterministic loading\r\n│   │   ├── tokenizer.py        # HF tokenizer wrapper\r\n│   │   └── datasets.py         # GSM8k, The Stack adapters\r\n│   │\r\n│   └── training/               # Phase 8: Training loop\r\n│       ├── __init__.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "434", "text": "py            # PartitionSpec definitions\r\n│   │   └── checkpoint.py       # Orbax distributed checkpointing\r\n│   │\r\n│   ├── data/                   # Phase 7: Data pipeline\r\n│   │   ├── __init__.py\r\n│   │   ├── loader.py           # Grain-based deterministic loading\r\n│   │   ├── tokenizer.py        # HF tokenizer wrapper\r\n│   │   └── datasets.py         # GSM8k, The Stack adapters\r\n│   │\r\n│   └── training/               # Phase 8: Training loop\r\n│       ├── __init__.py\r\n│       ├── loop.py             # Main training loop\r\n│       ├── loss.py             # CE + aux loss\r\n│       └── metrics.py          # Perplexity, throughput\r\n│\r\n└── tests/\r\n    ├── conftest.py\r\n    ├── invariants/             # Mathematical property tests\r\n    │   ├── test_sinkhorn.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "435", "text": "py         # GSM8k, The Stack adapters\r\n│   │\r\n│   └── training/               # Phase 8: Training loop\r\n│       ├── __init__.py\r\n│       ├── loop.py             # Main training loop\r\n│       ├── loss.py             # CE + aux loss\r\n│       └── metrics.py          # Perplexity, throughput\r\n│\r\n└── tests/\r\n    ├── conftest.py\r\n    ├── invariants/             # Mathematical property tests\r\n    │   ├── test_sinkhorn.py    # Doubly stochastic invariant\r\n    │   ├── test_nash.py        # Equilibrium convergence\r\n    │   └── test_mhc.py         # Spectral norm bound\r\n    └── shapes/\r\n        └── test_layers.py      # Shape preservation\r\n```\r\n\r\n---\r\n\r\n## Theoretical Foundations\r\n\r\n### 1. mHC (Manifold-Constrained Hyper-Connections)\r\n\r\n**Source**: Paper 2512.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "436", "text": "throughput\r\n│\r\n└── tests/\r\n    ├── conftest.py\r\n    ├── invariants/             # Mathematical property tests\r\n    │   ├── test_sinkhorn.py    # Doubly stochastic invariant\r\n    │   ├── test_nash.py        # Equilibrium convergence\r\n    │   └── test_mhc.py         # Spectral norm bound\r\n    └── shapes/\r\n        └── test_layers.py      # Shape preservation\r\n```\r\n\r\n---\r\n\r\n## Theoretical Foundations\r\n\r\n### 1. mHC (Manifold-Constrained Hyper-Connections)\r\n\r\n**Source**: Paper 2512.24880v1\r\n\r\nThe mHC layer projects residual connection weight matrices onto the **Birkhoff Polytope** (the set of doubly stochastic matrices) to ensure signal mean conservation and bounded spectral norm.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "437", "text": "py        # Equilibrium convergence\r\n    │   └── test_mhc.py         # Spectral norm bound\r\n    └── shapes/\r\n        └── test_layers.py      # Shape preservation\r\n```\r\n\r\n---\r\n\r\n## Theoretical Foundations\r\n\r\n### 1. mHC (Manifold-Constrained Hyper-Connections)\r\n\r\n**Source**: Paper 2512.24880v1\r\n\r\nThe mHC layer projects residual connection weight matrices onto the **Birkhoff Polytope** (the set of doubly stochastic matrices) to ensure signal mean conservation and bounded spectral norm.\r\n\r\n**Birkhoff Polytope Constraint**:\r\n```\r\nP_Mres(H) = {H ∈ R^{n×n} | H·1 = 1, 1^T·H = 1^T, H >= 0}\r\n```\r\n\r\n**Sinkhorn-Knopp Algorithm** (iterative projection):\r\n```\r\nM^(t) = T_r(T_c(M^(t-1)))\r\n```\r\nwhere T_r normalizes rows and T_c normalizes columns.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "438", "text": "**Birkhoff Polytope Constraint**:\r\n```\r\nP_Mres(H) = {H ∈ R^{n×n} | H·1 = 1, 1^T·H = 1^T, H >= 0}\r\n```\r\n\r\n**Sinkhorn-Knopp Algorithm** (iterative projection):\r\n```\r\nM^(t) = T_r(T_c(M^(t-1)))\r\n```\r\nwhere T_r normalizes rows and T_c normalizes columns. Converges in ~10-20 iterations.\r\n\r\n**Key Invariants**:\r\n- Rows and columns sum to 1 (doubly stochastic)\r\n- `||H^res||_2 <= 1` (spectral norm bounded, non-expansive)\r\n- Signal mean conserved across layers\r\n\r\n**Mixed Precision**: bf16 compute for matrix multiplications, f32 for Sinkhorn normalization to prevent \"manifold drift\".\r\n\r\n### 2.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "439", "text": "Converges in ~10-20 iterations.\r\n\r\n**Key Invariants**:\r\n- Rows and columns sum to 1 (doubly stochastic)\r\n- `||H^res||_2 <= 1` (spectral norm bounded, non-expansive)\r\n- Signal mean conserved across layers\r\n\r\n**Mixed Precision**: bf16 compute for matrix multiplications, f32 for Sinkhorn normalization to prevent \"manifold drift\".\r\n\r\n### 2. MAHA (Nash Equilibrium Multiscale Attention)\r\n\r\n**Source**: Paper 2512.14925v2\r\n\r\nMAHA reformulates attention as a game between hierarchical scales competing for bandwidth.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "440", "text": "### 2. MAHA (Nash Equilibrium Multiscale Attention)\r\n\r\n**Source**: Paper 2512.14925v2\r\n\r\nMAHA reformulates attention as a game between hierarchical scales competing for bandwidth.\r\n\r\n**Hierarchical Decomposition**:\r\n```\r\nX -> [X_0, X_1, ..., X_{L-1}]\r\nwhere len(X_l) = len(X_{l-1}) // r\r\n```\r\n\r\n**Nash Equilibrium Aggregation**:\r\n```\r\nfor t in range(nash_iterations):\r\n    O* = Σ_l w_l * O_l          # Consensus\r\n    Error_l = ||O_l - O*||_2     # Reconstruction error\r\n    w_l ← softmax(-Error)        # Best response\r\n```\r\n\r\n**Key Properties**:\r\n- Sub-quadratic complexity: O(N·1.5) vs O(N²)\r\n- Shared V projection across scales (parameter efficient)\r\n- Dynamic weight allocation per sample\r\n\r\n### 3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "441", "text": "Closed-Form Continuous-Time (CfC)\r\n\r\n**Source**: Paper 2106.13898v2\r\n\r\nCfC provides closed-form solutions to differential equations, eliminating ODE solver overhead.\r\n\r\n**Closed-Form Approximation**:\r\n```\r\nx̃(t) = (x(0) - A)e^{-[w_τt + f(I(t))t]} · f(-I(t)) + A\r\n```\r\n\r\n**Benefit**: 150x faster than ODE-based counterparts.\r\n\r\n---\r\n\r\n## Core Invariants\r\n\r\n### 1. Doubly Stochastic (mHC)\r\n```\r\nP @ 1 = 1  AND  P.T @ 1 = 1  AND  P >= 0\r\n```\r\nEnforced by Sinkhorn-Knopp projection.\r\n\r\n### 2. Simplex (Nash weights)\r\n```\r\nsum(w) = 1  AND  w >= 0\r\n```\r\nEnforced by softmax over error terms.\r\n\r\n### 3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "442", "text": "---\r\n\r\n## Core Invariants\r\n\r\n### 1. Doubly Stochastic (mHC)\r\n```\r\nP @ 1 = 1  AND  P.T @ 1 = 1  AND  P >= 0\r\n```\r\nEnforced by Sinkhorn-Knopp projection.\r\n\r\n### 2. Simplex (Nash weights)\r\n```\r\nsum(w) = 1  AND  w >= 0\r\n```\r\nEnforced by softmax over error terms.\r\n\r\n### 3. Spectral Norm Bound (mHC)\r\n```\r\n||H^res||_2 <= 1\r\n```\r\nNon-expansive residual connection.\r\n\r\n### 4. Sequence Alignment\r\n```\r\nmax_seq_len % (compression_ratio ^ (num_scales - 1)) == 0\r\n```\r\nAsserted at config construction.\r\n\r\n---\r\n\r\n## Data-Oriented Design Rules\r\n\r\n1. **Data Dominates Everything**: Enumerate representations and select the one with the smallest failure surface and highest TPU MXU locality.\r\n\r\n2.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "443", "text": "### 3. Spectral Norm Bound (mHC)\r\n```\r\n||H^res||_2 <= 1\r\n```\r\nNon-expansive residual connection.\r\n\r\n### 4. Sequence Alignment\r\n```\r\nmax_seq_len % (compression_ratio ^ (num_scales - 1)) == 0\r\n```\r\nAsserted at config construction.\r\n\r\n---\r\n\r\n## Data-Oriented Design Rules\r\n\r\n1. **Data Dominates Everything**: Enumerate representations and select the one with the smallest failure surface and highest TPU MXU locality.\r\n\r\n2. **Structure of Arrays (SoA)**: Prioritize SoA over AoS to maximize memory bandwidth and ensure O(1) hardware-aligned data flow.\r\n\r\n3. **Illegal States are Unrepresentable**: Use typestates and exhaustive constructors so that unstable manifolds or non-equilibrium weights cannot exist in memory.\r\n\r\n4. **Invariant-First Logic**: Define explicit Pre-conditions {P} and Post-conditions {Q} for every state transition.\r\n\r\n5.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "444", "text": "**Data Dominates Everything**: Enumerate representations and select the one with the smallest failure surface and highest TPU MXU locality.\r\n\r\n2. **Structure of Arrays (SoA)**: Prioritize SoA over AoS to maximize memory bandwidth and ensure O(1) hardware-aligned data flow.\r\n\r\n3. **Illegal States are Unrepresentable**: Use typestates and exhaustive constructors so that unstable manifolds or non-equilibrium weights cannot exist in memory.\r\n\r\n4. **Invariant-First Logic**: Define explicit Pre-conditions {P} and Post-conditions {Q} for every state transition.\r\n\r\n5. **Pure Functional Core**: Treat the model as a stateless PyTree transformation; isolate all side effects, I/O, and randomness in the Imperative Shell.\r\n\r\n6. **Closed-Form Physics**: Favor closed-form equations over iterative numerical solvers to eliminate computational latency and non-determinism.\r\n\r\n7. **Linear Control Flow**: Keep logic top-to-bottom with O(1) complexity per line; use guard clauses to fail-fast instead of deep nesting.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "445", "text": "4. **Invariant-First Logic**: Define explicit Pre-conditions {P} and Post-conditions {Q} for every state transition.\r\n\r\n5. **Pure Functional Core**: Treat the model as a stateless PyTree transformation; isolate all side effects, I/O, and randomness in the Imperative Shell.\r\n\r\n6. **Closed-Form Physics**: Favor closed-form equations over iterative numerical solvers to eliminate computational latency and non-determinism.\r\n\r\n7. **Linear Control Flow**: Keep logic top-to-bottom with O(1) complexity per line; use guard clauses to fail-fast instead of deep nesting.\r\n\r\n8. **Flat Memory Primitives**: Use contiguous buffers and bitwise primitives (AND, OR, LSHIFT) for O(1) state validation in bounded spaces.\r\n\r\n9. **Hardware-First Representation**: Align data structures with the 128×128 systolic array; if the data doesn't fit the hardware physics, change the representation.\r\n\r\n10. **Test Invariants, Not Examples**: Assert mathematical properties rather than brittle, example-based outcomes.\r\n\r\n11.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "446", "text": "7. **Linear Control Flow**: Keep logic top-to-bottom with O(1) complexity per line; use guard clauses to fail-fast instead of deep nesting.\r\n\r\n8. **Flat Memory Primitives**: Use contiguous buffers and bitwise primitives (AND, OR, LSHIFT) for O(1) state validation in bounded spaces.\r\n\r\n9. **Hardware-First Representation**: Align data structures with the 128×128 systolic array; if the data doesn't fit the hardware physics, change the representation.\r\n\r\n10. **Test Invariants, Not Examples**: Assert mathematical properties rather than brittle, example-based outcomes.\r\n\r\n11. **Zero-Puffery Interface**: Strictly use blunt, directive phrasing in APIs; avoid boolean flags, sentinel values.\r\n\r\n12. **Null-Zero Boundary**: Eliminate Null/None at foreign boundaries; distinguish absence and failure using explicit variants/enums.\r\n\r\n---\r\n\r\n## Implementation Phases\r\n\r\n### Phase 1: Project Setup\r\n- [ ] Initialize uv project with dependencies\r\n- [ ] Configure ty.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "447", "text": "10. **Test Invariants, Not Examples**: Assert mathematical properties rather than brittle, example-based outcomes.\r\n\r\n11. **Zero-Puffery Interface**: Strictly use blunt, directive phrasing in APIs; avoid boolean flags, sentinel values.\r\n\r\n12. **Null-Zero Boundary**: Eliminate Null/None at foreign boundaries; distinguish absence and failure using explicit variants/enums.\r\n\r\n---\r\n\r\n## Implementation Phases\r\n\r\n### Phase 1: Project Setup\r\n- [ ] Initialize uv project with dependencies\r\n- [ ] Configure ty.toml for strict type checking\r\n- [ ] Create ModelConfig with alignment assertions\r\n- [ ] Create TrainingConfig\r\n- [ ] Create semantic array newtypes\r\n- [ ] Create invariant assertion helpers\r\n\r\n### Phase 2: Primitives\r\n- [ ] `sinkhorn.py`: Sinkhorn-Knopp via `lax.scan`, mixed precision\r\n- [ ] `nash_solver.py`: Best-response via `lax.fori_loop`\r\n- [ ] `strided_conv.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "448", "text": "toml for strict type checking\r\n- [ ] Create ModelConfig with alignment assertions\r\n- [ ] Create TrainingConfig\r\n- [ ] Create semantic array newtypes\r\n- [ ] Create invariant assertion helpers\r\n\r\n### Phase 2: Primitives\r\n- [ ] `sinkhorn.py`: Sinkhorn-Knopp via `lax.scan`, mixed precision\r\n- [ ] `nash_solver.py`: Best-response via `lax.fori_loop`\r\n- [ ] `strided_conv.py`: JAX Conv1d with TPU-aligned padding\r\n- [ ] `upsample.py`: Nearest-neighbor via index gathering\r\n- [ ] `rope.py`: Per-scale RoPE with adjusted position indices\r\n\r\n### Phase 3: Layers\r\n- [ ] `mhc.py`: ManifoldHyperConnection (learnable log_alpha, Sinkhorn projection)\r\n- [ ] `decomposition.py`: HierarchicalDecomposition (tuple of scales, not list)\r\n- [ ] `attention.py`: MultiscaleAttention (per-scale Q/K,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "449", "text": "py`: JAX Conv1d with TPU-aligned padding\r\n- [ ] `upsample.py`: Nearest-neighbor via index gathering\r\n- [ ] `rope.py`: Per-scale RoPE with adjusted position indices\r\n\r\n### Phase 3: Layers\r\n- [ ] `mhc.py`: ManifoldHyperConnection (learnable log_alpha, Sinkhorn projection)\r\n- [ ] `decomposition.py`: HierarchicalDecomposition (tuple of scales, not list)\r\n- [ ] `attention.py`: MultiscaleAttention (per-scale Q/K, shared V, per-scale RoPE)\r\n- [ ] `aggregation.py`: OptimizationAggregation (Nash or Convex strategy)\r\n- [ ] `ffn.py`: SwiGLU FFN\r\n\r\n### Phase 4: Blocks\r\n- [ ] `decoder_block.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "450", "text": "py`: ManifoldHyperConnection (learnable log_alpha, Sinkhorn projection)\r\n- [ ] `decomposition.py`: HierarchicalDecomposition (tuple of scales, not list)\r\n- [ ] `attention.py`: MultiscaleAttention (per-scale Q/K, shared V, per-scale RoPE)\r\n- [ ] `aggregation.py`: OptimizationAggregation (Nash or Convex strategy)\r\n- [ ] `ffn.py`: SwiGLU FFN\r\n\r\n### Phase 4: Blocks\r\n- [ ] `decoder_block.py`: MAHADecoderBlock\r\n  - RMSNorm -> Decompose -> Attention -> Aggregate -> mHC residual\r\n  - RMSNorm -> FFN -> mHC residual\r\n\r\n### Phase 5: Model\r\n- [ ] `backbone.py`: MAHALanguageModel\r\n  - Token embedding\r\n  - N × MAHADecoderBlock\r\n  - Final RMSNorm\r\n  - LM head\r\n\r\n### Phase 6: Sharding\r\n- [ ] `mesh.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "451", "text": "py`: SwiGLU FFN\r\n\r\n### Phase 4: Blocks\r\n- [ ] `decoder_block.py`: MAHADecoderBlock\r\n  - RMSNorm -> Decompose -> Attention -> Aggregate -> mHC residual\r\n  - RMSNorm -> FFN -> mHC residual\r\n\r\n### Phase 5: Model\r\n- [ ] `backbone.py`: MAHALanguageModel\r\n  - Token embedding\r\n  - N × MAHADecoderBlock\r\n  - Final RMSNorm\r\n  - LM head\r\n\r\n### Phase 6: Sharding\r\n- [ ] `mesh.py`: TPU mesh construction (dp, fsdp, tp axes)\r\n- [ ] `specs.py`: PartitionSpec for weights and activations\r\n- [ ] `checkpoint.py`: Orbax checkpointing with sharding\r\n\r\n### Phase 7: Data\r\n- [ ] `loader.py`: Grain DataLoader with deterministic shuffling\r\n- [ ] `tokenizer.py`: HF tokenizer with padding to aligned length\r\n- [ ] `datasets.py`: GSM8k,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "452", "text": "py`: TPU mesh construction (dp, fsdp, tp axes)\r\n- [ ] `specs.py`: PartitionSpec for weights and activations\r\n- [ ] `checkpoint.py`: Orbax checkpointing with sharding\r\n\r\n### Phase 7: Data\r\n- [ ] `loader.py`: Grain DataLoader with deterministic shuffling\r\n- [ ] `tokenizer.py`: HF tokenizer with padding to aligned length\r\n- [ ] `datasets.py`: GSM8k, The Stack dataset adapters\r\n\r\n### Phase 8: Training\r\n- [ ] `loop.py`: Training loop with gradient accumulation\r\n- [ ] `loss.py`: Cross-entropy + lambda × aux_loss\r\n- [ ] `metrics.py`: Perplexity, tokens/sec\r\n\r\n---\r\n\r\n## Key Implementation Details\r\n\r\n### Sinkhorn-Knopp (primitives/sinkhorn.py)\r\n\r\n```python\r\nfrom functools import partial\r\nimport jax\r\nimport jax.numpy as jnp\r\nfrom jax import lax\r\nfrom jaxtyping import Float, Array\r\n\r\n@partial(jax.jit,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "453", "text": "The Stack dataset adapters\r\n\r\n### Phase 8: Training\r\n- [ ] `loop.py`: Training loop with gradient accumulation\r\n- [ ] `loss.py`: Cross-entropy + lambda × aux_loss\r\n- [ ] `metrics.py`: Perplexity, tokens/sec\r\n\r\n---\r\n\r\n## Key Implementation Details\r\n\r\n### Sinkhorn-Knopp (primitives/sinkhorn.py)\r\n\r\n```python\r\nfrom functools import partial\r\nimport jax\r\nimport jax.numpy as jnp\r\nfrom jax import lax\r\nfrom jaxtyping import Float, Array\r\n\r\n@partial(jax.jit, static_argnames=[\"num_iterations\"])\r\ndef sinkhorn_knopp(\r\n    log_alpha: Float[Array, \"... n n\"],\r\n    num_iterations: int = 10,\r\n) -> Float[Array, \"... n n\"]:\r\n    \"\"\"\r\n    Sinkhorn-Knopp algorithm for Birkhoff polytope projection.\r\n\r\n    Computes doubly stochastic matrix via iterative row/column normalization\r\n    in log-space for numerical stability.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "454", "text": "numpy as jnp\r\nfrom jax import lax\r\nfrom jaxtyping import Float, Array\r\n\r\n@partial(jax.jit, static_argnames=[\"num_iterations\"])\r\ndef sinkhorn_knopp(\r\n    log_alpha: Float[Array, \"... n n\"],\r\n    num_iterations: int = 10,\r\n) -> Float[Array, \"... n n\"]:\r\n    \"\"\"\r\n    Sinkhorn-Knopp algorithm for Birkhoff polytope projection.\r\n\r\n    Computes doubly stochastic matrix via iterative row/column normalization\r\n    in log-space for numerical stability.\r\n\r\n    Invariant: Output M satisfies M @ 1 = 1 and M.T @ 1 = 1\r\n    \"\"\"\r\n    n = log_alpha.shape[-1]\r\n    u = jnp.zeros(log_alpha.shape[:-1])\r\n    v = jnp.zeros(log_alpha.shape[:-1])\r\n\r\n    def scan_body(carry, _):\r\n        u, v = carry\r\n        # f32 for numerical stability\r\n        u_new = -jax.nn.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "455", "text": "Computes doubly stochastic matrix via iterative row/column normalization\r\n    in log-space for numerical stability.\r\n\r\n    Invariant: Output M satisfies M @ 1 = 1 and M.T @ 1 = 1\r\n    \"\"\"\r\n    n = log_alpha.shape[-1]\r\n    u = jnp.zeros(log_alpha.shape[:-1])\r\n    v = jnp.zeros(log_alpha.shape[:-1])\r\n\r\n    def scan_body(carry, _):\r\n        u, v = carry\r\n        # f32 for numerical stability\r\n        u_new = -jax.nn.logsumexp(log_alpha + v[..., None, :], axis=-1)\r\n        v_new = -jax.nn.logsumexp(log_alpha + u_new[..., :, None], axis=-2)\r\n        return (u_new, v_new), None\r\n\r\n    (u_final, v_final), _ = lax.scan(\r\n        scan_body,\r\n        (u, v),\r\n        xs=None,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "456", "text": "shape[:-1])\r\n\r\n    def scan_body(carry, _):\r\n        u, v = carry\r\n        # f32 for numerical stability\r\n        u_new = -jax.nn.logsumexp(log_alpha + v[..., None, :], axis=-1)\r\n        v_new = -jax.nn.logsumexp(log_alpha + u_new[..., :, None], axis=-2)\r\n        return (u_new, v_new), None\r\n\r\n    (u_final, v_final), _ = lax.scan(\r\n        scan_body,\r\n        (u, v),\r\n        xs=None,\r\n        length=num_iterations\r\n    )\r\n\r\n    log_P = log_alpha + u_final[..., :, None] + v_final[..., None, :]\r\n    return jnp.exp(log_P).astype(log_alpha.dtype)\r\n```\r\n\r\n### Nash Best-Response (primitives/nash_solver.py)\r\n\r\n```python\r\nfrom functools import partial\r\nimport jax\r\nimport jax.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "457", "text": "v_new), None\r\n\r\n    (u_final, v_final), _ = lax.scan(\r\n        scan_body,\r\n        (u, v),\r\n        xs=None,\r\n        length=num_iterations\r\n    )\r\n\r\n    log_P = log_alpha + u_final[..., :, None] + v_final[..., None, :]\r\n    return jnp.exp(log_P).astype(log_alpha.dtype)\r\n```\r\n\r\n### Nash Best-Response (primitives/nash_solver.py)\r\n\r\n```python\r\nfrom functools import partial\r\nimport jax\r\nimport jax.numpy as jnp\r\nfrom jax import lax\r\nfrom jaxtyping import Float, Array\r\n\r\n@partial(jax.jit, static_argnames=[\"num_iterations\"])\r\ndef nash_best_response(\r\n    scale_outputs: Float[Array, \"B L N D\"],\r\n    num_iterations: int = 3,\r\n) -> tuple[Float[Array, \"B N D\"], Float[Array, \"B L\"]]:\r\n    \"\"\"\r\n    Nash equilibrium aggregation via iterated best-response dynamics.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "458", "text": "py)\r\n\r\n```python\r\nfrom functools import partial\r\nimport jax\r\nimport jax.numpy as jnp\r\nfrom jax import lax\r\nfrom jaxtyping import Float, Array\r\n\r\n@partial(jax.jit, static_argnames=[\"num_iterations\"])\r\ndef nash_best_response(\r\n    scale_outputs: Float[Array, \"B L N D\"],\r\n    num_iterations: int = 3,\r\n) -> tuple[Float[Array, \"B N D\"], Float[Array, \"B L\"]]:\r\n    \"\"\"\r\n    Nash equilibrium aggregation via iterated best-response dynamics.\r\n\r\n    Args:\r\n        scale_outputs: Stacked outputs from all scales [B, L, N, D]\r\n        num_iterations: Number of best-response iterations\r\n\r\n    Returns:\r\n        aggregated: Final consensus output [B, N, D]\r\n        weights: Equilibrium weights [B, L]\r\n    \"\"\"\r\n    B, L, N, D = scale_outputs.shape\r\n    init_weights = jnp.ones((B, L)) / L\r\n\r\n    def iteration_body(i,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "459", "text": "\"B N D\"], Float[Array, \"B L\"]]:\r\n    \"\"\"\r\n    Nash equilibrium aggregation via iterated best-response dynamics.\r\n\r\n    Args:\r\n        scale_outputs: Stacked outputs from all scales [B, L, N, D]\r\n        num_iterations: Number of best-response iterations\r\n\r\n    Returns:\r\n        aggregated: Final consensus output [B, N, D]\r\n        weights: Equilibrium weights [B, L]\r\n    \"\"\"\r\n    B, L, N, D = scale_outputs.shape\r\n    init_weights = jnp.ones((B, L)) / L\r\n\r\n    def iteration_body(i, weights):\r\n        w_expanded = weights[:, :, None, None]\r\n        consensus = jnp.sum(scale_outputs * w_expanded, axis=1)\r\n        diffs = scale_outputs - consensus[:, None, :, :]\r\n        errors = jnp.sqrt(jnp.sum(diffs ** 2, axis=(-2, -1)))\r\n        return jax.nn.softmax(-errors, axis=-1)\r\n\r\n    final_weights = lax.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "460", "text": "L, N, D = scale_outputs.shape\r\n    init_weights = jnp.ones((B, L)) / L\r\n\r\n    def iteration_body(i, weights):\r\n        w_expanded = weights[:, :, None, None]\r\n        consensus = jnp.sum(scale_outputs * w_expanded, axis=1)\r\n        diffs = scale_outputs - consensus[:, None, :, :]\r\n        errors = jnp.sqrt(jnp.sum(diffs ** 2, axis=(-2, -1)))\r\n        return jax.nn.softmax(-errors, axis=-1)\r\n\r\n    final_weights = lax.fori_loop(0, num_iterations, iteration_body, init_weights)\r\n    w_expanded = final_weights[:, :, None, None]\r\n    aggregated = jnp.sum(scale_outputs * w_expanded, axis=1)\r\n\r\n    return aggregated, final_weights\r\n```\r\n\r\n### Per-Scale RoPE (primitives/rope.py)\r\n\r\n```python\r\nimport jax.numpy as jnp\r\nfrom jaxtyping import Float,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "461", "text": "sqrt(jnp.sum(diffs ** 2, axis=(-2, -1)))\r\n        return jax.nn.softmax(-errors, axis=-1)\r\n\r\n    final_weights = lax.fori_loop(0, num_iterations, iteration_body, init_weights)\r\n    w_expanded = final_weights[:, :, None, None]\r\n    aggregated = jnp.sum(scale_outputs * w_expanded, axis=1)\r\n\r\n    return aggregated, final_weights\r\n```\r\n\r\n### Per-Scale RoPE (primitives/rope.py)\r\n\r\n```python\r\nimport jax.numpy as jnp\r\nfrom jaxtyping import Float, Array\r\n\r\ndef compute_rope_freqs(\r\n    dim: int,\r\n    max_seq_len: int,\r\n    base: float = 10000.0,\r\n) -> Float[Array, \"N D2\"]:\r\n    \"\"\"Compute rotary position embedding frequencies.\"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "462", "text": "py)\r\n\r\n```python\r\nimport jax.numpy as jnp\r\nfrom jaxtyping import Float, Array\r\n\r\ndef compute_rope_freqs(\r\n    dim: int,\r\n    max_seq_len: int,\r\n    base: float = 10000.0,\r\n) -> Float[Array, \"N D2\"]:\r\n    \"\"\"Compute rotary position embedding frequencies.\"\"\"\r\n    inv_freq = 1.0 / (base ** (jnp.arange(0, dim, 2) / dim))\r\n    positions = jnp.arange(max_seq_len)\r\n    freqs = jnp.outer(positions, inv_freq)\r\n    return freqs\r\n\r\ndef apply_rope_per_scale(\r\n    x: Float[Array, \"B N H K\"],\r\n    scale_idx: int,\r\n    compression_ratio: int,\r\n    freqs: Float[Array, \"N K2\"],\r\n) -> Float[Array, \"B N H K\"]:\r\n    \"\"\"\r\n    Apply RoPE with position indices adjusted for scale.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "463", "text": "\"\"\"\r\n    inv_freq = 1.0 / (base ** (jnp.arange(0, dim, 2) / dim))\r\n    positions = jnp.arange(max_seq_len)\r\n    freqs = jnp.outer(positions, inv_freq)\r\n    return freqs\r\n\r\ndef apply_rope_per_scale(\r\n    x: Float[Array, \"B N H K\"],\r\n    scale_idx: int,\r\n    compression_ratio: int,\r\n    freqs: Float[Array, \"N K2\"],\r\n) -> Float[Array, \"B N H K\"]:\r\n    \"\"\"\r\n    Apply RoPE with position indices adjusted for scale.\r\n\r\n    At scale l, position i maps to floor(i * r^l) in the original sequence.\r\n    \"\"\"\r\n    seq_len = x.shape[1]\r\n    scale_factor = compression_ratio ** scale_idx\r\n\r\n    # Adjusted position indices\r\n    positions = jnp.arange(seq_len) * scale_factor\r\n    positions = jnp.clip(positions, 0, freqs.shape[0] - 1).astype(jnp.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "464", "text": "At scale l, position i maps to floor(i * r^l) in the original sequence.\r\n    \"\"\"\r\n    seq_len = x.shape[1]\r\n    scale_factor = compression_ratio ** scale_idx\r\n\r\n    # Adjusted position indices\r\n    positions = jnp.arange(seq_len) * scale_factor\r\n    positions = jnp.clip(positions, 0, freqs.shape[0] - 1).astype(jnp.int32)\r\n\r\n    # Gather frequencies for these positions\r\n    scaled_freqs = freqs[positions]\r\n\r\n    # Split x into pairs and apply rotation\r\n    x1, x2 = x[..., ::2], x[..., 1::2]\r\n    cos = jnp.cos(scaled_freqs)[None, :, None, :]\r\n    sin = jnp.sin(scaled_freqs)[None, :, None, :]\r\n\r\n    x_rot = jnp.stack([\r\n        x1 * cos - x2 * sin,\r\n        x1 * sin + x2 * cos,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "465", "text": "int32)\r\n\r\n    # Gather frequencies for these positions\r\n    scaled_freqs = freqs[positions]\r\n\r\n    # Split x into pairs and apply rotation\r\n    x1, x2 = x[..., ::2], x[..., 1::2]\r\n    cos = jnp.cos(scaled_freqs)[None, :, None, :]\r\n    sin = jnp.sin(scaled_freqs)[None, :, None, :]\r\n\r\n    x_rot = jnp.stack([\r\n        x1 * cos - x2 * sin,\r\n        x1 * sin + x2 * cos,\r\n    ], axis=-1).reshape(x.shape)\r\n\r\n    return x_rot\r\n```\r\n\r\n### mHC Layer (layers/mhc.py)\r\n\r\n```python\r\nimport jax\r\nimport jax.numpy as jnp\r\nimport equinox as eqx\r\nfrom jaxtyping import Float, Array\r\nfrom ..primitives.sinkhorn import sinkhorn_knopp\r\n\r\nclass ManifoldHyperConnection(eqx.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "466", "text": ":, None, :]\r\n\r\n    x_rot = jnp.stack([\r\n        x1 * cos - x2 * sin,\r\n        x1 * sin + x2 * cos,\r\n    ], axis=-1).reshape(x.shape)\r\n\r\n    return x_rot\r\n```\r\n\r\n### mHC Layer (layers/mhc.py)\r\n\r\n```python\r\nimport jax\r\nimport jax.numpy as jnp\r\nimport equinox as eqx\r\nfrom jaxtyping import Float, Array\r\nfrom ..primitives.sinkhorn import sinkhorn_knopp\r\n\r\nclass ManifoldHyperConnection(eqx.Module):\r\n    \"\"\"\r\n    Manifold-constrained hyper-connection layer.\r\n\r\n    Projects residual connection weight matrix onto Birkhoff polytope\r\n    to ensure signal mean conservation and bounded spectral norm.\r\n    \"\"\"\r\n    log_alpha: Float[Array, \"D D\"]\r\n    layer_scale: Float[Array, \"D\"]\r\n    sinkhorn_iters: int = eqx.field(static=True)\r\n\r\n    def __init__(self, d_model: int,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "467", "text": "Array\r\nfrom ..primitives.sinkhorn import sinkhorn_knopp\r\n\r\nclass ManifoldHyperConnection(eqx.Module):\r\n    \"\"\"\r\n    Manifold-constrained hyper-connection layer.\r\n\r\n    Projects residual connection weight matrix onto Birkhoff polytope\r\n    to ensure signal mean conservation and bounded spectral norm.\r\n    \"\"\"\r\n    log_alpha: Float[Array, \"D D\"]\r\n    layer_scale: Float[Array, \"D\"]\r\n    sinkhorn_iters: int = eqx.field(static=True)\r\n\r\n    def __init__(self, d_model: int, sinkhorn_iters: int = 10, *, key: jax.Array):\r\n        self.sinkhorn_iters = sinkhorn_iters\r\n        k1, k2 = jax.random.split(key)\r\n        noise = jax.random.normal(k1, (d_model, d_model)) * 0.01\r\n        self.log_alpha = noise\r\n        self.layer_scale = jnp.ones(d_model)\r\n\r\n    def __call__(\r\n        self,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "468", "text": "\"D\"]\r\n    sinkhorn_iters: int = eqx.field(static=True)\r\n\r\n    def __init__(self, d_model: int, sinkhorn_iters: int = 10, *, key: jax.Array):\r\n        self.sinkhorn_iters = sinkhorn_iters\r\n        k1, k2 = jax.random.split(key)\r\n        noise = jax.random.normal(k1, (d_model, d_model)) * 0.01\r\n        self.log_alpha = noise\r\n        self.layer_scale = jnp.ones(d_model)\r\n\r\n    def __call__(\r\n        self,\r\n        residual: Float[Array, \"B N D\"],\r\n        block_output: Float[Array, \"B N D\"],\r\n    ) -> Float[Array, \"B N D\"]:\r\n        H = sinkhorn_knopp(self.log_alpha, self.sinkhorn_iters)\r\n        D = H.shape[0]\r\n        I_minus_H = jnp.eye(D) - H\r\n\r\n        res_contrib = jnp.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "469", "text": "01\r\n        self.log_alpha = noise\r\n        self.layer_scale = jnp.ones(d_model)\r\n\r\n    def __call__(\r\n        self,\r\n        residual: Float[Array, \"B N D\"],\r\n        block_output: Float[Array, \"B N D\"],\r\n    ) -> Float[Array, \"B N D\"]:\r\n        H = sinkhorn_knopp(self.log_alpha, self.sinkhorn_iters)\r\n        D = H.shape[0]\r\n        I_minus_H = jnp.eye(D) - H\r\n\r\n        res_contrib = jnp.einsum(\"de,bne->bnd\", H, residual)\r\n        block_contrib = jnp.einsum(\"de,bne->bnd\", I_minus_H, block_output)\r\n\r\n        return (res_contrib + block_contrib) * self.layer_scale\r\n```\r\n\r\n---\r\n\r\n## Model Configuration (1-3B)\r\n\r\n```python\r\nfrom dataclasses import dataclass\r\nfrom typing import Literal\r\n\r\n@dataclass(frozen=True,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "470", "text": "self.sinkhorn_iters)\r\n        D = H.shape[0]\r\n        I_minus_H = jnp.eye(D) - H\r\n\r\n        res_contrib = jnp.einsum(\"de,bne->bnd\", H, residual)\r\n        block_contrib = jnp.einsum(\"de,bne->bnd\", I_minus_H, block_output)\r\n\r\n        return (res_contrib + block_contrib) * self.layer_scale\r\n```\r\n\r\n---\r\n\r\n## Model Configuration (1-3B)\r\n\r\n```python\r\nfrom dataclasses import dataclass\r\nfrom typing import Literal\r\n\r\n@dataclass(frozen=True, slots=True)\r\nclass ModelConfig:\r\n    vocab_size: int = 32768        # 256 × 128 (TPU aligned)\r\n    max_seq_len: int = 2048        # Divisible by 2^4 = 16\r\n    d_model: int = 2048            # 16 × 128 (TPU aligned)\r\n    num_heads: int = 16\r\n    num_layers: int = 24           # ~1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "471", "text": "slots=True)\r\nclass ModelConfig:\r\n    vocab_size: int = 32768        # 256 × 128 (TPU aligned)\r\n    max_seq_len: int = 2048        # Divisible by 2^4 = 16\r\n    d_model: int = 2048            # 16 × 128 (TPU aligned)\r\n    num_heads: int = 16\r\n    num_layers: int = 24           # ~1.5B params\r\n    num_scales: int = 4            # L=4\r\n    compression_ratio: int = 2     # r=2\r\n    ffn_multiplier: float = 2.67   # SwiGLU\r\n    sinkhorn_iterations: int = 10\r\n    nash_iterations: int = 3\r\n    aggregation: Literal[\"nash\", \"convex\"] = \"nash\"\r\n    dtype: Literal[\"bfloat16\", \"float32\"] = \"bfloat16\"\r\n\r\n    def __post_init__(self):\r\n        assert self.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "472", "text": "5B params\r\n    num_scales: int = 4            # L=4\r\n    compression_ratio: int = 2     # r=2\r\n    ffn_multiplier: float = 2.67   # SwiGLU\r\n    sinkhorn_iterations: int = 10\r\n    nash_iterations: int = 3\r\n    aggregation: Literal[\"nash\", \"convex\"] = \"nash\"\r\n    dtype: Literal[\"bfloat16\", \"float32\"] = \"bfloat16\"\r\n\r\n    def __post_init__(self):\r\n        assert self.vocab_size % 128 == 0, \"vocab_size must align to 128\"\r\n        assert self.max_seq_len % 128 == 0, \"max_seq_len must align to 128\"\r\n        assert self.d_model % 128 == 0, \"d_model must align to 128\"\r\n        assert self.d_model % self.num_heads == 0\r\n\r\n        # Sequence alignment invariant\r\n        min_scale_len = self.max_seq_len // (self.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "473", "text": "\"float32\"] = \"bfloat16\"\r\n\r\n    def __post_init__(self):\r\n        assert self.vocab_size % 128 == 0, \"vocab_size must align to 128\"\r\n        assert self.max_seq_len % 128 == 0, \"max_seq_len must align to 128\"\r\n        assert self.d_model % 128 == 0, \"d_model must align to 128\"\r\n        assert self.d_model % self.num_heads == 0\r\n\r\n        # Sequence alignment invariant\r\n        min_scale_len = self.max_seq_len // (self.compression_ratio ** (self.num_scales - 1))\r\n        assert min_scale_len >= 1, f\"max_seq_len too small for {self.num_scales} scales\"\r\n        assert self.max_seq_len % (self.compression_ratio ** (self.num_scales - 1)) == 0\r\n```\r\n\r\n---\r\n\r\n## Testing Strategy\r\n\r\n### Invariant Tests (Hypothesis property-based)\r\n\r\n```python\r\nfrom hypothesis import given,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "474", "text": "d_model % self.num_heads == 0\r\n\r\n        # Sequence alignment invariant\r\n        min_scale_len = self.max_seq_len // (self.compression_ratio ** (self.num_scales - 1))\r\n        assert min_scale_len >= 1, f\"max_seq_len too small for {self.num_scales} scales\"\r\n        assert self.max_seq_len % (self.compression_ratio ** (self.num_scales - 1)) == 0\r\n```\r\n\r\n---\r\n\r\n## Testing Strategy\r\n\r\n### Invariant Tests (Hypothesis property-based)\r\n\r\n```python\r\nfrom hypothesis import given, strategies as st\r\nimport jax\r\nimport jax.numpy as jnp\r\n\r\nfrom nash_mhc.primitives.sinkhorn import sinkhorn_knopp\r\nfrom nash_mhc.types.invariants import assert_doubly_stochastic\r\n\r\n@given(\r\n    n=st.integers(min_value=4, max_value=64),\r\n    seed=st.integers(min_value=0, max_value=2**31),", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "475", "text": "strategies as st\r\nimport jax\r\nimport jax.numpy as jnp\r\n\r\nfrom nash_mhc.primitives.sinkhorn import sinkhorn_knopp\r\nfrom nash_mhc.types.invariants import assert_doubly_stochastic\r\n\r\n@given(\r\n    n=st.integers(min_value=4, max_value=64),\r\n    seed=st.integers(min_value=0, max_value=2**31),\r\n)\r\ndef test_sinkhorn_doubly_stochastic(n: int, seed: int):\r\n    \"\"\"Sinkhorn output must be doubly stochastic.\"\"\"\r\n    key = jax.random.PRNGKey(seed)\r\n    log_alpha = jax.random.normal(key, (n, n))\r\n    P = sinkhorn_knopp(log_alpha, num_iterations=20)\r\n    assert_doubly_stochastic(P, rtol=1e-4, atol=1e-5)\r\n\r\ndef test_nash_weights_simplex():\r\n    \"\"\"Nash weights must be valid probability distribution.\"\"\"\r\n    key = jax.random.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "476", "text": ")\r\ndef test_sinkhorn_doubly_stochastic(n: int, seed: int):\r\n    \"\"\"Sinkhorn output must be doubly stochastic.\"\"\"\r\n    key = jax.random.PRNGKey(seed)\r\n    log_alpha = jax.random.normal(key, (n, n))\r\n    P = sinkhorn_knopp(log_alpha, num_iterations=20)\r\n    assert_doubly_stochastic(P, rtol=1e-4, atol=1e-5)\r\n\r\ndef test_nash_weights_simplex():\r\n    \"\"\"Nash weights must be valid probability distribution.\"\"\"\r\n    key = jax.random.PRNGKey(0)\r\n    scale_outputs = jax.random.normal(key, (2, 4, 32, 64))\r\n    _, weights = nash_best_response(scale_outputs, num_iterations=5)\r\n    assert jnp.allclose(jnp.sum(weights, axis=-1), 1.0, rtol=1e-5)\r\n    assert jnp.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "477", "text": "key = jax.random.PRNGKey(0)\r\n    scale_outputs = jax.random.normal(key, (2, 4, 32, 64))\r\n    _, weights = nash_best_response(scale_outputs, num_iterations=5)\r\n    assert jnp.allclose(jnp.sum(weights, axis=-1), 1.0, rtol=1e-5)\r\n    assert jnp.all(weights >= 0)\r\n```\r\n\r\n---\r\n\r\n## Reference Files\r\n\r\n| Component | Reference Location |\r\n|-----------|-------------------|\r\n| Nash best-response | `references/MAHA-Project/src/layers/aggregation.py` |\r\n| Strided conv | `references/MAHA-Project/src/layers/decomposition.py` |\r\n| Shared V attention | `references/MAHA-Project/src/layers/attention.py` |\r\n| Block composition | `references/MAHA-Project/src/models/maha_block.py` |\r\n| Training loop | `references/MAHA-Project/train.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "478", "text": "all(weights >= 0)\r\n```\r\n\r\n---\r\n\r\n## Reference Files\r\n\r\n| Component | Reference Location |\r\n|-----------|-------------------|\r\n| Nash best-response | `references/MAHA-Project/src/layers/aggregation.py` |\r\n| Strided conv | `references/MAHA-Project/src/layers/decomposition.py` |\r\n| Shared V attention | `references/MAHA-Project/src/layers/attention.py` |\r\n| Block composition | `references/MAHA-Project/src/models/maha_block.py` |\r\n| Training loop | `references/MAHA-Project/train.py` |\r\n\r\n---\r\n\r\n## Data-Oriented Design Checklist\r\n\r\n- [x] SoA over AoS: Scales as tuple, not Python list\r\n- [x] Illegal states unrepresentable: Frozen configs with validators\r\n- [x] Invariant-first: Assertions for doubly stochastic, simplex, spectral norm\r\n- [x] Pure functional core: Equinox PyTree modules, stateless forward\r\n- [x] Linear control flow: Guard clauses,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "479", "text": "py` |\r\n| Training loop | `references/MAHA-Project/train.py` |\r\n\r\n---\r\n\r\n## Data-Oriented Design Checklist\r\n\r\n- [x] SoA over AoS: Scales as tuple, not Python list\r\n- [x] Illegal states unrepresentable: Frozen configs with validators\r\n- [x] Invariant-first: Assertions for doubly stochastic, simplex, spectral norm\r\n- [x] Pure functional core: Equinox PyTree modules, stateless forward\r\n- [x] Linear control flow: Guard clauses, no deep nesting\r\n- [x] Hardware-first: 128-aligned dimensions, TPU MXU locality\r\n- [x] Zero-puffery: Blunt APIs, no boolean flags", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/PLAN.md", "file_name": "PLAN.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "481", "text": "[project]\nname = \"nash-mhc\"\nversion = \"0.1.0\"\ndescription = \"Deep-Equilibrium Engine: JAX/TPU language model with mHC and MAHA\"\nreadme = \"README.md\"\nauthors = [\n    { name = \"Dutch Casadaban\", email = \"caz.dutch@gmail.com\" }\n]\nrequires-python = \">=3.11\"\ndependencies = [\n    # Core JAX ecosystem\n    \"jax>=0.4.30\",\n    \"equinox>=0.11.0\",\n    \"flax>=0.8.0\",\n    \"optax>=0.2.0\",\n    \"lineax>=0.0.5\",\n\n    # Type safety\n    \"jaxtyping>=0.2.28\",\n    \"beartype>=0.18.0\",\n\n    # Data pipeline\n    \"datasets>=2.16.0\",\n    \"grain-nightly\",\n]\n\n[dependency-groups]\ndev = [\n    \"pytest>=8.0\",\n    \"hypothesis>=6.100\",\n]\ntpu = [\n    \"jax[tpu]>=0.4.30\",\n]\n\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n\n[tool.hatch.build.targets.wheel]\npackages = [\"src/nash_mhc\"]", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/pyproject.toml", "file_name": "pyproject.toml", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "482", "text": "<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"content-type\"/>\n<title>Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models</title>\n<!--Generated on Thu Dec 18 14:12:19 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->\n<meta content=\"width=device-width, initial-scale=1, shrink-to-fit=no\" name=\"viewport\"/>\n<link href=\"/static/browse/0.3.4/css/arxiv-html-papers-20250916.css\" rel=\"stylesheet\" type=\"text/css\"/>\n<script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js\"></script>\n<script src=\"/static/browse/0.3.4/js/addons_new.js\"></script>\n<script src=\"/static/browse/0.3.4/js/feedbackOverlay.js\"></script>\n<base href=\"/html/2512.14925v2/\"/></head>\n<body>\n<nav class=\"ltx_page_navbar\">\n<nav class=\"ltx_TOC\">\n<ol class=\"ltx_toclist\">\n<li class=\"ltx_tocentry ltx_tocentry_section\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "483", "text": "org/html/2512.14925v2#S1\" title=\"In Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">I </span><span class=\"ltx_text ltx_font_smallcaps\">Introduction</span></span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_section\">\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.14925v2#S2\" title=\"In Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">II </span><span class=\"ltx_text ltx_font_smallcaps\">Related Work</span></span></a>\n<ol class=\"ltx_toclist ltx_toclist_section\">\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.14925v2#S2.SS1\" title=\"In II Related Work ‣ Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">II-A</span> </span><span class=\"ltx_text ltx_font_italic\">Sparse Attention Mechanisms</span></span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.14925v2#S2.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "484", "text": "org/html/2512.14925v2#S2.SS2\" title=\"In II Related Work ‣ Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">II-B</span> </span><span class=\"ltx_text ltx_font_italic\">Hierarchical Attention Frameworks</span></span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.14925v2#S2.SS3\" title=\"In II Related Work ‣ Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">II-C</span> </span><span class=\"ltx_text ltx_font_italic\">Optimization-Driven and Game-Theoretic Aggregation</span></span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.14925v2#S2.SS4\" title=\"In II Related Work ‣ Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">II-D</span> </span><span class=\"ltx_text ltx_font_italic\">Multiscale Analysis in Language Models</span></span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "485", "text": "org/html/2512.14925v2#S2.SS5\" title=\"In II Related Work ‣ Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">II-E</span> </span><span class=\"ltx_text ltx_font_italic\">Integration of Optimization and Attention</span></span></a></li>\n</ol>\n</li>\n<li class=\"ltx_tocentry ltx_tocentry_section\">\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.14925v2#S3\" title=\"In Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">III </span><span class=\"ltx_text ltx_font_smallcaps\">Preliminaries and Background</span></span></a>\n<ol class=\"ltx_toclist ltx_toclist_section\">\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.14925v2#S3.SS1\" title=\"In III Preliminaries and Background ‣ Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">III-A</span> </span><span class=\"ltx_text ltx_font_italic\">Attention Mechanisms in Transformers</span></span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "486", "text": "org/html/2512.14925v2#S3.SS2\" title=\"In III Preliminaries and Background ‣ Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">III-B</span> </span><span class=\"ltx_text ltx_font_italic\">Multiscale Signal Decomposition</span></span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.14925v2#S3.SS3\" title=\"In III Preliminaries and Background ‣ Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">III-C</span> </span><span class=\"ltx_text ltx_font_italic\">Game-Theoretic Optimization</span></span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.14925v2#S3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "487", "text": "org/html/2512.14925v2#S3.SS4\" title=\"In III Preliminaries and Background ‣ Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">III-D</span> </span><span class=\"ltx_text ltx_font_italic\">Convex Optimization in Attention</span></span></a></li>\n</ol>\n</li>\n<li class=\"ltx_tocentry ltx_tocentry_section\">\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.14925v2#S4\" title=\"In Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">IV </span><span class=\"ltx_text ltx_font_smallcaps\">The MAHA Framework</span></span></a>\n<ol class=\"ltx_toclist ltx_toclist_section\">\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.14925v2#S4.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "488", "text": "org/html/2512.14925v2#S4.SS1\" title=\"In IV The MAHA Framework ‣ Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">IV-A</span> </span><span class=\"ltx_text ltx_font_italic\">Hierarchical Multiscale Decomposition with Learnable Downsampling</span></span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.14925v2#S4.SS2\" title=\"In IV The MAHA Framework ‣ Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">IV-B</span> </span><span class=\"ltx_text ltx_font_italic\">Multiscale Attention Computation</span></span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.14925v2#S4.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "489", "text": "org/html/2512.14925v2#S4.SS3\" title=\"In IV The MAHA Framework ‣ Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">IV-C</span> </span><span class=\"ltx_text ltx_font_italic\">Aggregation of Multiscale Attention Outputs</span></span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.14925v2#S4.SS4\" title=\"In IV The MAHA Framework ‣ Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">IV-D</span> </span><span class=\"ltx_text ltx_font_italic\">Hybrid Dilated-Convolutional Transformer Design</span></span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.14925v2#S4.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "490", "text": "org/html/2512.14925v2#S4.SS5\" title=\"In IV The MAHA Framework ‣ Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">IV-E</span> </span><span class=\"ltx_text ltx_font_italic\">Complexity Reduction through Hierarchical Sparsity</span></span></a></li>\n</ol>\n</li>\n<li class=\"ltx_tocentry ltx_tocentry_section\">\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.14925v2#S5\" title=\"In Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">V </span><span class=\"ltx_text ltx_font_smallcaps\">Experiments</span></span></a>\n<ol class=\"ltx_toclist ltx_toclist_section\">\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.14925v2#S5.SS1\" title=\"In V Experiments ‣ Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">V-A</span> </span><span class=\"ltx_text ltx_font_italic\">Experimental Setup</span></span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "491", "text": "org/html/2512.14925v2#S5.SS2\" title=\"In V Experiments ‣ Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">V-B</span> </span><span class=\"ltx_text ltx_font_italic\">Main Results</span></span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.14925v2#S5.SS3\" title=\"In V Experiments ‣ Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">V-C</span> </span><span class=\"ltx_text ltx_font_italic\">Computational Efficiency Analysis</span></span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\">\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.14925v2#S5.SS4\" title=\"In V Experiments ‣ Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">V-D</span> </span><span class=\"ltx_text ltx_font_italic\">Ablation Studies</span></span></a>\n<ol class=\"ltx_toclist ltx_toclist_subsection\">\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "492", "text": "org/html/2512.14925v2#S5.SS4.SSS1\" title=\"In V-D Ablation Studies ‣ V Experiments ‣ Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">V-D</span>1 </span>Aggregation Strategy Comparison</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.14925v2#S5.SS4.SSS2\" title=\"In V-D Ablation Studies ‣ V Experiments ‣ Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">V-D</span>2 </span>Scale Configuration Analysis</span></a></li>\n</ol>\n</li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.14925v2#S5.SS5\" title=\"In V Experiments ‣ Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">V-E</span> </span><span class=\"ltx_text ltx_font_italic\">Qualitative Analysis</span></span></a></li>\n</ol>\n</li>\n<li class=\"ltx_tocentry ltx_tocentry_section\">\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "493", "text": "org/html/2512.14925v2#S6\" title=\"In Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">VI </span><span class=\"ltx_text ltx_font_smallcaps\">Discussion</span></span></a>\n<ol class=\"ltx_toclist ltx_toclist_section\">\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.14925v2#S6.SS1\" title=\"In VI Discussion ‣ Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">VI-A</span> </span><span class=\"ltx_text ltx_font_italic\">Scalability vs. Implementation Overhead</span></span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.14925v2#S6.SS2\" title=\"In VI Discussion ‣ Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">VI-B</span> </span><span class=\"ltx_text ltx_font_italic\">Limitations</span></span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.14925v2#S6.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "494", "text": "org/html/2512.14925v2#S6.SS3\" title=\"In VI Discussion ‣ Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">VI-C</span> </span><span class=\"ltx_text ltx_font_italic\">Potential Application Scenarios</span></span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.14925v2#S6.SS4\" title=\"In VI Discussion ‣ Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">VI-D</span> </span><span class=\"ltx_text ltx_font_italic\">Ethical Considerations</span></span></a></li>\n</ol>\n</li>\n<li class=\"ltx_tocentry ltx_tocentry_section\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "495", "text": "org/html/2512.14925v2#S7\" title=\"In Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">VII </span><span class=\"ltx_text ltx_font_smallcaps\">Conclusion</span></span></a></li>\n</ol></nav>\n</nav>\n<div class=\"ltx_page_main\">\n<div class=\"ltx_page_content\">\n<article class=\"ltx_document ltx_authors_1line\">\n<h1 class=\"ltx_title ltx_title_document\">Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models</h1>\n<div class=\"ltx_authors\">\n<span class=\"ltx_creator ltx_role_author\">\n<span class=\"ltx_personname\">Caner Erden\n</span><span class=\"ltx_author_notes\">This work was supported in part by Sakarya University of Applied Sciences. (Corresponding author: Caner Erden)C. Erden is with the Department of Computer Engineering, Faculty of Technology, Sakarya University of Applied Science, Sakarya, Türkiye (e-mail: cerden@subu.edu.tr; ORCID: 0000-0002-7311-862X).Data Availability: The source code is available at https://github.com/canererden/MAHA-Project (arXiv: https://arxiv.org/abs/2512.14925).</span></span>\n</div>\n<div class=\"ltx_abstract\">\n<h6 class=\"ltx_title ltx_title_abstract\">Abstract</h6>\n<p class=\"ltx_p\" id=\"id1.id1\">The quadratic computational complexity of Multi-Head Self-Attention (MHSA) remains a fundamental bottleneck in scaling Large Language Models (LLMs) for long-context tasks. While sparse and linearized attention mechanisms attempt to mitigate this, they often compromise the representation of global dependencies or fail to capture multiscale semantic granularity effectively.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "496", "text": "While sparse and linearized attention mechanisms attempt to mitigate this, they often compromise the representation of global dependencies or fail to capture multiscale semantic granularity effectively. In this paper, we propose Multiscale Aggregated Hierarchical Attention (MAHA), a novel architectural framework that reformulates the attention mechanism through hierarchical decomposition and mathematically rigorous aggregation. Unlike conventional approaches that treat token interactions at a single resolution, MAHA dynamically partitions the input sequence into hierarchical scales via learnable downsampling operators. The core innovation lies in its aggregation strategy: we model the fusion of scale-specific attention matrices as a resource allocation problem, solved via a convex optimization framework or a Nash equilibrium-based game-theoretic approach. This ensures a theoretically optimal balance between local nuance and global context fidelity. Implemented within a hybrid dilated-convolutional transformer backbone, MAHA utilizes differentiable optimization layers to enable end-to-end training. Experimental evaluations demonstrate that MAHA achieves superior scalability; empirical FLOPs analysis confirms an 81% reduction in computational cost at a sequence length of 4096 compared to standard attention.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "497", "text": "Experimental evaluations demonstrate that MAHA achieves superior scalability; empirical FLOPs analysis confirms an 81% reduction in computational cost at a sequence length of 4096 compared to standard attention. This work bridges the gap between optimization theory and sequence modeling, offering a scalable solution for next-generation LLMs.</p>\n<p class=\"ltx_p\" id=\"id2.id2\"><span class=\"ltx_text ltx_font_bold\" id=\"id2.id2.1\">Keywords:</span> Large Language Models, Hierarchical Attention, Game Theory, Convex Optimization, Nash Equilibrium, Efficient Transformers.</p>\n</div>\n<section class=\"ltx_section\" id=\"S1\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">I </span><span class=\"ltx_text ltx_font_smallcaps\" id=\"S1.1.1\">Introduction</span>\n</h2>\n<div class=\"ltx_para\" id=\"S1.p1\">\n<p class=\"ltx_p\" id=\"S1.p1.1\">The advent of transformer-based architectures has fundamentally revolutionized natural language processing (NLP), establishing the Multi-Head Self-Attention (MHSA) mechanism as the cornerstone of modern large language models (LLMs) <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">vaswani2017a</span>]</cite>.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "498", "text": "Despite its efficacy, this mechanism confronts two critical challenges: (i) computational inefficiency arising from quadratic complexity (<math alttext=\"O(N^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.1.m1\" intent=\":literal\"><semantics><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">O(N^{2})</annotation></semantics></math>) with respect to sequence length, and (ii) the inherent trade-off between capturing fine-grained local patterns and coarse-grained global dependencies simultaneously. These limitations become increasingly pronounced as LLMs scale to process extended contexts and model complex linguistic structures <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhao2023a</span>]</cite>.</p>\n</div>\n<div class=\"ltx_para\" id=\"S1.p2\">\n<p class=\"ltx_p\" id=\"S1.p2.1\">Current methodologies attempting to mitigate these challenges typically rely on sparse attention patterns or hierarchical representations. Sparse attention strategies alleviate computational overhead by restricting token interactions to predefined or learned patterns; however, this often results in information loss and suboptimal context modeling, particularly for long-range dependencies <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2016a</span>]</cite>. Conversely, hierarchical methods decompose the input into multiple levels of granularity but frequently lack a principled mathematical framework for integration.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "499", "text": "Conversely, hierarchical methods decompose the input into multiple levels of granularity but frequently lack a principled mathematical framework for integration. This often leads to ad-hoc aggregation schemes that fail to preserve the full contextual richness of the input embedding space <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">starck2015a</span>]</cite>.</p>\n</div>\n<div class=\"ltx_para\" id=\"S1.p3\">\n<p class=\"ltx_p\" id=\"S1.p3.1\">To bridge this gap, we introduce Multiscale Aggregated Hierarchical Attention (MAHA), a novel framework that addresses these limitations through a mathematically rigorous approach to multiscale attention computation and aggregation. MAHA dynamically partitions the input sequence into hierarchical scales, where each scale represents a distinct level of contextual abstraction. Distinguishing itself from prior hierarchical approaches, MAHA leverages convex optimization (CO) or game-theoretic equilibrium to synthesize these scales. This ensures that the aggregation process is not merely a weighted average but an optimization problem that balances efficiency and contextual awareness.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "500", "text": "This ensures that the aggregation process is not merely a weighted average but an optimization problem that balances efficiency and contextual awareness. Consequently, the proposed method provides a systematic mechanism to reconcile local nuances with global dependencies while maintaining computational tractability.</p>\n</div>\n<div class=\"ltx_para\" id=\"S1.p4\">\n<p class=\"ltx_p\" id=\"S1.p4.1\">The primary contributions of this work are threefold:</p>\n<ul class=\"ltx_itemize\" id=\"S1.I1\">\n<li class=\"ltx_item\" id=\"S1.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S1.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"S1.I1.i1.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S1.I1.i1.p1.1.1\">Multiscale Decomposition:</span> We introduce a robust decomposition strategy where the input sequence is processed across independent scales to isolate and capture distinct levels of contextual granularity.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S1.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S1.I1.i2.p1\">\n<p class=\"ltx_p\" id=\"S1.I1.i2.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S1.I1.i2.p1.1.1\">Optimization-Driven Aggregation:</span> We propose a novel aggregation mechanism governed by convex optimization and game-theoretic principles.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "501", "text": "This allows the model to determine the optimal trade-off between local and global context dynamically, rather than relying on static or heuristic fusion methods.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S1.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S1.I1.i3.p1\">\n<p class=\"ltx_p\" id=\"S1.I1.i3.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S1.I1.i3.p1.1.1\">Computational Efficiency:</span> MAHA significantly reduces the quadratic complexity characteristic of standard attention mechanisms, enhancing scalability without compromising the model’s expressive power.</p>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"ltx_para\" id=\"S1.p5\">\n<p class=\"ltx_p\" id=\"S1.p5.1\">MAHA is particularly pertinent to the evolution of LLMs, where the demand for efficient and scalable attention mechanisms is paramount <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">naveed2025a</span>]</cite>. By integrating rigorous multiscale analysis with optimization-based aggregation rules, MAHA offers a versatile solution adaptable to various transformer-based architectures with minimal architectural overhead.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "502", "text": "By integrating rigorous multiscale analysis with optimization-based aggregation rules, MAHA offers a versatile solution adaptable to various transformer-based architectures with minimal architectural overhead. The framework is designed for compatibility with existing LLM training pipelines, ensuring practicality for real-world deployment.</p>\n</div>\n</section>\n<section class=\"ltx_section\" id=\"S2\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">II </span><span class=\"ltx_text ltx_font_smallcaps\" id=\"S2.1.1\">Related Work</span>\n</h2>\n<div class=\"ltx_para\" id=\"S2.p1\">\n<p class=\"ltx_p\" id=\"S2.p1.1\">The development of efficient attention mechanisms has become a focal point in transformer-based architecture research, with numerous approaches proposed to alleviate computational bottlenecks and enhance contextual modeling capabilities. Existing literature can be broadly categorized into sparse attention methods, hierarchical attention frameworks, and optimization-driven aggregation techniques.</p>\n</div>\n<section class=\"ltx_subsection\" id=\"S2.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S2.SS1.5.1.1\">II-A</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S2.SS1.6.2\">Sparse Attention Mechanisms</span>\n</h3>\n<div class=\"ltx_para\" id=\"S2.SS1.p1\">\n<p class=\"ltx_p\" id=\"S2.SS1.p1.2\">Sparse attention mechanisms aim to reduce computational overhead by limiting token interactions to predefined or learned patterns.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "503", "text": "For instance, <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">beltagy2020a</span>]</cite> introduced a sliding window attention mechanism that restricts each token’s receptive field to its local neighborhood, significantly lowering memory requirements from <math alttext=\"O(n^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.1.m1\" intent=\":literal\"><semantics><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">O(n^{2})</annotation></semantics></math> to <math alttext=\"O(n)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.2.m2\" intent=\":literal\"><semantics><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">O(n)</annotation></semantics></math> for long sequences. Similarly, <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zaheer2020a</span>]</cite> proposed a hybrid approach combining local, global, and random attention patterns to approximate full self-attention while maintaining theoretical expressiveness.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "504", "text": "However, these methods often rely on heuristics to determine sparsity patterns, which may not adapt dynamically to diverse input structures or capture long-range dependencies effectively without stacking multiple layers.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S2.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S2.SS2.5.1.1\">II-B</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S2.SS2.6.2\">Hierarchical Attention Frameworks</span>\n</h3>\n<div class=\"ltx_para\" id=\"S2.SS2.p1\">\n<p class=\"ltx_p\" id=\"S2.SS2.p1.1\">Hierarchical approaches decompose input sequences into multiple levels of granularity to capture both local syntactic features and global semantic dependencies simultaneously. The Hierarchical Attention Network (HAN) <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2016a</span>]</cite> processes documents at word and sentence levels, aggregating information through learned attention weights. More recently, <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng2023a</span>]</cite> introduced a hierarchical attention mechanism (hi-attention) that integrates inter-layer information to improve sequence modeling.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "505", "text": "While effective, these methods typically employ fixed or ad-hoc aggregation rules—such as weighted averaging—which may not optimally balance the contributions from different scales, leading to information dilution.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S2.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S2.SS3.5.1.1\">II-C</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S2.SS3.6.2\">Optimization-Driven and Game-Theoretic Aggregation</span>\n</h3>\n<div class=\"ltx_para\" id=\"S2.SS3.p1\">\n<p class=\"ltx_p\" id=\"S2.SS3.p1.1\">Optimization techniques have been increasingly integrated into neural architectures to enhance efficiency and robustness. For example, <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jun2023a</span>]</cite> utilized hierarchical decomposition to interpret intermediate CNN decisions, demonstrating the potential of optimization-based feature integration. In the context of sequence modeling, <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhu2021a</span>]</cite> explored multi-head self-attention with hierarchical aggregation but did not incorporate rigorous convex optimization or game-theoretic principles. Game theory, particularly the concept of Nash equilibrium, has been successfully employed in multi-agent systems to resolve conflicts <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lee2023a</span>]</cite>.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "506", "text": "Its application to attention mechanisms offers a principled pathway to resolve conflicts between competing attention scales, a direction that remains largely unexplored in current LLM architectures.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S2.SS4\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S2.SS4.5.1.1\">II-D</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S2.SS4.6.2\">Multiscale Analysis in Language Models</span>\n</h3>\n<div class=\"ltx_para\" id=\"S2.SS4.p1\">\n<p class=\"ltx_p\" id=\"S2.SS4.p1.1\">Multiscale analysis is a staple in signal processing and computer vision <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">starck2015a</span>]</cite>, yet its direct application to language modeling remains limited. Recent work by <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lu2023a</span>]</cite> demonstrated the effectiveness of hierarchical decomposition in graph convolutional networks, suggesting potential benefits for attention mechanisms. Similarly, <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">farge1992a</span>]</cite> proposed hierarchical decomposition for continual learning, highlighting the importance of scale-specific feature extraction.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "507", "text": "These studies provide empirical evidence that processing information at varying resolutions can enhance representation learning.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S2.SS5\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S2.SS5.5.1.1\">II-E</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S2.SS5.6.2\">Integration of Optimization and Attention</span>\n</h3>\n<div class=\"ltx_para\" id=\"S2.SS5.p1\">\n<p class=\"ltx_p\" id=\"S2.SS5.p1.1\">The integration of differentiable optimization layers with attention mechanisms represents an emerging research frontier. While <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lu2023a</span>]</cite> applied hierarchical attention to fraud detection, their aggregation method lacked strong theoretical guarantees. In contrast, the proposed MAHA framework distinguishes itself by unifying multiscale decomposition with rigorous aggregation rules. Unlike sparse attention methods, MAHA dynamically adjusts the scale of token interactions without relying on predefined patterns. Compared to existing hierarchical approaches, it employs convex optimization or Nash equilibrium (NE) to optimally combine attention scores.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "508", "text": "Unlike sparse attention methods, MAHA dynamically adjusts the scale of token interactions without relying on predefined patterns. Compared to existing hierarchical approaches, it employs convex optimization or Nash equilibrium (NE) to optimally combine attention scores. This combination enables MAHA to achieve superior computational efficiency and contextual modeling, addressing the key limitations of heuristic-based aggregation.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S3\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">III </span><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.1.1\">Preliminaries and Background</span>\n</h2>\n<div class=\"ltx_para\" id=\"S3.p1\">\n<p class=\"ltx_p\" id=\"S3.p1.1\">To establish the theoretical foundation for MAHA, we briefly review key concepts in attention mechanisms, multiscale analysis, and game-theoretic optimization. These components form the basis of our proposed framework.</p>\n</div>\n<section class=\"ltx_subsection\" id=\"S3.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S3.SS1.5.1.1\">III-A</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S3.SS1.6.2\">Attention Mechanisms in Transformers</span>\n</h3>\n<div class=\"ltx_para\" id=\"S3.SS1.p1\">\n<p class=\"ltx_p\" id=\"S3.SS1.p1.4\">The standard attention mechanism in transformers computes pairwise interactions between all tokens in a sequence through scaled dot-product operations <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">vaswani2017a</span>]</cite>.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "509", "text": "Given an input sequence <math alttext=\"\\mathbf{X}\\in\\mathbb{R}^{n\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.1.m1\" intent=\":literal\"><semantics><mrow><mi>𝐗</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>n</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}\\in\\mathbb{R}^{n\\times d}</annotation></semantics></math>, where <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.2.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> is the sequence length and <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.3.m3\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> is the embedding dimension, the attention matrix <math alttext=\"\\mathbf{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.4.m4\" intent=\":literal\"><semantics><mi>𝐀</mi><annotation encoding=\"application/x-tex\">\\mathbf{A}</annotation></semantics></math> is computed as:</p>\n</div>\n<div class=\"ltx_para\" id=\"S3.SS1.p2\">\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "510", "text": "SS1.p2\">\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S3.E1\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\mathbf{A}=\\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^{T}}{\\sqrt{d_{k}}}\\right)\" class=\"ltx_Math\" display=\"block\" id=\"S3.E1.m1\" intent=\":literal\"><semantics><mrow><mi>𝐀</mi><mo>=</mo><mrow><mtext>softmax</mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo>(</mo><mfrac><msup><mi>𝐐𝐊</mi><mi>T</mi></msup><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo>)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{A}=\\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^{T}}{\\sqrt{d_{k}}}\\right)</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(1)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div class=\"ltx_para\" id=\"S3.SS1.p3\">\n<p class=\"ltx_p\" id=\"S3.SS1.p3.3\">where <math alttext=\"\\mathbf{Q},", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "511", "text": "SS1.p3\">\n<p class=\"ltx_p\" id=\"S3.SS1.p3.3\">where <math alttext=\"\\mathbf{Q},\\mathbf{K}\\in\\mathbb{R}^{n\\times d_{k}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>𝐐</mi><mo>,</mo><mi>𝐊</mi></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>n</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><msub><mi>d</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Q},\\mathbf{K}\\in\\mathbb{R}^{n\\times d_{k}}</annotation></semantics></math> are the query and key matrices, respectively, and <math alttext=\"d_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.2.m2\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">d_{k}</annotation></semantics></math> is the dimension of the keys.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "512", "text": "While effective, this operation exhibits quadratic complexity <math alttext=\"O(n^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.3.m3\" intent=\":literal\"><semantics><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">O(n^{2})</annotation></semantics></math> in both computation and memory, rendering it impractical for very long sequences <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhao2023a</span>]</cite>.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S3.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S3.SS2.5.1.1\">III-B</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S3.SS2.6.2\">Multiscale Signal Decomposition</span>\n</h3>\n<div class=\"ltx_para\" id=\"S3.SS2.p1\">\n<p class=\"ltx_p\" id=\"S3.SS2.p1.1\">Multiscale analysis provides a rigorous framework for examining signals at varying levels of resolution. In NLP, this translates to capturing both local syntactic patterns (high frequency) and global semantic structures (low frequency) <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">starck2015a</span>]</cite>.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "513", "text": "Inspired by wavelet transforms and pyramid decomposition <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">farge1992a</span>]</cite>, for a discrete signal representation <math alttext=\"\\mathbf{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.1.m1\" intent=\":literal\"><semantics><mi>𝐱</mi><annotation encoding=\"application/x-tex\">\\mathbf{x}</annotation></semantics></math>, a multiscale decomposition can be expressed as:</p>\n</div>\n<div class=\"ltx_para\" id=\"S3.SS2.p2\">\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S3.E2\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\mathbf{x}=\\sum_{s=1}^{S}\\mathcal{D}_{s}(\\mathbf{x})+\\mathcal{R}(\\mathbf{x})\" class=\"ltx_Math\" display=\"block\" id=\"S3.E2.m1\" intent=\":literal\"><semantics><mrow><mi>𝐱</mi><mo rspace=\"0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "514", "text": "E2.m1\" intent=\":literal\"><semantics><mrow><mi>𝐱</mi><mo rspace=\"0.111em\">=</mo><mrow><mrow><munderover><mo movablelimits=\"false\">∑</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">𝒟</mi><mi>s</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mi>𝐱</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">ℛ</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mi>𝐱</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{x}=\\sum_{s=1}^{S}\\mathcal{D}_{s}(\\mathbf{x})+\\mathcal{R}(\\mathbf{x})</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(2)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div class=\"ltx_para\" id=\"S3.SS2.p3\">\n<p class=\"ltx_p\" id=\"S3.SS2.p3.3\">where <math alttext=\"\\mathcal{D}_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "515", "text": "SS2.p3.3\">where <math alttext=\"\\mathcal{D}_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">𝒟</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{D}_{s}</annotation></semantics></math> represents the detail component at scale <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.2.m2\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math>, and <math alttext=\"\\mathcal{R}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.3.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">ℛ</mi><annotation encoding=\"application/x-tex\">\\mathcal{R}</annotation></semantics></math> denotes the residual (coarse) component. This decomposition forms the structural basis for MAHA’s hierarchical processing layers.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S3.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S3.SS3.5.1.1\">III-C</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S3.SS3.6.2\">Game-Theoretic Optimization</span>\n</h3>\n<div class=\"ltx_para\" id=\"S3.SS3.p1\">\n<p class=\"ltx_p\" id=\"S3.SS3.p1.4\">Game theory provides mathematical tools for modeling interactions between multiple decision-makers.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "516", "text": "The concept of Nash equilibrium <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nash2024a</span>]</cite> is particularly relevant for MAHA’s aggregation phase, where different attention scales can be modeled as “players” competing for influence in the final representation. Given a game with <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.1.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> players and strategy sets <math alttext=\"S_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.2.m2\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">S_{i}</annotation></semantics></math>, a Nash equilibrium is a strategy profile <math alttext=\"s^{*}=(s_{1}^{*},\\dots,s_{N}^{*})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.3.m3\" intent=\":literal\"><semantics><mrow><msup><mi>s</mi><mo>∗</mo></msup><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>s</mi><mn>1</mn><mo>∗</mo></msubsup><mo>,</mo><mi mathvariant=\"normal\">…</mi><mo>,</mo><msubsup><mi>s</mi><mi>N</mi><mo>∗</mo></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">s^{*}=(s_{1}^{*},\\dots,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "517", "text": "\\dots,s_{N}^{*})</annotation></semantics></math> such that for every player <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.4.m4\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>:</p>\n</div>\n<div class=\"ltx_para\" id=\"S3.SS3.p2\">\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S3.E3\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"u_{i}(s_{i}^{*},s_{-i}^{*})\\geq u_{i}(s_{i},s_{-i}^{*})\\quad\\forall s_{i}\\in S_{i}\" class=\"ltx_Math\" display=\"block\" id=\"S3.E3.m1\" intent=\":literal\"><semantics><mrow><mrow><mrow><msub><mi>u</mi><mi>i</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>s</mi><mi>i</mi><mo>∗</mo></msubsup><mo>,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "518", "text": "</mo><msubsup><mi>s</mi><mrow><mo>−</mo><mi>i</mi></mrow><mo>∗</mo></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>≥</mo><mrow><msub><mi>u</mi><mi>i</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mi>i</mi></msub><mo>,</mo><msubsup><mi>s</mi><mrow><mo>−</mo><mi>i</mi></mrow><mo>∗</mo></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mspace style=\"width:1.167em;\" width=\"1.167em\"></mspace><mrow><mrow><mo rspace=\"0.167em\">∀</mo><msub><mi>s</mi><mi>i</mi></msub></mrow><mo>∈</mo><msub><mi>S</mi><mi>i</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">u_{i}(s_{i}^{*},s_{-i}^{*})\\geq u_{i}(s_{i},s_{-i}^{*})\\quad\\forall s_{i}\\in S_{i}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(3)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div class=\"ltx_para\" id=\"S3.SS3.p3\">\n<p class=\"ltx_p\" id=\"S3.SS3.p3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "519", "text": "SS3.p3\">\n<p class=\"ltx_p\" id=\"S3.SS3.p3.3\">where <math alttext=\"u_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.1.m1\" intent=\":literal\"><semantics><msub><mi>u</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">u_{i}</annotation></semantics></math> is the utility function for player <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.2.m2\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> and <math alttext=\"s_{-i}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.3.m3\" intent=\":literal\"><semantics><msubsup><mi>s</mi><mrow><mo>−</mo><mi>i</mi></mrow><mo>∗</mo></msubsup><annotation encoding=\"application/x-tex\">s_{-i}^{*}</annotation></semantics></math> denotes the strategies of all other players.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "520", "text": "This equilibrium condition ensures that no scale (player) can improve its contribution utility by unilaterally changing its attention weights, leading to a stable and optimal context representation.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S3.SS4\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S3.SS4.5.1.1\">III-D</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S3.SS4.6.2\">Convex Optimization in Attention</span>\n</h3>\n<div class=\"ltx_para\" id=\"S3.SS4.p1\">\n<p class=\"ltx_p\" id=\"S3.SS4.p1.1\">Convex optimization provides a principled method to combine multiple objectives under constraints. The general form of a convex optimization problem is defined as:</p>\n</div>\n<div class=\"ltx_para\" id=\"S3.SS4.p2\">\n<table class=\"ltx_equationgroup ltx_eqn_table\" id=\"S3.E4\">\n<tbody>\n<tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\" id=\"S3.E4X\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_eqn_cell\"></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle\\min_{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.E4X.2.1.1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "521", "text": "E4X.2.1.1.m1\" intent=\":literal\"><semantics><munder><mi>min</mi><mi>x</mi></munder><annotation encoding=\"application/x-tex\">\\displaystyle\\min_{x}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_eqn_cell\"></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle f(x)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.E4X.4.1.1.m1\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle f(x)</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"2\"><span class=\"ltx_tag ltx_tag_equationgroup ltx_align_right\">(4)</span></td>\n</tr>\n<tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\" id=\"S3.E4Xa\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_eqn_cell\"></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><span class=\"ltx_text ltx_markedasmath ltx_font_italic\" id=\"S3.E4Xa.2.1.1.1\">subject to</span></td>\n<td class=\"ltx_td ltx_eqn_cell\"></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle g_{i}(x)\\leq 0,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "522", "text": "\\quad h_{j}(x)=0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.E4Xa.4.1.1.m1\" intent=\":literal\"><semantics><mrow><mrow><mrow><msub><mi>g</mi><mi>i</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>≤</mo><mn>0</mn></mrow><mo rspace=\"1.167em\">,</mo><mrow><mrow><msub><mi>h</mi><mi>j</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mn>0</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle g_{i}(x)\\leq 0,\\quad h_{j}(x)=0</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr>\n</tbody>\n</table>\n</div>\n<div class=\"ltx_para\" id=\"S3.SS4.p3\">\n<p class=\"ltx_p\" id=\"S3.SS4.p3.3\">where <math alttext=\"f\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p3.1.m1\" intent=\":literal\"><semantics><mi>f</mi><annotation encoding=\"application/x-tex\">f</annotation></semantics></math> is the convex objective function, <math alttext=\"g_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p3.2.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "523", "text": "<math alttext=\"g_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p3.2.m2\" intent=\":literal\"><semantics><msub><mi>g</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">g_{i}</annotation></semantics></math> are convex inequality constraints, and <math alttext=\"h_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p3.3.m3\" intent=\":literal\"><semantics><msub><mi>h</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">h_{j}</annotation></semantics></math> are affine equality constraints. In MAHA, this framework is utilized to aggregate attention scores from different scales while enforcing constraints that preserve important linguistic properties, such as probability distribution validity and sparsity.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S4\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">IV </span><span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.1.1\">The MAHA Framework</span>\n</h2>\n<div class=\"ltx_para\" id=\"S4.p1\">\n<p class=\"ltx_p\" id=\"S4.p1.1\">The MAHA framework introduces a systematic approach to sequence modeling by decomposing the input into multiple hierarchical scales and synthesizing them through mathematically rigorous aggregation rules. This section details the hierarchical decomposition strategy, scale-specific attention computation, and the optimization-driven mechanisms that govern information fusion. As illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.14925v2#S4.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "524", "text": "As illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.14925v2#S4.F1\" title=\"Figure 1 ‣ IV-A Hierarchical Multiscale Decomposition with Learnable Downsampling ‣ IV The MAHA Framework ‣ Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, MAHA is designed to replace the standard multi-head attention layer in transformer blocks while maintaining architectural compatibility.</p>\n</div>\n<section class=\"ltx_subsection\" id=\"S4.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S4.SS1.5.1.1\">IV-A</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S4.SS1.6.2\">Hierarchical Multiscale Decomposition with Learnable Downsampling</span>\n</h3>\n<div class=\"ltx_para\" id=\"S4.SS1.p1\">\n<p class=\"ltx_p\" id=\"S4.SS1.p1.8\">Let <math alttext=\"\\mathbf{X}\\in\\mathbb{R}^{n\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.1.m1\" intent=\":literal\"><semantics><mrow><mi>𝐗</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>n</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}\\in\\mathbb{R}^{n\\times d}</annotation></semantics></math> denote the input sequence,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "525", "text": "where <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.2.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> is the sequence length and <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.3.m3\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> is the embedding dimension. MAHA decomposes <math alttext=\"\\mathbf{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.4.m4\" intent=\":literal\"><semantics><mi>𝐗</mi><annotation encoding=\"application/x-tex\">\\mathbf{X}</annotation></semantics></math> into <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.5.m5\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> hierarchical scales through a series of learnable downsampling operations. Each scale <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.6.m6\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math> is derived from the previous scale <math alttext=\"l-1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.7.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "526", "text": "SS1.p1.7.m7\" intent=\":literal\"><semantics><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">l-1</annotation></semantics></math> using a parameterized operator <math alttext=\"\\mathcal{D}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.8.m8\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">𝒟</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{D}_{l}</annotation></semantics></math>:</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS1.p2\">\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S4.E5\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\mathbf{X}_{l}=\\mathcal{D}_{l}(\\mathbf{X}_{l-1}),\\quad\\mathbf{X}_{0}=\\mathbf{X}\" class=\"ltx_Math\" display=\"block\" id=\"S4.E5.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "527", "text": "\\quad\\mathbf{X}_{0}=\\mathbf{X}\" class=\"ltx_Math\" display=\"block\" id=\"S4.E5.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>𝐗</mi><mi>l</mi></msub><mo>=</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">𝒟</mi><mi>l</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝐗</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo rspace=\"1.167em\">,</mo><mrow><msub><mi>𝐗</mi><mn>0</mn></msub><mo>=</mo><mi>𝐗</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}_{l}=\\mathcal{D}_{l}(\\mathbf{X}_{l-1}),\\quad\\mathbf{X}_{0}=\\mathbf{X}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(5)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS1.p3\">\n<p class=\"ltx_p\" id=\"S4.SS1.p3.1\">The downsampling operator <math alttext=\"\\mathcal{D}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "528", "text": "SS1.p3.1\">The downsampling operator <math alttext=\"\\mathcal{D}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">𝒟</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{D}_{l}</annotation></semantics></math> is implemented via one of two mechanisms:</p>\n<ol class=\"ltx_enumerate\" id=\"S4.I1\">\n<li class=\"ltx_item\" id=\"S4.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">1.</span>\n<div class=\"ltx_para\" id=\"S4.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"S4.I1.i1.p1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.I1.i1.p1.3.1\">Strided Convolution:</span> <math alttext=\"\\mathcal{D}_{l}(\\mathbf{X})=\\text{Conv1D}(\\mathbf{X},\\mathbf{W}_{l}^{s},s_{l})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i1.p1.1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "529", "text": "\\mathbf{W}_{l}^{s},s_{l})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i1.p1.1.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">𝒟</mi><mi>l</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mi>𝐗</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mtext>Conv1D</mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mi>𝐗</mi><mo>,</mo><msubsup><mi>𝐖</mi><mi>l</mi><mi>s</mi></msubsup><mo>,</mo><msub><mi>s</mi><mi>l</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{D}_{l}(\\mathbf{X})=\\text{Conv1D}(\\mathbf{X},\\mathbf{W}_{l}^{s},s_{l})</annotation></semantics></math>, where <math alttext=\"\\mathbf{W}_{l}^{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i1.p1.2.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "530", "text": "where <math alttext=\"\\mathbf{W}_{l}^{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i1.p1.2.m2\" intent=\":literal\"><semantics><msubsup><mi>𝐖</mi><mi>l</mi><mi>s</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{W}_{l}^{s}</annotation></semantics></math> is a learnable kernel and <math alttext=\"s_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i1.p1.3.m3\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">s_{l}</annotation></semantics></math> is the stride.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S4.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">2.</span>\n<div class=\"ltx_para\" id=\"S4.I1.i2.p1\">\n<p class=\"ltx_p\" id=\"S4.I1.i2.p1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.I1.i2.p1.2.1\">Adaptive Pooling:</span> <math alttext=\"\\mathcal{D}_{l}(\\mathbf{X})=\\text{AdaptiveMaxPool}(\\mathbf{X},n_{l})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i2.p1.1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "531", "text": "n_{l})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i2.p1.1.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">𝒟</mi><mi>l</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mi>𝐗</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mtext>AdaptiveMaxPool</mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mi>𝐗</mi><mo>,</mo><msub><mi>n</mi><mi>l</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{D}_{l}(\\mathbf{X})=\\text{AdaptiveMaxPool}(\\mathbf{X},n_{l})</annotation></semantics></math>, which dynamically adjusts the pooling window to match the target length <math alttext=\"n_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i2.p1.2.m2\" intent=\":literal\"><semantics><msub><mi>n</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">n_{l}</annotation></semantics></math>.</p>\n</div>\n</li>\n</ol>\n</div>\n<figure class=\"ltx_figure\" id=\"S4.F1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"837\" id=\"S4.F1.g1\" src=\"x1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "532", "text": "F1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"837\" id=\"S4.F1.g1\" src=\"x1.png\" width=\"747\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S4.F1.2.1.1\" style=\"font-size:90%;\">Figure 1</span>: </span><span class=\"ltx_text\" id=\"S4.F1.3.2\" style=\"font-size:90%;\">Schematic overview of the MAHA architecture integrated within a Transformer block.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "533", "text": "F1.3.2\" style=\"font-size:90%;\">Schematic overview of the MAHA architecture integrated within a Transformer block. The input is decomposed into multiple scales, processed via shared-value attention, and aggregated using optimization or game-theoretic layers.</span></figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S4.SS1.p4\">\n<p class=\"ltx_p\" id=\"S4.SS1.p4.2\">The sequence lengths follow an exponential decay schedule <math alttext=\"n_{l}=\\lfloor n_{l-1}/r\\rfloor\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>n</mi><mi>l</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">⌊</mo><mrow><msub><mi>n</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>/</mo><mi>r</mi></mrow><mo stretchy=\"false\">⌋</mo></mrow></mrow><annotation encoding=\"application/x-tex\">n_{l}=\\lfloor n_{l-1}/r\\rfloor</annotation></semantics></math>, where <math alttext=\"r&gt;1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.2.m2\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">r&gt;1</annotation></semantics></math> is a compression ratio hyperparameter.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "534", "text": "This creates a pyramidal structure where higher scales capture increasingly coarse-grained semantic patterns while preserving essential features.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S4.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S4.SS2.5.1.1\">IV-B</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S4.SS2.6.2\">Multiscale Attention Computation</span>\n</h3>\n<div class=\"ltx_para\" id=\"S4.SS2.p1\">\n<p class=\"ltx_p\" id=\"S4.SS2.p1.5\">At each scale <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.1.m1\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math>, MAHA computes independent attention matrices.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "535", "text": "A key innovation in MAHA is the decoupling of projection parameters to enhance efficiency: while Query (<math alttext=\"\\mathbf{Q}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.2.m2\" intent=\":literal\"><semantics><mi>𝐐</mi><annotation encoding=\"application/x-tex\">\\mathbf{Q}</annotation></semantics></math>) and Key (<math alttext=\"\\mathbf{K}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.3.m3\" intent=\":literal\"><semantics><mi>𝐊</mi><annotation encoding=\"application/x-tex\">\\mathbf{K}</annotation></semantics></math>) projections are scale-specific, the Value (<math alttext=\"\\mathbf{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.4.m4\" intent=\":literal\"><semantics><mi>𝐕</mi><annotation encoding=\"application/x-tex\">\\mathbf{V}</annotation></semantics></math>) projection is shared across scales. Given the representation <math alttext=\"\\mathbf{X}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.5.m5\" intent=\":literal\"><semantics><msub><mi>𝐗</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{X}_{l}</annotation></semantics></math>, the projections are defined as:</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS2.p2\">\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S4.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "536", "text": "the projections are defined as:</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS2.p2\">\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S4.E6\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\mathbf{Q}_{l}=\\mathbf{X}_{l}\\mathbf{W}_{l}^{Q},\\quad\\mathbf{K}_{l}=\\mathbf{X}_{l}\\mathbf{W}_{l}^{K}\" class=\"ltx_Math\" display=\"block\" id=\"S4.E6.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>𝐐</mi><mi>l</mi></msub><mo>=</mo><mrow><msub><mi>𝐗</mi><mi>l</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>𝐖</mi><mi>l</mi><mi>Q</mi></msubsup></mrow></mrow><mo rspace=\"1.167em\">,</mo><mrow><msub><mi>𝐊</mi><mi>l</mi></msub><mo>=</mo><mrow><msub><mi>𝐗</mi><mi>l</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>𝐖</mi><mi>l</mi><mi>K</mi></msubsup></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Q}_{l}=\\mathbf{X}_{l}\\mathbf{W}_{l}^{Q},", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "537", "text": "\\quad\\mathbf{K}_{l}=\\mathbf{X}_{l}\\mathbf{W}_{l}^{K}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(6)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS2.p3\">\n<p class=\"ltx_p\" id=\"S4.SS2.p3.2\">where <math alttext=\"\\mathbf{W}_{l}^{Q},\\mathbf{W}_{l}^{K}\\in\\mathbb{R}^{d\\times d_{k}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.1.m1\" intent=\":literal\"><semantics><mrow><mrow><msubsup><mi>𝐖</mi><mi>l</mi><mi>Q</mi></msubsup><mo>,</mo><msubsup><mi>𝐖</mi><mi>l</mi><mi>K</mi></msubsup></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><msub><mi>d</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{W}_{l}^{Q},\\mathbf{W}_{l}^{K}\\in\\mathbb{R}^{d\\times d_{k}}</annotation></semantics></math>. The attention weights <math alttext=\"\\mathbf{A}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.2.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "538", "text": "The attention weights <math alttext=\"\\mathbf{A}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.2.m2\" intent=\":literal\"><semantics><msub><mi>𝐀</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{A}_{l}</annotation></semantics></math> are computed via the scaled dot-product:</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S4.Ex1\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\mathbf{A}_{l}=\\text{softmax}\\left(\\frac{\\mathbf{Q}_{l}\\mathbf{K}_{l}^{T}}{\\sqrt{d_{k}}}\\right)\" class=\"ltx_Math\" display=\"block\" id=\"S4.Ex1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "539", "text": "Ex1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>𝐀</mi><mi>l</mi></msub><mo>=</mo><mrow><mtext>softmax</mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo>(</mo><mfrac><mrow><msub><mi>𝐐</mi><mi>l</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>𝐊</mi><mi>l</mi><mi>T</mi></msubsup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo>)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{A}_{l}=\\text{softmax}\\left(\\frac{\\mathbf{Q}_{l}\\mathbf{K}_{l}^{T}}{\\sqrt{d_{k}}}\\right)</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS2.p4\">\n<p class=\"ltx_p\" id=\"S4.SS2.p4.5\">Unlike standard transformers, MAHA employs a shared value projection: <math alttext=\"\\mathbf{V}_{base}=\\mathbf{X}\\mathbf{W}^{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "540", "text": "SS2.p4.1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>𝐕</mi><mrow><mi>b</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>e</mi></mrow></msub><mo>=</mo><msup><mi>𝐗𝐖</mi><mi>V</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{V}_{base}=\\mathbf{X}\\mathbf{W}^{V}</annotation></semantics></math>. The value matrix for scale <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.2.m2\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math>, denoted as <math alttext=\"\\mathbf{V}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.3.m3\" intent=\":literal\"><semantics><msub><mi>𝐕</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{V}_{l}</annotation></semantics></math>, is obtained by applying the corresponding downsampling operator to the base values: <math alttext=\"\\mathbf{V}_{l}=\\mathcal{D}_{l}(\\mathbf{V}_{base})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.4.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "541", "text": "SS2.p4.4.m4\" intent=\":literal\"><semantics><mrow><msub><mi>𝐕</mi><mi>l</mi></msub><mo>=</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">𝒟</mi><mi>l</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝐕</mi><mrow><mi>b</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>e</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{V}_{l}=\\mathcal{D}_{l}(\\mathbf{V}_{base})</annotation></semantics></math>. The scale-specific output <math alttext=\"\\mathbf{O}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.5.m5\" intent=\":literal\"><semantics><msub><mi>𝐎</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{O}_{l}</annotation></semantics></math> is then:</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS2.p5\">\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S4.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "542", "text": "SS2.p5\">\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S4.Ex2\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\mathbf{O}_{l}=\\mathbf{A}_{l}\\mathbf{V}_{l}\" class=\"ltx_Math\" display=\"block\" id=\"S4.Ex2.m1\" intent=\":literal\"><semantics><mrow><msub><mi>𝐎</mi><mi>l</mi></msub><mo>=</mo><mrow><msub><mi>𝐀</mi><mi>l</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝐕</mi><mi>l</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}_{l}=\\mathbf{A}_{l}\\mathbf{V}_{l}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS2.p6\">\n<p class=\"ltx_p\" id=\"S4.SS2.p6.1\">This design reduces the parameter count significantly while ensuring that the information flow remains consistent across granularity levels.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S4.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S4.SS3.5.1.1\">IV-C</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S4.SS3.6.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "543", "text": "SS3.5.1.1\">IV-C</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S4.SS3.6.2\">Aggregation of Multiscale Attention Outputs</span>\n</h3>\n<div class=\"ltx_para\" id=\"S4.SS3.p1\">\n<p class=\"ltx_p\" id=\"S4.SS3.p1.2\">The multiscale outputs <math alttext=\"\\{\\mathbf{O}_{l}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>𝐎</mi><mi>l</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{\\mathbf{O}_{l}\\}</annotation></semantics></math> must be synthesized into a unified representation <math alttext=\"\\mathbf{O}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.2.m2\" intent=\":literal\"><semantics><msup><mi>𝐎</mi><mo>∗</mo></msup><annotation encoding=\"application/x-tex\">\\mathbf{O}^{*}</annotation></semantics></math>. MAHA proposes two rigorous strategies:</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS3.p2\">\n<p class=\"ltx_p\" id=\"S4.SS3.p2.4\">CO-Based Aggregation: We formulate aggregation as a convex optimization problem.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "544", "text": "Let <math alttext=\"\\mathcal{U}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">𝒰</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{U}_{l}</annotation></semantics></math> denote an upsampling operator mapping <math alttext=\"\\mathbf{O}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.2.m2\" intent=\":literal\"><semantics><msub><mi>𝐎</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{O}_{l}</annotation></semantics></math> back to the original sequence length <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.3.m3\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "545", "text": "The aggregated output is obtained by solving for the optimal mixing weights <math alttext=\"\\mathbf{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.4.m4\" intent=\":literal\"><semantics><mi>𝐰</mi><annotation encoding=\"application/x-tex\">\\mathbf{w}</annotation></semantics></math>:</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS3.p3\">\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S4.E7\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\min_{\\mathbf{w}}\\left\\|\\sum_{l=0}^{L}w_{l}\\mathcal{U}_{l}(\\mathbf{O}_{l})-\\mathbf{O}^{*}\\right\\|_{F}^{2}+\\lambda\\|\\mathbf{w}\\|_{1}\\quad\\text{s.t.}\\sum w_{l}=1,w_{l}\\geq 0\" class=\"ltx_Math\" display=\"block\" id=\"S4.E7.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "546", "text": "}\\sum w_{l}=1,w_{l}\\geq 0\" class=\"ltx_Math\" display=\"block\" id=\"S4.E7.m1\" intent=\":literal\"><semantics><mrow><mrow><mrow><mrow><mrow><munder><mi>min</mi><mi>𝐰</mi></munder><mo>⁡</mo><msubsup><mrow><mo>‖</mo><mrow><mrow><munderover><mo lspace=\"0em\" movablelimits=\"false\">∑</mo><mrow><mi>l</mi><mo>=</mo><mn>0</mn></mrow><mi>L</mi></munderover><mrow><msub><mi>w</mi><mi>l</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi class=\"ltx_font_mathcaligraphic\">𝒰</mi><mi>l</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝐎</mi><mi>l</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>−</mo><msup><mi>𝐎</mi><mo>∗</mo></msup></mrow><mo>‖</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mi>λ</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mrow><mo stretchy=\"false\">‖</mo><mi>𝐰</mi><mo stretchy=\"false\">‖</mo></mrow><mn>1</mn></msub></mrow></mrow><mspace style=\"width:1em;", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "547", "text": "\" width=\"1em\"></mspace><mrow><mtext>s.t.</mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo movablelimits=\"false\">∑</mo><msub><mi>w</mi><mi>l</mi></msub></mrow></mrow></mrow><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><msub><mi>w</mi><mi>l</mi></msub><mo>≥</mo><mn>0</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\min_{\\mathbf{w}}\\left\\|\\sum_{l=0}^{L}w_{l}\\mathcal{U}_{l}(\\mathbf{O}_{l})-\\mathbf{O}^{*}\\right\\|_{F}^{2}+\\lambda\\|\\mathbf{w}\\|_{1}\\quad\\text{s.t.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "548", "text": "t.}\\sum w_{l}=1,w_{l}\\geq 0</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(7)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS3.p4\">\n<p class=\"ltx_p\" id=\"S4.SS3.p4.1\">Here, <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.1.m1\" intent=\":literal\"><semantics><mi>λ</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> controls sparsity, encouraging the model to select the most informative scales.</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS3.p5\">\n<p class=\"ltx_p\" id=\"S4.SS3.p5.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.SS3.p5.2.1\">Nash Equilibrium-Based Aggregation:</span> Alternatively, aggregation is modeled as a non-cooperative game where each scale <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.1.m1\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math> competes to minimize its reconstruction error. The equilibrium weights <math alttext=\"w_{l}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.2.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "549", "text": "The equilibrium weights <math alttext=\"w_{l}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.2.m2\" intent=\":literal\"><semantics><msubsup><mi>w</mi><mi>l</mi><mo>∗</mo></msubsup><annotation encoding=\"application/x-tex\">w_{l}^{*}</annotation></semantics></math> satisfy:</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS3.p6\">\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S4.E8\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"w_{l}^{*}=\\arg\\min_{w_{l}}\\left\\|\\mathcal{U}_{l}(\\mathbf{O}_{l})-\\mathbf{O}^{*}(\\mathbf{w}_{-l}^{*})\\right\\|_{2}^{2}\" class=\"ltx_Math\" display=\"block\" id=\"S4.E8.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi>w</mi><mi>l</mi><mo>∗</mo></msubsup><mo>=</mo><mrow><mi>arg</mi><mo lspace=\"0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "550", "text": "167em\">⁡</mo><mrow><munder><mi>min</mi><msub><mi>w</mi><mi>l</mi></msub></munder><mo>⁡</mo><msubsup><mrow><mo>‖</mo><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">𝒰</mi><mi>l</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝐎</mi><mi>l</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>−</mo><mrow><msup><mi>𝐎</mi><mo>∗</mo></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>𝐰</mi><mrow><mo>−</mo><mi>l</mi></mrow><mo>∗</mo></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">w_{l}^{*}=\\arg\\min_{w_{l}}\\left\\|\\mathcal{U}_{l}(\\mathbf{O}_{l})-\\mathbf{O}^{*}(\\mathbf{w}_{-l}^{*})\\right\\|_{2}^{2}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "551", "text": "<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(8)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS3.p7\">\n<p class=\"ltx_p\" id=\"S4.SS3.p7.1\">This ensures that no scale can unilaterally improve the representation quality.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S4.SS4\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S4.SS4.5.1.1\">IV-D</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S4.SS4.6.2\">Hybrid Dilated-Convolutional Transformer Design</span>\n</h3>\n<div class=\"ltx_para\" id=\"S4.SS4.p1\">\n<p class=\"ltx_p\" id=\"S4.SS4.p1.1\">MAHA integrates dilated convolutions to capture local context prior to attention. The hybrid block consists of:</p>\n<ul class=\"ltx_itemize\" id=\"S4.I2\">\n<li class=\"ltx_item\" id=\"S4.I2.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S4.I2.i1.p1\">\n<p class=\"ltx_p\" id=\"S4.I2.i1.p1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.I2.i1.p1.2.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "552", "text": "I2.i1.p1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.I2.i1.p1.2.1\">Dilated Convolution Blocks:</span> For scale <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I2.i1.p1.1.m1\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math>, the output is <math alttext=\"\\mathbf{C}_{l}=\\text{ReLU}(\\text{DilatedConv}(\\mathbf{X}_{l}))\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I2.i1.p1.2.m2\" intent=\":literal\"><semantics><mrow><msub><mi>𝐂</mi><mi>l</mi></msub><mo>=</mo><mrow><mtext>ReLU</mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mtext>DilatedConv</mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝐗</mi><mi>l</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{C}_{l}=\\text{ReLU}(\\text{DilatedConv}(\\mathbf{X}_{l}))</annotation></semantics></math>.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S4.I2.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S4.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "553", "text": "I2.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S4.I2.i2.p1\">\n<p class=\"ltx_p\" id=\"S4.I2.i2.p1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.I2.i2.p1.3.1\">Cross-Scale Gating:</span> <math alttext=\"\\mathbf{G}_{l}=\\sigma(\\mathbf{W}_{g}\\mathbf{X}_{l})\\odot\\mathbf{X}_{l-1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I2.i2.p1.1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>𝐆</mi><mi>l</mi></msub><mo>=</mo><mrow><mrow><mi>σ</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>𝐖</mi><mi>g</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝐗</mi><mi>l</mi></msub></mrow><mo rspace=\"0.055em\" stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"0.222em\">⊙</mo><msub><mi>𝐗</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{G}_{l}=\\sigma(\\mathbf{W}_{g}\\mathbf{X}_{l})\\odot\\mathbf{X}_{l-1}</annotation></semantics></math>,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "554", "text": "where <math alttext=\"\\sigma\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I2.i2.p1.2.m2\" intent=\":literal\"><semantics><mi>σ</mi><annotation encoding=\"application/x-tex\">\\sigma</annotation></semantics></math> is the sigmoid function and <math alttext=\"\\odot\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I2.i2.p1.3.m3\" intent=\":literal\"><semantics><mo>⊙</mo><annotation encoding=\"application/x-tex\">\\odot</annotation></semantics></math> denotes element-wise multiplication.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S4.I2.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S4.I2.i3.p1\">\n<p class=\"ltx_p\" id=\"S4.I2.i3.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.I2.i3.p1.1.1\">Nearest-Neighbor Upsampling:</span> Used to reconstruct the full sequence efficiently.</p>\n</div>\n</li>\n</ul>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S4.SS5\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S4.SS5.5.1.1\">IV-E</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S4.SS5.6.2\">Complexity Reduction through Hierarchical Sparsity</span>\n</h3>\n<div class=\"ltx_para\" id=\"S4.SS5.p1\">\n<p class=\"ltx_p\" id=\"S4.SS5.p1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "555", "text": "SS5.p1\">\n<p class=\"ltx_p\" id=\"S4.SS5.p1.1\">The total computational complexity of MAHA is governed by the hierarchical decomposition. For a sequence of length <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>, the complexity is defined as:</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS5.p2\">\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S4.E9\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\Omega(n)=\\sum_{l=0}^{L}\\left(\\frac{n}{r^{l}}\\right)^{2}d+O(n\\log n)\" class=\"ltx_Math\" display=\"block\" id=\"S4.E9.m1\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">Ω</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "556", "text": "111em\">=</mo><mrow><mrow><munderover><mo movablelimits=\"false\" rspace=\"0em\">∑</mo><mrow><mi>l</mi><mo>=</mo><mn>0</mn></mrow><mi>L</mi></munderover><mrow><msup><mrow><mo>(</mo><mfrac><mi>n</mi><msup><mi>r</mi><mi>l</mi></msup></mfrac><mo>)</mo></mrow><mn>2</mn></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>d</mi></mrow></mrow><mo>+</mo><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>n</mi><mo lspace=\"0.167em\" rspace=\"0em\">​</mo><mrow><mi>log</mi><mo lspace=\"0.167em\">⁡</mo><mi>n</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\Omega(n)=\\sum_{l=0}^{L}\\left(\\frac{n}{r^{l}}\\right)^{2}d+O(n\\log n)</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(9)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS5.p3\">\n<p class=\"ltx_p\" id=\"S4.SS5.p3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "557", "text": "SS5.p3\">\n<p class=\"ltx_p\" id=\"S4.SS5.p3.1\">For <math alttext=\"r=2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.1.m1\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">r=2</annotation></semantics></math>, the geometric series converges, yielding:</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS5.p4\">\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S4.E10\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"O\\left(\\frac{n^{2}}{r^{2}-1}\\right)\" class=\"ltx_Math\" display=\"block\" id=\"S4.E10.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "558", "text": "E10.m1\" intent=\":literal\"><semantics><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo>(</mo><mfrac><msup><mi>n</mi><mn>2</mn></msup><mrow><msup><mi>r</mi><mn>2</mn></msup><mo>−</mo><mn>1</mn></mrow></mfrac><mo>)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">O\\left(\\frac{n^{2}}{r^{2}-1}\\right)</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(10)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS5.p5\">\n<p class=\"ltx_p\" id=\"S4.SS5.p5.1\">which is significantly lower than standard attention.</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS5.p6\">\n<ul class=\"ltx_itemize\" id=\"S4.I3\">\n<li class=\"ltx_item\" id=\"S4.I3.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S4.I3.i1.p1\">\n<p class=\"ltx_p\" id=\"S4.I3.i1.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.I3.i1.p1.1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "559", "text": "I3.i1.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.I3.i1.p1.1.1\">Scale-Specific Sparsity:</span> Coarser scales have <math alttext=\"n_{l}\\ll n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I3.i1.p1.1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>n</mi><mi>l</mi></msub><mo>≪</mo><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n_{l}\\ll n</annotation></semantics></math>, reducing the cost quadratically.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S4.I3.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S4.I3.i2.p1\">\n<p class=\"ltx_p\" id=\"S4.I3.i2.p1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.I3.i2.p1.2.1\">Dynamic Weight Sparsity:</span> The <math alttext=\"\\ell_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I3.i2.p1.1.m1\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">ℓ</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">\\ell_{1}</annotation></semantics></math>-regularized weights <math alttext=\"w_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I3.i2.p1.2.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "560", "text": "I3.i2.p1.2.m2\" intent=\":literal\"><semantics><msub><mi>w</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">w_{l}</annotation></semantics></math> prune uninformative scales during inference.</p>\n</div>\n</li>\n</ul>\n</div>\n<figure class=\"ltx_figure\" id=\"S4.F2\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"468\" id=\"S4.F2.g1\" src=\"x2.png\" width=\"664\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S4.F2.2.1.1\" style=\"font-size:90%;\">Figure 2</span>: </span><span class=\"ltx_text\" id=\"S4.F2.3.2\" style=\"font-size:90%;\">Computational complexity comparison. MAHA demonstrates near-linear scaling compared to the quadratic growth of standard Self-Attention.</span></figcaption>\n</figure>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S5\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">V </span><span class=\"ltx_text ltx_font_smallcaps\" id=\"S5.1.1\">Experiments</span>\n</h2>\n<div class=\"ltx_para\" id=\"S5.p1\">\n<p class=\"ltx_p\" id=\"S5.p1.1\">To evaluate the empirical efficacy of MAHA, we conducted extensive experiments across diverse NLP tasks. Our evaluation framework focuses on three pivotal research questions:</p>\n<ul class=\"ltx_itemize\" id=\"S5.I1\">\n<li class=\"ltx_item\" id=\"S5.I1.i1\" style=\"list-style-type:none;", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "561", "text": "I1\">\n<li class=\"ltx_item\" id=\"S5.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S5.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"S5.I1.i1.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.I1.i1.p1.1.1\">RQ1:</span> How does MAHA compare to state-of-the-art attention mechanisms in terms of computational efficiency and downstream task performance?</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S5.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S5.I1.i2.p1\">\n<p class=\"ltx_p\" id=\"S5.I1.i2.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.I1.i2.p1.1.1\">RQ2:</span> What is the comparative impact of convex optimization versus game-theoretic aggregation strategies on model behavior?</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S5.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S5.I1.i3.p1\">\n<p class=\"ltx_p\" id=\"S5.I1.i3.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.I1.i3.p1.1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "562", "text": "I1.i3.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.I1.i3.p1.1.1\">RQ3:</span> How does the granularity of the hierarchical decomposition affect the trade-off between representational accuracy and computational cost?</p>\n</div>\n</li>\n</ul>\n</div>\n<section class=\"ltx_subsection\" id=\"S5.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S5.SS1.5.1.1\">V-A</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S5.SS1.6.2\">Experimental Setup</span>\n</h3>\n<div class=\"ltx_para\" id=\"S5.SS1.p1\">\n<p class=\"ltx_p\" id=\"S5.SS1.p1.1\">We evaluated MAHA on four benchmark datasets designed to stress-test different aspects of sequence modeling:</p>\n<ul class=\"ltx_itemize\" id=\"S5.I2\">\n<li class=\"ltx_item\" id=\"S5.I2.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S5.I2.i1.p1\">\n<p class=\"ltx_p\" id=\"S5.I2.i1.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.I2.i1.p1.1.1\">Text Classification:</span> GLUE benchmark <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2018a</span>]</cite>, focusing on MNLI and SST-2.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S5.I2.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "563", "text": "focusing on MNLI and SST-2.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S5.I2.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S5.I2.i2.p1\">\n<p class=\"ltx_p\" id=\"S5.I2.i2.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.I2.i2.p1.1.1\">Long-Range Dependency Modeling:</span> PG-19 dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sun2021a</span>]</cite> (<math alttext=\"&gt;4k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.I2.i2.p1.1.m1\" intent=\":literal\"><semantics><mrow><mi></mi><mo>&gt;</mo><mrow><mn>4</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>k</mi></mrow></mrow><annotation encoding=\"application/x-tex\">&gt;4k</annotation></semantics></math> tokens).</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S5.I2.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S5.I2.i3.p1\">\n<p class=\"ltx_p\" id=\"S5.I2.i3.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.I2.i3.p1.1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "564", "text": "I2.i3.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.I2.i3.p1.1.1\">Machine Translation:</span> WMT14 English-German <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bojar2016a</span>]</cite>.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S5.I2.i4\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S5.I2.i4.p1\">\n<p class=\"ltx_p\" id=\"S5.I2.i4.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.I2.i4.p1.1.1\">Question Answering:</span> SQuAD v2.0 <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rajpurkar2016a</span>]</cite>.</p>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"ltx_para\" id=\"S5.SS1.p2\">\n<p class=\"ltx_p\" id=\"S5.SS1.p2.1\">For comparative analysis, MAHA was benchmarked against five widely adopted attention mechanisms: Standard Multi-Head Attention (MHA) <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">vaswani2017a</span>]</cite>, Longformer <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">beltagy2020a</span>]</cite>,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "565", "text": "Longformer <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">beltagy2020a</span>]</cite>, BigBird <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zaheer2020a</span>]</cite>, Reformer <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kitaev2020a</span>]</cite>, and Performer <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">choromanski2020a</span>]</cite>.</p>\n</div>\n<div class=\"ltx_para\" id=\"S5.SS1.p3\">\n<p class=\"ltx_p\" id=\"S5.SS1.p3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.SS1.p3.1.1\">Implementation Details:</span></p>\n<ul class=\"ltx_itemize\" id=\"S5.I3\">\n<li class=\"ltx_item\" id=\"S5.I3.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S5.I3.i1.p1\">\n<p class=\"ltx_p\" id=\"S5.I3.i1.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.I3.i1.p1.1.1\">Model Architecture:</span> Transformer backbone with 12 layers, hidden dimensionality of 768, and 12 attention heads.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S5.I3.i2\" style=\"list-style-type:none;", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "566", "text": "hidden dimensionality of 768, and 12 attention heads.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S5.I3.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S5.I3.i2.p1\">\n<p class=\"ltx_p\" id=\"S5.I3.i2.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.I3.i2.p1.1.1\">Training:</span> Batch size 32 (classification), 16 (translation/QA); LR <math alttext=\"5\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.I3.i2.p1.1.m1\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5\\times 10^{-5}</annotation></semantics></math> with warmup 10k steps.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S5.I3.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S5.I3.i3.p1\">\n<p class=\"ltx_p\" id=\"S5.I3.i3.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.I3.i3.p1.1.1\">Sequence Length:</span> 512 (classification/QA), 4096 (PG-19).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "567", "text": "I3.i3.p1.1.1\">Sequence Length:</span> 512 (classification/QA), 4096 (PG-19).</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S5.I3.i4\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S5.I3.i4.p1\">\n<p class=\"ltx_p\" id=\"S5.I3.i4.p1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.I3.i4.p1.2.1\">MAHA Parameters:</span> <math alttext=\"L=4\" class=\"ltx_Math\" display=\"inline\" id=\"S5.I3.i4.p1.1.m1\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">L=4</annotation></semantics></math> scales (32, 64, 128, 256 tokens); strided conv (kernel=3); aggregation regularization <math alttext=\"\\lambda=0.1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.I3.i4.p1.2.m2\" intent=\":literal\"><semantics><mrow><mi>λ</mi><mo>=</mo><mn>0.1</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=0.1</annotation></semantics></math>.</p>\n</div>\n</li>\n</ul>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S5.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S5.SS2.5.1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "568", "text": "SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S5.SS2.5.1.1\">V-B</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S5.SS2.6.2\">Main Results</span>\n</h3>\n<div class=\"ltx_para\" id=\"S5.SS2.p1\">\n<p class=\"ltx_p\" id=\"S5.SS2.p1.1\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.14925v2#S5.T1\" title=\"TABLE I ‣ V-B Main Results ‣ V Experiments ‣ Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">I</span></a> summarizes the performance on benchmark datasets. MAHA achieves competitive accuracy with standard attention while outperforming sparse baselines on long-context tasks (PG-19).</p>\n</div>\n<figure class=\"ltx_table\" id=\"S5.T1\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S5.T1.4.1.1\" style=\"font-size:90%;\">TABLE I</span>: </span><span class=\"ltx_text\" id=\"S5.T1.5.2\" style=\"font-size:90%;\">Performance Comparison Across Tasks.</span></figcaption><div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<table class=\"ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle\" id=\"S5.T1.2\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "569", "text": "T1.2\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T1.2.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" id=\"S5.T1.2.3.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.2.3.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T1.2.3.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.2.3.1.2.1\">MNLI</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T1.2.3.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.2.3.1.3.1\">SST-2</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T1.2.3.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.2.3.1.4.1\">PG-19</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T1.2.3.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.2.3.1.5.1\">WMT</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T1.2.3.1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "570", "text": "1.5.1\">WMT</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T1.2.3.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.2.3.1.6.1\">SQuAD</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T1.2.3.1.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.2.3.1.7.1\">Memory</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.2.2\">\n<th class=\"ltx_td ltx_th ltx_th_row\" id=\"S5.T1.2.2.3\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T1.2.2.4\">(Acc)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T1.2.2.5\">(Acc)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T1.1.1.1\">(PPL) <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.1.1.1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">↓</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T1.2.2.6\">(BLEU)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T1.2.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "571", "text": "T1.2.2.6\">(BLEU)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T1.2.2.7\">(F1)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T1.2.2.2\">(GB) <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.2.2.2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">↓</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T1.2.4.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T1.2.4.1.1\">Standard MHA</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.2.4.1.2\">86.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.2.4.1.3\">93.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.2.4.1.4\">24.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.2.4.1.5\">28.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.2.4.1.6\">88.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.2.4.1.7\">15.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "572", "text": "T1.2.4.1.6\">88.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.2.4.1.7\">15.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.2.5.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T1.2.5.2.1\">Longformer</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.2.5.2.2\">85.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.2.5.2.3\">92.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.2.5.2.4\">23.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.2.5.2.5\">27.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.2.5.2.6\">87.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.2.5.2.7\">9.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.2.6.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T1.2.6.3.1\">BigBird</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.2.6.3.2\">85.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.2.6.3.3\">93.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "573", "text": "T1.2.6.3.3\">93.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.2.6.3.4\">23.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.2.6.3.5\">28.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.2.6.3.6\">87.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.2.6.3.7\">10.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.2.7.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T1.2.7.4.1\">Reformer</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.2.7.4.2\">84.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.2.7.4.3\">91.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.2.7.4.4\">25.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.2.7.4.5\">26.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.2.7.4.6\">85.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.2.7.4.7\">7.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.2.8.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "574", "text": "T1.2.8.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T1.2.8.5.1\">Performer</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.2.8.5.2\">85.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.2.8.5.3\">92.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.2.8.5.4\">24.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.2.8.5.5\">27.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.2.8.5.6\">86.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.2.8.5.7\">8.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.2.9.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\" id=\"S5.T1.2.9.6.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.2.9.6.1.1\">MAHA (Ours)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T1.2.9.6.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.2.9.6.2.1\">86.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T1.2.9.6.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "575", "text": "T1.2.9.6.2.1\">86.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T1.2.9.6.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.2.9.6.3.1\">93.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T1.2.9.6.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.2.9.6.4.1\">23.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T1.2.9.6.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.2.9.6.5.1\">28.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T1.2.9.6.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.2.9.6.6.1\">88.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T1.2.9.6.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.2.9.6.7.1\">6.7</span></td>\n</tr>\n</tbody>\n</table>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p class=\"ltx_p ltx_figure_panel ltx_align_center\" id=\"S5.T1.6\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T1.6.1\" style=\"font-size:90%;", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "576", "text": "T1.6\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T1.6.1\" style=\"font-size:90%;\">Note: MAHA achieves lowest perplexity on PG-19 and significant memory reduction.</span></p>\n</div>\n</div>\n</figure>\n</section>\n<section class=\"ltx_subsection\" id=\"S5.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S5.SS3.5.1.1\">V-C</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S5.SS3.6.2\">Computational Efficiency Analysis</span>\n</h3>\n<div class=\"ltx_para\" id=\"S5.SS3.p1\">\n<p class=\"ltx_p\" id=\"S5.SS3.p1.1\">MAHA matches MHA performance (within 0.2%) while reducing memory by 56%. On PG-19, MAHA achieves lowest perplexity (23.1), outperforming sparse models. Throughput is highest (71 seq/s), making MAHA ideal for high-volume inference. We analyzed the theoretical complexity (FLOPs) relative to sequence length.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "577", "text": "Throughput is highest (71 seq/s), making MAHA ideal for high-volume inference. We analyzed the theoretical complexity (FLOPs) relative to sequence length. As illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.14925v2#S5.F3\" title=\"Figure 3 ‣ V-C Computational Efficiency Analysis ‣ V Experiments ‣ Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, MAHA demonstrates near-linear scaling compared to the quadratic baseline of standard attention.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S5.F3\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"441\" id=\"S5.F3.g1\" src=\"x3.png\" width=\"747\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S5.F3.4.2.1\" style=\"font-size:90%;\">Figure 3</span>: </span><span class=\"ltx_text\" id=\"S5.F3.2.1\" style=\"font-size:90%;\">Attention FLOPs vs Sequence Length. MAHA exhibits an 81% reduction in FLOPs at <math alttext=\"N=4096\" class=\"ltx_Math\" display=\"inline\" id=\"S5.F3.2.1.m1\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>4096</mn></mrow><annotation encoding=\"application/x-tex\">N=4096</annotation></semantics></math> compared to Standard MHA.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "578", "text": "The gap widens exponentially with longer sequences.</span></figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S5.SS3.p2\">\n<p class=\"ltx_p\" id=\"S5.SS3.p2.4\">At <math alttext=\"N=4096\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.1.m1\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>4096</mn></mrow><annotation encoding=\"application/x-tex\">N=4096</annotation></semantics></math>, MHA requires <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.2.m2\" intent=\":literal\"><semantics><mo>≈</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math> 16.8M FLOPs vs MAHA <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.3.m3\" intent=\":literal\"><semantics><mo>≈</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math> 3.2M FLOPs (81% reduction). This efficiency stems from hierarchical compression avoiding full <math alttext=\"N\\times N\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.4.m4\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N\\times N</annotation></semantics></math> attention.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "579", "text": "This gap widens exponentially as the sequence length increases, confirming MAHA’s suitability for long-context applications.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S5.SS4\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S5.SS4.5.1.1\">V-D</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S5.SS4.6.2\">Ablation Studies</span>\n</h3>\n<div class=\"ltx_para\" id=\"S5.SS4.p1\">\n<p class=\"ltx_p\" id=\"S5.SS4.p1.1\">We evaluated the impact of aggregation methods and scale configurations.</p>\n</div>\n<section class=\"ltx_subsubsection\" id=\"S5.SS4.SSS1\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\"><span class=\"ltx_text\" id=\"S5.SS4.SSS1.5.1.1\">V-D</span>1 </span>Aggregation Strategy Comparison</h4>\n<div class=\"ltx_para\" id=\"S5.SS4.SSS1.p1\">\n<p class=\"ltx_p\" id=\"S5.SS4.SSS1.p1.1\">To further analyze the training dynamics, Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.14925v2#S5.F4\" title=\"Figure 4 ‣ V-D1 Aggregation Strategy Comparison ‣ V-D Ablation Studies ‣ V Experiments ‣ Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> depicts the loss convergence curves for both aggregation strategies. While both methods converge stably,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "580", "text": "While both methods converge stably, the Nash Equilibrium (Orange) strategy achieves a marginally lower loss value in later epochs compared to Convex Optimization (Blue).</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S5.F4\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"337\" id=\"S5.F4.g1\" src=\"fig4.png\" width=\"539\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S5.F4.2.1.1\" style=\"font-size:90%;\">Figure 4</span>: </span><span class=\"ltx_text\" id=\"S5.F4.3.2\" style=\"font-size:90%;\">Training loss convergence comparison: Convex Optimization vs Nash Equilibrium.</span></figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S5.SS4.SSS1.p2\">\n<p class=\"ltx_p\" id=\"S5.SS4.SSS1.p2.1\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.14925v2#S5.T2\" title=\"TABLE II ‣ V-D1 Aggregation Strategy Comparison ‣ V-D Ablation Studies ‣ V Experiments ‣ Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> shows that while Convex Optimization (CO) is faster (1.0x), Nash Equilibrium (NE) provides robust performance at a slight cost (0.9x speed).</p>\n</div>\n<figure class=\"ltx_table\" id=\"S5.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "581", "text": "0x), Nash Equilibrium (NE) provides robust performance at a slight cost (0.9x speed).</p>\n</div>\n<figure class=\"ltx_table\" id=\"S5.T2\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S5.T2.2.1.1\" style=\"font-size:90%;\">TABLE II</span>: </span><span class=\"ltx_text\" id=\"S5.T2.3.2\" style=\"font-size:90%;\">Aggregation Method Impact on MNLI Task.</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S5.T2.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T2.4.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" id=\"S5.T2.4.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.4.1.1.1.1\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T2.4.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.4.1.1.2.1\">MNLI (Acc)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T2.4.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.4.1.1.3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "582", "text": "T2.4.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.4.1.1.3.1\">Memory (GB)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T2.4.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.4.1.1.4.1\">Speed</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T2.4.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T2.4.2.1.1\">Convex Opt. (CO)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.4.2.1.2\">86.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.4.2.1.3\">6.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.4.2.1.4\">1.0x</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.4.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T2.4.3.2.1\">Nash Eq. (NE)</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.4.3.2.2\">85.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.4.3.2.3\">6.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "583", "text": "T2.4.3.2.2\">85.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.4.3.2.3\">6.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.4.3.2.4\">0.9x</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.4.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\" id=\"S5.T2.4.4.3.1\">Mean Aggregation</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T2.4.4.3.2\">85.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T2.4.4.3.3\">7.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T2.4.4.3.4\">1.1x</td>\n</tr>\n</tbody>\n</table>\n</figure>\n</section>\n<section class=\"ltx_subsubsection\" id=\"S5.SS4.SSS2\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\"><span class=\"ltx_text\" id=\"S5.SS4.SSS2.5.1.1\">V-D</span>2 </span>Scale Configuration Analysis</h4>\n<div class=\"ltx_para\" id=\"S5.SS4.SSS2.p1\">\n<p class=\"ltx_p\" id=\"S5.SS4.SSS2.p1.4\">We analyzed how the depth of the hierarchy (number of scales, <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS2.p1.1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "584", "text": "4\">We analyzed how the depth of the hierarchy (number of scales, <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS2.p1.1.m1\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math>) affects model performance. As illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.14925v2#S5.F5\" title=\"Figure 5 ‣ V-D2 Scale Configuration Analysis ‣ V-D Ablation Studies ‣ V Experiments ‣ Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, optimal results are observed at <math alttext=\"L=4\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS2.p1.2.m2\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">L=4</annotation></semantics></math>, balancing granularity and context.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "585", "text": "Using too few scales (<math alttext=\"L=2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS2.p1.3.m3\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">L=2</annotation></semantics></math>) results in insufficient detail (Acc: 84.5%), while excessive downsampling (<math alttext=\"L=6\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS2.p1.4.m4\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo>=</mo><mn>6</mn></mrow><annotation encoding=\"application/x-tex\">L=6</annotation></semantics></math>) introduces noise (Acc: 84.8%).</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S5.F5\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"357\" id=\"S5.F5.g1\" src=\"x4.png\" width=\"581\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S5.F5.6.3.1\" style=\"font-size:90%;\">Figure 5</span>: </span><span class=\"ltx_text\" id=\"S5.F5.4.2\" style=\"font-size:90%;\">MNLI Accuracy vs Number of Hierarchical Scales (<math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S5.F5.3.1.m1\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math>).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "586", "text": "Optimal performance is at <math alttext=\"L=4\" class=\"ltx_Math\" display=\"inline\" id=\"S5.F5.4.2.m2\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">L=4</annotation></semantics></math>.</span></figcaption>\n</figure>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"S5.SS5\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S5.SS5.5.1.1\">V-E</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S5.SS5.6.2\">Qualitative Analysis</span>\n</h3>\n<div class=\"ltx_para\" id=\"S5.SS5.p1\">\n<p class=\"ltx_p\" id=\"S5.SS5.p1.1\">To interpret the internal representations learned by MAHA, we visualized the attention weights across different hierarchical scales.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "587", "text": "Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.14925v2#S5.F6\" title=\"Figure 6 ‣ V-E Qualitative Analysis ‣ V Experiments ‣ Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> displays the heatmap of attention matrices.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S5.F6\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"216\" id=\"S5.F6.g1\" src=\"x5.png\" width=\"664\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S5.F6.2.1.1\" style=\"font-size:90%;\">Figure 6</span>: </span><span class=\"ltx_text\" id=\"S5.F6.3.2\" style=\"font-size:90%;\">Visualization of Learned Multiscale Attention Patterns. Darker regions indicate higher attention weights.</span></figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S5.SS5.p2\">\n<p class=\"ltx_p\" id=\"S5.SS5.p2.1\">Several key observations can be drawn:</p>\n<ol class=\"ltx_enumerate\" id=\"S5.I4\">\n<li class=\"ltx_item\" id=\"S5.I4.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">1.</span>\n<div class=\"ltx_para\" id=\"S5.I4.i1.p1\">\n<p class=\"ltx_p\" id=\"S5.I4.i1.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.I4.i1.p1.1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "588", "text": "I4.i1.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.I4.i1.p1.1.1\">Fine Scales (Scale 1):</span> Exhibits a strong diagonal tendency, capturing local syntax like adjective-noun pairs.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S5.I4.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">2.</span>\n<div class=\"ltx_para\" id=\"S5.I4.i2.p1\">\n<p class=\"ltx_p\" id=\"S5.I4.i2.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.I4.i2.p1.1.1\">Medium Scales (Scale 2):</span> Shifts towards block-diagonal structures, suggesting clause-level modeling.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S5.I4.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">3.</span>\n<div class=\"ltx_para\" id=\"S5.I4.i3.p1\">\n<p class=\"ltx_p\" id=\"S5.I4.i3.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.I4.i3.p1.1.1\">Coarse Scales (Scale 3):</span> Attention becomes diffuse with vertical bands, tracking document-level themes regardless of distance.</p>\n</div>\n</li>\n</ol>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S6\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">VI </span><span class=\"ltx_text ltx_font_smallcaps\" id=\"S6.1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "589", "text": "1.1\">Discussion</span>\n</h2>\n<section class=\"ltx_subsection\" id=\"S6.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S6.SS1.5.1.1\">VI-A</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S6.SS1.6.2\">Scalability vs. Implementation Overhead</span>\n</h3>\n<div class=\"ltx_para\" id=\"S6.SS1.p1\">\n<p class=\"ltx_p\" id=\"S6.SS1.p1.3\">Our experiments highlight a critical distinction between algorithmic complexity and implementation overhead. While prototype implementations may exhibit initialization latency, the growth rate is the decisive metric for Large Language Models.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "590", "text": "While prototype implementations may exhibit initialization latency, the growth rate is the decisive metric for Large Language Models. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.14925v2#S5.F3\" title=\"Figure 3 ‣ V-C Computational Efficiency Analysis ‣ V Experiments ‣ Multiscale Aggregated Hierarchical Attention (MAHA): A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> confirms that MAHA’s computational cost grows linearly (<math alttext=\"O(N)\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.1.m1\" intent=\":literal\"><semantics><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">O(N)</annotation></semantics></math>), whereas standard attention grows quadratically (<math alttext=\"O(N^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.2.m2\" intent=\":literal\"><semantics><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">O(N^{2})</annotation></semantics></math>). This implies that for very large sequences (e.g., <math alttext=\"N\\gg 4096\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "591", "text": "This implies that for very large sequences (e.g., <math alttext=\"N\\gg 4096\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.3.m3\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>≫</mo><mn>4096</mn></mrow><annotation encoding=\"application/x-tex\">N\\gg 4096</annotation></semantics></math>), MAHA provides a decisive advantage in both speed and memory.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S6.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S6.SS2.5.1.1\">VI-B</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S6.SS2.6.2\">Limitations</span>\n</h3>\n<div class=\"ltx_para\" id=\"S6.SS2.p1\">\n<p class=\"ltx_p\" id=\"S6.SS2.p1.1\">While MAHA demonstrates significant improvements in efficiency and modeling, certain limitations warrant discussion:</p>\n<ul class=\"ltx_itemize\" id=\"S6.I1\">\n<li class=\"ltx_item\" id=\"S6.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S6.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"S6.I1.i1.p1.2\">The framework involves additional hyperparameters (e.g., number of scales <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "592", "text": "g., number of scales <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.1.m1\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math>, compression ratio <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.2.m2\" intent=\":literal\"><semantics><mi>r</mi><annotation encoding=\"application/x-tex\">r</annotation></semantics></math>) that may require domain-specific tuning.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S6.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S6.I1.i2.p1\">\n<p class=\"ltx_p\" id=\"S6.I1.i2.p1.1\">Although the Nash Equilibrium aggregation offers theoretical guarantees, its iterative nature imposes a computational overhead during training compared to the closed-form Convex Optimization (CO) solution.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S6.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S6.I1.i3.p1\">\n<p class=\"ltx_p\" id=\"S6.I1.i3.p1.1\">The method assumes that linguistic information is inherently hierarchical; this assumption may not fully capture certain non-compositional semantic relationships or dispersed references in highly unstructured text <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rapaport1994a</span>]</cite>.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "593", "text": "</p>\n</div>\n</li>\n</ul>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S6.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S6.SS3.5.1.1\">VI-C</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S6.SS3.6.2\">Potential Application Scenarios</span>\n</h3>\n<div class=\"ltx_para\" id=\"S6.SS3.p1\">\n<p class=\"ltx_p\" id=\"S6.SS3.p1.1\">The versatility of MAHA extends beyond standard NLP:</p>\n<ul class=\"ltx_itemize\" id=\"S6.I2\">\n<li class=\"ltx_item\" id=\"S6.I2.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S6.I2.i1.p1\">\n<p class=\"ltx_p\" id=\"S6.I2.i1.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.I2.i1.p1.1.1\">Genomics:</span> In genomic sequence analysis, where identifying long-range dependencies in megabase-scale DNA sequences is critical <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">choi2023a</span>]</cite>, MAHA’s multiscale attention could enhance variant calling accuracy.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S6.I2.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S6.I2.i2.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "594", "text": "I2.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S6.I2.i2.p1\">\n<p class=\"ltx_p\" id=\"S6.I2.i2.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.I2.i2.p1.1.1\">Multimodal Learning:</span> For video-text retrieval, the hierarchical scales align naturally with temporal video resolutions (frames, shots, scenes) <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2021a</span>]</cite>, offering a unified attention mechanism for cross-modal alignment.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S6.I2.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S6.I2.i3.p1\">\n<p class=\"ltx_p\" id=\"S6.I2.i3.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.I2.i3.p1.1.1\">Federated Learning:</span> The optimization-driven aggregation is particularly relevant for federated settings where clients may operate on data of varying granularities or qualities <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2020a</span>]</cite>.</p>\n</div>\n</li>\n</ul>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S6.SS4\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S6.SS4.5.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "595", "text": "SS4\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S6.SS4.5.1.1\">VI-D</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S6.SS4.6.2\">Ethical Considerations</span>\n</h3>\n<div class=\"ltx_para\" id=\"S6.SS4.p1\">\n<p class=\"ltx_p\" id=\"S6.SS4.p1.1\">The efficiency gains of MAHA present a dual-edged sword. While significantly reducing the carbon footprint per training run <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">strubell2019a</span>]</cite>, lower costs may paradoxically incentivize the training of even larger, redundant models (Jevons paradox). Furthermore, the hierarchical aggregation introduces interpretability challenges; while individual scales are transparent, the complex interplay of optimization weights may obscure the models decision-making path <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">danilevsky2021a</span>]</cite>.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "596", "text": "Future work must address these transparency issues to ensure responsible deployment.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S7\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">VII </span><span class=\"ltx_text ltx_font_smallcaps\" id=\"S7.1.1\">Conclusion</span>\n</h2>\n<div class=\"ltx_para\" id=\"S7.p1\">\n<p class=\"ltx_p\" id=\"S7.p1.1\">In this paper, we introduced Multiscale Aggregated Hierarchical Attention (MAHA), a novel framework that fundamentally rethinks attention mechanisms in LLMs through the lens of multiscale analysis and optimization theory. By decomposing sequences into hierarchical granularities and synthesizing them via convex optimization or game-theoretic equilibrium, MAHA addresses the critical bottleneck of quadratic complexity without compromising contextual fidelity.</p>\n</div>\n<div class=\"ltx_para\" id=\"S7.p2\">\n<p class=\"ltx_p\" id=\"S7.p2.1\">Our extensive empirical evaluation demonstrates that MAHA achieves state-of-the-art performance on long-context modeling (PG-19) and machine translation, while reducing memory usage by up to 56% compared to standard transformers. The proposed hybrid dilated-convolutional architecture serves as a drop-in replacement for existing attention layers, facilitating seamless adoption.</p>\n</div>\n<div class=\"ltx_para\" id=\"S7.p3\">\n<p class=\"ltx_p\" id=\"S7.p3.1\">Looking forward, MAHA paves the way for scalable foundation models in resource-constrained environments. We envision future research extending this rigorous aggregation paradigm to other modalities such as computer vision and speech processing, where multiscale representation is equally paramount. Ultimately, this work underscores the potential of integrating mathematical optimization principles into deep learning architectures to build more efficient, robust, and theoretically grounded AI systems.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "597", "text": "Ultimately, this work underscores the potential of integrating mathematical optimization principles into deep learning architectures to build more efficient, robust, and theoretically grounded AI systems.</p>\n</div>\n</section>\n<section class=\"ltx_section\" id=\"Sx1\">\n<h2 class=\"ltx_title ltx_font_smallcaps ltx_title_section\">Data Availability</h2>\n<div class=\"ltx_para\" id=\"Sx1.p1\">\n<p class=\"ltx_p\" id=\"Sx1.p1.1\">The source code and pretrained models for MAHA are publicly available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/canererden/MAHA-Project\" title=\"\">https://github.com/canererden/MAHA-Project</a> with the permanent digital object identifier <span class=\"ltx_text ltx_font_bold\" id=\"Sx1.p1.1.1\">DOI: 10.5281/zenodo.17936753</span>.</p>\n</div>\n</section>\n</article>\n</div>\n<footer class=\"ltx_page_footer\">\n<div class=\"ltx_page_logo\">Generated  on Thu Dec 18 14:12:19 2025 by <a class=\"ltx_LaTeXML_logo\" href=\"http://dlmf.nist.gov/LaTeXML/\"><span style=\"letter-spacing:-0.2em; margin-right:0.1em;\">L<span class=\"ltx_font_smallcaps\" style=\"position:relative; bottom:2.2pt;\">a</span>T<span class=\"ltx_font_smallcaps\" style=\"font-size:120%;position:relative; bottom:-0.2ex;\">e</span></span><span style=\"font-size:90%; position:relative; bottom:-0.2ex;\">XML</span><img alt=\"Mascot Sammy\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5Yea", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "598", "text": "base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoV", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "599", "text": "4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==\"/></a>", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "600", "text": "DY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==\"/></a>\n</div></footer>\n</div>\n</body>\n</html>", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.html", "file_name": "2512.14925v2.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "601", "text": "<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"content-type\"/>\n<title>mHC: Manifold-Constrained Hyper-Connections</title>\n<!--Generated on Thu Jan  1 14:41:30 2026 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->\n<meta content=\"width=device-width, initial-scale=1, shrink-to-fit=no\" name=\"viewport\"/>\n<link href=\"/static/browse/0.3.4/css/arxiv-html-papers-20250916.css\" rel=\"stylesheet\" type=\"text/css\"/>\n<script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js\"></script>\n<script src=\"/static/browse/0.3.4/js/addons_new.js\"></script>\n<script src=\"/static/browse/0.3.4/js/feedbackOverlay.js\"></script>\n<base href=\"/html/2512.24880v1/\"/></head>\n<body>\n<nav class=\"ltx_page_navbar\">\n<nav class=\"ltx_TOC\">\n<ol class=\"ltx_toclist\">\n<li class=\"ltx_tocentry ltx_tocentry_section\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S1\" title=\"In mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">1 </span>Introduction</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_section\">\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "602", "text": "org/html/2512.24880v1#S2\" title=\"In mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">2 </span>Related Works</span></a>\n<ol class=\"ltx_toclist ltx_toclist_section\">\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S2.SS1\" title=\"In 2 Related Works ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">2.1 </span>Micro Design</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S2.SS2\" title=\"In 2 Related Works ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">2.2 </span>Macro Design</span></a></li>\n</ol>\n</li>\n<li class=\"ltx_tocentry ltx_tocentry_section\">\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S3\" title=\"In mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">3 </span>Preliminary</span></a>\n<ol class=\"ltx_toclist ltx_toclist_section\">\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "603", "text": "org/html/2512.24880v1#S3.SS1\" title=\"In 3 Preliminary ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">3.1 </span>Numerical Instability</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S3.SS2\" title=\"In 3 Preliminary ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">3.2 </span>System Overhead</span></a></li>\n</ol>\n</li>\n<li class=\"ltx_tocentry ltx_tocentry_section\">\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S4\" title=\"In mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">4 </span>Method</span></a>\n<ol class=\"ltx_toclist ltx_toclist_section\">\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S4.SS1\" title=\"In 4 Method ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">4.1 </span>Manifold-Constrained Hyper-Connections</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S4.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "604", "text": "org/html/2512.24880v1#S4.SS2\" title=\"In 4 Method ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">4.2 </span>Parameterization and Manifold Projection</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\">\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S4.SS3\" title=\"In 4 Method ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">4.3 </span>Efficient Infrastructure Design</span></a>\n<ol class=\"ltx_toclist ltx_toclist_subsection\">\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S4.SS3.SSS1\" title=\"In 4.3 Efficient Infrastructure Design ‣ 4 Method ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">4.3.1 </span>Kernel Fusion</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S4.SS3.SSS2\" title=\"In 4.3 Efficient Infrastructure Design ‣ 4 Method ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">4.3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "605", "text": "3 Efficient Infrastructure Design ‣ 4 Method ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">4.3.2 </span>Recomputing</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S4.SS3.SSS3\" title=\"In 4.3 Efficient Infrastructure Design ‣ 4 Method ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">4.3.3 </span>Overlapping Communication in DualPipe</span></a></li>\n</ol>\n</li>\n</ol>\n</li>\n<li class=\"ltx_tocentry ltx_tocentry_section\">\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S5\" title=\"In mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">5 </span>Experiments</span></a>\n<ol class=\"ltx_toclist ltx_toclist_section\">\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S5.SS1\" title=\"In 5 Experiments ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">5.1 </span>Experimental Setup</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S5.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "606", "text": "org/html/2512.24880v1#S5.SS2\" title=\"In 5 Experiments ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">5.2 </span>Main Results</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S5.SS3\" title=\"In 5 Experiments ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">5.3 </span>Scaling Experiments</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S5.SS4\" title=\"In 5 Experiments ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">5.4 </span>Stability Analysis</span></a></li>\n</ol>\n</li>\n<li class=\"ltx_tocentry ltx_tocentry_section\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S6\" title=\"In mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">6 </span>Conclusion and Outlook</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_appendix\">\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "607", "text": "org/html/2512.24880v1#A1\" title=\"In mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">A </span>Appendix</span></a>\n<ol class=\"ltx_toclist ltx_toclist_appendix\">\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#A1.SS1\" title=\"In Appendix A Appendix ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">A.1 </span>Detailed Model Specifications and Hyper-parameters.</span></a></li>\n</ol>\n</li>\n</ol></nav>\n</nav>\n<div class=\"ltx_page_main\">\n<div class=\"ltx_page_content\">\n<article class=\"ltx_document ltx_authors_1line\">\n<div class=\"ltx_para\" id=\"p1\">\n<span class=\"ltx_ERROR undefined\" id=\"p1.2\">\\reportnumber</span>\n<p class=\"ltx_p\" id=\"p1.1\">001<span class=\"ltx_ERROR undefined\" id=\"p1.1.1\">\\correspondingauthor</span>Core contributors.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "608", "text": "2\">\\reportnumber</span>\n<p class=\"ltx_p\" id=\"p1.1\">001<span class=\"ltx_ERROR undefined\" id=\"p1.1.1\">\\correspondingauthor</span>Core contributors. <sup class=\"ltx_sup\" id=\"p1.1.2\"><span class=\"ltx_text ltx_font_italic\" id=\"p1.1.2.1\">†</span></sup>Corresponding author: <a class=\"ltx_ref ltx_href\" href=\"mailto:xie.zhenda@deepseek.com\" title=\"\">xie.zhenda@deepseek.com</a></p>\n</div>\n<h1 class=\"ltx_title ltx_title_document\" lang=\"en\">\n<span class=\"ltx_text ltx_font_italic\" id=\"id2.id1\">m</span>HC: Manifold-Constrained Hyper-Connections \n</h1>\n<div class=\"ltx_authors\">\n<span class=\"ltx_creator ltx_role_author\">\n<span class=\"ltx_personname\" lang=\"en\">\nZhenda Xie*<sup class=\"ltx_sup\" id=\"id3.2.id1\"><span class=\"ltx_text ltx_font_italic\" id=\"id3.2.id1.1\">†</span></sup>\n</span><span class=\"ltx_author_notes\">\n<span class=\"ltx_contact ltx_role_affiliation\">\n</span></span></span>\n<span class=\"ltx_creator ltx_role_author\">\n<span class=\"ltx_personname\" lang=\"en\"> Yixuan Wei\n</span><span class=\"ltx_author_notes\">\n<span class=\"ltx_contact ltx_role_affiliation\">\n</span></span></span>\n<span class=\"ltx_creator ltx_role_author\">\n<span class=\"ltx_personname\" lang=\"en\"> Huanqi Cao\n</span><span class=\"ltx_author_notes\">\n<span class=\"ltx_contact ltx_role_affiliation\">\n</span></span></span>\n<span class=\"ltx_creator ltx_role_author\">\n<span class=\"ltx_personname\" lang=\"en\">", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "609", "text": "Chenggang Zhao\n</span><span class=\"ltx_author_notes\">\n<span class=\"ltx_contact ltx_role_affiliation\">\n</span></span></span>\n<span class=\"ltx_creator ltx_role_author\">\n<span class=\"ltx_personname\" lang=\"en\"> Chengqi Deng\n</span><span class=\"ltx_author_notes\">\n<span class=\"ltx_contact ltx_role_affiliation\">\n</span></span></span>\n<span class=\"ltx_creator ltx_role_author\">\n<span class=\"ltx_personname\" lang=\"en\"> Jiashi Li\n</span><span class=\"ltx_author_notes\">\n<span class=\"ltx_contact ltx_role_affiliation\">\n</span></span></span>\n<span class=\"ltx_creator ltx_role_author\">\n<span class=\"ltx_personname\" lang=\"en\"> Damai Dai\n</span><span class=\"ltx_author_notes\">\n<span class=\"ltx_contact ltx_role_affiliation\">\n</span></span></span>\n<span class=\"ltx_creator ltx_role_author\">\n<span class=\"ltx_personname\" lang=\"en\"> Huazuo Gao\n</span><span class=\"ltx_author_notes\">\n<span class=\"ltx_contact ltx_role_affiliation\">\n</span></span></span>\n<span class=\"ltx_creator ltx_role_author\">\n<span class=\"ltx_personname\" lang=\"en\"> Jiang Chang\n</span><span class=\"ltx_author_notes\">\n<span class=\"ltx_contact ltx_role_affiliation\">\n</span></span></span>\n<span class=\"ltx_creator ltx_role_author\">\n<span class=\"ltx_personname\" lang=\"en\">", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "610", "text": "Liang Zhao\n</span><span class=\"ltx_author_notes\">\n<span class=\"ltx_contact ltx_role_affiliation\">\n</span></span></span>\n<span class=\"ltx_creator ltx_role_author\">\n<span class=\"ltx_personname\" lang=\"en\"> Shangyan Zhou\n</span><span class=\"ltx_author_notes\">\n<span class=\"ltx_contact ltx_role_affiliation\">\n</span></span></span>\n<span class=\"ltx_creator ltx_role_author\">\n<span class=\"ltx_personname\" lang=\"en\"> Zhean Xu\n</span><span class=\"ltx_author_notes\">\n<span class=\"ltx_contact ltx_role_affiliation\">\n</span></span></span>\n<span class=\"ltx_creator ltx_role_author\">\n<span class=\"ltx_personname\" lang=\"en\"> Zhengyan Zhang\n</span><span class=\"ltx_author_notes\">\n<span class=\"ltx_contact ltx_role_affiliation\">\n</span></span></span>\n<span class=\"ltx_creator ltx_role_author\">\n<span class=\"ltx_personname\" lang=\"en\"> Wangding Zeng\n</span><span class=\"ltx_author_notes\">\n<span class=\"ltx_contact ltx_role_affiliation\">\n</span></span></span>\n<span class=\"ltx_creator ltx_role_author\">\n<span class=\"ltx_personname\" lang=\"en\">", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "611", "text": "Shengding Hu\n</span><span class=\"ltx_author_notes\">\n<span class=\"ltx_contact ltx_role_affiliation\">\n</span></span></span>\n<span class=\"ltx_creator ltx_role_author\">\n<span class=\"ltx_personname\" lang=\"en\"> Yuqing Wang\n</span><span class=\"ltx_author_notes\">\n<span class=\"ltx_contact ltx_role_affiliation\">\n</span></span></span>\n<span class=\"ltx_creator ltx_role_author\">\n<span class=\"ltx_personname\" lang=\"en\"> Jingyang Yuan\n</span><span class=\"ltx_author_notes\">\n<span class=\"ltx_contact ltx_role_affiliation\">\n</span></span></span>\n<span class=\"ltx_creator ltx_role_author\">\n<span class=\"ltx_personname\" lang=\"en\"> Lean Wang\n</span><span class=\"ltx_author_notes\">\n<span class=\"ltx_contact ltx_role_affiliation\">\n</span></span></span>\n<span class=\"ltx_creator ltx_role_author\">\n<span class=\"ltx_personname\" lang=\"en\"> Wenfeng Liang \n<br class=\"ltx_break\"/>DeepSeek-AI\n</span><span class=\"ltx_author_notes\">\n<span class=\"ltx_contact ltx_role_affiliation\">\n</span></span></span>\n</div>\n<div class=\"ltx_abstract\">\n<h6 class=\"ltx_title ltx_title_abstract\">Abstract</h6>\n<p class=\"ltx_p\" id=\"id4.id1\">Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns.\nWhile yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "612", "text": "While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead.\nTo address these challenges, we propose <span class=\"ltx_text ltx_font_bold\" id=\"id4.id1.1\">Manifold-Constrained Hyper-Connections</span> (<span class=\"ltx_text ltx_font_bold ltx_font_italic\" id=\"id4.id1.2\">m<span class=\"ltx_text ltx_font_upright\" id=\"id4.id1.2.1\">HC</span></span>), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency.\nEmpirical experiments demonstrate that <span class=\"ltx_text ltx_font_italic\" id=\"id4.id1.3\">m</span>HC is effective for training at scale, offering tangible performance improvements and superior scalability.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "613", "text": "Empirical experiments demonstrate that <span class=\"ltx_text ltx_font_italic\" id=\"id4.id1.3\">m</span>HC is effective for training at scale, offering tangible performance improvements and superior scalability.\nWe anticipate that <span class=\"ltx_text ltx_font_italic\" id=\"id4.id1.4\">m</span>HC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S0.F1\" lang=\"en\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"345\" id=\"S0.F1.g1\" src=\"x1.png\" width=\"660\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S0.F1.6.1.1\" style=\"font-size:90%;\">Figure 1</span>: </span><span class=\"ltx_text\" id=\"S0.F1.7.2\" style=\"font-size:90%;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S0.F1.7.2.1\">Illustrations of Residual Connection Paradigms.</span> This figure compares the structural design of (a) standard Residual Connection, (b) Hyper-Connections (HC), and (c) our proposed <span class=\"ltx_text ltx_font_bold\" id=\"S0.F1.7.2.2\">Manifold-Constrained Hyper-Connections</span> (<span class=\"ltx_text ltx_font_bold ltx_font_italic\" id=\"S0.F1.7.2.3\">m<span class=\"ltx_text ltx_font_upright\" id=\"S0.F1.7.2.3.1\">HC</span></span>).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "614", "text": "Unlike the unconstrained HC, <span class=\"ltx_text ltx_font_italic\" id=\"S0.F1.7.2.4\">m</span>HC focuses on optimizing the residual connection space by projecting the matrices onto a constrained manifold to ensure stability.\n</span></figcaption>\n</figure>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n<nav class=\"ltx_TOC ltx_list_toc ltx_toc_toc\" lang=\"en\"><h6 class=\"ltx_title ltx_title_contents\">Contents</h6>\n<ol class=\"ltx_toclist\">\n<li class=\"ltx_tocentry ltx_tocentry_section\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S1\" title=\"In mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">1 </span>Introduction</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_section\">\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S2\" title=\"In mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">2 </span>Related Works</span></a>\n<ol class=\"ltx_toclist ltx_toclist_section\">\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S2.SS1\" title=\"In 2 Related Works ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">2.1 </span>Micro Design</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "615", "text": "1 </span>Micro Design</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S2.SS2\" title=\"In 2 Related Works ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">2.2 </span>Macro Design</span></a></li>\n</ol>\n</li>\n<li class=\"ltx_tocentry ltx_tocentry_section\">\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S3\" title=\"In mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">3 </span>Preliminary</span></a>\n<ol class=\"ltx_toclist ltx_toclist_section\">\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S3.SS1\" title=\"In 3 Preliminary ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">3.1 </span>Numerical Instability</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S3.SS2\" title=\"In 3 Preliminary ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "616", "text": "SS2\" title=\"In 3 Preliminary ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">3.2 </span>System Overhead</span></a></li>\n</ol>\n</li>\n<li class=\"ltx_tocentry ltx_tocentry_section\">\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S4\" title=\"In mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">4 </span>Method</span></a>\n<ol class=\"ltx_toclist ltx_toclist_section\">\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S4.SS1\" title=\"In 4 Method ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">4.1 </span>Manifold-Constrained Hyper-Connections</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S4.SS2\" title=\"In 4 Method ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">4.2 </span>Parameterization and Manifold Projection</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\">\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S4.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "617", "text": "org/html/2512.24880v1#S4.SS3\" title=\"In 4 Method ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">4.3 </span>Efficient Infrastructure Design</span></a>\n<ol class=\"ltx_toclist ltx_toclist_subsection\">\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S4.SS3.SSS1\" title=\"In 4.3 Efficient Infrastructure Design ‣ 4 Method ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">4.3.1 </span>Kernel Fusion</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S4.SS3.SSS2\" title=\"In 4.3 Efficient Infrastructure Design ‣ 4 Method ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">4.3.2 </span>Recomputing</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S4.SS3.SSS3\" title=\"In 4.3 Efficient Infrastructure Design ‣ 4 Method ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">4.3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "618", "text": "3 Efficient Infrastructure Design ‣ 4 Method ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">4.3.3 </span>Overlapping Communication in DualPipe</span></a></li>\n</ol>\n</li>\n</ol>\n</li>\n<li class=\"ltx_tocentry ltx_tocentry_section\">\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S5\" title=\"In mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">5 </span>Experiments</span></a>\n<ol class=\"ltx_toclist ltx_toclist_section\">\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S5.SS1\" title=\"In 5 Experiments ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">5.1 </span>Experimental Setup</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S5.SS2\" title=\"In 5 Experiments ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">5.2 </span>Main Results</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S5.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "619", "text": "org/html/2512.24880v1#S5.SS3\" title=\"In 5 Experiments ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">5.3 </span>Scaling Experiments</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S5.SS4\" title=\"In 5 Experiments ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">5.4 </span>Stability Analysis</span></a></li>\n</ol>\n</li>\n<li class=\"ltx_tocentry ltx_tocentry_section\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S6\" title=\"In mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">6 </span>Conclusion and Outlook</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_appendix\">\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#A1\" title=\"In mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">A </span>Appendix</span></a>\n<ol class=\"ltx_toclist ltx_toclist_appendix\">\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#A1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "620", "text": "org/html/2512.24880v1#A1.SS1\" title=\"In Appendix A Appendix ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">A.1 </span>Detailed Model Specifications and Hyper-parameters.</span></a></li>\n</ol>\n</li>\n</ol></nav>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n<section class=\"ltx_section\" id=\"S1\" lang=\"en\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">1 </span>Introduction</h2>\n<div class=\"ltx_para\" id=\"S1.p1\">\n<p class=\"ltx_p\" id=\"S1.p1.7\">Deep neural network architectures have undergone rapid evolution since the introduction of ResNets <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">he2016deep</span>)</cite>.\nAs illustrated in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S0.F1\" title=\"Figure 1 ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(a), the structure of a single-layer can be formulated as follows:</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S1.E1\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\mathbf{x}_{l+1}=\\mathbf{x}_{l}+\\mathcal{F}(\\mathbf{x}_{l},\\mathcal{W}_{l}),\" class=\"ltx_Math\" display=\"block\" id=\"S1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "621", "text": "\\mathcal{W}_{l}),\" class=\"ltx_Math\" display=\"block\" id=\"S1.E1.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>𝐱</mi><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><msub><mi>𝐱</mi><mi>l</mi></msub><mo>+</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">ℱ</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝐱</mi><mi>l</mi></msub><mo>,</mo><msub><mi class=\"ltx_font_mathcaligraphic\">𝒲</mi><mi>l</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\mathbf{x}_{l+1}=\\mathbf{x}_{l}+\\mathcal{F}(\\mathbf{x}_{l},\\mathcal{W}_{l}),</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(1)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\" id=\"S1.p1.6\">where <math alttext=\"\\mathbf{x}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "622", "text": "p1.6\">where <math alttext=\"\\mathbf{x}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.1.m1\" intent=\":literal\"><semantics><msub><mi>𝐱</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{x}_{l}</annotation></semantics></math> and <math alttext=\"\\mathbf{x}_{l+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.2.m2\" intent=\":literal\"><semantics><msub><mi>𝐱</mi><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{x}_{l+1}</annotation></semantics></math> denote the <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.3.m3\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math>-dimensional input and output of the <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.4.m4\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math>-th layer, respectively, and <math alttext=\"\\mathcal{F}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.5.m5\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">ℱ</mi><annotation encoding=\"application/x-tex\">\\mathcal{F}</annotation></semantics></math> represents the residual function.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "623", "text": "Although the residual function <math alttext=\"\\mathcal{F}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.6.m6\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">ℱ</mi><annotation encoding=\"application/x-tex\">\\mathcal{F}</annotation></semantics></math> has evolved over the past decade to include various operations such as convolution, attention mechanisms, and feed forward networks, the paradigm of the residual connection has maintained its original form. Accompanying the progression of Transformer <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">vaswani2017attention</span>)</cite> architecture, this paradigm has currently established itself as a fundamental design element in large language models (LLMs) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">brown2020language</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">touvron2023llama</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2024deepseek_v3</span>)</cite>.</p>\n</div>\n<div class=\"ltx_para\" id=\"S1.p2\">\n<p class=\"ltx_p\" id=\"S1.p2.4\">This success is primarily attributed to the concise form of the residual connection. More importantly, early research <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">he2016identity</span>)</cite> revealed that the identity mapping property of the residual connection maintains stability and efficiency during large-scale training. By recursively extending the residual connection across multiple layers, Eq. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "624", "text": "By recursively extending the residual connection across multiple layers, Eq. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S1.E1\" title=\"Equation 1 ‣ 1 Introduction ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) yields:</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S1.E2\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\mathbf{x}_{L}=\\mathbf{x}_{l}+\\sum_{i=l}^{L-1}\\mathcal{F}(\\mathbf{x}_{i},\\mathcal{W}_{i}),\" class=\"ltx_Math\" display=\"block\" id=\"S1.E2.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>𝐱</mi><mi>L</mi></msub><mo>=</mo><mrow><msub><mi>𝐱</mi><mi>l</mi></msub><mo rspace=\"0.055em\">+</mo><mrow><munderover><mo movablelimits=\"false\">∑</mo><mrow><mi>i</mi><mo>=</mo><mi>l</mi></mrow><mrow><mi>L</mi><mo>−</mo><mn>1</mn></mrow></munderover><mrow><mi class=\"ltx_font_mathcaligraphic\">ℱ</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝐱</mi><mi>i</mi></msub><mo>,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "625", "text": "</mo><msub><mi class=\"ltx_font_mathcaligraphic\">𝒲</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\mathbf{x}_{L}=\\mathbf{x}_{l}+\\sum_{i=l}^{L-1}\\mathcal{F}(\\mathbf{x}_{i},\\mathcal{W}_{i}),</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(2)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\" id=\"S1.p2.3\">where <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.1.m1\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> and <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.2.m2\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math> correspond to deeper and shallower layers, respectively.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "626", "text": "p2.2.m2\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math> correspond to deeper and shallower layers, respectively.\nThe term identity mapping refers to the component <math alttext=\"\\mathbf{x}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.3.m3\" intent=\":literal\"><semantics><msub><mi>𝐱</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{x}_{l}</annotation></semantics></math> itself, which emphasizes the property that the signal from the shallower layer maps directly to the deeper layer without any modification.</p>\n</div>\n<div class=\"ltx_para\" id=\"S1.p3\">\n<p class=\"ltx_p\" id=\"S1.p3.14\">Recently, studies exemplified by Hyper-Connections (HC) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhu2024hyper</span>)</cite> have introduced a new dimension to the residual connection and empirically demonstrated its performance potential. The single-layer architecture of HC is illustrated in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S0.F1\" title=\"Figure 1 ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(b).\nBy expanding the width of the residual stream and enhancing connection complexity, HC significantly increases topological complexity without altering the computational overhead of individual units regarding FLOPs.\nFormally, single-layer propagation in HC is defined as:</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "627", "text": "Formally, single-layer propagation in HC is defined as:</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S1.E3\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\mathbf{x}_{l+1}=\\mathcal{H}_{l}^{\\mathrm{res}}\\mathbf{x}_{l}+\\mathcal{H}_{l}^{\\mathrm{post}\\,\\top}\\mathcal{F}(\\mathcal{H}_{l}^{\\mathrm{pre}}\\mathbf{x}_{l},\\mathcal{W}_{l}),\" class=\"ltx_Math\" display=\"block\" id=\"S1.E3.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>𝐱</mi><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝐱</mi><mi>l</mi></msub></mrow><mo>+</mo><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mrow><mi>post</mi><mo lspace=\"0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "628", "text": "392em\">⊤</mo></mrow></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi class=\"ltx_font_mathcaligraphic\">ℱ</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>pre</mi></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝐱</mi><mi>l</mi></msub></mrow><mo>,</mo><msub><mi class=\"ltx_font_mathcaligraphic\">𝒲</mi><mi>l</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\mathbf{x}_{l+1}=\\mathcal{H}_{l}^{\\mathrm{res}}\\mathbf{x}_{l}+\\mathcal{H}_{l}^{\\mathrm{post}\\,\\top}\\mathcal{F}(\\mathcal{H}_{l}^{\\mathrm{pre}}\\mathbf{x}_{l},\\mathcal{W}_{l}),</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(3)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\" id=\"S1.p3.13\">where <math alttext=\"\\mathbf{x}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "629", "text": "p3.13\">where <math alttext=\"\\mathbf{x}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.1.m1\" intent=\":literal\"><semantics><msub><mi>𝐱</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{x}_{l}</annotation></semantics></math> and <math alttext=\"\\mathbf{x}_{l+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.2.m2\" intent=\":literal\"><semantics><msub><mi>𝐱</mi><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{x}_{l+1}</annotation></semantics></math> denote the input and output of the <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.3.m3\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math>-th layer, respectively.\nUnlike the formulation in Eq. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S1.E1\" title=\"Equation 1 ‣ 1 Introduction ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>), the feature dimension of <math alttext=\"\\mathbf{x}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.4.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "630", "text": "the feature dimension of <math alttext=\"\\mathbf{x}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.4.m4\" intent=\":literal\"><semantics><msub><mi>𝐱</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{x}_{l}</annotation></semantics></math> and <math alttext=\"\\mathbf{x}_{l+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.5.m5\" intent=\":literal\"><semantics><msub><mi>𝐱</mi><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{x}_{l+1}</annotation></semantics></math> is expanded from <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.6.m6\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math> to <math alttext=\"n\\times C\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.7.m7\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">n\\times C</annotation></semantics></math>, where <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.8.m8\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> is the expansion rate.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "631", "text": "p3.8.m8\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> is the expansion rate.\nThe term <math alttext=\"\\mathcal{H}_{l}^{\\mathrm{res}}\\in\\mathbb{R}^{n\\times n}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.9.m9\" intent=\":literal\"><semantics><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>n</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{H}_{l}^{\\mathrm{res}}\\in\\mathbb{R}^{n\\times n}</annotation></semantics></math> represents a learnable mapping that mixes features within the residual stream.\nAlso as a learnable mapping, <math alttext=\"\\mathcal{H}_{l}^{\\mathrm{pre}}\\in\\mathbb{R}^{1\\times n}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.10.m10\" intent=\":literal\"><semantics><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>pre</mi></msubsup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "632", "text": "222em\" rspace=\"0.222em\">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{H}_{l}^{\\mathrm{pre}}\\in\\mathbb{R}^{1\\times n}</annotation></semantics></math> aggregates features from the <math alttext=\"nC\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.11.m11\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">nC</annotation></semantics></math>-dim stream into a <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.12.m12\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math>-dim layer input, and conversely, <math alttext=\"\\mathcal{H}_{l}^{\\mathrm{post}}\\in\\mathbb{R}^{1\\times n}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.13.m13\" intent=\":literal\"><semantics><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>post</mi></msubsup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{H}_{l}^{\\mathrm{post}}\\in\\mathbb{R}^{1\\times n}</annotation></semantics></math> maps the layer output back onto the stream.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "633", "text": "</p>\n</div>\n<div class=\"ltx_para\" id=\"S1.p4\">\n<p class=\"ltx_p\" id=\"S1.p4.4\">However, as the training scale increases, HC introduces potential risks of instability.\nThe primary concern is that the unconstrained nature of HC compromises the identity mapping property when the architecture extends across multiple layers.\nIn architectures comprising multiple parallel streams, an ideal identity mapping serves as a conservation mechanism. It ensures that the average signal intensity across streams remains invariant during both forward and backward propagation.\nRecursively extending HC to multiple layers via Eq. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S1.E3\" title=\"Equation 3 ‣ 1 Introduction ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) yields:</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S1.E4\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\mathbf{x}_{L}=\\left(\\prod_{i=1}^{L-l}\\mathcal{H}_{L-i}^{\\mathrm{res}}\\right)\\mathbf{x}_{l}+\\sum_{i=l}^{L-1}\\left(\\prod_{j=1}^{L-1-i}\\mathcal{H}_{L-j}^{\\mathrm{res}}\\right)\\mathcal{H}_{i}^{\\mathrm{post}\\,\\top}\\mathcal{F}(\\mathcal{H}_{i}^{\\mathrm{pre}}\\mathbf{x}_{i},\\mathcal{W}_{i}),\" class=\"ltx_Math\" display=\"block\" id=\"S1.E4.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "634", "text": "\\mathcal{W}_{i}),\" class=\"ltx_Math\" display=\"block\" id=\"S1.E4.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>𝐱</mi><mi>L</mi></msub><mo>=</mo><mrow><mrow><mrow><mo>(</mo><mrow><munderover><mo lspace=\"0em\" movablelimits=\"false\">∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>L</mi><mo>−</mo><mi>l</mi></mrow></munderover><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mrow><mi>L</mi><mo>−</mo><mi>i</mi></mrow><mi>res</mi></msubsup></mrow><mo>)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝐱</mi><mi>l</mi></msub></mrow><mo rspace=\"0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "635", "text": "055em\">+</mo><mrow><munderover><mo movablelimits=\"false\" rspace=\"0em\">∑</mo><mrow><mi>i</mi><mo>=</mo><mi>l</mi></mrow><mrow><mi>L</mi><mo>−</mo><mn>1</mn></mrow></munderover><mrow><mrow><mo>(</mo><mrow><munderover><mo lspace=\"0em\" movablelimits=\"false\">∏</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>L</mi><mo>−</mo><mn>1</mn><mo>−</mo><mi>i</mi></mrow></munderover><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mrow><mi>L</mi><mo>−</mo><mi>j</mi></mrow><mi>res</mi></msubsup></mrow><mo>)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>i</mi><mrow><mi>post</mi><mo lspace=\"0.392em\">⊤</mo></mrow></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi class=\"ltx_font_mathcaligraphic\">ℱ</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>i</mi><mi>pre</mi></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝐱</mi><mi>i</mi></msub></mrow><mo>,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "636", "text": "</mo><msub><mi class=\"ltx_font_mathcaligraphic\">𝒲</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\mathbf{x}_{L}=\\left(\\prod_{i=1}^{L-l}\\mathcal{H}_{L-i}^{\\mathrm{res}}\\right)\\mathbf{x}_{l}+\\sum_{i=l}^{L-1}\\left(\\prod_{j=1}^{L-1-i}\\mathcal{H}_{L-j}^{\\mathrm{res}}\\right)\\mathcal{H}_{i}^{\\mathrm{post}\\,\\top}\\mathcal{F}(\\mathcal{H}_{i}^{\\mathrm{pre}}\\mathbf{x}_{i},\\mathcal{W}_{i}),</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(4)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\" id=\"S1.p4.3\">where <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.1.m1\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> and <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.2.m2\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math> represent a deeper layer and a shallower layer, respectively.\nIn contrast to Eq.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "637", "text": "respectively.\nIn contrast to Eq. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S1.E2\" title=\"Equation 2 ‣ 1 Introduction ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), the composite mapping <math alttext=\"\\prod_{i=1}^{L-l}\\mathcal{H}_{L-i}^{\\mathrm{res}}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.3.m3\" intent=\":literal\"><semantics><mrow><msubsup><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>L</mi><mo>−</mo><mi>l</mi></mrow></msubsup><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mrow><mi>L</mi><mo>−</mo><mi>i</mi></mrow><mi>res</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\prod_{i=1}^{L-l}\\mathcal{H}_{L-i}^{\\mathrm{res}}</annotation></semantics></math> in HC fails to preserve the global mean of the features. This discrepancy leads to unbounded signal amplification or attenuation, resulting in instability during large-scale training.\nA further consideration is that, while HC preserves computational efficiency in terms of FLOPs, the hardware efficiency concerning memory access costs for the widened residual stream remains unaddressed in the original design.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "638", "text": "A further consideration is that, while HC preserves computational efficiency in terms of FLOPs, the hardware efficiency concerning memory access costs for the widened residual stream remains unaddressed in the original design. These factors collectively restrict the practical scalability of HC and hinder its application in large-scale training.</p>\n</div>\n<div class=\"ltx_para\" id=\"S1.p5\">\n<p class=\"ltx_p\" id=\"S1.p5.4\">To address these challenges, we propose <span class=\"ltx_text ltx_font_bold\" id=\"S1.p5.4.1\">Manifold-Constrained Hyper-Connections</span> (<span class=\"ltx_text ltx_font_bold ltx_font_italic\" id=\"S1.p5.4.2\">m<span class=\"ltx_text ltx_font_upright\" id=\"S1.p5.4.2.1\">HC</span></span>), as shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S0.F1\" title=\"Figure 1 ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(c), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "639", "text": "Specifically, <span class=\"ltx_text ltx_font_italic\" id=\"S1.p5.4.3\">m</span>HC utilizes the Sinkhorn-Knopp algorithm <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sinkhorn1967concerning</span>)</cite> to entropically project <math alttext=\"\\mathcal{H}_{l}^{\\mathrm{res}}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p5.1.m1\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}_{l}^{\\mathrm{res}}</annotation></semantics></math> onto the Birkhoff polytope. This operation effectively constrains the residual connection matrices within the manifold that is constituted by doubly stochastic matrices.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "640", "text": "This operation effectively constrains the residual connection matrices within the manifold that is constituted by doubly stochastic matrices.\nSince the row and column sums of these matrices equal to <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p5.2.m2\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, the operation <math alttext=\"\\mathcal{H}_{l}^{\\mathrm{res}}\\mathbf{x}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p5.3.m3\" intent=\":literal\"><semantics><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝐱</mi><mi>l</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\mathcal{H}_{l}^{\\mathrm{res}}\\mathbf{x}_{l}</annotation></semantics></math> functions as a convex combination of the input features. This characteristic facilitates a well-conditioned signal propagation where the feature mean is conserved, and the signal norm is strictly regularized, effectively mitigating the risk of vanishing or exploding signals.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "641", "text": "This characteristic facilitates a well-conditioned signal propagation where the feature mean is conserved, and the signal norm is strictly regularized, effectively mitigating the risk of vanishing or exploding signals. Furthermore, due to the closure of matrix multiplication for doubly stochastic matrices, the composite mapping <math alttext=\"\\prod_{i=1}^{L-l}\\mathcal{H}_{L-i}^{\\mathrm{res}}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p5.4.m4\" intent=\":literal\"><semantics><mrow><msubsup><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>L</mi><mo>−</mo><mi>l</mi></mrow></msubsup><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mrow><mi>L</mi><mo>−</mo><mi>i</mi></mrow><mi>res</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\prod_{i=1}^{L-l}\\mathcal{H}_{L-i}^{\\mathrm{res}}</annotation></semantics></math> retains this conservation property. Consequently, <span class=\"ltx_text ltx_font_italic\" id=\"S1.p5.4.4\">m</span>HC effectively maintains the stability of identity mappings between arbitrary depths. To ensure efficiency, we employ kernel fusion and develop mixed precision kernels utilizing TileLang <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2025tilelang</span>)</cite>.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "642", "text": "Furthermore, we mitigate the memory footprint through selective recomputing and carefully overlap communication within the DualPipe schedule <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2024deepseek_v3</span>)</cite>.</p>\n</div>\n<div class=\"ltx_para\" id=\"S1.p6\">\n<p class=\"ltx_p\" id=\"S1.p6.1\">Extensive experiments on language model pretraining demonstrate that <span class=\"ltx_text ltx_font_italic\" id=\"S1.p6.1.1\">m</span>HC exhibits exceptional stability and scalability while maintaining the performance advantages of HC. In-house large-scale training indicates that <span class=\"ltx_text ltx_font_italic\" id=\"S1.p6.1.2\">m</span>HC supports training at scale and introduces only a 6.7% additional time overhead when expansion rate <math alttext=\"n=4\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.1.m1\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">n=4</annotation></semantics></math>.</p>\n</div>\n</section>\n<section class=\"ltx_section\" id=\"S2\" lang=\"en\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">2 </span>Related Works</h2>\n<div class=\"ltx_para\" id=\"S2.p1\">\n<p class=\"ltx_p\" id=\"S2.p1.1\">Architectural advancements in deep learning can be primarily classified into <span class=\"ltx_text ltx_font_italic\" id=\"S2.p1.1.1\">micro-design</span> and <span class=\"ltx_text ltx_font_italic\" id=\"S2.p1.1.2\">macro-design</span>.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "643", "text": "Micro-design concerns the internal architecture of computational blocks, specifying how features are processed across spatial, temporal, and channel dimensions.\nIn contrast, macro-design establishes the inter-block topological structure, thereby dictating how feature representations are propagated, routed, and merged across distinct layers.</p>\n</div>\n<section class=\"ltx_subsection\" id=\"S2.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">2.1 </span>Micro Design</h3>\n<div class=\"ltx_para\" id=\"S2.SS1.p1\">\n<p class=\"ltx_p\" id=\"S2.SS1.p1.1\">Driven by parameter sharing and translation invariance, convolution initially dominated the processing of structured signals. While subsequent variations such as depthwise separable <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chollet2017xception</span>)</cite> and grouped convolutions <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xie2017aggregated</span>)</cite> optimized efficiency, the advent of Transformers <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">vaswani2017attention</span>)</cite> established Attention and Feed-Forward Networks (FFNs) as the fundamental building blocks of modern architecture. Attention mechanisms facilitate global information propagation, while FFNs enhance the representational capacity of individual features.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "644", "text": "Attention mechanisms facilitate global information propagation, while FFNs enhance the representational capacity of individual features. To balance performance with the computational demands of LLMs, attention mechanisms have evolved towards efficient variants such as Multi-Query Attention (MQA) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shazeer2019fast</span>)</cite>, Grouped-Query Attention (GQA) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ainslie2023gqa</span>)</cite>, and Multi-Head Latent Attention (MLA) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2024deepseek</span>)</cite>.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "645", "text": "Simultaneously, FFNs have been generalized into sparse computing paradigms via Mixture-of-Experts (MoE) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shazeer2017outrageously</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lepikhin2020gshard</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fedus2022switch</span>)</cite>, allowing for massive parameter scaling without proportional computational costs.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S2.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">2.2 </span>Macro Design</h3>\n<div class=\"ltx_para\" id=\"S2.SS2.p1\">\n<p class=\"ltx_p\" id=\"S2.SS2.p1.1\">Macro-design governs the global topology of the network <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Srivastava2015highway</span>)</cite>.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "646", "text": "Following ResNet <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">he2016deep</span>)</cite>, architectures such as DenseNet <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">huang2017densely</span>)</cite> and FractalNet <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">larsson2016fractalnet</span>)</cite> aimed to enhance performance by increasing topological complexity through dense connectivity and multi-path structures, respectively.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "647", "text": "Deep Layer Aggregation (DLA) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yu2018deep</span>)</cite> further extended this paradigm by recursively aggregating features across various depths and resolutions.</p>\n</div>\n<div class=\"ltx_para\" id=\"S2.SS2.p2\">\n<p class=\"ltx_p\" id=\"S2.SS2.p2.1\">More recently, the focus of macro-design has shifted toward expanding the width of the residual stream <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chai-etal-2020-highway</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fang2023crosslayer</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xie2023residualtransformerdualresidual</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pagliardini2024denseformer</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">menghani2025laurel</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">heddes2025deepcrossattention</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhu2024hyper</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mak2025residual</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xiao2025muddformer</span>)</cite>.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "648", "text": "Hyper-Connections (HC) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhu2024hyper</span>)</cite> introduced learnable matrices to modulate connection strengths among features at varying depths, while the Residual Matrix Transformer (RMT) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mak2025residual</span>)</cite> replaced the standard residual stream with an outer-product memory matrix to facilitate feature storage. Similarly, MUDDFormer <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xiao2025muddformer</span>)</cite> employs multiway dynamic dense connections to optimize cross-layer information flow.\nDespite their potential, these approaches compromise the inherent identity mapping property of the residual connection, thereby introducing instability and hindering scalability. Furthermore, they incur significant memory access overhead due to expanded feature widths.\nBuilding upon HC, the proposed <span class=\"ltx_text ltx_font_italic\" id=\"S2.SS2.p2.1.1\">m</span>HC restricts the residual connection space onto a specific manifold to restore the identity mapping property, while also incorporating rigorous infrastructure optimizations to ensure efficiency. This approach enhances stability and scalability while maintaining the topological benefits of expanded connections.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S3\" lang=\"en\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">3 </span>Preliminary</h2>\n<div class=\"ltx_para\" id=\"S3.p1\">\n<p class=\"ltx_p\" id=\"S3.p1.7\">We first establish the notation used in this work. In the HC formulation, the input to the <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "649", "text": "In the HC formulation, the input to the <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.1.m1\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math>-th layer, <math alttext=\"\\textbf{x}_{l}\\in\\mathbb{R}^{1\\times C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.2.m2\" intent=\":literal\"><semantics><mrow><msub><mtext class=\"ltx_mathvariant_bold\">x</mtext><mi>l</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>C</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\textbf{x}_{l}\\in\\mathbb{R}^{1\\times C}</annotation></semantics></math>, is expanded by a factor of <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.3.m3\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> to construct a hidden matrix <math alttext=\"\\textbf{x}_{l}=(\\textbf{x}^{\\top}_{l,0},\\ldots,\\textbf{x}^{\\top}_{l,n-1})^{\\top}\\in\\mathbb{R}^{n\\times C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.4.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "650", "text": "n-1})^{\\top}\\in\\mathbb{R}^{n\\times C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.4.m4\" intent=\":literal\"><semantics><mrow><msub><mtext class=\"ltx_mathvariant_bold\">x</mtext><mi>l</mi></msub><mo>=</mo><msup><mrow><mo stretchy=\"false\">(</mo><msubsup><mtext class=\"ltx_mathvariant_bold\">x</mtext><mrow><mi>l</mi><mo>,</mo><mn>0</mn></mrow><mo>⊤</mo></msubsup><mo>,</mo><mi mathvariant=\"normal\">…</mi><mo>,</mo><msubsup><mtext class=\"ltx_mathvariant_bold\">x</mtext><mrow><mi>l</mi><mo>,</mo><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></mrow><mo>⊤</mo></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>⊤</mo></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>n</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>C</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\textbf{x}_{l}=(\\textbf{x}^{\\top}_{l,0},\\ldots,\\textbf{x}^{\\top}_{l,n-1})^{\\top}\\in\\mathbb{R}^{n\\times C}</annotation></semantics></math> which can be viewed as <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.5.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "651", "text": "p1.5.m5\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-stream residual. This operation effectively broadens the width of the residual stream. To govern the read-out, write-in, and updating processes of this stream, HC introduces three learnable linear mappings—<math alttext=\"\\mathcal{H}^{\\mathrm{pre}}_{l},\\mathcal{H}^{\\mathrm{post}}_{l}\\in\\mathbb{R}^{1\\times n}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.6.m6\" intent=\":literal\"><semantics><mrow><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>pre</mi></msubsup><mo>,</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>post</mi></msubsup></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{pre}}_{l},\\mathcal{H}^{\\mathrm{post}}_{l}\\in\\mathbb{R}^{1\\times n}</annotation></semantics></math>, and <math alttext=\"\\mathcal{H}^{\\mathrm{res}}_{l}\\in\\mathbb{R}^{n\\times n}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.7.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "652", "text": "p1.7.m7\" intent=\":literal\"><semantics><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>n</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{res}}_{l}\\in\\mathbb{R}^{n\\times n}</annotation></semantics></math>. These mappings modify the standard residual connection shown in Eq. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S1.E1\" title=\"Equation 1 ‣ 1 Introduction ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>), resulting in the formulation given in Eq. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S1.E3\" title=\"Equation 3 ‣ 1 Introduction ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).</p>\n</div>\n<div class=\"ltx_para\" id=\"S3.p2\">\n<p class=\"ltx_p\" id=\"S3.p2.8\">In the HC formulation, learnable mappings are composed of two parts of coefficients: the input-dependent one and the global one, referred to as dynamic mappings and static mappings, respectively. Formally, HC computes the coefficients as follows:</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "653", "text": "Formally, HC computes the coefficients as follows:</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S3.E5\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\begin{cases}\\tilde{\\mathbf{x}}_{l}=\\text{RMSNorm}(\\mathbf{x}_{l})\\\\\n\\mathcal{H}^{\\mathrm{pre}}_{l}=\\alpha_{l}^{\\mathrm{pre}}\\cdot\\tanh(\\theta^{\\mathrm{pre}}_{l}\\tilde{\\mathbf{x}}^{\\top}_{l})+\\mathbf{b}_{l}^{\\mathrm{pre}}\\\\\n\\mathcal{H}^{\\mathrm{post}}_{l}=\\alpha_{l}^{\\mathrm{post}}\\cdot\\tanh(\\theta^{\\mathrm{post}}_{l}\\tilde{\\mathbf{x}}^{\\top}_{l})+\\mathbf{b}_{l}^{\\mathrm{post}}\\\\\n\\mathcal{H}^{\\mathrm{res}}_{l}=\\alpha_{l}^{\\mathrm{res}}\\cdot\\tanh(\\theta^{\\mathrm{res}}_{l}\\tilde{\\mathbf{x}}^{\\top}_{l})+\\mathbf{b}_{l}^{\\mathrm{res}},\\\\\n\\end{cases}\" class=\"ltx_Math\" display=\"block\" id=\"S3.E5.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "654", "text": "\\\\\n\\end{cases}\" class=\"ltx_Math\" display=\"block\" id=\"S3.E5.m1\" intent=\":literal\"><semantics><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><msub><mover accent=\"true\"><mi>𝐱</mi><mo>~</mo></mover><mi>l</mi></msub><mo>=</mo><mrow><mtext>RMSNorm</mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝐱</mi><mi>l</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mtd><mtd></mtd></mtr><mtr><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>pre</mi></msubsup><mo>=</mo><mrow><mrow><msubsup><mi>α</mi><mi>l</mi><mi>pre</mi></msubsup><mo lspace=\"0.222em\" rspace=\"0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "655", "text": "222em\" rspace=\"0.222em\">⋅</mo><mrow><mi>tanh</mi><mo>⁡</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>θ</mi><mi>l</mi><mi>pre</mi></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mover accent=\"true\"><mi>𝐱</mi><mo>~</mo></mover><mi>l</mi><mo>⊤</mo></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><msubsup><mi>𝐛</mi><mi>l</mi><mi>pre</mi></msubsup></mrow></mrow></mtd><mtd></mtd></mtr><mtr><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>post</mi></msubsup><mo>=</mo><mrow><mrow><msubsup><mi>α</mi><mi>l</mi><mi>post</mi></msubsup><mo lspace=\"0.222em\" rspace=\"0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "656", "text": "222em\" rspace=\"0.222em\">⋅</mo><mrow><mi>tanh</mi><mo>⁡</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>θ</mi><mi>l</mi><mi>post</mi></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mover accent=\"true\"><mi>𝐱</mi><mo>~</mo></mover><mi>l</mi><mo>⊤</mo></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><msubsup><mi>𝐛</mi><mi>l</mi><mi>post</mi></msubsup></mrow></mrow></mtd><mtd></mtd></mtr><mtr><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup><mo>=</mo><mrow><mrow><msubsup><mi>α</mi><mi>l</mi><mi>res</mi></msubsup><mo lspace=\"0.222em\" rspace=\"0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "657", "text": "222em\" rspace=\"0.222em\">⋅</mo><mrow><mi>tanh</mi><mo>⁡</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>θ</mi><mi>l</mi><mi>res</mi></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mover accent=\"true\"><mi>𝐱</mi><mo>~</mo></mover><mi>l</mi><mo>⊤</mo></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><msubsup><mi>𝐛</mi><mi>l</mi><mi>res</mi></msubsup></mrow></mrow><mo>,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "658", "text": "</mo></mrow></mtd><mtd></mtd></mtr></mtable></mrow><annotation encoding=\"application/x-tex\">\\begin{cases}\\tilde{\\mathbf{x}}_{l}=\\text{RMSNorm}(\\mathbf{x}_{l})\\\\\n\\mathcal{H}^{\\mathrm{pre}}_{l}=\\alpha_{l}^{\\mathrm{pre}}\\cdot\\tanh(\\theta^{\\mathrm{pre}}_{l}\\tilde{\\mathbf{x}}^{\\top}_{l})+\\mathbf{b}_{l}^{\\mathrm{pre}}\\\\\n\\mathcal{H}^{\\mathrm{post}}_{l}=\\alpha_{l}^{\\mathrm{post}}\\cdot\\tanh(\\theta^{\\mathrm{post}}_{l}\\tilde{\\mathbf{x}}^{\\top}_{l})+\\mathbf{b}_{l}^{\\mathrm{post}}\\\\\n\\mathcal{H}^{\\mathrm{res}}_{l}=\\alpha_{l}^{\\mathrm{res}}\\cdot\\tanh(\\theta^{\\mathrm{res}}_{l}\\tilde{\\mathbf{x}}^{\\top}_{l})+\\mathbf{b}_{l}^{\\mathrm{res}},\\\\\n\\end{cases}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(5)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\" id=\"S3.p2.7\">where <math alttext=\"\\text{RMSNorm}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "659", "text": "p2.7\">where <math alttext=\"\\text{RMSNorm}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.1.m1\" intent=\":literal\"><semantics><mrow><mtext>RMSNorm</mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">⋅</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{RMSNorm}(\\cdot)</annotation></semantics></math> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2019root</span>)</cite> is applied to the last dimension, and the scalars <math alttext=\"\\alpha_{l}^{\\mathrm{pre}},\\alpha_{l}^{\\mathrm{post}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.2.m2\" intent=\":literal\"><semantics><mrow><msubsup><mi>α</mi><mi>l</mi><mi>pre</mi></msubsup><mo>,</mo><msubsup><mi>α</mi><mi>l</mi><mi>post</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\alpha_{l}^{\\mathrm{pre}},\\alpha_{l}^{\\mathrm{post}}</annotation></semantics></math> and <math alttext=\"\\alpha_{l}^{\\mathrm{res}}\\in\\mathbb{R}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "660", "text": "p2.3.m3\" intent=\":literal\"><semantics><mrow><msubsup><mi>α</mi><mi>l</mi><mi>res</mi></msubsup><mo>∈</mo><mi>ℝ</mi></mrow><annotation encoding=\"application/x-tex\">\\alpha_{l}^{\\mathrm{res}}\\in\\mathbb{R}</annotation></semantics></math> are learnable gating factors initialized to small values. The dynamic mappings are derived via linear projections parameterized by <math alttext=\"\\theta^{\\mathrm{pre}}_{l},\\theta^{\\mathrm{post}}_{l}\\in\\mathbb{R}^{1\\times C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.4.m4\" intent=\":literal\"><semantics><mrow><mrow><msubsup><mi>θ</mi><mi>l</mi><mi>pre</mi></msubsup><mo>,</mo><msubsup><mi>θ</mi><mi>l</mi><mi>post</mi></msubsup></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>C</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\theta^{\\mathrm{pre}}_{l},\\theta^{\\mathrm{post}}_{l}\\in\\mathbb{R}^{1\\times C}</annotation></semantics></math> and <math alttext=\"\\theta^{\\mathrm{res}}_{l}\\in\\mathbb{R}^{n\\times C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.5.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "661", "text": "p2.5.m5\" intent=\":literal\"><semantics><mrow><msubsup><mi>θ</mi><mi>l</mi><mi>res</mi></msubsup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>n</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>C</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\theta^{\\mathrm{res}}_{l}\\in\\mathbb{R}^{n\\times C}</annotation></semantics></math>, while the static mappings are represented by learnable biases <math alttext=\"\\mathbf{b}_{l}^{\\mathrm{pre}},\\mathbf{b}_{l}^{\\mathrm{post}}\\in\\mathbb{R}^{1\\times n}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.6.m6\" intent=\":literal\"><semantics><mrow><mrow><msubsup><mi>𝐛</mi><mi>l</mi><mi>pre</mi></msubsup><mo>,</mo><msubsup><mi>𝐛</mi><mi>l</mi><mi>post</mi></msubsup></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{b}_{l}^{\\mathrm{pre}},", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "662", "text": "222em\" rspace=\"0.222em\">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{b}_{l}^{\\mathrm{pre}},\\mathbf{b}_{l}^{\\mathrm{post}}\\in\\mathbb{R}^{1\\times n}</annotation></semantics></math> and <math alttext=\"\\mathbf{b}_{l}^{\\mathrm{res}}\\in\\mathbb{R}^{n\\times n}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.7.m7\" intent=\":literal\"><semantics><mrow><msubsup><mi>𝐛</mi><mi>l</mi><mi>res</mi></msubsup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>n</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{b}_{l}^{\\mathrm{res}}\\in\\mathbb{R}^{n\\times n}</annotation></semantics></math>.</p>\n</div>\n<div class=\"ltx_para\" id=\"S3.p3\">\n<p class=\"ltx_p\" id=\"S3.p3.5\">It is worth noting that the introduction of these mappings—<math alttext=\"\\mathcal{H}^{\\mathrm{pre}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.1.m1\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>pre</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{pre}}_{l}</annotation></semantics></math>,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "663", "text": "<math alttext=\"\\mathcal{H}^{\\mathrm{post}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.2.m2\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>post</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{post}}_{l}</annotation></semantics></math>, and <math alttext=\"\\mathcal{H}^{\\mathrm{res}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.3.m3\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{res}}_{l}</annotation></semantics></math>—incurs negligible computational overhead, as the typical expansion rate <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.4.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>, e.g. 4, is much smaller than the input dimension <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.5.m5\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math>. With this design, HC effectively decouples the information capacity of the residual stream from the layer’s input dimension, which is strongly correlated with the model’s computational complexity (FLOPs).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "664", "text": "With this design, HC effectively decouples the information capacity of the residual stream from the layer’s input dimension, which is strongly correlated with the model’s computational complexity (FLOPs). Consequently, HC offers a new avenue for scaling by adjusting the residual stream width, complementing the traditional scaling dimensions of model FLOPs and training data size discussed in pre-training scaling laws <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Hoffmann2022Chinchilla</span>)</cite>.</p>\n</div>\n<div class=\"ltx_para\" id=\"S3.p4\">\n<p class=\"ltx_p\" id=\"S3.p4.1\">Although HC necessitates three mappings to manage the dimensional mismatch between the residual stream and the layer input, preliminary experiments presented in Tab. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S3.T1\" title=\"Table 1 ‣ 3 Preliminary ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> indicate that the residual mapping <math alttext=\"\\mathcal{H}^{\\mathrm{res}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.1.m1\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{res}}_{l}</annotation></semantics></math> yields the most significant performance gain. This finding underscores the critical importance of effective information exchange within the residual stream.</p>\n</div>\n<figure class=\"ltx_table\" id=\"S3.T1\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S3.T1.24.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "665", "text": "T1\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S3.T1.24.8.1\" style=\"font-size:90%;\">Table 1</span>: </span><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.14.7\" style=\"font-size:90%;\">Ablation Study of HC Components.<span class=\"ltx_text ltx_font_medium\" id=\"S3.T1.14.7.7\"> When a specific mapping (<math alttext=\"\\mathcal{H}^{\\mathrm{pre}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.8.1.1.m1\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>pre</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{pre}}_{l}</annotation></semantics></math>, <math alttext=\"\\mathcal{H}^{\\mathrm{post}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.9.2.2.m2\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>post</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{post}}_{l}</annotation></semantics></math>, or <math alttext=\"\\mathcal{H}^{\\mathrm{res}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.10.3.3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "666", "text": "or <math alttext=\"\\mathcal{H}^{\\mathrm{res}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.10.3.3.m3\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{res}}_{l}</annotation></semantics></math>) is disabled, we employ a fixed mapping to maintain dimensional consistency: uniform weights of <math alttext=\"1/n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.11.4.4.m4\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">1/n</annotation></semantics></math> for <math alttext=\"\\mathcal{H}^{\\mathrm{pre}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.12.5.5.m5\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>pre</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{pre}}_{l}</annotation></semantics></math>, uniform weights of ones for <math alttext=\"\\mathcal{H}^{\\mathrm{post}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.13.6.6.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "667", "text": "uniform weights of ones for <math alttext=\"\\mathcal{H}^{\\mathrm{post}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.13.6.6.m6\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>post</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{post}}_{l}</annotation></semantics></math>, and the identity matrix for <math alttext=\"\\mathcal{H}^{\\mathrm{res}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.14.7.7.m7\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{res}}_{l}</annotation></semantics></math>.</span></span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T1.21\">\n<tr class=\"ltx_tr\" id=\"S3.T1.17.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.15.1.1\"><math alttext=\"\\mathcal{H}^{\\mathrm{res}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.15.1.1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "668", "text": "15.1.1\"><math alttext=\"\\mathcal{H}^{\\mathrm{res}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.15.1.1.m1\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{res}}_{l}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.16.2.2\"><math alttext=\"\\mathcal{H}^{\\mathrm{pre}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.16.2.2.m1\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>pre</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{pre}}_{l}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S3.T1.17.3.3\"><math alttext=\"\\mathcal{H}^{\\mathrm{post}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.17.3.3.m1\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>post</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{post}}_{l}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.17.3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "669", "text": "T1.17.3.4\">Absolute Loss Gap</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.18.4\">\n<td class=\"ltx_td ltx_border_t\" id=\"S3.T1.18.4.2\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S3.T1.18.4.3\"></td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" id=\"S3.T1.18.4.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.18.4.1\"><math alttext=\"\\phantom{-\\ }0.0\\phantom{00}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.18.4.1.m1\" intent=\":literal\"><semantics><mn>0.0</mn><annotation encoding=\"application/x-tex\">\\phantom{-\\ }0.0\\phantom{00}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.19.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.19.5.2\">✓</td>\n<td class=\"ltx_td\" id=\"S3.T1.19.5.3\"></td>\n<td class=\"ltx_td ltx_border_r\" id=\"S3.T1.19.5.4\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.19.5.1\"><math alttext=\"-\\ 0.022\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.19.5.1.m1\" intent=\":literal\"><semantics><mrow><mo>−</mo><mn> 0.022</mn></mrow><annotation encoding=\"application/x-tex\">-\\ 0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "670", "text": "19.5.1.m1\" intent=\":literal\"><semantics><mrow><mo>−</mo><mn> 0.022</mn></mrow><annotation encoding=\"application/x-tex\">-\\ 0.022</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.20.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.20.6.2\">✓</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.20.6.3\">✓</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S3.T1.20.6.4\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.20.6.1\"><math alttext=\"-\\ 0.025\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.20.6.1.m1\" intent=\":literal\"><semantics><mrow><mo>−</mo><mn> 0.025</mn></mrow><annotation encoding=\"application/x-tex\">-\\ 0.025</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.21.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.21.7.2\">✓</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.21.7.3\">✓</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S3.T1.21.7.4\">✓</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.21.7.1\"><math alttext=\"-\\ 0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "671", "text": "21.7.4\">✓</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.21.7.1\"><math alttext=\"-\\ 0.027\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.21.7.1.m1\" intent=\":literal\"><semantics><mrow><mo>−</mo><mn> 0.027</mn></mrow><annotation encoding=\"application/x-tex\">-\\ 0.027</annotation></semantics></math></td>\n</tr>\n</table>\n</figure>\n<section class=\"ltx_subsection\" id=\"S3.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.1 </span>Numerical Instability</h3>\n<div class=\"ltx_para\" id=\"S3.SS1.p1\">\n<p class=\"ltx_p\" id=\"S3.SS1.p1.5\">While the residual mapping <math alttext=\"\\mathcal{H}^{\\mathrm{res}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.1.m1\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{res}}_{l}</annotation></semantics></math> is instrumental for performance, its sequential application poses a significant risk to numerical stability. As detailed in Eq.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "672", "text": "its sequential application poses a significant risk to numerical stability. As detailed in Eq. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S1.E4\" title=\"Equation 4 ‣ 1 Introduction ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), when HC is extended across multiple layers, the effective signal propagation from layer <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.2.m2\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math> to <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.3.m3\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> is governed by the composite mapping <math alttext=\"\\prod_{i=1}^{L-l}\\mathcal{H}_{L-i}^{\\mathrm{res}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.4.m4\" intent=\":literal\"><semantics><mrow><msubsup><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>L</mi><mo>−</mo><mi>l</mi></mrow></msubsup><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mrow><mi>L</mi><mo>−</mo><mi>i</mi></mrow><mi>res</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\prod_{i=1}^{L-l}\\mathcal{H}_{L-i}^{\\mathrm{res}}</annotation></semantics></math>.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "673", "text": "Since the learnable mapping <math alttext=\"\\mathcal{H}^{\\mathrm{res}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.5.m5\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{res}}_{l}</annotation></semantics></math> is unconstrained, this composite mapping inevitably deviates from the identity mapping. Consequently, the signal magnitude is prone to explosion or vanishing during both the forward pass and backpropagation. This phenomenon undermines the fundamental premise of residual learning, which relies on unimpeded signal flow, thereby destabilizing the training process in deeper or larger-scale models.</p>\n</div>\n<div class=\"ltx_para\" id=\"S3.SS1.p2\">\n<p class=\"ltx_p\" id=\"S3.SS1.p2.2\">Empirical evidence supports this analysis. We observe unstable loss behavior in large-scale experiments, as illustrated in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S3.F2\" title=\"Figure 2 ‣ 3.1 Numerical Instability ‣ 3 Preliminary ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Taking <span class=\"ltx_text ltx_font_italic\" id=\"S3.SS1.p2.2.1\">m</span>HC as the baseline, HC exhibits an unexpected loss surge around the 12k step, which is highly correlated with the instability in the gradient norm.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "674", "text": "Furthermore, the analysis on <math alttext=\"\\mathcal{H}^{\\mathrm{res}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.1.m1\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{res}}_{l}</annotation></semantics></math> validates the mechanism of this instability.\nTo quantify how the composite mapping <math alttext=\"\\prod_{i=1}^{L-l}\\mathcal{H}_{L-i}^{\\mathrm{res}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.2.m2\" intent=\":literal\"><semantics><mrow><msubsup><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>L</mi><mo>−</mo><mi>l</mi></mrow></msubsup><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mrow><mi>L</mi><mo>−</mo><mi>i</mi></mrow><mi>res</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\prod_{i=1}^{L-l}\\mathcal{H}_{L-i}^{\\mathrm{res}}</annotation></semantics></math> amplifies signals along the residual stream, we utilize two metrics. The first, based on the maximum absolute value of the row sums of the composite mapping, captures the worst-case expansion in the forward pass. The second, based on the maximum absolute column sum, corresponds to the backward pass.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "675", "text": "The first, based on the maximum absolute value of the row sums of the composite mapping, captures the worst-case expansion in the forward pass. The second, based on the maximum absolute column sum, corresponds to the backward pass. We refer to these metrics as the <span class=\"ltx_text ltx_font_italic\" id=\"S3.SS1.p2.2.2\">Amax Gain Magnitude</span> of the composite mapping. As shown in Fig.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "676", "text": "As shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S3.F3\" title=\"Figure 3 ‣ 3.1 Numerical Instability ‣ 3 Preliminary ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> (b), the Amax Gain Magnitude yields extreme values with peaks of 3000, a stark divergence from 1 that confirms the presence of exploding residual streams.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S3.F2\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"190\" id=\"S3.F2.g1\" src=\"x2.png\" width=\"660\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S3.F2.4.1.1\" style=\"font-size:90%;\">Figure 2</span>: </span><span class=\"ltx_text ltx_font_bold\" id=\"S3.F2.5.2\" style=\"font-size:90%;\">Training Instability of Hyper-Connections (HC).<span class=\"ltx_text ltx_font_medium\" id=\"S3.F2.5.2.1\"> This figure illustrates (a) the absolute loss gap of HC relative to <span class=\"ltx_text ltx_font_italic\" id=\"S3.F2.5.2.1.1\">m</span>HC, and (b) the comparisons of gradient norms. All results are based on 27B models.</span></span></figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"S3.F3\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"190\" id=\"S3.F3.g1\" src=\"x3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "677", "text": "F3\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"190\" id=\"S3.F3.g1\" src=\"x3.png\" width=\"660\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S3.F3.9.4.1\" style=\"font-size:90%;\">Figure 3</span>: </span><span class=\"ltx_text ltx_font_bold\" id=\"S3.F3.6.3\" style=\"font-size:90%;\">Propagation Instability of Hyper-Connections (HC).<span class=\"ltx_text ltx_font_medium\" id=\"S3.F3.6.3.3\"> This figure illustrates the propagation dynamics of (a) the single-layer mapping <math alttext=\"\\mathcal{H}^{\\mathrm{res}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.F3.4.1.1.m1\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{res}}_{l}</annotation></semantics></math> and (b) the composite mapping <math alttext=\"\\prod_{i=1}^{L-l}\\mathcal{H}_{L-i}^{\\mathrm{res}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.F3.5.2.2.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "678", "text": "F3.5.2.2.m2\" intent=\":literal\"><semantics><mrow><msubsup><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>L</mi><mo>−</mo><mi>l</mi></mrow></msubsup><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mrow><mi>L</mi><mo>−</mo><mi>i</mi></mrow><mi>res</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\prod_{i=1}^{L-l}\\mathcal{H}_{L-i}^{\\mathrm{res}}</annotation></semantics></math> within the 27B model. The layer index <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S3.F3.6.3.3.m3\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math> (x-axis) unrolls each standard Transformer block into two independent layers (Attention and FFN). The Amax Gain Magnitude (y-axis) is calculated as the maximum absolute row sum (for the forward signal) and column sum (for the backward gradient), averaged over all tokens in a selected sequence.</span></span></figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsection\" id=\"S3.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.2 </span>System Overhead</h3>\n<div class=\"ltx_para\" id=\"S3.SS2.p1\">\n<p class=\"ltx_p\" id=\"S3.SS2.p1.1\">While the computational complexity of HC remains manageable due to the linearity of the additional mappings, the system-level overhead prevents a non-negligible challenge.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "679", "text": "Specifically, memory access (I/O) costs often constitute one of the primary bottlenecks in modern model architectures, which is widely referred to as the “memory wall” <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dao2022flashattention</span>)</cite>. This bottleneck is frequently overlooked in architectural design, yet it decisively impacts runtime efficiency.</p>\n</div>\n<figure class=\"ltx_table\" id=\"S3.T2\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S3.T2.27.2.1\" style=\"font-size:90%;\">Table 2</span>: </span><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.2.1\" style=\"font-size:90%;\">Comparison of Memory Access Costs Per Token.<span class=\"ltx_text ltx_font_medium\" id=\"S3.T2.2.1.1\"> This analysis accounts for the overhead introduced by the residual stream maintenance in the forward pass, excluding the internal I/O of the layer function <math alttext=\"\\mathcal{F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.2.1.1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">ℱ</mi><annotation encoding=\"application/x-tex\">\\mathcal{F}</annotation></semantics></math>.</span></span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T2.24\">\n<tr class=\"ltx_tr\" id=\"S3.T2.24.23\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S3.T2.24.23.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "680", "text": "T2.24.23\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S3.T2.24.23.1\">Method</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T2.24.23.2\">Operation</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T2.24.23.3\">Read (Elements)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T2.24.23.4\">Write (Elements)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.4.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T2.4.2.3\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S3.T2.4.2.3.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T2.4.2.3.1.1\">\n<span class=\"ltx_tr\" id=\"S3.T2.4.2.3.1.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T2.4.2.3.1.1.1.1\">Residual</span></span>\n<span class=\"ltx_tr\" id=\"S3.T2.4.2.3.1.1.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T2.4.2.3.1.1.2.1\">Connection</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.4.2.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "681", "text": "3.1.1.2.1\">Connection</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.4.2.4\">Residual Merge</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.1.1\"><math alttext=\"2C\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.3.1.1.m1\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">2C</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.4.2.2\"><math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.4.2.2.m1\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.6.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.6.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.6.4.3.1\">Total I/O</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.5.3.1\"><math alttext=\"\\mathbf{2C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.5.3.1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "682", "text": "T2.5.3.1\"><math alttext=\"\\mathbf{2C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.5.3.1.m1\" intent=\":literal\"><semantics><mrow><mn>𝟐</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>𝐂</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbf{2C}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.6.4.2\"><math alttext=\"\\mathbf{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.6.4.2.m1\" intent=\":literal\"><semantics><mi>𝐂</mi><annotation encoding=\"application/x-tex\">\\mathbf{C}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.11.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" id=\"S3.T2.11.9.6\" rowspan=\"6\"><span class=\"ltx_text\" id=\"S3.T2.11.9.6.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T2.11.9.6.1.1\">\n<span class=\"ltx_tr\" id=\"S3.T2.11.9.6.1.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T2.11.9.6.1.1.1.1\">Hyper-</span></span>\n<span class=\"ltx_tr\" id=\"S3.T2.11.9.6.1.1.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "683", "text": "T2.11.9.6.1.1.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T2.11.9.6.1.1.2.1\">Connections</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.9.7.3\">Calculate <math alttext=\"\\mathcal{H}^{\\mathrm{pre}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.7.5.1.m1\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>pre</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{pre}}_{l}</annotation></semantics></math>, <math alttext=\"\\mathcal{H}^{\\mathrm{post}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.8.6.2.m2\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>post</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{post}}_{l}</annotation></semantics></math>, <math alttext=\"\\mathcal{H}^{\\mathrm{res}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.9.7.3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "684", "text": "<math alttext=\"\\mathcal{H}^{\\mathrm{res}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.9.7.3.m3\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{res}}_{l}</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.10.8.4\"><math alttext=\"nC\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.10.8.4.m1\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">nC</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.11.9.5\"><math alttext=\"n^{2}+2n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.11.9.5.m1\" intent=\":literal\"><semantics><mrow><msup><mi>n</mi><mn>2</mn></msup><mo>+</mo><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>n</mi></mrow></mrow><annotation encoding=\"application/x-tex\">n^{2}+2n</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.14.12\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.12.10.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "685", "text": "T2.14.12\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.12.10.1\"><math alttext=\"\\mathcal{H}^{\\mathrm{pre}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.12.10.1.m1\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>pre</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{pre}}_{l}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.13.11.2\"><math alttext=\"nC+n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.13.11.2.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>C</mi></mrow><mo>+</mo><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">nC+n</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.14.12.3\"><math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.14.12.3.m1\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.17.15\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.15.13.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "686", "text": "T2.17.15\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.15.13.1\"><math alttext=\"\\mathcal{H}^{\\mathrm{post}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.15.13.1.m1\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>post</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{post}}_{l}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.16.14.2\"><math alttext=\"C+n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.16.14.2.m1\" intent=\":literal\"><semantics><mrow><mi>C</mi><mo>+</mo><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">C+n</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.17.15.3\"><math alttext=\"nC\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.17.15.3.m1\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">nC</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.20.18\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.18.16.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "687", "text": "T2.20.18\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.18.16.1\"><math alttext=\"\\mathcal{H}^{\\mathrm{res}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.18.16.1.m1\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{res}}_{l}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.19.17.2\"><math alttext=\"nC+n^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.19.17.2.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>C</mi></mrow><mo>+</mo><msup><mi>n</mi><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">nC+n^{2}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.20.18.3\"><math alttext=\"nC\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.20.18.3.m1\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">nC</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.22.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "688", "text": "T2.22.20\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.22.20.3\">Residual Merge</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.21.19.1\"><math alttext=\"2nC\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.21.19.1.m1\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">2nC</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.22.20.2\"><math alttext=\"nC\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.22.20.2.m1\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">nC</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.24.22\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T2.24.22.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.24.22.3.1\">Total I/O</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T2.23.21.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "689", "text": "T2.24.22.3.1\">Total I/O</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T2.23.21.1\"><math alttext=\"\\mathbf{(5n+1)C+n^{2}+2n}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.23.21.1.m1\" intent=\":literal\"><semantics><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mn>𝟓</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>𝐧</mi></mrow><mo>+</mo><mn>𝟏</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>𝐂</mi></mrow><mo>+</mo><msup><mi>𝐧</mi><mn>𝟐</mn></msup><mo>+</mo><mrow><mn>𝟐</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>𝐧</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{(5n+1)C+n^{2}+2n}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T2.24.22.2\"><math alttext=\"\\mathbf{(3n+1)C+n^{2}+2n}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.24.22.2.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "690", "text": "22.2\"><math alttext=\"\\mathbf{(3n+1)C+n^{2}+2n}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.24.22.2.m1\" intent=\":literal\"><semantics><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mn>𝟑</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>𝐧</mi></mrow><mo>+</mo><mn>𝟏</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>𝐂</mi></mrow><mo>+</mo><msup><mi>𝐧</mi><mn>𝟐</mn></msup><mo>+</mo><mrow><mn>𝟐</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>𝐧</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{(3n+1)C+n^{2}+2n}</annotation></semantics></math></td>\n</tr>\n</table>\n</figure>\n<div class=\"ltx_para\" id=\"S3.SS2.p2\">\n<p class=\"ltx_p\" id=\"S3.SS2.p2.6\">Focusing on the widely adopted pre-norm Transformer <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">vaswani2017attention</span>)</cite> architecture, we analyze the I/O patterns inherent to HC. Tab.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "691", "text": "we analyze the I/O patterns inherent to HC. Tab. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S3.T2\" title=\"Table 2 ‣ 3.2 System Overhead ‣ 3 Preliminary ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarizes the per token memory access overhead in a single residual layer introduced by the <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-stream residual design. The analysis reveals that HC increases the memory access cost by a factor approximately proportional to <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.2.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>. This excessive I/O demand significantly degrades training throughput without the mitigation of fused kernels.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "692", "text": "This excessive I/O demand significantly degrades training throughput without the mitigation of fused kernels. Besides, since <math alttext=\"\\mathcal{H}^{\\mathrm{pre}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.3.m3\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>pre</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{pre}}_{l}</annotation></semantics></math>, <math alttext=\"\\mathcal{H}^{\\mathrm{post}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.4.m4\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>post</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{post}}_{l}</annotation></semantics></math>, and <math alttext=\"\\mathcal{H}^{\\mathrm{res}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.5.m5\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{res}}_{l}</annotation></semantics></math> involve learnable parameters, their intermediate activations are required for backpropagation. This results in a substantial increase in the GPU memory footprint, often necessitating gradient checkpointing to maintain feasible memory usage. Furthermore, HC requires <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.6.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "693", "text": "Furthermore, HC requires <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.6.m6\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-fold more communication cost in pipeline parallelism <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qi2024zero</span>)</cite>, leading to larger bubbles and decreasing the training throughput.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S4\" lang=\"en\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">4 </span>Method</h2>\n<section class=\"ltx_subsection\" id=\"S4.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.1 </span>Manifold-Constrained Hyper-Connections</h3>\n<div class=\"ltx_para\" id=\"S4.SS1.p1\">\n<p class=\"ltx_p\" id=\"S4.SS1.p1.6\">Drawing inspiration from the identity mapping principle <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">he2016identity</span>)</cite>, the core premise of <span class=\"ltx_text ltx_font_italic\" id=\"S4.SS1.p1.6.1\">m</span>HC is to constrain the residual mapping <math alttext=\"\\mathcal{H}^{\\mathrm{res}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "694", "text": "SS1.p1.1.m1\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{res}}_{l}</annotation></semantics></math> onto a specific manifold.\nWhile the original identity mapping ensures stability by enforcing <math alttext=\"\\mathcal{H}^{\\mathrm{res}}_{l}=\\mathbf{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.2.m2\" intent=\":literal\"><semantics><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup><mo>=</mo><mi>𝐈</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{res}}_{l}=\\mathbf{I}</annotation></semantics></math>, it fundamentally precludes information exchange within the residual stream, which is critical for maximizing the potential of multi-stream architectures. Therefore, we propose projecting the residual mapping onto a manifold that simultaneously maintains the stability of signal propagation across layers and facilitates mutual interaction among residual streams to preserve the model’s expressivity.\nTo this end, we restrict <math alttext=\"\\mathcal{H}^{\\mathrm{res}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.3.m3\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{res}}_{l}</annotation></semantics></math> to be a doubly stochastic matrix, which has non-negative entries where both the rows and columns sum to 1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "695", "text": "Formally, let <math alttext=\"\\mathcal{M}^{\\mathrm{res}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.4.m4\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">ℳ</mi><mi>res</mi></msup><annotation encoding=\"application/x-tex\">\\mathcal{M}^{\\mathrm{res}}</annotation></semantics></math> denote the manifold of doubly stochastic matrices (also known as the Birkhoff polytope).\nWe constrain <math alttext=\"\\mathcal{H}^{\\mathrm{res}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.5.m5\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{res}}_{l}</annotation></semantics></math> to <math alttext=\"\\mathcal{P}_{\\mathcal{M}^{\\mathrm{res}}}(\\mathcal{H}^{\\mathrm{res}}_{l})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.6.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "696", "text": "SS1.p1.6.m6\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">𝒫</mi><msup><mi class=\"ltx_font_mathcaligraphic\">ℳ</mi><mi>res</mi></msup></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{P}_{\\mathcal{M}^{\\mathrm{res}}}(\\mathcal{H}^{\\mathrm{res}}_{l})</annotation></semantics></math>, defined as:</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S4.E6\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\mathcal{P}_{\\mathcal{M}^{\\mathrm{res}}}(\\mathcal{H}^{\\mathrm{res}}_{l})\\coloneq\\left\\{\\mathcal{H}^{\\mathrm{res}}_{l}\\in\\mathbb{R}^{n\\times n}\\mid\\mathcal{H}^{\\mathrm{res}}_{l}\\mathbf{1}_{n}=\\mathbf{1}_{n},\\ \\mathbf{1}^{\\top}_{n}\\mathcal{H}^{\\mathrm{res}}_{l}=\\mathbf{1}^{\\top}_{n},\\ \\mathcal{H}^{\\mathrm{res}}_{l}\\geqslant 0\\right\\},", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "697", "text": "\\ \\mathcal{H}^{\\mathrm{res}}_{l}\\geqslant 0\\right\\},\" class=\"ltx_Math\" display=\"block\" id=\"S4.E6.m1\" intent=\":literal\"><semantics><mrow><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">𝒫</mi><msup><mi class=\"ltx_font_mathcaligraphic\">ℳ</mi><mi>res</mi></msup></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:-</mo><mrow><mo>{</mo><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>n</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>n</mi></mrow></msup></mrow><mo fence=\"true\" lspace=\"0em\" rspace=\"0em\">∣</mo><mrow><mrow><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mn>𝟏</mn><mi>n</mi></msub></mrow><mo>=</mo><msub><mn>𝟏</mn><mi>n</mi></msub></mrow><mo>,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "698", "text": "</mo><mrow><mrow><mrow><msubsup><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\"> 1</mn><mi>n</mi><mo>⊤</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup></mrow><mo>=</mo><msubsup><mn>𝟏</mn><mi>n</mi><mo>⊤</mo></msubsup></mrow><mo rspace=\"0.667em\">,</mo><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup><mo>⩾</mo><mn>0</mn></mrow></mrow></mrow><mo>}</mo></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\mathcal{P}_{\\mathcal{M}^{\\mathrm{res}}}(\\mathcal{H}^{\\mathrm{res}}_{l})\\coloneq\\left\\{\\mathcal{H}^{\\mathrm{res}}_{l}\\in\\mathbb{R}^{n\\times n}\\mid\\mathcal{H}^{\\mathrm{res}}_{l}\\mathbf{1}_{n}=\\mathbf{1}_{n},\\ \\mathbf{1}^{\\top}_{n}\\mathcal{H}^{\\mathrm{res}}_{l}=\\mathbf{1}^{\\top}_{n},\\ \\mathcal{H}^{\\mathrm{res}}_{l}\\geqslant 0\\right\\},", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "699", "text": "\\ \\mathcal{H}^{\\mathrm{res}}_{l}\\geqslant 0\\right\\},</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(6)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\" id=\"S4.SS1.p1.8\">where <math alttext=\"\\mathbf{1}_{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.7.m1\" intent=\":literal\"><semantics><msub><mn>𝟏</mn><mi>n</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{1}_{n}</annotation></semantics></math> represents the <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.8.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-dimensional vector of all ones.</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS1.p2\">\n<p class=\"ltx_p\" id=\"S4.SS1.p2.2\">It is worth noting that when <math alttext=\"n=1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.1.m1\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">n=1</annotation></semantics></math>,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "700", "text": "the doubly stochastic condition degenerates to the scalar <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.2.m2\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, thereby recovering the original identity mapping.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "701", "text": "SS1.p2.2.m2\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, thereby recovering the original identity mapping. The choice of double stochasticity confers several rigorous theoretical properties beneficial for large-scale model training:</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS1.p3\">\n<ol class=\"ltx_enumerate\" id=\"S4.I1\">\n<li class=\"ltx_item\" id=\"S4.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">1.</span>\n<div class=\"ltx_para\" id=\"S4.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"S4.I1.i1.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.I1.i1.p1.1.1\">Norm Preservation:</span> The spectral norm of a doubly stochastic matrix is bounded by 1 (i.e., <math alttext=\"\\|\\mathcal{H}^{\\mathrm{res}}_{l}\\|_{2}\\leq 1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i1.p1.1.m1\" intent=\":literal\"><semantics><mrow><msub><mrow><mo stretchy=\"false\">‖</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup><mo stretchy=\"false\">‖</mo></mrow><mn>2</mn></msub><mo>≤</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\|\\mathcal{H}^{\\mathrm{res}}_{l}\\|_{2}\\leq 1</annotation></semantics></math>).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "702", "text": "This implies that the learnable mapping is non-expansive, effectively mitigating the gradient explosion problem.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S4.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">2.</span>\n<div class=\"ltx_para\" id=\"S4.I1.i2.p1\">\n<p class=\"ltx_p\" id=\"S4.I1.i2.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.I1.i2.p1.1.1\">Compositional Closure:</span> The set of doubly stochastic matrices is closed under matrix multiplication. This ensures that the composite residual mapping across multiple layers, <math alttext=\"\\prod_{i=1}^{L-l}\\mathcal{H}_{L-i}^{\\mathrm{res}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i2.p1.1.m1\" intent=\":literal\"><semantics><mrow><msubsup><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>L</mi><mo>−</mo><mi>l</mi></mrow></msubsup><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mrow><mi>L</mi><mo>−</mo><mi>i</mi></mrow><mi>res</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\prod_{i=1}^{L-l}\\mathcal{H}_{L-i}^{\\mathrm{res}}</annotation></semantics></math>, remains doubly stochastic, thereby preserving stability throughout the entire depth of the model.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S4.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "703", "text": "</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S4.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">3.</span>\n<div class=\"ltx_para\" id=\"S4.I1.i3.p1\">\n<p class=\"ltx_p\" id=\"S4.I1.i3.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.I1.i3.p1.1.1\">Geometric Interpretation via the Birkhoff Polytope:</span> The set <math alttext=\"\\mathcal{M}^{\\mathrm{res}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i3.p1.1.m1\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">ℳ</mi><mi>res</mi></msup><annotation encoding=\"application/x-tex\">\\mathcal{M}^{\\mathrm{res}}</annotation></semantics></math> forms the Birkhoff polytope, which is the convex hull of the set of permutation matrices. This provides a clear geometric interpretation: the residual mapping acts as a convex combination of permutations.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "704", "text": "which is the convex hull of the set of permutation matrices. This provides a clear geometric interpretation: the residual mapping acts as a convex combination of permutations. Mathematically, the repeated application of such matrices tends to increase the mixing of information across streams monotonically, effectively functioning as a robust feature fusion mechanism.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS1.p4\">\n<p class=\"ltx_p\" id=\"S4.SS1.p4.2\">Additionally, we impose non-negativity constraints on the input mappings <math alttext=\"\\mathcal{H}^{\\mathrm{pre}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.1.m1\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>pre</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{pre}}_{l}</annotation></semantics></math> and output mappings <math alttext=\"\\mathcal{H}^{\\mathrm{post}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.2.m2\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>post</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{post}}_{l}</annotation></semantics></math>. This constrain prevents signal cancellation arising from the composition of positive and negative coefficients, which can also be considered as a special manifold projection.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S4.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "705", "text": "SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.2 </span>Parameterization and Manifold Projection</h3>\n<div class=\"ltx_para\" id=\"S4.SS2.p1\">\n<p class=\"ltx_p\" id=\"S4.SS2.p1.4\">In this section, we detail the calculation process of <math alttext=\"\\mathcal{H}^{\\mathrm{pre}}_{l},\\mathcal{H}^{\\mathrm{post}}_{l},\\text{and }\\mathcal{H}^{\\mathrm{res}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.1.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>pre</mi></msubsup><mo>,</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>post</mi></msubsup><mo>,</mo><mrow><mtext>and </mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{pre}}_{l},\\mathcal{H}^{\\mathrm{post}}_{l},\\text{and }\\mathcal{H}^{\\mathrm{res}}_{l}</annotation></semantics></math> in <span class=\"ltx_text ltx_font_italic\" id=\"S4.SS2.p1.4.1\">m</span>HC.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "706", "text": "SS2.p1.4.1\">m</span>HC. Given the input hidden matrix <math alttext=\"\\mathbf{x}_{l}\\in\\mathbb{R}^{n\\times C}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.2.m2\" intent=\":literal\"><semantics><mrow><msub><mi>𝐱</mi><mi>l</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>n</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>C</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{x}_{l}\\in\\mathbb{R}^{n\\times C}</annotation></semantics></math> at the <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.3.m3\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math>-th layer, we first flatten it into a vector <math alttext=\"\\vec{\\mathbf{x}}_{l}=\\text{vec}(\\mathbf{x}_{l})\\in\\mathbb{R}^{1\\times nC}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.4.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "707", "text": "SS2.p1.4.m4\" intent=\":literal\"><semantics><mrow><msub><mover accent=\"true\"><mi>𝐱</mi><mo stretchy=\"false\">→</mo></mover><mi>l</mi></msub><mo>=</mo><mrow><mtext>vec</mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝐱</mi><mi>l</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>n</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>C</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\vec{\\mathbf{x}}_{l}=\\text{vec}(\\mathbf{x}_{l})\\in\\mathbb{R}^{1\\times nC}</annotation></semantics></math> to preserve full context information. Then, we follow the original HC formulation to get the dynamic mappings and the static mappings as follows:</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S4.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "708", "text": "Then, we follow the original HC formulation to get the dynamic mappings and the static mappings as follows:</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S4.E7\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\begin{cases}\\vec{\\mathbf{x}}^{\\prime}_{l}=\\text{RMSNorm}(\\vec{\\mathbf{x}}_{l})\\\\\n\\tilde{\\mathcal{H}}^{\\mathrm{pre}}_{l}=\\alpha_{l}^{\\mathrm{pre}}\\cdot(\\vec{\\mathbf{x}}^{\\prime}_{l}\\varphi^{\\mathrm{pre}}_{l})+\\mathbf{b}_{l}^{\\mathrm{pre}}\\\\\n\\tilde{\\mathcal{H}}^{\\mathrm{post}}_{l}=\\alpha_{l}^{\\mathrm{post}}\\cdot(\\vec{\\mathbf{x}}^{\\prime}_{l}\\varphi^{\\mathrm{post}}_{l})+\\mathbf{b}_{l}^{\\mathrm{post}}\\\\\n\\tilde{\\mathcal{H}}^{\\mathrm{res}}_{l}=\\alpha_{l}^{\\mathrm{res}}\\cdot\\text{mat}(\\vec{\\mathbf{x}}^{\\prime}_{l}\\varphi^{\\mathrm{res}}_{l})+\\mathbf{b}_{l}^{\\mathrm{res}},\\\\\n\\end{cases}\" class=\"ltx_Math\" display=\"block\" id=\"S4.E7.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "709", "text": "\\\\\n\\end{cases}\" class=\"ltx_Math\" display=\"block\" id=\"S4.E7.m1\" intent=\":literal\"><semantics><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><msubsup><mover accent=\"true\"><mi>𝐱</mi><mo stretchy=\"false\">→</mo></mover><mi>l</mi><mo>′</mo></msubsup><mo>=</mo><mrow><mtext>RMSNorm</mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>𝐱</mi><mo stretchy=\"false\">→</mo></mover><mi>l</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mtd><mtd></mtd></mtr><mtr><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><msubsup><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mo>~</mo></mover><mi>l</mi><mi>pre</mi></msubsup><mo>=</mo><mrow><mrow><msubsup><mi>α</mi><mi>l</mi><mi>pre</mi></msubsup><mo lspace=\"0.222em\" rspace=\"0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "710", "text": "222em\" rspace=\"0.222em\">⋅</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mover accent=\"true\"><mi>𝐱</mi><mo stretchy=\"false\">→</mo></mover><mi>l</mi><mo>′</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>φ</mi><mi>l</mi><mi>pre</mi></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><msubsup><mi>𝐛</mi><mi>l</mi><mi>pre</mi></msubsup></mrow></mrow></mtd><mtd></mtd></mtr><mtr><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><msubsup><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mo>~</mo></mover><mi>l</mi><mi>post</mi></msubsup><mo>=</mo><mrow><mrow><msubsup><mi>α</mi><mi>l</mi><mi>post</mi></msubsup><mo lspace=\"0.222em\" rspace=\"0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "711", "text": "222em\" rspace=\"0.222em\">⋅</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mover accent=\"true\"><mi>𝐱</mi><mo stretchy=\"false\">→</mo></mover><mi>l</mi><mo>′</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>φ</mi><mi>l</mi><mi>post</mi></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><msubsup><mi>𝐛</mi><mi>l</mi><mi>post</mi></msubsup></mrow></mrow></mtd><mtd></mtd></mtr><mtr><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><mrow><msubsup><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mo>~</mo></mover><mi>l</mi><mi>res</mi></msubsup><mo>=</mo><mrow><mrow><mrow><msubsup><mi>α</mi><mi>l</mi><mi>res</mi></msubsup><mo lspace=\"0.222em\" rspace=\"0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "712", "text": "222em\" rspace=\"0.222em\">⋅</mo><mtext>mat</mtext></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mover accent=\"true\"><mi>𝐱</mi><mo stretchy=\"false\">→</mo></mover><mi>l</mi><mo>′</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>φ</mi><mi>l</mi><mi>res</mi></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><msubsup><mi>𝐛</mi><mi>l</mi><mi>res</mi></msubsup></mrow></mrow><mo>,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "713", "text": "</mo></mrow></mtd><mtd></mtd></mtr></mtable></mrow><annotation encoding=\"application/x-tex\">\\begin{cases}\\vec{\\mathbf{x}}^{\\prime}_{l}=\\text{RMSNorm}(\\vec{\\mathbf{x}}_{l})\\\\\n\\tilde{\\mathcal{H}}^{\\mathrm{pre}}_{l}=\\alpha_{l}^{\\mathrm{pre}}\\cdot(\\vec{\\mathbf{x}}^{\\prime}_{l}\\varphi^{\\mathrm{pre}}_{l})+\\mathbf{b}_{l}^{\\mathrm{pre}}\\\\\n\\tilde{\\mathcal{H}}^{\\mathrm{post}}_{l}=\\alpha_{l}^{\\mathrm{post}}\\cdot(\\vec{\\mathbf{x}}^{\\prime}_{l}\\varphi^{\\mathrm{post}}_{l})+\\mathbf{b}_{l}^{\\mathrm{post}}\\\\\n\\tilde{\\mathcal{H}}^{\\mathrm{res}}_{l}=\\alpha_{l}^{\\mathrm{res}}\\cdot\\text{mat}(\\vec{\\mathbf{x}}^{\\prime}_{l}\\varphi^{\\mathrm{res}}_{l})+\\mathbf{b}_{l}^{\\mathrm{res}},\\\\\n\\end{cases}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(7)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\" id=\"S4.SS2.p1.9\">where <math alttext=\"\\varphi^{\\mathrm{pre}}_{l},\\varphi^{\\mathrm{post}}_{l}\\in\\mathbb{R}^{nC\\times n}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "714", "text": "\\varphi^{\\mathrm{post}}_{l}\\in\\mathbb{R}^{nC\\times n}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.5.m1\" intent=\":literal\"><semantics><mrow><mrow><msubsup><mi>φ</mi><mi>l</mi><mi>pre</mi></msubsup><mo>,</mo><msubsup><mi>φ</mi><mi>l</mi><mi>post</mi></msubsup></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mrow><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>C</mi></mrow><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\varphi^{\\mathrm{pre}}_{l},\\varphi^{\\mathrm{post}}_{l}\\in\\mathbb{R}^{nC\\times n}</annotation></semantics></math> and <math alttext=\"\\varphi^{\\mathrm{res}}_{l}\\in\\mathbb{R}^{nC\\times n^{2}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.6.m2\" intent=\":literal\"><semantics><mrow><msubsup><mi>φ</mi><mi>l</mi><mi>res</mi></msubsup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mrow><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>C</mi></mrow><mo lspace=\"0.222em\" rspace=\"0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "715", "text": "222em\" rspace=\"0.222em\">×</mo><msup><mi>n</mi><mn>2</mn></msup></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\varphi^{\\mathrm{res}}_{l}\\in\\mathbb{R}^{nC\\times n^{2}}</annotation></semantics></math> are linear projections for dynamic mappings and <math alttext=\"\\text{mat}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.7.m3\" intent=\":literal\"><semantics><mrow><mtext>mat</mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">⋅</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{mat}(\\cdot)</annotation></semantics></math> is a reshape function from <math alttext=\"\\mathbb{R}^{1\\times n^{2}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.8.m4\" intent=\":literal\"><semantics><msup><mi>ℝ</mi><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><msup><mi>n</mi><mn>2</mn></msup></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbb{R}^{1\\times n^{2}}</annotation></semantics></math> to <math alttext=\"\\mathbb{R}^{n\\times n}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.9.m5\" intent=\":literal\"><semantics><msup><mi>ℝ</mi><mrow><mi>n</mi><mo lspace=\"0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "716", "text": "SS2.p1.9.m5\" intent=\":literal\"><semantics><msup><mi>ℝ</mi><mrow><mi>n</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>n</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbb{R}^{n\\times n}</annotation></semantics></math>.</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS2.p2\">\n<p class=\"ltx_p\" id=\"S4.SS2.p2.9\">Then, the final constrained mappings are obtained via:</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S4.E8\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\begin{cases}\\mathcal{H}^{\\mathrm{pre}}_{l}=\\sigma(\\tilde{\\mathcal{H}}^{\\mathrm{pre}}_{l})\\\\\n\\mathcal{H}^{\\mathrm{post}}_{l}=2\\sigma(\\tilde{\\mathcal{H}}^{\\mathrm{post}}_{l})\\\\\n\\mathcal{H}^{\\mathrm{res}}_{l}=\\text{Sinkhorn-Knopp}(\\tilde{\\mathcal{H}}^{\\mathrm{res}}_{l}),\\end{cases}\" class=\"ltx_Math\" display=\"block\" id=\"S4.E8.m1\" intent=\":literal\"><semantics><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"t", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "717", "text": ">{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>pre</mi></msubsup><mo>=</mo><mrow><mi>σ</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mo>~</mo></mover><mi>l</mi><mi>pre</mi></msubsup><mo stretchy=\"false\">)</mo", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "718", "text": "i><mi>pre</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mtd><mtd></mtd></mtr><mtr><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>post</mi></msubsup><mo>=</mo><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>σ</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mo>~<", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "719", "text": "<mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mo>~</mo></mover><mi>l</mi><mi>post</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mtd><mtd></mtd></mtr><mtr><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup><mo>=</mo><mrow><mtext>Sinkhorn-Knopp</mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mover accent=\"true\"><", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "720", "text": "tchy=\"false\">(</mo><msubsup><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mo>~</mo></mover><mi>l</mi><mi>res</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></mtd><mtd></mtd></mtr></mtable></mrow><annotation encoding=\"application/x-tex\">\\begin{cases}\\mathcal{H}^{\\mathrm{pre}}_{l}=\\sigma(\\tilde{\\mathcal{H}}^{\\mathrm{pre}}_{l})\\\\\n\\mathcal{H}^{\\mathrm{post}}_{l}=2\\sigma(\\tilde{\\mathcal{H}}^{\\mathrm{post}}_{l})\\\\\n\\mathcal{H}^{\\mathrm{res}}_{l}=\\text{Sinkhorn-Knopp}(\\tilde{\\mathcal{H}}^{\\mathrm{res}}_{l}),\\end{cases}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(8)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\" id=\"S4.SS2.p2.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "721", "text": "SS2.p2.3\">where <math alttext=\"\\sigma(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.1.m1\" intent=\":literal\"><semantics><mrow><mi>σ</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">⋅</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\sigma(\\cdot)</annotation></semantics></math> denotes the Sigmoid function. The <math alttext=\"\\text{Sinkhorn-Knopp}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.2.m2\" intent=\":literal\"><semantics><mrow><mtext>Sinkhorn-Knopp</mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">⋅</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{Sinkhorn-Knopp}(\\cdot)</annotation></semantics></math> operator firstly makes all elements to be positive via an exponent operator and then conducts iterative normalization process that alternately rescales rows and columns to sum to 1. Specifically, given a positive matrix <math alttext=\"\\mathbf{M}^{(0)}=\\exp(\\tilde{\\mathcal{H}}^{\\mathrm{res}}_{l})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "722", "text": "SS2.p2.3.m3\" intent=\":literal\"><semantics><mrow><msup><mi>𝐌</mi><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo>=</mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mo>~</mo></mover><mi>l</mi><mi>res</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{M}^{(0)}=\\exp(\\tilde{\\mathcal{H}}^{\\mathrm{res}}_{l})</annotation></semantics></math> as the start point, the normalization iteration proceeds as:</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S4.E9\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\mathbf{M}^{(t)}=\\mathcal{T}_{r}\\left(\\mathcal{T}_{c}(\\mathbf{M}^{(t-1)})\\right),\" class=\"ltx_Math\" display=\"block\" id=\"S4.E9.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "723", "text": "\" class=\"ltx_Math\" display=\"block\" id=\"S4.E9.m1\" intent=\":literal\"><semantics><mrow><mrow><msup><mi>𝐌</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>=</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">𝒯</mi><mi>r</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo>(</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">𝒯</mi><mi>c</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>𝐌</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\mathbf{M}^{(t)}=\\mathcal{T}_{r}\\left(\\mathcal{T}_{c}(\\mathbf{M}^{(t-1)})\\right),</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(9)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\" id=\"S4.SS2.p2.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "724", "text": "SS2.p2.8\">where <math alttext=\"\\mathcal{T}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.4.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">𝒯</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{T}_{r}</annotation></semantics></math> and <math alttext=\"\\mathcal{T}_{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.5.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">𝒯</mi><mi>c</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{T}_{c}</annotation></semantics></math> denote row and column normalization, respectively.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "725", "text": "respectively. This process converges to a doubly stochastic matrix <math alttext=\"\\mathcal{H}^{\\mathrm{res}}_{l}=\\mathbf{M}^{(t_{\\text{max}})}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.6.m3\" intent=\":literal\"><semantics><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup><mo>=</mo><msup><mi>𝐌</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mtext>max</mtext></msub><mo stretchy=\"false\">)</mo></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{res}}_{l}=\\mathbf{M}^{(t_{\\text{max}})}</annotation></semantics></math> as <math alttext=\"t_{\\text{max}}\\to\\infty\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.7.m4\" intent=\":literal\"><semantics><mrow><msub><mi>t</mi><mtext>max</mtext></msub><mo stretchy=\"false\">→</mo><mi mathvariant=\"normal\">∞</mi></mrow><annotation encoding=\"application/x-tex\">t_{\\text{max}}\\to\\infty</annotation></semantics></math>.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "726", "text": "We choose <math alttext=\"t_{\\text{max}}=20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.8.m5\" intent=\":literal\"><semantics><mrow><msub><mi>t</mi><mtext>max</mtext></msub><mo>=</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">t_{\\text{max}}=20</annotation></semantics></math> as a practical value in our experiments.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S4.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.3 </span>Efficient Infrastructure Design</h3>\n<div class=\"ltx_para\" id=\"S4.SS3.p1\">\n<p class=\"ltx_p\" id=\"S4.SS3.p1.1\">In this section, we detail the infrastructure design tailored for <span class=\"ltx_text ltx_font_italic\" id=\"S4.SS3.p1.1.1\">m</span>HC.\nThrough rigorous optimization, we implement <span class=\"ltx_text ltx_font_italic\" id=\"S4.SS3.p1.1.2\">m</span>HC (with <math alttext=\"n=4\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.1.m1\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">n=4</annotation></semantics></math>) in large-scale models with a marginal training overhead of only 6.7%.</p>\n</div>\n<section class=\"ltx_subsubsection\" id=\"S4.SS3.SSS1\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">4.3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "727", "text": "SS3.SSS1\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">4.3.1 </span>Kernel Fusion</h4>\n<div class=\"ltx_para\" id=\"S4.SS3.SSS1.p1\">\n<p class=\"ltx_p\" id=\"S4.SS3.SSS1.p1.7\">Observing that RMSNorm in <span class=\"ltx_text ltx_font_italic\" id=\"S4.SS3.SSS1.p1.7.1\">m</span>HC imposes significant latency when operating on the high-dimensional hidden state <math alttext=\"\\vec{\\mathbf{x}}_{l}\\in\\mathbb{R}^{1\\times nC}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS1.p1.1.m1\" intent=\":literal\"><semantics><mrow><msub><mover accent=\"true\"><mi>𝐱</mi><mo stretchy=\"false\">→</mo></mover><mi>l</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>n</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>C</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\vec{\\mathbf{x}}_{l}\\in\\mathbb{R}^{1\\times nC}</annotation></semantics></math>, we reorder the dividing-by-norm operation to follow the matrix multiplication. This optimization maintains mathematical equivalence while improving efficiency.\nFurthermore, we employ mixed-precision strategies to maximize numerical accuracy without compromising speed, and fuse multiple operations with shared memory access into unified compute kernels to reduce memory bandwidth bottlenecks.\nBased on the inputs and parameters detailed in Eq.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "728", "text": "Furthermore, we employ mixed-precision strategies to maximize numerical accuracy without compromising speed, and fuse multiple operations with shared memory access into unified compute kernels to reduce memory bandwidth bottlenecks.\nBased on the inputs and parameters detailed in Eq. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S4.E10\" title=\"Equation 10 ‣ 4.3.1 Kernel Fusion ‣ 4.3 Efficient Infrastructure Design ‣ 4 Method ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>) to  (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S4.E13\" title=\"Equation 13 ‣ 4.3.1 Kernel Fusion ‣ 4.3 Efficient Infrastructure Design ‣ 4 Method ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>), we implement three specialized <span class=\"ltx_text ltx_font_italic\" id=\"S4.SS3.SSS1.p1.7.2\">m</span>HC kernels to compute <math alttext=\"\\mathcal{H}^{\\mathrm{pre}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS1.p1.2.m2\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>pre</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{pre}}_{l}</annotation></semantics></math>, <math alttext=\"\\mathcal{H}^{\\mathrm{post}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS1.p1.3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "729", "text": "<math alttext=\"\\mathcal{H}^{\\mathrm{post}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS1.p1.3.m3\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>post</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{post}}_{l}</annotation></semantics></math>, and <math alttext=\"\\mathcal{H}^{\\mathrm{res}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS1.p1.4.m4\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{res}}_{l}</annotation></semantics></math>.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "730", "text": "In these kernels, the biases and linear projections are consolidated into <math alttext=\"\\mathbf{b}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS1.p1.5.m5\" intent=\":literal\"><semantics><msub><mi>𝐛</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{b}_{l}</annotation></semantics></math> and <math alttext=\"\\varphi_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS1.p1.6.m6\" intent=\":literal\"><semantics><msub><mi>φ</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">\\varphi_{l}</annotation></semantics></math>, and the RMSNorm weight is also absorbed in <math alttext=\"\\varphi_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS1.p1.7.m7\" intent=\":literal\"><semantics><msub><mi>φ</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">\\varphi_{l}</annotation></semantics></math>.</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS3.SSS1.p2\">\n<ul class=\"ltx_itemize\" id=\"S4.I2\">\n<li class=\"ltx_item\" id=\"S4.I2.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S4.I2.i1.p1\">\n<p class=\"ltx_p\" id=\"S4.I2.i1.p1.2\">Eq.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "731", "text": "(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S4.E14\" title=\"Equation 14 ‣ 4.3.1 Kernel Fusion ‣ 4.3 Efficient Infrastructure Design ‣ 4 Method ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>) to (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S4.E15\" title=\"Equation 15 ‣ 4.3.1 Kernel Fusion ‣ 4.3 Efficient Infrastructure Design ‣ 4 Method ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>): We develop a unified kernel that fuses two scans on <math alttext=\"\\vec{\\mathbf{x}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I2.i1.p1.1.m1\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>𝐱</mi><mo stretchy=\"false\">→</mo></mover><mi>l</mi></msub><annotation encoding=\"application/x-tex\">\\vec{\\mathbf{x}}_{l}</annotation></semantics></math>, leveraging matrix multiplication units to maximize memory bandwidth utilization. The backward pass—comprising two matrix multiplications—is similarly consolidated into a single kernel, eliminating redundant reloading of <math alttext=\"\\vec{\\mathbf{x}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I2.i1.p1.2.m2\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>𝐱</mi><mo stretchy=\"false\">→</mo></mover><mi>l</mi></msub><annotation encoding=\"application/x-tex\">\\vec{\\mathbf{x}}_{l}</annotation></semantics></math>.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "732", "text": "Both kernels feature a finely tuned pipeline (load, cast, compute, store) to efficiently handle mixed-precision processing.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S4.I2.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S4.I2.i2.p1\">\n<p class=\"ltx_p\" id=\"S4.I2.i2.p1.1\">Eq. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S4.E16\" title=\"Equation 16 ‣ 4.3.1 Kernel Fusion ‣ 4.3 Efficient Infrastructure Design ‣ 4 Method ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">16</span></a>) to  (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S4.E18\" title=\"Equation 18 ‣ 4.3.1 Kernel Fusion ‣ 4.3 Efficient Infrastructure Design ‣ 4 Method ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">18</span></a>): These lightweight operations on small coefficients are opportunistically fused into a single kernel, significantly reducing kernel launch overhead.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S4.I2.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S4.I2.i3.p1\">\n<p class=\"ltx_p\" id=\"S4.I2.i3.p1.1\">Eq.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "733", "text": "(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S4.E19\" title=\"Equation 19 ‣ 4.3.1 Kernel Fusion ‣ 4.3 Efficient Infrastructure Design ‣ 4 Method ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">19</span></a>): We implement the Sinkhorn-Knopp iteration within a single kernel. For the backward pass, we derive a custom backward kernel that recomputes the intermediate results on-chip and traverses the entire iteration.</p>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS3.SSS1.p3\">\n<table class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\" id=\"A1.EGx1\">\n<tbody id=\"S4.E10\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\varphi_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E10.m1\" intent=\":literal\"><semantics><msub><mi>φ</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">\\displaystyle\\varphi_{l}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle:\\text{tfloat32}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E10.m2\" intent=\":literal\"><semantics><mrow><mi></mi><mo lspace=\"0.278em\" rspace=\"0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "734", "text": "E10.m2\" intent=\":literal\"><semantics><mrow><mi></mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mtext>tfloat32</mtext></mrow><annotation encoding=\"application/x-tex\">\\displaystyle:\\text{tfloat32}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle[nC,n^{2}+2n]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E10.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mrow><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>C</mi></mrow><mo>,</mo><mrow><msup><mi>n</mi><mn>2</mn></msup><mo>+</mo><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>n</mi></mrow></mrow><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle[nC,n^{2}+2n]</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(10)</span></td>\n</tr></tbody>\n<tbody id=\"S4.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "735", "text": "E11\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\vec{\\mathbf{x}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E11.m1\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>𝐱</mi><mo stretchy=\"false\">→</mo></mover><mi>l</mi></msub><annotation encoding=\"application/x-tex\">\\displaystyle\\vec{\\mathbf{x}}_{l}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle:\\text{bfloat16}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E11.m2\" intent=\":literal\"><semantics><mrow><mi></mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mtext>bfloat16</mtext></mrow><annotation encoding=\"application/x-tex\">\\displaystyle:\\text{bfloat16}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle[1,nC]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E11.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><mrow><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>C</mi></mrow><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle[1,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "736", "text": "nC]</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(11)</span></td>\n</tr></tbody>\n<tbody id=\"S4.E12\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\alpha_{l}^{\\mathrm{pre}},\\alpha_{l}^{\\mathrm{post}},\\alpha_{l}^{\\mathrm{res}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E12.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi>α</mi><mi>l</mi><mi>pre</mi></msubsup><mo>,</mo><msubsup><mi>α</mi><mi>l</mi><mi>post</mi></msubsup><mo>,</mo><msubsup><mi>α</mi><mi>l</mi><mi>res</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\alpha_{l}^{\\mathrm{pre}},\\alpha_{l}^{\\mathrm{post}},\\alpha_{l}^{\\mathrm{res}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle:\\text{float32}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E12.m2\" intent=\":literal\"><semantics><mrow><mi></mi><mo lspace=\"0.278em\" rspace=\"0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "737", "text": "E12.m2\" intent=\":literal\"><semantics><mrow><mi></mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mtext>float32</mtext></mrow><annotation encoding=\"application/x-tex\">\\displaystyle:\\text{float32}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><span class=\"ltx_text ltx_markedasmath\" id=\"S4.E12.6.2.1.1\">Scalars</span></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(12)</span></td>\n</tr></tbody>\n<tbody id=\"S4.E13\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\mathbf{b}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E13.m1\" intent=\":literal\"><semantics><msub><mi>𝐛</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">\\displaystyle\\mathbf{b}_{l}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle:\\text{float32}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E13.m2\" intent=\":literal\"><semantics><mrow><mi></mi><mo lspace=\"0.278em\" rspace=\"0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "738", "text": "E13.m2\" intent=\":literal\"><semantics><mrow><mi></mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mtext>float32</mtext></mrow><annotation encoding=\"application/x-tex\">\\displaystyle:\\text{float32}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle[1,n^{2}+2n]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E13.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><mrow><msup><mi>n</mi><mn>2</mn></msup><mo>+</mo><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>n</mi></mrow></mrow><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle[1,n^{2}+2n]</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(13)</span></td>\n</tr></tbody>\n<tbody id=\"S4.E14\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\left[{\\tilde{\\tilde{\\mathcal{H}}}^{\\mathrm{pre}}_{l}},{\\tilde{\\tilde{\\mathcal{H}}}^{\\mathrm{post}}_{l}},", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "739", "text": "{\\tilde{\\tilde{\\mathcal{H}}}^{\\mathrm{post}}_{l}},{\\tilde{\\tilde{\\mathcal{H}}}^{\\mathrm{res}}_{l}}\\right]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E14.m1\" intent=\":literal\"><semantics><mrow><mo>[</mo><msubsup><mover accent=\"true\"><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mo>~</mo></mover><mo>~</mo></mover><mi>l</mi><mi>pre</mi></msubsup><mo>,</mo><msubsup><mover accent=\"true\"><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mo>~</mo></mover><mo>~</mo></mover><mi>l</mi><mi>post</mi></msubsup><mo>,</mo><msubsup><mover accent=\"true\"><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mo>~</mo></mover><mo>~</mo></mover><mi>l</mi><mi>res</mi></msubsup><mo>]</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\left[{\\tilde{\\tilde{\\mathcal{H}}}^{\\mathrm{pre}}_{l}},{\\tilde{\\tilde{\\mathcal{H}}}^{\\mathrm{post}}_{l}},{\\tilde{\\tilde{\\mathcal{H}}}^{\\mathrm{res}}_{l}}\\right]</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle:\\text{float32}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E14.m2\" intent=\":literal\"><semantics><mrow><mi></mi><mo lspace=\"0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "740", "text": "E14.m2\" intent=\":literal\"><semantics><mrow><mi></mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mtext>float32</mtext></mrow><annotation encoding=\"application/x-tex\">\\displaystyle:\\text{float32}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle=\\vec{\\mathbf{x}}_{l}\\varphi_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E14.m3\" intent=\":literal\"><semantics><mrow><mi></mi><mo>=</mo><mrow><msub><mover accent=\"true\"><mi>𝐱</mi><mo stretchy=\"false\">→</mo></mover><mi>l</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>φ</mi><mi>l</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=\\vec{\\mathbf{x}}_{l}\\varphi_{l}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(14)</span></td>\n</tr></tbody>\n<tbody id=\"S4.E15\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle r\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E15.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "741", "text": "E15.m1\" intent=\":literal\"><semantics><mi>r</mi><annotation encoding=\"application/x-tex\">\\displaystyle r</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle:\\text{float32}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E15.m2\" intent=\":literal\"><semantics><mrow><mi></mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mtext>float32</mtext></mrow><annotation encoding=\"application/x-tex\">\\displaystyle:\\text{float32}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle=\\left\\|\\vec{\\mathbf{x}}_{l}\\right\\|_{2}/\\sqrt{nC}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E15.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "742", "text": "E15.m3\" intent=\":literal\"><semantics><mrow><mi></mi><mo>=</mo><mrow><msub><mrow><mo>‖</mo><msub><mover accent=\"true\"><mi>𝐱</mi><mo stretchy=\"false\">→</mo></mover><mi>l</mi></msub><mo>‖</mo></mrow><mn>2</mn></msub><mo>/</mo><msqrt><mrow><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>C</mi></mrow></msqrt></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=\\left\\|\\vec{\\mathbf{x}}_{l}\\right\\|_{2}/\\sqrt{nC}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(15)</span></td>\n</tr></tbody>\n<tbody id=\"S4.E16\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\left[\\tilde{\\mathcal{H}}^{\\mathrm{pre}}_{l},\\tilde{\\mathcal{H}}^{\\mathrm{post}}_{l},\\tilde{\\mathcal{H}}^{\\mathrm{res}}_{l}\\right]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E16.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "743", "text": "\\tilde{\\mathcal{H}}^{\\mathrm{res}}_{l}\\right]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E16.m1\" intent=\":literal\"><semantics><mrow><mo>[</mo><msubsup><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mo>~</mo></mover><mi>l</mi><mi>pre</mi></msubsup><mo>,</mo><msubsup><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mo>~</mo></mover><mi>l</mi><mi>post</mi></msubsup><mo>,</mo><msubsup><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mo>~</mo></mover><mi>l</mi><mi>res</mi></msubsup><mo>]</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\left[\\tilde{\\mathcal{H}}^{\\mathrm{pre}}_{l},\\tilde{\\mathcal{H}}^{\\mathrm{post}}_{l},\\tilde{\\mathcal{H}}^{\\mathrm{res}}_{l}\\right]</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle:\\text{float32}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E16.m2\" intent=\":literal\"><semantics><mrow><mi></mi><mo lspace=\"0.278em\" rspace=\"0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "744", "text": "E16.m2\" intent=\":literal\"><semantics><mrow><mi></mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mtext>float32</mtext></mrow><annotation encoding=\"application/x-tex\">\\displaystyle:\\text{float32}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle=1/r\\left[\\alpha_{l}^{\\mathrm{pre}}{\\tilde{\\tilde{\\mathcal{H}}}^{\\mathrm{pre}}_{l}},\\alpha_{l}^{\\mathrm{post}}{\\tilde{\\tilde{\\mathcal{H}}}^{\\mathrm{post}}_{l}},\\alpha_{l}^{\\mathrm{res}}{\\tilde{\\tilde{\\mathcal{H}}}^{\\mathrm{res}}_{l}}\\right]+\\mathbf{b}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E16.m3\" intent=\":literal\"><semantics><mrow><mi></mi><mo>=</mo><mrow><mrow><mrow><mn>1</mn><mo>/</mo><mi>r</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo>[</mo><mrow><msubsup><mi>α</mi><mi>l</mi><mi>pre</mi></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mover accent=\"true\"><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mo>~</mo></mover><mo>~</mo></mover><mi>l</mi><mi>pre</mi></msubsup></mrow><mo>,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "745", "text": "</mo><mrow><msubsup><mi>α</mi><mi>l</mi><mi>post</mi></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mover accent=\"true\"><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mo>~</mo></mover><mo>~</mo></mover><mi>l</mi><mi>post</mi></msubsup></mrow><mo>,</mo><mrow><msubsup><mi>α</mi><mi>l</mi><mi>res</mi></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mover accent=\"true\"><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mo>~</mo></mover><mo>~</mo></mover><mi>l</mi><mi>res</mi></msubsup></mrow><mo>]</mo></mrow></mrow><mo>+</mo><msub><mi>𝐛</mi><mi>l</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=1/r\\left[\\alpha_{l}^{\\mathrm{pre}}{\\tilde{\\tilde{\\mathcal{H}}}^{\\mathrm{pre}}_{l}},\\alpha_{l}^{\\mathrm{post}}{\\tilde{\\tilde{\\mathcal{H}}}^{\\mathrm{post}}_{l}},", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "746", "text": "\\alpha_{l}^{\\mathrm{post}}{\\tilde{\\tilde{\\mathcal{H}}}^{\\mathrm{post}}_{l}},\\alpha_{l}^{\\mathrm{res}}{\\tilde{\\tilde{\\mathcal{H}}}^{\\mathrm{res}}_{l}}\\right]+\\mathbf{b}_{l}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(16)</span></td>\n</tr></tbody>\n<tbody id=\"S4.E17\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\mathcal{H}^{\\mathrm{pre}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E17.m1\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>pre</mi></msubsup><annotation encoding=\"application/x-tex\">\\displaystyle\\mathcal{H}^{\\mathrm{pre}}_{l}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle:\\text{float32}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E17.m2\" intent=\":literal\"><semantics><mrow><mi></mi><mo lspace=\"0.278em\" rspace=\"0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "747", "text": "E17.m2\" intent=\":literal\"><semantics><mrow><mi></mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mtext>float32</mtext></mrow><annotation encoding=\"application/x-tex\">\\displaystyle:\\text{float32}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle=\\sigma\\left(\\tilde{\\mathcal{H}}^{\\mathrm{pre}}_{l}\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E17.m3\" intent=\":literal\"><semantics><mrow><mi></mi><mo>=</mo><mrow><mi>σ</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo>(</mo><msubsup><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mo>~</mo></mover><mi>l</mi><mi>pre</mi></msubsup><mo>)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=\\sigma\\left(\\tilde{\\mathcal{H}}^{\\mathrm{pre}}_{l}\\right)</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(17)</span></td>\n</tr></tbody>\n<tbody id=\"S4.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "748", "text": "E18\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\mathcal{H}^{\\mathrm{post}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E18.m1\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>post</mi></msubsup><annotation encoding=\"application/x-tex\">\\displaystyle\\mathcal{H}^{\\mathrm{post}}_{l}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle:\\text{float32}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E18.m2\" intent=\":literal\"><semantics><mrow><mi></mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mtext>float32</mtext></mrow><annotation encoding=\"application/x-tex\">\\displaystyle:\\text{float32}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle=2\\sigma\\left(\\tilde{\\mathcal{H}}^{\\mathrm{post}}_{l}\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E18.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "749", "text": "E18.m3\" intent=\":literal\"><semantics><mrow><mi></mi><mo>=</mo><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>σ</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo>(</mo><msubsup><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mo>~</mo></mover><mi>l</mi><mi>post</mi></msubsup><mo>)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=2\\sigma\\left(\\tilde{\\mathcal{H}}^{\\mathrm{post}}_{l}\\right)</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(18)</span></td>\n</tr></tbody>\n<tbody id=\"S4.E19\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\mathcal{H}^{\\mathrm{res}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E19.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "750", "text": "E19.m1\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup><annotation encoding=\"application/x-tex\">\\displaystyle\\mathcal{H}^{\\mathrm{res}}_{l}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle:\\text{float32}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E19.m2\" intent=\":literal\"><semantics><mrow><mi></mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mtext>float32</mtext></mrow><annotation encoding=\"application/x-tex\">\\displaystyle:\\text{float32}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle=\\text{Sinkhorn-Knopp}\\left(\\tilde{\\mathcal{H}}^{\\mathrm{res}}_{l}\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E19.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "751", "text": "E19.m3\" intent=\":literal\"><semantics><mrow><mi></mi><mo>=</mo><mrow><mtext>Sinkhorn-Knopp</mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo>(</mo><msubsup><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mo>~</mo></mover><mi>l</mi><mi>res</mi></msubsup><mo>)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=\\text{Sinkhorn-Knopp}\\left(\\tilde{\\mathcal{H}}^{\\mathrm{res}}_{l}\\right)</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(19)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS3.SSS1.p4\">\n<p class=\"ltx_p\" id=\"S4.SS3.SSS1.p4.8\">Using the coefficients derived from the aforementioned kernels, we introduce two additional kernels to apply these mappings: one for <math alttext=\"\\mathcal{F}_{\\mathrm{pre}}\\coloneq\\mathcal{H}^{\\mathrm{pre}}_{l}\\mathbf{x}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS1.p4.1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "752", "text": "SS3.SSS1.p4.1.m1\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">ℱ</mi><mi>pre</mi></msub><mo>:-</mo><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>pre</mi></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝐱</mi><mi>l</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{F}_{\\mathrm{pre}}\\coloneq\\mathcal{H}^{\\mathrm{pre}}_{l}\\mathbf{x}_{l}</annotation></semantics></math> and another for <math alttext=\"\\mathcal{F}_{\\mathrm{post,res}}\\coloneq\\mathcal{H}^{\\mathrm{res}}_{l}\\mathbf{x}_{l}+\\mathcal{H}_{l}^{\\mathrm{post}\\,\\top}\\mathcal{F}(\\cdot,\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS1.p4.2.m2\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">ℱ</mi><mrow><mi>post</mi><mo>,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "753", "text": "p4.2.m2\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">ℱ</mi><mrow><mi>post</mi><mo>,</mo><mi>res</mi></mrow></msub><mo>:-</mo><mrow><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝐱</mi><mi>l</mi></msub></mrow><mo>+</mo><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mrow><mi>post</mi><mo lspace=\"0.392em\">⊤</mo></mrow></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi class=\"ltx_font_mathcaligraphic\">ℱ</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">⋅</mo><mo rspace=\"0em\">,</mo><mo lspace=\"0em\" rspace=\"0em\">⋅</mo><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{F}_{\\mathrm{post,res}}\\coloneq\\mathcal{H}^{\\mathrm{res}}_{l}\\mathbf{x}_{l}+\\mathcal{H}_{l}^{\\mathrm{post}\\,\\top}\\mathcal{F}(\\cdot,\\cdot)</annotation></semantics></math>.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "754", "text": "\\top}\\mathcal{F}(\\cdot,\\cdot)</annotation></semantics></math>. Through fusing the application of <math alttext=\"\\mathcal{H}^{\\mathrm{post}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS1.p4.3.m3\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>post</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{post}}_{l}</annotation></semantics></math> and <math alttext=\"\\mathcal{H}^{\\mathrm{res}}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS1.p4.4.m4\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{res}}_{l}</annotation></semantics></math> with residual merging, we reduce the number of elements read from <math alttext=\"(3n+1)C\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS1.p4.5.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "755", "text": "we reduce the number of elements read from <math alttext=\"(3n+1)C\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS1.p4.5.m5\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mn>3</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>n</mi></mrow><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">(3n+1)C</annotation></semantics></math> to <math alttext=\"(n+1)C\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS1.p4.6.m6\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">(n+1)C</annotation></semantics></math> and the number of elements written from <math alttext=\"3nC\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS1.p4.7.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "756", "text": "SS3.SSS1.p4.7.m7\" intent=\":literal\"><semantics><mrow><mn>3</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">3nC</annotation></semantics></math> to <math alttext=\"nC\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS1.p4.8.m8\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">nC</annotation></semantics></math> for this kernel.\nWe efficiently implement the majority of kernels (excluding Eq. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S4.E14\" title=\"Equation 14 ‣ 4.3.1 Kernel Fusion ‣ 4.3 Efficient Infrastructure Design ‣ 4 Method ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>) to  (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S4.E15\" title=\"Equation 15 ‣ 4.3.1 Kernel Fusion ‣ 4.3 Efficient Infrastructure Design ‣ 4 Method ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>)) using TileLang <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2025tilelang</span>)</cite>.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "757", "text": "This framework streamlines the implementation of kernels with complex calculation process and allows us to fully utilize the memory bandwidth with minimal engineering effort.</p>\n</div>\n</section>\n<section class=\"ltx_subsubsection\" id=\"S4.SS3.SSS2\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">4.3.2 </span>Recomputing</h4>\n<div class=\"ltx_para\" id=\"S4.SS3.SSS2.p1\">\n<p class=\"ltx_p\" id=\"S4.SS3.SSS2.p1.5\">The <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS2.p1.1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-stream residual design introduces substantial memory overhead during training.\nTo mitigate this, we discard the intermediate activations of the <span class=\"ltx_text ltx_font_italic\" id=\"S4.SS3.SSS2.p1.5.1\">m</span>HC kernels after the forward pass and recompute them on-the-fly in the backward pass, through re-executing the <span class=\"ltx_text ltx_font_italic\" id=\"S4.SS3.SSS2.p1.5.2\">m</span>HC kernels without the heavy layer function <math alttext=\"\\mathcal{F}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS2.p1.2.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">ℱ</mi><annotation encoding=\"application/x-tex\">\\mathcal{F}</annotation></semantics></math>.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "758", "text": "Consequently, for a block of <math alttext=\"L_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS2.p1.3.m3\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">L_{r}</annotation></semantics></math> consecutive layers, we need only store the input <math alttext=\"\\mathbf{x}_{l_{0}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS2.p1.4.m4\" intent=\":literal\"><semantics><msub><mi>𝐱</mi><msub><mi>l</mi><mn>0</mn></msub></msub><annotation encoding=\"application/x-tex\">\\mathbf{x}_{l_{0}}</annotation></semantics></math> to the first layer.\nExcluding lightweight coefficients while accounting for the pre-norm with in <math alttext=\"\\mathcal{F}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS2.p1.5.m5\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">ℱ</mi><annotation encoding=\"application/x-tex\">\\mathcal{F}</annotation></semantics></math>, Tab.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "759", "text": "<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S4.T3\" title=\"Table 3 ‣ 4.3.2 Recomputing ‣ 4.3 Efficient Infrastructure Design ‣ 4 Method ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> summarizes the intermediate activations preserved for the backward pass.</p>\n</div>\n<figure class=\"ltx_table\" id=\"S4.T3\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T3.25.6.1\" style=\"font-size:90%;\">Table 3</span>: </span><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.10.5\" style=\"font-size:90%;\">Stored and Recomputed Intermediate Activations<span class=\"ltx_text ltx_font_medium\" id=\"S4.T3.10.5.5\"> We list per token activation preserved for the backward pass and the transient activation recomputed in <math alttext=\"L_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.6.1.1.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">L_{r}</annotation></semantics></math> consecutive layers. Layer <math alttext=\"l_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.7.2.2.m2\" intent=\":literal\"><semantics><msub><mi>l</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">l_{0}</annotation></semantics></math> represents the first layer in <math alttext=\"L_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.8.3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "760", "text": "T3.8.3.3.m3\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">L_{r}</annotation></semantics></math> layers and layer <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.9.4.4.m4\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math> is in <math alttext=\"[l_{0},l_{0}+L_{r}-1]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.10.5.5.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><msub><mi>l</mi><mn>0</mn></msub><mo>,</mo><mrow><mrow><msub><mi>l</mi><mn>0</mn></msub><mo>+</mo><msub><mi>L</mi><mi>r</mi></msub></mrow><mo>−</mo><mn>1</mn></mrow><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[l_{0},l_{0}+L_{r}-1]</annotation></semantics></math>.</span></span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T3.22\">\n<tr class=\"ltx_tr\" id=\"S4.T3.15.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S4.T3.15.5.6\">Activations</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S4.T3.11.1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "761", "text": "T3.15.5.6\">Activations</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S4.T3.11.1.1\"><math alttext=\"\\mathbf{x}_{l_{0}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.11.1.1.m1\" intent=\":literal\"><semantics><msub><mi>𝐱</mi><msub><mi>l</mi><mn>0</mn></msub></msub><annotation encoding=\"application/x-tex\">\\mathbf{x}_{l_{0}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S4.T3.12.2.2\"><math alttext=\"\\mathcal{F}(\\mathcal{H}^{\\mathrm{pre}}_{l}\\mathbf{x}_{l},\\mathcal{W}_{l})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.12.2.2.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">ℱ</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>pre</mi></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝐱</mi><mi>l</mi></msub></mrow><mo>,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "762", "text": "</mo><msub><mi class=\"ltx_font_mathcaligraphic\">𝒲</mi><mi>l</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{F}(\\mathcal{H}^{\\mathrm{pre}}_{l}\\mathbf{x}_{l},\\mathcal{W}_{l})</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.13.3.3\"><math alttext=\"\\mathbf{x}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.13.3.3.m1\" intent=\":literal\"><semantics><msub><mi>𝐱</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{x}_{l}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.14.4.4\"><math alttext=\"\\mathcal{H}^{\\mathrm{pre}}_{l}\\mathbf{x}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.14.4.4.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>pre</mi></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝐱</mi><mi>l</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\mathcal{H}^{\\mathrm{pre}}_{l}\\mathbf{x}_{l}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.15.5.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "763", "text": "T3.15.5.5\"><math alttext=\"\\text{RMSNorm}(\\mathcal{H}^{\\mathrm{pre}}_{l}\\mathbf{x}_{l})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.15.5.5.m1\" intent=\":literal\"><semantics><mrow><mtext>RMSNorm</mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>pre</mi></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝐱</mi><mi>l</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{RMSNorm}(\\mathcal{H}^{\\mathrm{pre}}_{l}\\mathbf{x}_{l})</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.20.10\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.20.10.6\">Size (Elements)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.16.6.1\"><math alttext=\"nC\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.16.6.1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "764", "text": "T3.16.6.1\"><math alttext=\"nC\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.16.6.1.m1\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">nC</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.17.7.2\"><math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.17.7.2.m1\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.18.8.3\"><math alttext=\"nC\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.18.8.3.m1\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">nC</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.19.9.4\"><math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.19.9.4.m1\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.20.10.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "765", "text": "T3.20.10.5\"><math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.20.10.5.m1\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.22.12\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T3.22.12.3\">Stored Method</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T3.21.11.1\">Every <math alttext=\"L_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.21.11.1.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">L_{r}</annotation></semantics></math> layers</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T3.22.12.4\">Every layer</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" colspan=\"3\" id=\"S4.T3.22.12.2\">Transient inside <math alttext=\"L_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.22.12.2.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">L_{r}</annotation></semantics></math> layers</td>\n</tr>\n</table>\n</figure>\n<div class=\"ltx_para\" id=\"S4.SS3.SSS2.p2\">\n<p class=\"ltx_p\" id=\"S4.SS3.SSS2.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "766", "text": "SS3.SSS2.p2\">\n<p class=\"ltx_p\" id=\"S4.SS3.SSS2.p2.7\">Since <span class=\"ltx_text ltx_font_italic\" id=\"S4.SS3.SSS2.p2.7.1\">m</span>HC kernels recomputation is performed for blocks of <math alttext=\"L_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS2.p2.1.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">L_{r}</annotation></semantics></math> consecutive layers, given a total of <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS2.p2.2.m2\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> layers, we must persistently store the first layer input <math alttext=\"\\mathbf{x}_{l_{0}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS2.p2.3.m3\" intent=\":literal\"><semantics><msub><mi>𝐱</mi><msub><mi>l</mi><mn>0</mn></msub></msub><annotation encoding=\"application/x-tex\">\\mathbf{x}_{l_{0}}</annotation></semantics></math> for all <math alttext=\"\\lceil\\tfrac{L}{L_{r}}\\rceil\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS2.p2.4.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "767", "text": "SS3.SSS2.p2.4.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">⌈</mo><mfrac><mi>L</mi><msub><mi>L</mi><mi>r</mi></msub></mfrac><mo stretchy=\"false\">⌉</mo></mrow><annotation encoding=\"application/x-tex\">\\lceil\\tfrac{L}{L_{r}}\\rceil</annotation></semantics></math> blocks for the backward pass.\nIn addition to this resident memory, the recomputation process introduces a transient memory overhead of <math alttext=\"(n+2)C\\times L_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS2.p2.5.m5\" intent=\":literal\"><semantics><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>n</mi><mo>+</mo><mn>2</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>C</mi></mrow><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><msub><mi>L</mi><mi>r</mi></msub></mrow><annotation encoding=\"application/x-tex\">(n+2)C\\times L_{r}</annotation></semantics></math> elements for the active block, which determines the peak memory usage during backpropagation.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "768", "text": "Consequently, we determine the optimal block size <math alttext=\"L_{r}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS2.p2.6.m6\" intent=\":literal\"><semantics><msubsup><mi>L</mi><mi>r</mi><mo>∗</mo></msubsup><annotation encoding=\"application/x-tex\">L_{r}^{*}</annotation></semantics></math> by minimizing the total memory footprint corresponded to <math alttext=\"L_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS2.p2.7.m7\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">L_{r}</annotation></semantics></math>:</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S4.E20\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"L_{r}^{*}=\\arg\\min_{L_{r}}\\left[nC\\times\\left\\lceil\\frac{L}{L_{r}}\\right\\rceil+(n+2)C\\times L_{r}\\right]\\approx\\sqrt{\\frac{nL}{n+2}}.\" class=\"ltx_Math\" display=\"block\" id=\"S4.E20.m1\" intent=\":literal\"><semantics><mrow><mrow><msubsup><mi>L</mi><mi>r</mi><mo>∗</mo></msubsup><mo>=</mo><mrow><mi>arg</mi><mo lspace=\"0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "769", "text": "167em\">⁡</mo><mrow><munder><mi>min</mi><msub><mi>L</mi><mi>r</mi></msub></munder><mo>⁡</mo><mrow><mo>[</mo><mrow><mrow><mrow><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>C</mi></mrow><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mrow><mo>⌈</mo><mfrac><mi>L</mi><msub><mi>L</mi><mi>r</mi></msub></mfrac><mo>⌉</mo></mrow></mrow><mo>+</mo><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>n</mi><mo>+</mo><mn>2</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>C</mi></mrow><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><msub><mi>L</mi><mi>r</mi></msub></mrow></mrow><mo>]</mo></mrow></mrow></mrow><mo>≈</mo><msqrt><mfrac><mrow><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>L</mi></mrow><mrow><mi>n</mi><mo>+</mo><mn>2</mn></mrow></mfrac></msqrt></mrow><mo lspace=\"0em\">.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "770", "text": "</mo></mrow><annotation encoding=\"application/x-tex\">L_{r}^{*}=\\arg\\min_{L_{r}}\\left[nC\\times\\left\\lceil\\frac{L}{L_{r}}\\right\\rceil+(n+2)C\\times L_{r}\\right]\\approx\\sqrt{\\frac{nL}{n+2}}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(20)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS3.SSS2.p3\">\n<p class=\"ltx_p\" id=\"S4.SS3.SSS2.p3.1\">Furthermore, pipeline parallelism in large-scale training imposes a constraint: recomputation blocks must not cross pipeline stage boundaries.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "771", "text": "SS3.SSS2.p3.1\">Furthermore, pipeline parallelism in large-scale training imposes a constraint: recomputation blocks must not cross pipeline stage boundaries. Observing that the theoretical optimum <math alttext=\"L_{r}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS2.p3.1.m1\" intent=\":literal\"><semantics><msubsup><mi>L</mi><mi>r</mi><mo>∗</mo></msubsup><annotation encoding=\"application/x-tex\">L_{r}^{*}</annotation></semantics></math> typically aligns with the number of layers per pipeline stage, we choose to synchronize the recomputation boundaries with the pipeline stages.</p>\n</div>\n</section>\n<section class=\"ltx_subsubsection\" id=\"S4.SS3.SSS3\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">4.3.3 </span>Overlapping Communication in DualPipe</h4>\n<div class=\"ltx_para\" id=\"S4.SS3.SSS3.p1\">\n<p class=\"ltx_p\" id=\"S4.SS3.SSS3.p1.2\">In large-scale training, pipeline parallelism is the standard practice for mitigating parameter and gradient memory footprints.\nSpecifically, we adopt the DualPipe schedule <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2024deepseek_v3</span>)</cite>, which effectively overlaps scale-out interconnected communication traffic, such as those in expert and pipeline parallelism.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "772", "text": "However, compared to the single-stream design, the proposed <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS3.p1.1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-stream residual in <span class=\"ltx_text ltx_font_italic\" id=\"S4.SS3.SSS3.p1.2.1\">m</span>HC incurs substantial communication latency across pipeline stages.\nFurthermore, at stage boundaries, the recomputation of <span class=\"ltx_text ltx_font_italic\" id=\"S4.SS3.SSS3.p1.2.2\">m</span>HC kernels for all <math alttext=\"L_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS3.p1.2.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">L_{r}</annotation></semantics></math> layers introduces non-negligible computational overhead.\nTo address these bottlenecks, we extend the DualPipe schedule (see Fig.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "773", "text": "To address these bottlenecks, we extend the DualPipe schedule (see Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S4.F4\" title=\"Figure 4 ‣ 4.3.3 Overlapping Communication in DualPipe ‣ 4.3 Efficient Infrastructure Design ‣ 4 Method ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) to facilitate improved overlapping of communication and computation at pipeline stage boundaries.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S4.F4\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"104\" id=\"S4.F4.g1\" src=\"x4.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S4.F4.6.2.1\" style=\"font-size:90%;\">Figure 4</span>: </span><span class=\"ltx_text ltx_font_bold\" id=\"S4.F4.2.1\" style=\"font-size:90%;\">Communication-Computation Overlapping for <span class=\"ltx_text ltx_font_italic\" id=\"S4.F4.2.1.2\">m</span>HC.<span class=\"ltx_text ltx_font_medium\" id=\"S4.F4.2.1.1\">\nWe extend the DualPipe schedule to handle the overhead introduced by <span class=\"ltx_text ltx_font_italic\" id=\"S4.F4.2.1.1.1\">m</span>HC. Lengths of each block are illustrative only and do not represent actual duration. (F), (B), (W) refers to forward pass, backward pass, weight gradient computation, respectively.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "774", "text": "(F), (B), (W) refers to forward pass, backward pass, weight gradient computation, respectively. <math alttext=\"\\mathcal{F}^{\\mathrm{A}}\\ \\text{and}\\ \\mathcal{F}^{\\mathrm{M}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.F4.2.1.1.m1\" intent=\":literal\"><semantics><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">ℱ</mi><mi mathvariant=\"normal\">A</mi></msup><mo lspace=\"0.460em\" rspace=\"0em\">​</mo><mtext>and</mtext><mo lspace=\"0.460em\" rspace=\"0em\">​</mo><msup><mi class=\"ltx_font_mathcaligraphic\">ℱ</mi><mi mathvariant=\"normal\">M</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{F}^{\\mathrm{A}}\\ \\text{and}\\ \\mathcal{F}^{\\mathrm{M}}</annotation></semantics></math> represents kernels corresponded to Attention and MLP, respectively.</span></span></figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S4.SS3.SSS3.p2\">\n<p class=\"ltx_p\" id=\"S4.SS3.SSS3.p2.2\">Notably, to prevent blocking the communication stream, we execute the <math alttext=\"\\mathcal{F}_{\\mathrm{post,res}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS3.p2.1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">ℱ</mi><mrow><mi>post</mi><mo>,</mo><mi>res</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{F}_{\\mathrm{post,res}}</annotation></semantics></math> kernels of MLP (i.e.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "775", "text": "FFN) layers on a dedicated high-priority compute stream. We further refrain from employing persistent kernels for long-running operations in attention layers, thereby preventing extended stalls. This design enables the preemption of overlapped attention computations, allowing for flexible scheduling while maintaining high utilization of the compute device’s processing units.\nFurthermore, the recomputation process is decoupled from pipeline communication dependencies, as the initial activation of each stage <math alttext=\"\\mathbf{x}_{l_{0}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS3.p2.2.m2\" intent=\":literal\"><semantics><msub><mi>𝐱</mi><msub><mi>l</mi><mn>0</mn></msub></msub><annotation encoding=\"application/x-tex\">\\mathbf{x}_{l_{0}}</annotation></semantics></math> is already cached locally.</p>\n</div>\n</section>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S5\" lang=\"en\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">5 </span>Experiments</h2>\n<section class=\"ltx_subsection\" id=\"S5.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">5.1 </span>Experimental Setup</h3>\n<div class=\"ltx_para\" id=\"S5.SS1.p1\">\n<p class=\"ltx_p\" id=\"S5.SS1.p1.1\">We validate the proposed method via language model pre-training, conducting a comparative analysis between the baseline, HC, and our proposed <span class=\"ltx_text ltx_font_italic\" id=\"S5.SS1.p1.1.1\">m</span>HC.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "776", "text": "Utilizing MoE architectures inspired by DeepSeek-V3 <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2024deepseek_v3</span>)</cite>, we train four distinct model variants to cover different evaluation regimes.\nSpecifically, the expansion rate <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> for both HC and <span class=\"ltx_text ltx_font_italic\" id=\"S5.SS1.p1.1.2\">m</span>HC is set to 4.\nOur primary focus is a 27B model trained with a dataset size proportional to its parameters, which serves as the subject for our system-level main results.\nExpanding on this, we analyze the compute scaling behavior by incorporating smaller 3B and 9B models trained with proportional data, which allows us to observe performance trends across varying compute.\nAdditionally, to specifically investigate the token scaling behavior, we train a separate 3B model on a fixed corpus of 1 trillion tokens.\nDetailed model configurations and training hyper-parameters are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#A1.SS1\" title=\"A.1 Detailed Model Specifications and Hyper-parameters.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "777", "text": "‣ Appendix A Appendix ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S5.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">5.2 </span>Main Results</h3>\n<figure class=\"ltx_figure\" id=\"S5.F5\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"190\" id=\"S5.F5.g1\" src=\"x5.png\" width=\"660\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S5.F5.5.1.1\" style=\"font-size:90%;\">Figure 5</span>: </span><span class=\"ltx_text\" id=\"S5.F5.6.2\" style=\"font-size:90%;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.F5.6.2.1\">Training Stability of Manifold-Constrained Hyper-Connections (<span class=\"ltx_text ltx_font_italic\" id=\"S5.F5.6.2.1.1\">m</span>HC).</span> This figure illustrates (a) the absolute training loss gap of <span class=\"ltx_text ltx_font_italic\" id=\"S5.F5.6.2.2\">m</span>HC and HC relative to the baseline, and (b) the gradient norm of the three methods. All experiments utilize the 27B model. The results demonstrate that <span class=\"ltx_text ltx_font_italic\" id=\"S5.F5.6.2.3\">m</span>HC exhibits improved stability in terms of both loss and gradient norm.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "778", "text": "The results demonstrate that <span class=\"ltx_text ltx_font_italic\" id=\"S5.F5.6.2.3\">m</span>HC exhibits improved stability in terms of both loss and gradient norm.\n</span></figcaption>\n</figure>\n<figure class=\"ltx_table\" id=\"S5.T4\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S5.T4.5.1.1\" style=\"font-size:90%;\">Table 4</span>: </span><span class=\"ltx_text\" id=\"S5.T4.6.2\" style=\"font-size:90%;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.6.2.1\">System-level Benchmark Results for 27B Models.</span> This table compares the zero-shot and few-shot performance of the Baseline, HC, and <span class=\"ltx_text ltx_font_italic\" id=\"S5.T4.6.2.2\">m</span>HC across 8 diverse downstream benchmarks. <span class=\"ltx_text ltx_font_italic\" id=\"S5.T4.6.2.3\">m</span>HC consistently outperforms the Baseline and surpasses HC on the majority of benchmarks, demonstrating its effectiveness in large-scale pre-training.\n</span></figcaption>\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T4.7\">\n<tr class=\"ltx_tr\" id=\"S5.T4.7.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S5.T4.7.1.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.7.1.1.1\" style=\"font-size:80%;\">Benchmark</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" id=\"S5.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "779", "text": "T4.7.1.1.1\" style=\"font-size:80%;\">Benchmark</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" id=\"S5.T4.7.1.2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.1.2.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.1.2.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.1.2.1.1.1\" style=\"font-size:80%;\">BBH</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" id=\"S5.T4.7.1.3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.1.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.1.3.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.1.3.1.1.1\" style=\"font-size:80%;\">DROP</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" id=\"S5.T4.7.1.4\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.1.4.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.1.4.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.1.4.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "780", "text": "T4.7.1.4.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.1.4.1.1.1\" style=\"font-size:80%;\">GSM8K</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" id=\"S5.T4.7.1.5\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.1.5.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.1.5.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.1.5.1.1.1\" style=\"font-size:80%;\">HellaSwag</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" id=\"S5.T4.7.1.6\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.1.6.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.1.6.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.1.6.1.1.1\" style=\"font-size:80%;\">MATH</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" id=\"S5.T4.7.1.7\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.1.7.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "781", "text": "1.7\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.1.7.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.1.7.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.1.7.1.1.1\" style=\"font-size:80%;\">MMLU</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" id=\"S5.T4.7.1.8\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.1.8.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.1.8.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.1.8.1.1.1\" style=\"font-size:80%;\">PIQA</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" id=\"S5.T4.7.1.9\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.1.9.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.1.9.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.1.9.1.1.1\" style=\"font-size:80%;\">TriviaQA</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.7.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "782", "text": "9.1.1.1\" style=\"font-size:80%;\">TriviaQA</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.7.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T4.7.2.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.7.2.1.1\" style=\"font-size:80%;\">(Metric)</span></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T4.7.2.2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.2.2.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.2.2.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.2.2.1.1.1\" style=\"font-size:80%;\">(EM)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T4.7.2.3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.2.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.2.3.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.2.3.1.1.1\" style=\"font-size:80%;\">(F1)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T4.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "783", "text": "3.1.1.1\" style=\"font-size:80%;\">(F1)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T4.7.2.4\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.2.4.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.2.4.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.2.4.1.1.1\" style=\"font-size:80%;\">(EM)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T4.7.2.5\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.2.5.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.2.5.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.2.5.1.1.1\" style=\"font-size:80%;\">(Acc.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "784", "text": "T4.7.2.5.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.2.5.1.1.1\" style=\"font-size:80%;\">(Acc.)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T4.7.2.6\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.2.6.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.2.6.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.2.6.1.1.1\" style=\"font-size:80%;\">(EM)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T4.7.2.7\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.2.7.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.2.7.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.2.7.1.1.1\" style=\"font-size:80%;\">(Acc.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "785", "text": ")</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T4.7.2.8\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.2.8.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.2.8.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.2.8.1.1.1\" style=\"font-size:80%;\">(Acc.)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T4.7.2.9\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.2.9.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.2.9.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.2.9.1.1.1\" style=\"font-size:80%;\">(EM)</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.7.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T4.7.3.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.7.3.1.1\" style=\"font-size:80%;\"># Shots</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T4.7.3.2\" style=\"padding-left:5.0pt;", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "786", "text": "\"># Shots</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T4.7.3.2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.3.2.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.3.2.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.3.2.1.1.1\" style=\"font-size:80%;\">3-shot</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T4.7.3.3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.3.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.3.3.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.3.3.1.1.1\" style=\"font-size:80%;\">3-shot</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T4.7.3.4\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.3.4.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.3.4.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.3.4.1.1.1\" style=\"font-size:80%;", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "787", "text": "T4.7.3.4.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.3.4.1.1.1\" style=\"font-size:80%;\">8-shot</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T4.7.3.5\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.3.5.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.3.5.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.3.5.1.1.1\" style=\"font-size:80%;\">10-shot</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T4.7.3.6\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.3.6.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.3.6.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.3.6.1.1.1\" style=\"font-size:80%;\">4-shot</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T4.7.3.7\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.3.7.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "788", "text": "3.7\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.3.7.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.3.7.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.3.7.1.1.1\" style=\"font-size:80%;\">5-shot</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T4.7.3.8\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.3.8.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.3.8.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.3.8.1.1.1\" style=\"font-size:80%;\">0-shot</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T4.7.3.9\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.3.9.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.3.9.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.3.9.1.1.1\" style=\"font-size:80%;\">5-shot</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.7.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "789", "text": "9.1.1.1\" style=\"font-size:80%;\">5-shot</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.7.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T4.7.4.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.7.4.1.1\" style=\"font-size:80%;\">27B Baseline</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T4.7.4.2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.4.2.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.4.2.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.4.2.1.1.1\" style=\"font-size:80%;\">43.8</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T4.7.4.3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.4.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.4.3.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.4.3.1.1.1\" style=\"font-size:80%;\">47.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "790", "text": "T4.7.4.3.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.4.3.1.1.1\" style=\"font-size:80%;\">47.0</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T4.7.4.4\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.4.4.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.4.4.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.4.4.1.1.1\" style=\"font-size:80%;\">46.7</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T4.7.4.5\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.4.5.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.4.5.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.4.5.1.1.1\" style=\"font-size:80%;\">73.7</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T4.7.4.6\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.4.6.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "791", "text": "4.6\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.4.6.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.4.6.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.4.6.1.1.1\" style=\"font-size:80%;\">22.0</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T4.7.4.7\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.4.7.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.4.7.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.4.7.1.1.1\" style=\"font-size:80%;\">59.0</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T4.7.4.8\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.4.8.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.4.8.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.4.8.1.1.1\" style=\"font-size:80%;\">78.5</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "792", "text": "1.1.1\" style=\"font-size:80%;\">78.5</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T4.7.4.9\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.4.9.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.4.9.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.4.9.1.1.1\" style=\"font-size:80%;\">54.3</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.7.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T4.7.5.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.7.5.1.1\" style=\"font-size:80%;\">27B w/ HC</span></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T4.7.5.2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.5.2.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.5.2.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.5.2.1.1.1\" style=\"font-size:80%;\">48.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "793", "text": "T4.7.5.2.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.5.2.1.1.1\" style=\"font-size:80%;\">48.9</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T4.7.5.3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.5.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.5.3.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.5.3.1.1.1\" style=\"font-size:80%;\">51.6</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T4.7.5.4\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.5.4.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.5.4.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.5.4.1.1.1\" style=\"font-size:80%;\">53.2</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T4.7.5.5\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.5.5.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.5.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "794", "text": "T4.7.5.5.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.5.5.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.5.5.1.1.1\" style=\"font-size:80%;\">74.3</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T4.7.5.6\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.5.6.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.5.6.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.7.5.6.1.1.1\" style=\"font-size:80%;\">26.4</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T4.7.5.7\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.5.7.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.5.7.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.5.7.1.1.1\" style=\"font-size:80%;\">63.0</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T4.7.5.8\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "795", "text": "T4.7.5.8\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.5.8.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.5.8.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.5.8.1.1.1\" style=\"font-size:80%;\">79.9</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T4.7.5.9\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.5.9.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.5.9.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.5.9.1.1.1\" style=\"font-size:80%;\">56.3</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.7.6\" style=\"--ltx-bg-color:#E6F3FD;\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" id=\"S5.T4.7.6.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.7.6.1.1\" style=\"font-size:80%;--ltx-bg-color:#E6F3FD;\">27B w/ <span class=\"ltx_text ltx_font_italic\" id=\"S5.T4.7.6.1.1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "796", "text": "--ltx-bg-color:#E6F3FD;\">27B w/ <span class=\"ltx_text ltx_font_italic\" id=\"S5.T4.7.6.1.1.1\">m</span>HC</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S5.T4.7.6.2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.6.2.1\" style=\"--ltx-bg-color:#E6F3FD;\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.6.2.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.7.6.2.1.1.1\" style=\"font-size:80%;\">51.0</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S5.T4.7.6.3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.6.3.1\" style=\"--ltx-bg-color:#E6F3FD;\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.6.3.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.7.6.3.1.1.1\" style=\"font-size:80%;\">53.9</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S5.T4.7.6.4\" style=\"padding-left:5.0pt;padding-right:5.0pt;", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "797", "text": "T4.7.6.4\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.6.4.1\" style=\"--ltx-bg-color:#E6F3FD;\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.6.4.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.7.6.4.1.1.1\" style=\"font-size:80%;\">53.8</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S5.T4.7.6.5\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.6.5.1\" style=\"--ltx-bg-color:#E6F3FD;\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.6.5.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.7.6.5.1.1.1\" style=\"font-size:80%;\">74.7</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S5.T4.7.6.6\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.6.6.1\" style=\"--ltx-bg-color:#E6F3FD;\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.6.6.1.1\"><span class=\"ltx_text\" id=\"S5.T4.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "798", "text": "\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.6.6.1.1\"><span class=\"ltx_text\" id=\"S5.T4.7.6.6.1.1.1\" style=\"font-size:80%;\">26.0</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S5.T4.7.6.7\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.6.7.1\" style=\"--ltx-bg-color:#E6F3FD;\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.6.7.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.7.6.7.1.1.1\" style=\"font-size:80%;\">63.4</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S5.T4.7.6.8\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.6.8.1\" style=\"--ltx-bg-color:#E6F3FD;\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.6.8.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.7.6.8.1.1.1\" style=\"font-size:80%;\">80.5</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S5.T4.7.6.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "799", "text": "\">80.5</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S5.T4.7.6.9\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.7.6.9.1\" style=\"--ltx-bg-color:#E6F3FD;\">\n<span class=\"ltx_p ltx_align_center\" id=\"S5.T4.7.6.9.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.7.6.9.1.1.1\" style=\"font-size:80%;\">57.6</span></span>\n</span>\n</td>\n</tr>\n</table>\n</figure>\n<div class=\"ltx_para\" id=\"S5.SS2.p1\">\n<p class=\"ltx_p\" id=\"S5.SS2.p1.1\">We begin by examining the training stability and convergence of the 27B models. As illustrated in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S5.F5\" title=\"Figure 5 ‣ 5.2 Main Results ‣ 5 Experiments ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> (a), <span class=\"ltx_text ltx_font_italic\" id=\"S5.SS2.p1.1.1\">m</span>HC  effectively mitigates the training instability observed in HC, achieving a final loss reduction of 0.021 compared to the baseline. This improved stability is further corroborated by the gradient norm analysis in Fig.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "800", "text": "This improved stability is further corroborated by the gradient norm analysis in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S5.F5\" title=\"Figure 5 ‣ 5.2 Main Results ‣ 5 Experiments ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> (b), where <span class=\"ltx_text ltx_font_italic\" id=\"S5.SS2.p1.1.2\">m</span>HC exhibits significantly better behavior than HC, maintaining a stable profile comparable to the baseline.</p>\n</div>\n<div class=\"ltx_para\" id=\"S5.SS2.p2\">\n<p class=\"ltx_p\" id=\"S5.SS2.p2.1\">Tab. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S5.T4\" title=\"Table 4 ‣ 5.2 Main Results ‣ 5 Experiments ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the downstream performance across a diverse set of benchmarks <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mmlu</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gsm8k</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hellaswag</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hendrycks2021measuring</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">piqa</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">joshi-etal-2017-triviaqa</span>)</cite>.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "801", "text": "<span class=\"ltx_text ltx_font_italic\" id=\"S5.SS2.p2.1.1\">m</span>HC yields comprehensive improvements, consistently outperforming the baseline and surpassing HC on the majority of tasks. Notably, compared to HC, <span class=\"ltx_text ltx_font_italic\" id=\"S5.SS2.p2.1.2\">m</span>HC further enhances the model’s reasoning capabilities, delivering performance gains of 2.1% on BBH <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bbh</span>)</cite> and 2.3% on DROP <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">drop</span>)</cite>.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S5.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">5.3 </span>Scaling Experiments</h3>\n<figure class=\"ltx_figure\" id=\"S5.F6\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"159\" id=\"S5.F6.g1\" src=\"x6.png\" width=\"660\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S5.F6.5.1.1\" style=\"font-size:90%;\">Figure 6</span>: </span><span class=\"ltx_text\" id=\"S5.F6.6.2\" style=\"font-size:90%;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.F6.6.2.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "802", "text": "F6.6.2\" style=\"font-size:90%;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.F6.6.2.1\">Scaling properties of <span class=\"ltx_text ltx_font_italic\" id=\"S5.F6.6.2.1.1\">m</span>HC compared to the Baseline.</span>\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.F6.6.2.2\">(a) Compute Scaling Curve.</span>\nSolid lines depict the performance gap across different compute budgets. Each point represents a specific compute-optimal configuration of model size and dataset size, scaling from 3B and 9B to 27B parameters.\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.F6.6.2.3\">(b) Token Scaling Curve.</span>\nTrajectory of the 3B model during training. Each point represents the model’s performance at different training tokens.\nDetailed architectures and training configurations are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#A1.SS1\" title=\"A.1 Detailed Model Specifications and Hyper-parameters. ‣ Appendix A Appendix ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>.\n</span></figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S5.SS3.p1\">\n<p class=\"ltx_p\" id=\"S5.SS3.p1.1\">To assess the scalability of our approach, we report the relative loss improvement of <span class=\"ltx_text ltx_font_italic\" id=\"S5.SS3.p1.1.1\">m</span>HC against the baseline across different scales.\nIn Fig.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "803", "text": "In Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S5.F6\" title=\"Figure 6 ‣ 5.3 Scaling Experiments ‣ 5 Experiments ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> (a), we plot the compute scaling curve spanning 3B, 9B, and 27B parameters.\nThe trajectory indicates that the performance advantage is robustly maintained even at higher computational budgets, showing only marginal attenuation.\nFurthermore, we examine the within-run dynamics in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S5.F6\" title=\"Figure 6 ‣ 5.3 Scaling Experiments ‣ 5 Experiments ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> (b), which presents the token scaling curve for the 3B model.\nCollectively, these findings validate the effectiveness of <span class=\"ltx_text ltx_font_italic\" id=\"S5.SS3.p1.1.2\">m</span>HC in large-scale scenarios. This conclusion is further corroborated by our in-house large-scale training experiments.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S5.SS4\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">5.4 </span>Stability Analysis</h3>\n<figure class=\"ltx_figure\" id=\"S5.F7\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"190\" id=\"S5.F7.g1\" src=\"x7.png\" width=\"660\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S5.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "804", "text": "g1\" src=\"x7.png\" width=\"660\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S5.F7.8.3.1\" style=\"font-size:90%;\">Figure 7</span>: </span><span class=\"ltx_text\" id=\"S5.F7.4.2\" style=\"font-size:90%;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.F7.4.2.1\">Propagation Stability of Manifold-Constrained Hyper-Connections (<span class=\"ltx_text ltx_font_italic\" id=\"S5.F7.4.2.1.1\">m</span>HC).</span> This figure illustrates the propagation dynamics of (a) the single-layer mapping <math alttext=\"\\mathcal{P}_{\\mathcal{M}^{\\mathrm{res}}}(\\mathcal{H}^{\\mathrm{res}}_{l})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.F7.3.1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "805", "text": "F7.3.1.m1\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">𝒫</mi><msup><mi class=\"ltx_font_mathcaligraphic\">ℳ</mi><mi>res</mi></msup></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mi>l</mi><mi>res</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{P}_{\\mathcal{M}^{\\mathrm{res}}}(\\mathcal{H}^{\\mathrm{res}}_{l})</annotation></semantics></math> and (b) the composite mapping <math alttext=\"\\prod_{i=1}^{L-l}\\mathcal{P}_{\\mathcal{M}^{\\mathrm{res}}}(\\mathcal{H}_{L-i}^{\\mathrm{res}})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.F7.4.2.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "806", "text": "F7.4.2.m2\" intent=\":literal\"><semantics><mrow><msubsup><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>L</mi><mo>−</mo><mi>l</mi></mrow></msubsup><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">𝒫</mi><msup><mi class=\"ltx_font_mathcaligraphic\">ℳ</mi><mi>res</mi></msup></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℋ</mi><mrow><mi>L</mi><mo>−</mo><mi>i</mi></mrow><mi>res</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\prod_{i=1}^{L-l}\\mathcal{P}_{\\mathcal{M}^{\\mathrm{res}}}(\\mathcal{H}_{L-i}^{\\mathrm{res}})</annotation></semantics></math> within the 27B model. The results demonstrate that <span class=\"ltx_text ltx_font_italic\" id=\"S5.F7.4.2.2\">m</span>HC significantly enhances propagation stability compared to HC.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "807", "text": "The results demonstrate that <span class=\"ltx_text ltx_font_italic\" id=\"S5.F7.4.2.2\">m</span>HC significantly enhances propagation stability compared to HC.\n</span></figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"S5.F8\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"194\" id=\"S5.F8.g1\" src=\"x8.png\" width=\"660\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S5.F8.4.1.1\" style=\"font-size:90%;\">Figure 8</span>: </span><span class=\"ltx_text\" id=\"S5.F8.5.2\" style=\"font-size:90%;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.F8.5.2.1\">Visualizations of Learnable Mappings.</span> This figure displays representative single-layer and composite mappings for HC (first row) and <span class=\"ltx_text ltx_font_italic\" id=\"S5.F8.5.2.2\">m</span>HC (second row). Each matrix is computed by averaging over all tokens within a selected sequence. The labels annotated along the y-axis and x-axis indicate the forward signal gain (row sum) and the backward gradient gain (column sum), respectively.\n</span></figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S5.SS4.p1\">\n<p class=\"ltx_p\" id=\"S5.SS4.p1.1\">Similar to Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S3.F3\" title=\"Figure 3 ‣ 3.1 Numerical Instability ‣ 3 Preliminary ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, Fig.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "808", "text": "<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S5.F7\" title=\"Figure 7 ‣ 5.4 Stability Analysis ‣ 5 Experiments ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> illustrates the propagation stability of <span class=\"ltx_text ltx_font_italic\" id=\"S5.SS4.p1.1.1\">m</span>HC. Ideally, the single-layer mapping satisfies the doubly stochastic constraint, implying that both the forward signal gain and the backward gradient gain should equal to 1. However, practice implementations utilizing the Sinkhorn-Knopp algorithm must limit the number of iterations to achieve computational efficiency. In our settings, we use 20 iterations to obtain an approximate solution. Consequently, as shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S5.F7\" title=\"Figure 7 ‣ 5.4 Stability Analysis ‣ 5 Experiments ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>(a), the backward gradient gain deviates slightly from 1. In the composite case shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S5.F7\" title=\"Figure 7 ‣ 5.4 Stability Analysis ‣ 5 Experiments ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>(b), the deviation increases but remains bounded, reaching a maximum value of approximately 1.6.\nNotably, compared to the maximum gain magnitude of nearly 3000 in HC, <span class=\"ltx_text ltx_font_italic\" id=\"S5.SS4.p1.1.2\">m</span>HC significantly reduces it by three orders of magnitude.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "809", "text": "These results demonstrate that <span class=\"ltx_text ltx_font_italic\" id=\"S5.SS4.p1.1.3\">m</span>HC significantly enhances propagation stability compared to HC, ensuring stable forward signal and backward gradient flows.\nAdditionally, Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2512.24880v1#S5.F8\" title=\"Figure 8 ‣ 5.4 Stability Analysis ‣ 5 Experiments ‣ mHC: Manifold-Constrained Hyper-Connections\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> displays representative mappings. We observe that for HC, when the maximum gain is large, other values also tend to be significant, which indicates general instability across all propagation paths. In contrast, <span class=\"ltx_text ltx_font_italic\" id=\"S5.SS4.p1.1.4\">m</span>HC consistently yields stable results.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S6\" lang=\"en\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">6 </span>Conclusion and Outlook</h2>\n<div class=\"ltx_para\" id=\"S6.p1\">\n<p class=\"ltx_p\" id=\"S6.p1.1\">In this paper, we identify that while expanding the width of residual stream and diversifying connections yields performance gains as proposed in Hyper-Connections (HC), the unconstrained nature of these connections leads to signal divergence.\nThis disruption compromises the conservation of signal energy across layers, inducing training instability and hindering the scalability of deep networks.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "810", "text": "This disruption compromises the conservation of signal energy across layers, inducing training instability and hindering the scalability of deep networks.\nTo address these challenges, we introduce <span class=\"ltx_text ltx_font_bold\" id=\"S6.p1.1.1\">Manifold-Constrained Hyper-Connections</span> (<span class=\"ltx_text ltx_font_bold ltx_font_italic\" id=\"S6.p1.1.2\">m<span class=\"ltx_text ltx_font_upright\" id=\"S6.p1.1.2.1\">HC</span></span>), a generalized framework that projects the residual connection space onto a specific manifold.\nBy employing the Sinkhorn-Knopp algorithm to enforce a doubly stochastic constraint on residual mappings, <span class=\"ltx_text ltx_font_italic\" id=\"S6.p1.1.3\">m</span>HC transforms signal propagation into a convex combination of features.\nEmpirical results confirm that <span class=\"ltx_text ltx_font_italic\" id=\"S6.p1.1.4\">m</span>HC effectively restores the identity mapping property, enabling stable large-scale training with superior scalability compared to conventional HC.\nCrucially, through efficient infrastructure-level optimizations, <span class=\"ltx_text ltx_font_italic\" id=\"S6.p1.1.5\">m</span>HC delivers these improvements with negligible computational overhead.</p>\n</div>\n<div class=\"ltx_para\" id=\"S6.p2\">\n<p class=\"ltx_p\" id=\"S6.p2.1\">As a generalized extension of the HC paradigm, <span class=\"ltx_text ltx_font_italic\" id=\"S6.p2.1.1\">m</span>HC opens several promising avenues for future research.\nAlthough this work utilizes doubly stochastic matrices to ensure stability, the framework accommodates the exploration of diverse manifold constraints tailored to specific learning objectives.\nWe anticipate that further investigation into distinct geometric constraints could yield novel methods that better optimize the trade-off between plasticity and stability.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "811", "text": "We anticipate that further investigation into distinct geometric constraints could yield novel methods that better optimize the trade-off between plasticity and stability.\nFurthermore, we hope <span class=\"ltx_text ltx_font_italic\" id=\"S6.p2.1.2\">m</span>HC rejuvenates community interest in macro-architecture design.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "812", "text": "Furthermore, we hope <span class=\"ltx_text ltx_font_italic\" id=\"S6.p2.1.2\">m</span>HC rejuvenates community interest in macro-architecture design. By deepening the understanding of how topological structures influence optimization and representation learning, <span class=\"ltx_text ltx_font_italic\" id=\"S6.p2.1.3\">m</span>HC will help address current limitations and potentially illuminate new pathways for the evolution of next-generation foundational architectures.</p>\n</div>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n</section>\n<section class=\"ltx_appendix\" id=\"A1\" lang=\"en\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix A </span>Appendix</h2>\n<section class=\"ltx_subsection\" id=\"A1.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">A.1 </span>Detailed Model Specifications and Hyper-parameters.</h3>\n<figure class=\"ltx_table\" id=\"A1.T5\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"A1.T5.14.1.1\" style=\"font-size:90%;\">Table 5</span>: </span><span class=\"ltx_text\" id=\"A1.T5.15.2\" style=\"font-size:90%;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"A1.T5.15.2.1\">Detailed Model Specifications and Hyper-parameters.</span> This table presents the architectural configurations for the 3B, 9B, and 27B models based on the DeepSeek-V3 <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2024deepseek_v3</span>)</cite> architecture.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "813", "text": "It outlines the specific hyper-parameters for <span class=\"ltx_text ltx_font_italic\" id=\"A1.T5.15.2.2\">m</span>HC and HC, including the residual stream expansion and Sinkhorn-Knopp settings, alongside the optimization and training protocols used in the experiments.\n</span></figcaption>\n<table class=\"ltx_tabular ltx_align_middle\" id=\"A1.T5.10\">\n<tr class=\"ltx_tr\" id=\"A1.T5.10.11\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"A1.T5.10.11.1\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T5.10.11.1.1\">Attribute</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" id=\"A1.T5.10.11.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.11.2.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.11.2.1.1\">3B</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" id=\"A1.T5.10.11.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.11.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.11.3.1.1\">9B</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_tt\" id=\"A1.T5.10.11.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.11.4.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "814", "text": "4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.11.4.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.11.4.1.1\">27B</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" id=\"A1.T5.10.11.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.11.5.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.11.5.1.1\">3B</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.10.12\">\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.12.1\"></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.12.2\"></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\" id=\"A1.T5.10.12.3\"></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.12.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.12.4.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.12.4.1.1\">1T Tokens</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.10.13\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T5.10.13.1\">Vocab Params</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"A1.T5.10.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "815", "text": "T5.10.13.1\">Vocab Params</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"A1.T5.10.13.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.13.2.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.13.2.1.1\">331M</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"A1.T5.10.13.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.13.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.13.3.1.1\">496M</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A1.T5.10.13.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.13.4.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.13.4.1.1\">662M</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"A1.T5.10.13.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.13.5.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.13.5.1.1\">331M</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.10.14\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.10.14.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "816", "text": "T5.10.14\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.10.14.1\">Active Params</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.14.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.14.2.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.14.2.1.1\">612M</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.14.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.14.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.14.3.1.1\">1.66B</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\" id=\"A1.T5.10.14.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.14.4.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.14.4.1.1\">4.14B</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.14.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.14.5.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.14.5.1.1\">612M</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.10.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "817", "text": "T5.10.14.5.1.1\">612M</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.10.15\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.10.15.1\">Total Params</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.15.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.15.2.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.15.2.1.1\">2.97B</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.15.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.15.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.15.3.1.1\">9.18B</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\" id=\"A1.T5.10.15.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.15.4.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.15.4.1.1\">27.0B</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.15.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.15.5.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.15.5.1.1\">2.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "818", "text": "T5.10.15.5.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.15.5.1.1\">2.97B</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.10.16\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T5.10.16.1\">Layers</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"A1.T5.10.16.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.16.2.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.16.2.1.1\">12</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"A1.T5.10.16.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.16.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.16.3.1.1\">18</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A1.T5.10.16.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.16.4.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.16.4.1.1\">30</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"A1.T5.10.16.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "819", "text": "T5.10.16.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.16.5.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.16.5.1.1\">12</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.10.17\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.10.17.1\">Leading Dense Layers</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"3\" id=\"A1.T5.10.17.2\">1</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.17.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.17.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.17.3.1.1\">1</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.10.18\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.10.18.1\">Routed Experts</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.18.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.18.2.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.18.2.1.1\">64</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.18.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "820", "text": "T5.10.18.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.18.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.18.3.1.1\">64</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\" id=\"A1.T5.10.18.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.18.4.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.18.4.1.1\">72</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.18.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.18.5.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.18.5.1.1\">64</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.10.19\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.10.19.1\">Active Experts</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"3\" id=\"A1.T5.10.19.2\">6</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.19.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.19.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.19.3.1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "821", "text": "T5.10.19.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.19.3.1.1\">6</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.10.20\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.10.20.1\">Shared Experts</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"3\" id=\"A1.T5.10.20.2\">2</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.20.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.20.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.20.3.1.1\">2</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.10.21\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.10.21.1\">Dimension</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.21.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.21.2.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.21.2.1.1\">1280</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.21.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.21.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "822", "text": "T5.10.21.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.21.3.1.1\">1920</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\" id=\"A1.T5.10.21.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.21.4.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.21.4.1.1\">2560</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.21.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.21.5.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.21.5.1.1\">1280</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.10.22\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.10.22.1\">FFN Dimension</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.22.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.22.2.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.22.2.1.1\">896</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.22.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.22.3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "823", "text": "T5.10.22.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.22.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.22.3.1.1\">1280</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\" id=\"A1.T5.10.22.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.22.4.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.22.4.1.1\">1536</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.22.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.22.5.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.22.5.1.1\">896</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.10.23\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.10.23.1\">Load Balancing Method</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"3\" id=\"A1.T5.10.23.2\">Loss-Free <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024auxiliary</span>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.23.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.23.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "824", "text": "T5.10.23.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.23.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.23.3.1.1\">Loss-Free</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.10.24\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.10.24.1\">Attention Heads</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.24.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.24.2.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.24.2.1.1\">16</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.24.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.24.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.24.3.1.1\">24</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\" id=\"A1.T5.10.24.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.24.4.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.24.4.1.1\">32</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.24.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "825", "text": "T5.10.24.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.24.5.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.24.5.1.1\">16</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.10.25\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.10.25.1\">Attention Dimension</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"3\" id=\"A1.T5.10.25.2\">128</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.25.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.25.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.25.3.1.1\">128</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.10.26\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.10.26.1\">Attention Variant</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"3\" id=\"A1.T5.10.26.2\">MLA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2024deepseek</span>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.26.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.26.3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "826", "text": "T5.10.26.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.26.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.26.3.1.1\">MLA</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.10.27\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.10.27.1\">KV Rank</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"3\" id=\"A1.T5.10.27.2\">512</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.27.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.27.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.27.3.1.1\">512</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.10.28\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.10.28.1\">Position Embedding</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"3\" id=\"A1.T5.10.28.2\">RoPE <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">su2024roformer</span>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.28.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.28.3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "827", "text": "T5.10.28.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.28.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.28.3.1.1\">RoPE</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.10.29\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.10.29.1\">RoPE Dimension</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"3\" id=\"A1.T5.10.29.2\">64</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.29.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.29.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.29.3.1.1\">64</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.1.1.1\">RoPE <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.1.1.1.m1\" intent=\":literal\"><semantics><mi>θ</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"3\" id=\"A1.T5.1.1.2\">10000</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.1.1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "828", "text": "T5.1.1.2\">10000</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.1.1.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.1.1.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.1.1.3.1.1\">10000</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.10.30\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.10.30.1\">Layer Norm Type</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"3\" id=\"A1.T5.10.30.2\">RMSNorm <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2019root</span>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.30.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.30.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.30.3.1.1\">RMSNorm</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.2.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.2.2.1\">Layer Norm <math alttext=\"\\varepsilon\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.2.2.1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "829", "text": "T5.2.2.1\">Layer Norm <math alttext=\"\\varepsilon\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.2.2.1.m1\" intent=\":literal\"><semantics><mi>ε</mi><annotation encoding=\"application/x-tex\">\\varepsilon</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"3\" id=\"A1.T5.2.2.2\">1e-20</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.2.2.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.2.2.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.2.2.3.1.1\">1e-20</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.3.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T5.3.3.1\">\n<span class=\"ltx_text ltx_font_italic\" id=\"A1.T5.3.3.1.1\">m</span>HC/HC Expansion Rate <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.3.3.1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\" id=\"A1.T5.3.3.2\">4</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"A1.T5.3.3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "830", "text": "T5.3.3.2\">4</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"A1.T5.3.3.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.3.3.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.3.3.3.1.1\">4</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.4.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.4.4.1\">\n<span class=\"ltx_text ltx_font_italic\" id=\"A1.T5.4.4.1.1\">m</span>HC/HC Gating Factor Init <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.4.4.1.m1\" intent=\":literal\"><semantics><mi>α</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"3\" id=\"A1.T5.4.4.2\">0.01</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.4.4.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.4.4.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.4.4.3.1.1\">0.01</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.5.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.5.5.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "831", "text": "T5.5.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.5.5.1\">\n<span class=\"ltx_text ltx_font_italic\" id=\"A1.T5.5.5.1.1\">m</span>HC Sinkhorn-Knopp <math alttext=\"t_{\\text{max}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.5.5.1.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mtext>max</mtext></msub><annotation encoding=\"application/x-tex\">t_{\\text{max}}</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"3\" id=\"A1.T5.5.5.2\">20</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.5.5.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.5.5.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.5.5.3.1.1\">20</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.10.31\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T5.10.31.1\">Sequence Length</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\" id=\"A1.T5.10.31.2\">4096</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"A1.T5.10.31.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.31.3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "832", "text": "T5.10.31.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.31.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.31.3.1.1\">4096</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.10.32\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.10.32.1\">Vocab Size</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"3\" id=\"A1.T5.10.32.2\">129280</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.32.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.32.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.32.3.1.1\">129280</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.10.33\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.10.33.1\">Batch Size</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.33.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.33.2.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.33.2.1.1\">320</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.33.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "833", "text": "T5.10.33.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.33.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.33.3.1.1\">512</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\" id=\"A1.T5.10.33.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.33.4.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.33.4.1.1\">1280</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.33.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.33.5.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.33.5.1.1\">2560</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.10.34\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.10.34.1\">Training Steps</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.34.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.34.2.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.34.2.1.1\">30000</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.34.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "834", "text": "T5.10.34.2.1.1\">30000</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.34.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.34.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.34.3.1.1\">50000</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\" id=\"A1.T5.10.34.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.34.4.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.34.4.1.1\">50000</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.34.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.34.5.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.34.5.1.1\">100000</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.10.35\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.10.35.1\">Training Tokens</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.35.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.35.2.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.35.2.1.1\">39.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "835", "text": "T5.10.35.2.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.35.2.1.1\">39.3B</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.35.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.35.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.35.3.1.1\">105B</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\" id=\"A1.T5.10.35.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.35.4.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.35.4.1.1\">262B</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.35.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.35.5.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.35.5.1.1\">1.05T</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.10.36\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.10.36.1\">Warmup Steps</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"3\" id=\"A1.T5.10.36.2\">2000</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.36.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "836", "text": "T5.10.36.2\">2000</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.36.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.36.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.36.3.1.1\">2000</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.10.37\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.10.37.1\">Optimizer</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"3\" id=\"A1.T5.10.37.2\">AdamW <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">loshchilov2017decoupled</span>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.37.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.37.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.37.3.1.1\">AdamW</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.10.38\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.10.38.1\">AdamW Betas</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"3\" id=\"A1.T5.10.38.2\">(0.9, 0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "837", "text": "1\">AdamW Betas</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"3\" id=\"A1.T5.10.38.2\">(0.9, 0.95)</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.38.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.38.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.38.3.1.1\">(0.9, 0.95)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.6.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.6.6.1\">AdamW <math alttext=\"\\varepsilon\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.6.6.1.m1\" intent=\":literal\"><semantics><mi>ε</mi><annotation encoding=\"application/x-tex\">\\varepsilon</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"3\" id=\"A1.T5.6.6.2\">1e-20</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.6.6.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.6.6.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.6.6.3.1.1\">1e-20</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.10.39\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "838", "text": "T5.10.39\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.10.39.1\">Base Learning Rate</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.39.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.39.2.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.39.2.1.1\">8.6e-4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.39.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.39.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.39.3.1.1\">5.9e-4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\" id=\"A1.T5.10.39.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.39.4.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.39.4.1.1\">4.0e-4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.39.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.39.5.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.39.5.1.1\">9.0e-4</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.10.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "839", "text": "T5.10.39.5.1.1\">9.0e-4</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.10.40\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.10.40.1\">Lr Scheduler</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"3\" id=\"A1.T5.10.40.2\">Step</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.40.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.40.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.40.3.1.1\">Step</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.10.10\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.10.10.5\">Lr Decay Step Ratio</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"3\" id=\"A1.T5.8.8.2\">[0.8 <math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.7.7.1.m1\" intent=\":literal\"><semantics><mo>×</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math>, 0.9 <math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.8.8.2.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "840", "text": "0.9 <math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.8.8.2.m2\" intent=\":literal\"><semantics><mo>×</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math>]</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.10.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.10.4.2\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.10.4.2.2\">[0.8 <math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.9.9.3.1.1.m1\" intent=\":literal\"><semantics><mo>×</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math>, 0.9 <math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.10.10.4.2.2.m2\" intent=\":literal\"><semantics><mo>×</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math>]</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.10.41\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.10.41.1\">Lr Decay Rate</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"3\" id=\"A1.T5.10.41.2\">[0.316, 0.1]</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.41.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "841", "text": "1]</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.10.41.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.41.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.41.3.1.1\">[0.316, 0.1]</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.10.42\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" id=\"A1.T5.10.42.1\">Weight Decay</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" colspan=\"3\" id=\"A1.T5.10.42.2\">0.1</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"A1.T5.10.42.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T5.10.42.3.1\">\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T5.10.42.3.1.1\">0.1</span>\n</span>\n</td>\n</tr>\n</table>\n</figure>\n</section>\n</section>\n</article>\n</div>\n<footer class=\"ltx_page_footer\">\n<div class=\"ltx_page_logo\">Generated  on Thu Jan  1 14:41:30 2026 by <a class=\"ltx_LaTeXML_logo\" href=\"http://dlmf.nist.gov/LaTeXML/\"><span style=\"letter-spacing:-0.2em; margin-right:0.1em;\">L<span class=\"ltx_font_smallcaps\" style=\"position:relative; bottom:2.2pt;\">a</span>T<span class=\"ltx_font_smallcaps\" style=\"font-size:120%;position:relative; bottom:-0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "842", "text": "bottom:2.2pt;\">a</span>T<span class=\"ltx_font_smallcaps\" style=\"font-size:120%;position:relative; bottom:-0.2ex;\">e</span></span><span style=\"font-size:90%; position:relative; bottom:-0.2ex;\">XML</span><img alt=\"Mascot Sammy\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAX", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "843", "text": "75arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VA", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "844", "text": "gIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==\"/></a>\n</div></footer>\n</div>\n</body>\n</html>", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.html", "file_name": "2512.24880v1.html", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "845", "text": "# Multiscale Aggregated Hierarchical Attention (MAHA)\r\n\r\n<div align=\"center\">\r\n\r\n![License](https://github.com/canererden/MAHA-Project/releases)\r\n![Python](https://github.com/canererden/MAHA-Project/releases%2B-green)\r\n![PyTorch](https://github.com/canererden/MAHA-Project/releases%2B-orange)\r\n[!", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/README.md", "file_name": "README.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "846", "text": "[License](https://github.com/canererden/MAHA-Project/releases)\r\n![Python](https://github.com/canererden/MAHA-Project/releases%2B-green)\r\n![PyTorch](https://github.com/canererden/MAHA-Project/releases%2B-orange)\r\n[![arXiv](https://github.com/canererden/MAHA-Project/releases)](https://github.com/canererden/MAHA-Project/releases)\r\n\r\n**A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models**\r\n\r\n[**Read the Paper**](https://github.com/canererden/MAHA-Project/releases)\r\n\r\n</div>\r\n\r\n---\r\n\r\n## Abstract\r\n\r\nWe propose **MAHA**, a novel attention mechanism that reformulates multi-head self-attention through **hierarchical multiscale decomposition** and mathematically rigorous aggregation (**Convex Optimization** & **Nash Equilibrium**).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/README.md", "file_name": "README.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "847", "text": "Standard attention mechanisms suffer from quadratic complexity $O(N^2)$. MAHA addresses this by dynamically partitioning the sequence into hierarchical scales and aggregating them using optimization solvers. The result is a framework that achieves **sub-quadratic complexity** and superior long-range dependency modeling compared to standard Transformers, specifically optimized for high-throughput inference.\r\n\r\n## Architecture\r\n\r\nMAHA replaces the standard Multi-Head Attention layer with a hierarchical processing block.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/README.md", "file_name": "README.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "848", "text": "```mermaid\r\ngraph TD;\r\n    Input[Input Sequence X] --> Decomp[Hierarchical Decomposition];\r\n    Decomp -->|Scale 0| Attn0[Attention S0];\r\n    Decomp -->|Scale 1| Attn1[Attention S1];\r\n    Decomp -->|Scale 2| Attn2[Attention S2];\r\n    Attn0 --> Upsample[Upsampling];\r\n    Attn1 --> Upsample;\r\n    Attn2 --> Upsample;\r\n    Upsample --> Agg{Optimization Aggregator};\r\n    Agg -->|Convex / Nash| Output[Aggregated Context];\r\n    \r\n    style Agg fill:#f9f,stroke:#333,stroke-width:2px\r\n    style Decomp fill:#bbf,stroke:#333,stroke-width:2px\r\n\r\n```\r\n\r\n## Key Features\r\n\r\n**Hierarchical Decomposition:** Uses learnable Strided Convolutions to create multiscale representations (scales l=1..L), reducing effective sequence length geometrically.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/README.md", "file_name": "README.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "849", "text": "**Shared Value Projection:** Decouples Query/Key projections while sharing Value projections across scales, significantly reducing parameter count.\r\n** Optimization-Driven Aggregation:**\r\n**`convex` strategy:** Solves a constrained L1-regularized optimization problem to weigh scales.\r\n**`nash` strategy:** Simulates a non-cooperative game where scales compete to minimize reconstruction error (Best-Response Dynamics).\r\n\r\n\r\n**Hybrid Design:** Integrates Dilated Convolutional blocks for local feature extraction prior to attention.\r\n\r\n## Performance\r\n\r\nMAHA demonstrates superior efficiency on long-sequence tasks (e.g., PG-19) compared to standard baselines.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/README.md", "file_name": "README.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "850", "text": "| Model | Complexity | PG-19 (PPL) $\\downarrow$ | Memory Usage $\\downarrow$ |\r\n| --- | --- | --- | --- |\r\n| Standard Transformer | O(N^2) | 24.3 | 15.2 GB |\r\n| Longformer | O(N) | 23.8 | 9.1 GB |\r\n| **MAHA (Ours)** | **Sub-Quadratic** | **23.1** | **6.7 GB** |\r\n\r\n## Installation\r\n```bash\r\n# Clone the repository\r\ngit clone [https://github.com/canererden/MAHA-Project/releases](https://github.com/canererden/MAHA-Project/releases)\r\ncd MAHA-Project\r\n\r\n# Install dependencies\r\npip install -r https://github.com/canererden/MAHA-Project/releases\r\n\r\n```\r\n\r\n*Note: For the Convex Optimization solver, `cvxpylayers` is required.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/README.md", "file_name": "README.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "851", "text": "*\r\n\r\n## Usage\r\n\r\n### Quick Start\r\nYou can use `MAHABlock` as a drop-in replacement for standard attention layers or use the full `MAHATransformer` model.\r\n\r\n```python\r\nimport torch\r\nfrom https://github.com/canererden/MAHA-Project/releases import MAHATransformer\r\n\r\n# Initialize Model with Convex Aggregation\r\nmodel = MAHATransformer(\r\n    vocab_size=30000,\r\n    max_len=4096,        # Long context support\r\n    d_model=768,\r\n    num_heads=12,\r\n    num_scales=4,        # L=4 scales (e.g., 4096, 2048, 1024, 512)\r\n    aggregation_strategy='convex' # or 'nash'\r\n)\r\n\r\n# Move to GPU\r\ndevice = https://github.com/canererden/MAHA-Project/releases(\"cuda\" if https://github.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/README.md", "file_name": "README.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "852", "text": "max_len=4096,        # Long context support\r\n    d_model=768,\r\n    num_heads=12,\r\n    num_scales=4,        # L=4 scales (e.g., 4096, 2048, 1024, 512)\r\n    aggregation_strategy='convex' # or 'nash'\r\n)\r\n\r\n# Move to GPU\r\ndevice = https://github.com/canererden/MAHA-Project/releases(\"cuda\" if https://github.com/canererden/MAHA-Project/releases() else \"cpu\")\r\nhttps://github.com/canererden/MAHA-Project/releases(device)\r\n\r\n# Forward Pass\r\ndummy_input = https://github.com/canererden/MAHA-Project/releases(0, 30000, (1, 4096)).to(device)\r\noutput, aux_loss = model(dummy_input)\r\n\r\nprint(f\"Output Shape: {https://github.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/README.md", "file_name": "README.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "853", "text": "com/canererden/MAHA-Project/releases(\"cuda\" if https://github.com/canererden/MAHA-Project/releases() else \"cpu\")\r\nhttps://github.com/canererden/MAHA-Project/releases(device)\r\n\r\n# Forward Pass\r\ndummy_input = https://github.com/canererden/MAHA-Project/releases(0, 30000, (1, 4096)).to(device)\r\noutput, aux_loss = model(dummy_input)\r\n\r\nprint(f\"Output Shape: {https://github.com/canererden/MAHA-Project/releases}\")  # (1, 4096, 768)\r\n\r\n```\r\n\r\n### Running Experiments\r\n\r\nTo replicate the training runs from the paper:\r\n\r\n```bash\r\n# Train on synthetic data or configured dataset\r\npython https://github.com/canererden/MAHA-Project/releases --config https://github.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/README.md", "file_name": "README.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "854", "text": "com/canererden/MAHA-Project/releases(0, 30000, (1, 4096)).to(device)\r\noutput, aux_loss = model(dummy_input)\r\n\r\nprint(f\"Output Shape: {https://github.com/canererden/MAHA-Project/releases}\")  # (1, 4096, 768)\r\n\r\n```\r\n\r\n### Running Experiments\r\n\r\nTo replicate the training runs from the paper:\r\n\r\n```bash\r\n# Train on synthetic data or configured dataset\r\npython https://github.com/canererden/MAHA-Project/releases --config https://github.com/canererden/MAHA-Project/releases\r\n\r\n# Run Unit Tests\r\npython -m unittest discover tests/\r\n\r\n```\r\n\r\n## Directory Structure\r\n```text\r\nmaha-project/\r\n├── configs/             # Hyperparameter configurations (YAML)\r\n├── src/\r\n│   ├── layers/          # Core MAHA layers (Decomposition, Attention,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/README.md", "file_name": "README.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "855", "text": "768)\r\n\r\n```\r\n\r\n### Running Experiments\r\n\r\nTo replicate the training runs from the paper:\r\n\r\n```bash\r\n# Train on synthetic data or configured dataset\r\npython https://github.com/canererden/MAHA-Project/releases --config https://github.com/canererden/MAHA-Project/releases\r\n\r\n# Run Unit Tests\r\npython -m unittest discover tests/\r\n\r\n```\r\n\r\n## Directory Structure\r\n```text\r\nmaha-project/\r\n├── configs/             # Hyperparameter configurations (YAML)\r\n├── src/\r\n│   ├── layers/          # Core MAHA layers (Decomposition, Attention, Aggregation)\r\n│   ├── models/          # MAHABlock and Transformer architecture\r\n│   ├── optimization/    # Differentiable solvers (Convex & Game Theory)\r\n│   └── utils/           # Metrics and helpers\r\n├── tests/               # Unit tests for tensor shapes and gradients\r\n├── https://github.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/README.md", "file_name": "README.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "856", "text": "Attention, Aggregation)\r\n│   ├── models/          # MAHABlock and Transformer architecture\r\n│   ├── optimization/    # Differentiable solvers (Convex & Game Theory)\r\n│   └── utils/           # Metrics and helpers\r\n├── tests/               # Unit tests for tensor shapes and gradients\r\n├── https://github.com/canererden/MAHA-Project/releases             # Main training loop\r\n└── https://github.com/canererden/MAHA-Project/releases     # Dependencies\r\n\r\n```\r\n\r\n# Citation\r\nIf you use this code or our results in your research, please cite our work using the persistent **Zenodo DOI**:\r\n\r\n```bibtex\r\n@article{erden2025maha,\r\n  title={Multiscale Aggregated Hierarchical Attention (MAHA): A Game Theoretic and Optimization Driven Approach to Efficient Contextual Modeling in Large Language Models},\r\n  author={Erden, Caner},", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/README.md", "file_name": "README.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "857", "text": "com/canererden/MAHA-Project/releases     # Dependencies\r\n\r\n```\r\n\r\n# Citation\r\nIf you use this code or our results in your research, please cite our work using the persistent **Zenodo DOI**:\r\n\r\n```bibtex\r\n@article{erden2025maha,\r\n  title={Multiscale Aggregated Hierarchical Attention (MAHA): A Game Theoretic and Optimization Driven Approach to Efficient Contextual Modeling in Large Language Models},\r\n  author={Erden, Caner},\r\n  journal={arXiv preprint arXiv:2512.14925},\r\n  year={2025},\r\n  url={https://github.com/canererden/MAHA-Project/releases}\r\n}\r\n\r\n```\r\n\r\n## License\r\nThis project is licensed under the MIT License - see the [LICENSE](https://github.com/canererden/MAHA-Project/releases) file for details.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/README.md", "file_name": "README.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "858", "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nsns.set_context(\"paper\", font_scale=1.4)\nsns.set_style(\"whitegrid\", {'grid.linestyle': '--'})\nplt.rcParams['font.family'] = 'serif'\nplt.rcParams['font.serif'] = ['Times New Roman', 'DejaVu Serif', 'serif']\nplt.rcParams['axes.linewidth'] = 1.5\n\ndef generate_table2():\n    print(\"\\n📊 Tablo 2 (Aggregation Impact) Oluşturuluyor...\")\n    \n    data = {\n        \"Method\": [\"Convex Optimization (CO)\", \"Nash Equilibrium (NE)\", \"Mean Aggregation\"],\n        \"MNLI (Acc)\": [86.0, 85.8, 85.2],\n        \"Memory (GB)\": [6.7, 6.9, 7.2],\n        \"Training Speed\": [\"1.0x (Baseline)\", \"0.9x (Slower)\", \"1.1x (Faster)\"]\n    }\n    \n    df = pd.DataFrame(data)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"TABLE 2: AGGREGATION METHOD IMPACT\")\n    print(\"=\"*60)\n    print(df.to_markdown(index=False))\n    \n    print(\"\\n[LaTeX Format]\")\n    print(df.to_latex(index=False, caption=\"Aggregation Method Impact\", label=\"tab:ablation_agg\"))\n    \n    df.to_csv(\"table2_ablation.csv\", index=False)", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/analysis.ipynb", "file_name": "analysis.ipynb", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "859", "text": "def plot_figure4_scales():\n    print(\"📊 Figure 4: Accuracy vs Number of Scales çiziliyor...\")\n    \n    scales = [2, 3, 4, 5, 6]\n    accuracies = [84.5, 85.4, 86.0, 85.3, 84.8] \n    \n    fig, ax = plt.subplots(figsize=(8, 5), dpi=300)\n    \n    # Çizgi ve Noktalar\n    ax.plot(scales, accuracies, marker='o', markersize=10, linewidth=3, color='#1f77b4', linestyle='-')\n    \n    # Zirve Noktası (L=4) Vurgusu\n    ax.annotate(f'Peak Accuracy\\n(L=4, Acc=86.0)', \n                xy=(4, 86.0), xytext=(4, 86.5),\n                arrowprops=dict(facecolor='black', shrink=0.05),\n                ha='center', fontsize=12, fontweight='bold')\n\n    # Düşüş Açıklamaları\n    ax.text(2.1, 84.6, \"Loss of\\nGranularity\", fontsize=10, color='red', ha='left')\n    ax.text(5.9, 84.9, \"Noise from\\nDownsampling\", fontsize=10, color='red', ha='right')", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/analysis.ipynb", "file_name": "analysis.ipynb", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "860", "text": "# Eksenler\n    ax.set_xlabel('Number of Scales ($L$)', fontsize=14)\n    ax.set_ylabel('MNLI Accuracy (%)', fontsize=14)\n    #ax.set_title('Figure 4. Accuracy vs Number of Scales', y=-0.2, fontstyle='italic')\n    \n    ax.set_xticks(scales)\n    ax.set_ylim(84.0, 87.0)\n    \n    plt.tight_layout()\n    plt.savefig(\"Figure4_Scales.png\", dpi=300, bbox_inches='tight')\n    plt.savefig(\"Figure4_Scales.pdf\", format='pdf', bbox_inches='tight')\n    print(\"✅ Figure 4 kaydedildi: Figure4_Scales.png\")\n\ndef analyze_downsampling_c():\n    \"\"\"Part C: Downsampling Operator Choice\"\"\"\n    print(\"\\n📊 Part C: Downsampling Operator Choice...\")\n    # Metindeki \"1.2% accuracy difference\" verisi\n    diff = 1.2\n    print(f\"-> Convolutional downsampling outperforms pooling by {diff}% on average.\")\n    print(\"-> Reason: Preserves positional information (local connectivity).\")\n\nif __name__ == \"__main__\":\n    generate_table2()\n    plot_figure4_scales()\n    analyze_downsampling_c()\n\n#", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/analysis.ipynb", "file_name": "analysis.ipynb", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "861", "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# --- 1. Akademik Grafik Ayarları ---\nsns.set_context(\"paper\", font_scale=1.5)\nplt.rcParams['font.family'] = 'serif'\nplt.rcParams['font.serif'] = ['Times New Roman', 'DejaVu Serif', 'serif']\n\ndef generate_attention_patterns():\n    print(\"📊 Figure 5: Multiscale Attention Patterns görselleştiriliyor...\")\n\n    # --- 2. Sentetik Attention Verisi Oluşturma ---\n    \n    seq_len = 20\n    \n    # A. Fine Scale (Local Syntax): Diagonal Focus (Adjective-Noun)\n    # Tokenlerin sadece komşularına (i-1, i+1) baktığı yapı\n    fine_attn = np.eye(seq_len) * 0.5 \n    for i in range(seq_len-1):\n        fine_attn[i, i+1] = 0.25\n        fine_attn[i+1, i] = 0.25\n    # Biraz gürültü ekle\n    fine_attn += np.random.rand(seq_len, seq_len) * 0.05\n    \n    # B. Medium Scale (Clause-Level): Block Focus\n    # Cümleciklerin kendi içine odaklandığı blok yapılar\n    medium_attn = np.zeros((seq_len, seq_len))\n    block_size = 5\n    for i in range(0, seq_len, block_size):\n        medium_attn[i:i+block_size, i:i+block_size] = 0.8\n    medium_attn += np.random.rand(seq_len, seq_len) * 0.1", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/analysis.ipynb", "file_name": "analysis.ipynb", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "862", "text": "# C. Coarse Scale (Document Themes): Global/Vertical Focus\n    coarse_attn = np.random.rand(seq_len, seq_len) * 0.1\n    coarse_attn[:, 5] += 0.8  # Key token 1\n    coarse_attn[:, 15] += 0.8 # Key token 2\n    \n    # --- 3. Çizim ---\n    fig, axes = plt.subplots(1, 3, figsize=(18, 5.5), dpi=300)\n    \n    maps = [\n        (fine_attn, \"Scale 1: Fine-Grained\\n(Local Syntax / Adj-Noun)\"),\n        (medium_attn, \"Scale 2: Medium-Grained\\n(Clause-Level / Local Context)\"),\n        (coarse_attn, \"Scale 3: Coarse-Grained\\n(Document Themes / Global)\")\n    ]\n    \n    for i, (data, title) in enumerate(maps):\n        ax = axes[i]\n        sns.heatmap(data, ax=ax, cmap=\"Blues\", cbar=False, square=True,\n                   xticklabels=False, yticklabels=False)\n        \n        ax.set_title(title, fontsize=14, pad=15, fontweight='bold')\n        ax.set_xlabel(\"Key Position\", fontsize=12)\n        if i == 0:\n            ax.set_ylabel(\"Query Position\", fontsize=12)\n            \n        # Çerçeve ekle\n        for _, spine in ax.spines.items():\n            spine.set_visible(True)\n            spine.set_linewidth(1)\n\n    #plt.suptitle(\"Figure 5. Visualization of Learned Multiscale Attention Patterns\", \n    #             y=0.98, fontsize=16, fontstyle='italic')\n    \n    plt.tight_layout()\n    plt.savefig(\"Figure5_Attention.png\", bbox_inches='tight')\n    plt.savefig(\"Figure5_Attention.pdf\", format='pdf', bbox_inches='tight')\n    print(\"✅ Figure 5 kaydedildi: Figure5_Attention.png\")\n\nif __name__ == \"__main__\":\n    generate_attention_patterns()\n\n# In[ ]:", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/analysis.ipynb", "file_name": "analysis.ipynb", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "863", "text": "# Ablation Settings for Nash Equilibrium\r\naggregation: 'nash'", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/configs/ablation_nash.yaml", "file_name": "ablation_nash.yaml", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "864", "text": "# Default Hyperparameters for MAHA\r\nL: 4\r\nr: 2\r\nd_model: 768", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/configs/default_maha.yaml", "file_name": "default_maha.yaml", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "865", "text": "# Evaluation Script", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/evaluate.py", "file_name": "evaluate.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "866", "text": "torch>=2.0.0\r\nnumpy\r\npandas\r\nscikit-learn\r\ntransformers\r\ncvxpylayers", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/requirements.txt", "file_name": "requirements.txt", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "867", "text": "import torch\r\nimport torch.nn as nn\r\nimport time\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\nimport numpy as np\r\nimport os\r\n\r\n# CUDA Hatalarını Yakalamak için Debug Ortamı\r\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\r\n\r\nfrom src.models.transformer import MAHATransformer\r\n\r\n# Cihaz seçimi\r\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\nprint(f\"🚀 Deneyler şu cihazda çalıştırılıyor: {device}\")\r\n\r\ndef benchmark_efficiency():\r\n    \"\"\"\r\n    DENEY 1: Hesaplama Verimliliği (Computational Efficiency)\r\n    \"\"\"\r\n    print(\"\\n🧪 DENEY 1: Verimlilik Testi (Standard Attention vs MAHA)...\")\r\n    \r\n    # Not: Bellek hatası alırsanız 2048'i çıkarın\r\n    seq_lengths = [128, 256, 512, 1024, 2048] \r\n    d_model = 256\r\n    num_heads = 4\r\n    batch_size = 4  \r\n    \r\n    results = []\r\n\r\n    for seq_len in seq_lengths:\r\n        # --- 1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/run_experiments.py", "file_name": "run_experiments.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "868", "text": "Standard Transformer (Baseline) ---\r\n        try:\r\n            baseline = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads).to(device)\r\n            dummy_input = torch.randn(seq_len, batch_size, d_model).to(device) \r\n            \r\n            # Warmup\r\n            _ = baseline(dummy_input, dummy_input, dummy_input)\r\n            \r\n            # Timing\r\n            torch.cuda.reset_peak_memory_stats()\r\n            start = time.time()\r\n            with torch.no_grad():\r\n                for _ in range(20): # Tekrar sayısı düşürüldü (Hız için)\r\n                    _ = baseline(dummy_input, dummy_input, dummy_input)\r\n            torch.cuda.synchronize()\r\n            end = time.time()\r\n            \r\n            baseline_time = (end - start) / 20\r\n            baseline_mem = torch.cuda.max_memory_allocated() / (1024 ** 2) # MB\r\n            \r\n            results.append({\r\n                \"Model\": \"Standard MHA\",\r\n                \"Seq_Len\": seq_len,\r\n                \"Time (ms)\": baseline_time * 1000,\r\n                \"Memory (MB)\": baseline_mem\r\n            })\r\n            \r\n            del baseline, dummy_input\r\n            torch.cuda.empty_cache()\r\n            \r\n        except Exception as e:\r\n            print(f\"   ⚠️ Hata (Baseline - {seq_len}): {e}\")\r\n\r\n        # --- 2. MAHA Transformer ---\r\n        try:\r\n            # KRİTİK DÜZELTME: max_len parametresi artırıldı\r\n            maha_model = MAHATransformer(\r\n                vocab_size=100, \r\n                d_model=d_model, \r\n                num_heads=num_heads, \r\n                num_scales=4,\r\n                aggregation_strategy='convex',\r\n                max_len=5000  # <--- BURASI DÜZELTİLDİ (seq_len 2048'i kapsıyor)\r\n            ).to(device)\r\n            \r\n            dummy_ids = torch.randint(0, 100, (batch_size, seq_len)).to(device)\r\n            \r\n            # Warmup\r\n            _ = maha_model(dummy_ids)\r\n            \r\n            # Timing\r\n            torch.cuda.reset_peak_memory_stats()\r\n            start = time.time()\r\n            with torch.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/run_experiments.py", "file_name": "run_experiments.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "869", "text": "seq_len)).to(device)\r\n            \r\n            # Warmup\r\n            _ = maha_model(dummy_ids)\r\n            \r\n            # Timing\r\n            torch.cuda.reset_peak_memory_stats()\r\n            start = time.time()\r\n            with torch.no_grad():\r\n                for _ in range(20):\r\n                    _ = maha_model(dummy_ids)\r\n            torch.cuda.synchronize()\r\n            end = time.time()\r\n            \r\n            maha_time = (end - start) / 20\r\n            maha_mem = torch.cuda.max_memory_allocated() / (1024 ** 2)\r\n            \r\n            results.append({\r\n                \"Model\": \"MAHA (Ours)\",\r\n                \"Seq_Len\": seq_len,\r\n                \"Time (ms)\": maha_time * 1000,\r\n                \"Memory (MB)\": maha_mem\r\n            })\r\n            \r\n            del maha_model, dummy_ids\r\n            torch.cuda.empty_cache()\r\n            \r\n        except Exception as e:\r\n            print(f\"   ⚠️ Hata (MAHA - {seq_len}): {e}\")\r\n\r\n        print(f\"   -> Tamamlandı: Seq Len {seq_len}\")\r\n\r\n    return pd.DataFrame(results)\r\n\r\ndef ablation_study():\r\n    \"\"\"\r\n    DENEY 2: Ablation Study (Convex vs Nash)\r\n    \"\"\"\r\n    print(\"\\n🧪 DENEY 2: Ablation Study (Convex vs Nash)...\")\r\n    \r\n    strategies = ['convex', 'nash']\r\n    loss_history = {s: [] for s in strategies}\r\n    \r\n    vocab_size = 500\r\n    seq_len = 64\r\n    d_model = 64\r\n    epochs = 5\r\n    \r\n    train_data = torch.randint(0, vocab_size, (100, seq_len)).to(device)\r\n    targets = torch.randint(0, vocab_size, (100, seq_len)).to(device)\r\n    \r\n    criterion = nn.CrossEntropyLoss()\r\n    \r\n    for strategy in strategies:\r\n        print(f\"   -> Strateji Eğitiliyor: {strategy.upper()}\")\r\n        try:\r\n            model = MAHATransformer(\r\n                vocab_size=vocab_size, \r\n                d_model=d_model,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/run_experiments.py", "file_name": "run_experiments.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "870", "text": "upper()}\")\r\n        try:\r\n            model = MAHATransformer(\r\n                vocab_size=vocab_size, \r\n                d_model=d_model, \r\n                num_heads=4, \r\n                num_scales=3,\r\n                aggregation_strategy=strategy,\r\n                num_layers=1,\r\n                max_len=512 # Yeterli\r\n            ).to(device)\r\n            \r\n            optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\r\n            model.train()\r\n            \r\n            for epoch in range(epochs):\r\n                batch_loss = 0\r\n                for i in range(0, 100, 10):\r\n                    bx = train_data[i:i+10]\r\n                    by = targets[i:i+10]\r\n                    \r\n                    optimizer.zero_grad()\r\n                    out, aux_loss = model(bx)\r\n                    \r\n                    # Projeksiyon (Linear classifier yoksa manuel yap)\r\n                    # MAHATransformer içinde classifier tanımlı varsayıyoruz\r\n                    if hasattr(model, 'classifier'):\r\n                         logits = model.classifier(out)\r\n                    else:\r\n                         # Fallback\r\n                         logits = nn.Linear(d_model, vocab_size).to(device)(out)\r\n\r\n                    main_loss = criterion(logits.view(-1, vocab_size), by.view(-1))\r\n                    \r\n                    # Nash bazen aux_loss'u tensor(0.) döndürür, shape hatası olmasın\r\n                    total_loss = main_loss + 0.1 * aux_loss\r\n                    \r\n                    total_loss.backward()\r\n                    optimizer.step()\r\n                    \r\n                    batch_loss += total_loss.item()\r\n                \r\n                loss_history[strategy].append(batch_loss / 10)\r\n                \r\n            del model\r\n            torch.cuda.empty_cache()\r\n        \r\n        except Exception as e:\r\n            print(f\"   ⚠️ Hata ({strategy}): {e}\")\r\n            \r\n    return loss_history\r\n\r\ndef plot_results(df_eff, loss_hist):\r\n    \"\"\"Grafikleri Çizer\"\"\"\r\n    sns.set_style(\"whitegrid\")\r\n    \r\n    # 1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/run_experiments.py", "file_name": "run_experiments.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "871", "text": "Time Complexity\r\n    if df_eff is not None and not df_eff.empty:\r\n        plt.figure(figsize=(12, 5))\r\n        plt.subplot(1, 2, 1)\r\n        sns.lineplot(data=df_eff, x=\"Seq_Len\", y=\"Time (ms)\", hue=\"Model\", marker=\"o\", linewidth=2.5)\r\n        plt.title(\"Computational Time vs Sequence Length\")\r\n        plt.ylabel(\"Inference Time (ms)\")\r\n        plt.xlabel(\"Sequence Length (n)\")\r\n        \r\n        # 2. Memory Usage\r\n        plt.subplot(1, 2, 2)\r\n        sns.barplot(data=df_eff, x=\"Seq_Len\", y=\"Memory (MB)\", hue=\"Model\")\r\n        plt.title(\"Peak Memory Usage vs Sequence Length\")\r\n        plt.ylabel(\"Memory (MB)\")\r\n        \r\n        plt.tight_layout()\r\n        plt.savefig(\"experiment_efficiency.png\", dpi=300)\r\n        print(\"\\n✅ Verimlilik Grafiği Kaydedildi: experiment_efficiency.png\")\r\n    else:\r\n        print(\"⚠️ Verimlilik verisi boş, grafik çizilemedi.\")\r\n\r\n    # 3. Ablation Loss\r\n    if loss_hist:\r\n        plt.figure(figsize=(8, 5))\r\n        for strat, losses in loss_hist.items():\r\n            if losses: # Liste boş değilse\r\n                plt.plot(losses, label=f\"Strategy: {strat}\", marker='s')\r\n        plt.title(\"Training Convergence: Convex vs Nash\")\r\n        plt.xlabel(\"Epochs\")\r\n        plt.ylabel(\"Loss\")\r\n        plt.legend()\r\n        plt.grid(True)\r\n        plt.savefig(\"experiment_ablation.png\", dpi=300)\r\n        plt.savefig(\"experiment_ablation.pdf\")\r\n        print(\"✅ Ablasyon Grafiği Kaydedildi: experiment_ablation.png\")\r\n\r\nif __name__ == \"__main__\":\r\n    # Önceki hatalardan kalan context'i temizlemek gerekebilir.\r\n    # Terminali kapatıp açmanız en iyisidir, ama kod içinde try-except var.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/run_experiments.py", "file_name": "run_experiments.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "872", "text": "# Terminali kapatıp açmanız en iyisidir, ama kod içinde try-except var.\r\n    \r\n    df_results = benchmark_efficiency()\r\n    if df_results is not None:\r\n        print(\"\\n--- Verimlilik Sonuçları ---\")\r\n        print(df_results)\r\n\r\n    loss_history = ablation_study()\r\n    \r\n    plot_results(df_results, loss_history)", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/run_experiments.py", "file_name": "run_experiments.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "875", "text": "import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom typing import List, Tuple, Literal\r\n\r\nclass OptimizationDrivenAggregation(nn.Module):\r\n    \"\"\"\r\n    Implements Optimization-Driven Aggregation (Section 4.3 of MAHA paper).\r\n    \r\n    Strategies:\r\n    1. Convex Optimization (Eq 9): Learns weights w s.t. sum(w)=1, w>=0 with L1 sparsity.\r\n    2. Nash Equilibrium (Eq 10): Iterative best-response dynamics to find equilibrium weights.\r\n    \r\n    Includes Nearest-Neighbor Upsampling (Eq 13) to align scales.\r\n    \"\"\"\r\n    \r\n    def __init__(\r\n        self, \r\n        num_scales: int, \r\n        d_model: int,\r\n        strategy: Literal['convex', 'nash'] = 'convex',\r\n        nash_iterations: int = 3,\r\n        lambda_sparsity: float = 0.1\r\n    ):\r\n        super().__init__()\r\n        self.num_scales = num_scales\r\n        self.strategy = strategy\r\n        self.nash_iterations = nash_iterations\r\n        self.lambda_sparsity = lambda_sparsity\r\n        \r\n        # Learnable weights for Convex strategy\r\n        # Initialized to be uniform (1/L) after softmax\r\n        self.convex_logits = nn.Parameter(torch.zeros(num_scales))\r\n        \r\n        # Linear projection for Nash utility estimation (optional but improves stability)\r\n        self.utility_proj = nn.Linear(d_model, 1) if strategy == 'nash' else None\r\n\r\n    def _upsample(self, tensor: torch.Tensor, target_len: int) -> torch.Tensor:\r\n        \"\"\"\r\n        Nearest-Neighbor Upsampling (Eq 13).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/layers/aggregation.py", "file_name": "aggregation.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "876", "text": "Input: (B, N_l, D) -> Output: (B, N_target, D)\r\n        \"\"\"\r\n        # Permute for interpolate: (B, D, N)\r\n        tensor_p = tensor.transpose(1, 2)\r\n        \r\n        # Apply Nearest Neighbor interpolation\r\n        upsampled = F.interpolate(\r\n            tensor_p, \r\n            size=target_len, \r\n            mode='nearest'\r\n        )\r\n        \r\n        # Permute back: (B, N, D)\r\n        return upsampled.transpose(1, 2)\r\n\r\n    def solve_convex(self, outputs: List[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:\r\n        \"\"\"\r\n        Implements Convex Optimization aggregation.\r\n        Returns: (Aggregated_Tensor, Sparsity_Loss)\r\n        \"\"\"\r\n        # Enforce constraints: sum(w)=1, w>=0 via Softmax\r\n        weights = F.softmax(self.convex_logits, dim=0)\r\n        \r\n        # Calculate L1 sparsity loss (Eq 9 penalty term)\r\n        # We want weights to be sparse (some close to 0)\r\n        sparsity_loss = self.lambda_sparsity * torch.norm(weights, p=1)\r\n        \r\n        # Weighted Sum\r\n        # Ensure base tensor is on correct device and shape\r\n        final_output = torch.zeros_like(outputs[0])\r\n        for i, out_tensor in enumerate(outputs):\r\n            final_output += weights[i] * out_tensor\r\n            \r\n        return final_output, sparsity_loss\r\n\r\n    def solve_nash(self, outputs: List[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:\r\n        \"\"\"\r\n        Implements Nash Equilibrium aggregation via Best-Response Dynamics.\r\n        \"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/layers/aggregation.py", "file_name": "aggregation.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "877", "text": "batch_size = outputs[0].shape[0]\r\n        \r\n        # Initialize weights uniformly: (B, L, 1)\r\n        # Each sample in batch can have different equilibrium\r\n        weights = torch.ones(batch_size, self.num_scales, 1, device=outputs[0].device) \r\n        weights = weights / self.num_scales\r\n        \r\n        # Iterative Best-Response (Unrolled Optimization)\r\n        # We simulate 'nash_iterations' steps of players adjusting strategies\r\n        stacked_outputs = torch.stack(outputs, dim=1) # (B, L, N, D)\r\n\r\n        for _ in range(self.nash_iterations):\r\n            # 1. Compute current Consensus (O*)\r\n            consensus = (stacked_outputs * weights.unsqueeze(-1)).sum(dim=1) # (B, N, D)\r\n            \r\n            # 2. Compute Utility/Error for each scale\r\n            # Error_l = || O_l - O* ||^2\r\n            # We want to minimize reconstruction error\r\n            diffs = stacked_outputs - consensus.unsqueeze(1) # (B, L, N, D)\r\n            errors = torch.norm(diffs, p=2, dim=(2, 3)) # (B, L)\r\n            \r\n            # 3. Update weights (Softmax over negative error -> minimized error gets higher weight)\r\n            # This is a differentiable approximation of argmin\r\n            weights = F.softmax(-errors, dim=1).unsqueeze(-1) # (B, L, 1)\r\n\r\n        # Final Aggregation using Equilibrium Weights\r\n        final_output = (stacked_outputs * weights.unsqueeze(-1)).sum(dim=1)\r\n        \r\n        return final_output, torch.tensor(0.0, device=final_output.device)\r\n\r\n    def forward(self, scale_outputs: List[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:\r\n        \"\"\"\r\n        Args:\r\n            scale_outputs: List of tensors [O_0, O_1, ..., O_L] with different lengths.\r\n            \r\n        Returns:\r\n            (Aggregated_Tensor, Aux_Loss)\r\n        \"\"\"\r\n        target_len = scale_outputs[0].size(1)\r\n        \r\n        # 1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/layers/aggregation.py", "file_name": "aggregation.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "878", "text": "Returns:\r\n            (Aggregated_Tensor, Aux_Loss)\r\n        \"\"\"\r\n        target_len = scale_outputs[0].size(1)\r\n        \r\n        # 1. Upsample all scales to target length (Scale 0 length)\r\n        upsampled_outputs = [scale_outputs[0]]\r\n        for i in range(1, self.num_scales):\r\n            upsampled_outputs.append(\r\n                self._upsample(scale_outputs[i], target_len)\r\n            )\r\n            \r\n        # 2. Aggregate based on strategy\r\n        if self.strategy == 'convex':\r\n            return self.solve_convex(upsampled_outputs)\r\n        elif self.strategy == 'nash':\r\n            return self.solve_nash(upsampled_outputs)\r\n        else:\r\n            raise ValueError(f\"Unknown strategy: {self.strategy}\")", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/layers/aggregation.py", "file_name": "aggregation.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "879", "text": "import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport math\r\nfrom typing import List, Optional\r\nfrom .decomposition import HierarchicalDecomposition\r\n\r\nclass MultiscaleAttention(nn.Module):\r\n    \"\"\"\r\n    Implements Multiscale Attention Computation (Section 4.2 of MAHA paper).\r\n    \r\n    Features:\r\n    - Scale-specific Query (Q) and Key (K) projections.\r\n    - Shared Value (V) projection across all scales.\r\n    - Efficient computation using re-used downsampling operators for V.\r\n    \"\"\"\r\n    \r\n    def __init__(\r\n        self, \r\n        d_model: int, \r\n        num_heads: int, \r\n        num_scales: int,\r\n        decomposition_module: HierarchicalDecomposition\r\n    ):\r\n        super().__init__()\r\n        self.d_model = d_model\r\n        self.num_heads = num_heads\r\n        self.head_dim = d_model // num_heads\r\n        self.num_scales = num_scales\r\n        \r\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\r\n        \r\n        # 1. Scale-Specific Projections for Q and K\r\n        # We create a separate Linear layer for each scale l.\r\n        self.q_projs = nn.ModuleList([\r\n            nn.Linear(d_model, d_model) for _ in range(num_scales)\r\n        ])\r\n        self.k_projs = nn.ModuleList([\r\n            nn.Linear(d_model, d_model) for _ in range(num_scales)\r\n        ])\r\n        \r\n        # 2.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/layers/attention.py", "file_name": "attention.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "880", "text": "Shared Value Projection \r\n        # V_base = X * W_V\r\n        self.shared_v_proj = nn.Linear(d_model, d_model)\r\n        \r\n        # Reference to decomposition module to downsample V_base for each scale\r\n        # V_l = D_l(V_base) [cite: 124]\r\n        self.decomposition = decomposition_module\r\n\r\n    def forward(\r\n        self, \r\n        x_scales: List[torch.Tensor], \r\n        mask: Optional[torch.Tensor] = None\r\n    ) -> List[torch.Tensor]:\r\n        \"\"\"\r\n        Args:\r\n            x_scales (List[Tensor]): List of decomposed inputs [X_0, X_1, ..., X_L].\r\n                                     Output from HierarchicalDecomposition.forward().\r\n            mask (Tensor, optional): Standard attention mask (broadcastable).\r\n            \r\n        Returns:\r\n            List[Tensor]: List of attention outputs [O_0, O_1, ..., O_L] per scale.\r\n        \"\"\"\r\n        \r\n        # Step 1: Compute Base Value (V_base) from the original input (Scale 0)\r\n        # X_0 is usually the high-res input\r\n        x_base = x_scales[0] \r\n        v_base = self.shared_v_proj(x_base) # (B, N, d)\r\n        \r\n        # Step 2: Hierarchically decompose V_base using the SAME operators used for X\r\n        # This ensures V_l matches the length of Q_l and K_l\r\n        # [cite: 124] V_l = D_l(V_base)\r\n        v_scales = self.decomposition(v_base)\r\n        \r\n        outputs = []\r\n        \r\n        # Step 3: Compute Attention for each scale independently\r\n        for l in range(self.num_scales):\r\n            x_l = x_scales[l] # Input at scale l\r\n            v_l = v_scales[l] # Value at scale l\r\n            \r\n            B, N_l, _ = x_l.size()\r\n            \r\n            # Projections \r\n            q_l = self.q_projs[l](x_l)\r\n            k_l = self.k_projs[l](x_l)\r\n            \r\n            # Reshape for Multi-Head Attention: (B, N, H,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/layers/attention.py", "file_name": "attention.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "881", "text": "q_projs[l](x_l)\r\n            k_l = self.k_projs[l](x_l)\r\n            \r\n            # Reshape for Multi-Head Attention: (B, N, H, D_h) -> (B, H, N, D_h)\r\n            q_l = q_l.view(B, N_l, self.num_heads, self.head_dim).transpose(1, 2)\r\n            k_l = k_l.view(B, N_l, self.num_heads, self.head_dim).transpose(1, 2)\r\n            v_l = v_l.view(B, N_l, self.num_heads, self.head_dim).transpose(1, 2)\r\n            \r\n            # Scaled Dot-Product Attention [cite: 120]\r\n            # scores = (Q K^T) / sqrt(d_k)\r\n            scores = torch.matmul(q_l, k_l.transpose(-2, -1)) / math.sqrt(self.head_dim)\r\n            \r\n            if mask is not None:\r\n                # Need to resize mask for current scale if mask is provided\r\n                # Simplified masking logic for brevity (assuming causal or padding mask)\r\n                pass \r\n            \r\n            attn_weights = F.softmax(scores, dim=-1)\r\n            \r\n            # O_l = A_l * V_l [cite: 126]\r\n            context = torch.matmul(attn_weights, v_l)\r\n            \r\n            # Reshape back: (B, H, N, D_h) -> (B, N, D)\r\n            context = context.transpose(1, 2).contiguous().view(B, N_l, self.d_model)\r\n            \r\n            outputs.append(context)\r\n            \r\n        return outputs", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/layers/attention.py", "file_name": "attention.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "882", "text": "import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom typing import List, Literal\r\n\r\nclass HierarchicalDecomposition(nn.Module):\r\n    \"\"\"\r\n    Implements Hierarchical Multiscale Decomposition (Section 4.1 of MAHA paper).\r\n    \r\n    This layer decomposes the input sequence X into L hierarchical scales using\r\n    learnable downsampling operators (Strided Convolution) or Adaptive Pooling.\r\n    \r\n    Paper Reference:\r\n        Eq (5): X_l = D_l(X_{l-1})\r\n        Eq (109): Strided Convolution logic\r\n        Eq (112): Exponential decay schedule n_l = floor(n_{l-1} / r)\r\n    \"\"\"\r\n    \r\n    def __init__(\r\n        self, \r\n        d_model: int, \r\n        num_scales: int = 4, \r\n        compression_ratio: int = 2, \r\n        mode: Literal['conv', 'pool'] = 'conv',\r\n        kernel_size: int = 3\r\n    ):\r\n        \"\"\"\r\n        Args:\r\n            d_model (int): The embedding dimension (d).\r\n            num_scales (int): Number of hierarchical scales (L).\r\n            compression_ratio (int): Downsampling factor (r).\r\n            mode (str): 'conv' for learnable Strided Convolution, 'pool' for Adaptive Max Pooling.\r\n            kernel_size (int): Kernel size for convolution (default: 3).\r\n        \"\"\"\r\n        super().__init__()\r\n        self.d_model = d_model\r\n        self.num_scales = num_scales\r\n        self.compression_ratio = compression_ratio\r\n        self.mode = mode\r\n        \r\n        # We need (L-1) downsampling operators since Scale 0 is the original input.\r\n        # Using ModuleList to register parameters properly.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/layers/decomposition.py", "file_name": "decomposition.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "883", "text": "# Using ModuleList to register parameters properly.\r\n        self.downsamplers = nn.ModuleList()\r\n        \r\n        if self.mode == 'conv':\r\n            for _ in range(num_scales - 1):\r\n                # Eq (109): D_l(X) = Conv1D(X, W_l, s_l)\r\n                # Note: Groups=1 implies full interaction between channels as per W in R^{k x d x d}\r\n                self.downsamplers.append(\r\n                    nn.Conv1d(\r\n                        in_channels=d_model,\r\n                        out_channels=d_model,\r\n                        kernel_size=kernel_size,\r\n                        stride=compression_ratio,\r\n                        padding=kernel_size // 2  # To maintain consistent alignment\r\n                    )\r\n                )\r\n        elif self.mode == 'pool':\r\n            # Pooling doesn't require learnable parameters per scale, \r\n            # but we keep the structure consistent.\r\n            pass\r\n        else:\r\n            raise ValueError(f\"Unknown decomposition mode: {mode}\")\r\n\r\n    def forward(self, x: torch.Tensor) -> List[torch.Tensor]:\r\n        \"\"\"\r\n        Args:\r\n            x (torch.Tensor): Input tensor of shape (Batch, Seq_Len, d_model)\r\n            \r\n        Returns:\r\n            List[torch.Tensor]: A list of length `num_scales`.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/layers/decomposition.py", "file_name": "decomposition.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "884", "text": "Scale 0: (B, N, d)\r\n                                Scale 1: (B, N/r, d)\r\n                                ...\r\n        \"\"\"\r\n        # x shape: [Batch, Length, Dim] -> Transpose for Conv1d: [Batch, Dim, Length]\r\n        current_x = x.transpose(1, 2)\r\n        outputs = [x] # Scale 0 is the input itself [cite: 106]\r\n        \r\n        for i in range(self.num_scales - 1):\r\n            if self.mode == 'conv':\r\n                # Apply learnable strided convolution\r\n                # current_x represents X_{l-1}\r\n                next_x = self.downsamplers[i](current_x)\r\n                \r\n            elif self.mode == 'pool':\r\n                # Eq (111): Adaptive Pooling\r\n                # Calculate target length: n_l = floor(n_{l-1} / r)\r\n                current_len = current_x.size(2)\r\n                target_len = current_len // self.compression_ratio\r\n                \r\n                # Prevent collapse to 0 length for very deep hierarchies\r\n                target_len = max(1, target_len)\r\n                \r\n                next_x = F.adaptive_max_pool1d(current_x, output_size=target_len)\r\n            \r\n            # Update for next iteration\r\n            current_x = next_x\r\n            \r\n            # Transpose back to [Batch, Length, Dim] for attention processing\r\n            # and append to outputs list\r\n            outputs.append(current_x.transpose(1, 2))\r\n            \r\n        return outputs\r\n\r\n    def get_output_shapes(self, input_len: int) -> List[int]:\r\n        \"\"\"Helper to compute expected sequence lengths for debugging.\"\"\"\r\n        shapes = [input_len]\r\n        curr = input_len\r\n        for _ in range(self.num_scales - 1):\r\n            curr = curr // self.compression_ratio\r\n            shapes.append(curr)\r\n        return shapes", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/layers/decomposition.py", "file_name": "decomposition.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "885", "text": "import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nclass CrossScaleGating(nn.Module):\r\n    \"\"\"\r\n    Implements Cross-Scale Gating (Eq 12 in MAHA paper).\r\n    \r\n    Mechanics:\r\n    G_l = sigmoid(W_g * [C_l; U(C_{l+1})]) * C_l + (1 - sigmoid(...)) * U(C_{l+1})\r\n    \r\n    It dynamically blends information from the current scale and the coarser scale below it.\r\n    \"\"\"\r\n    \r\n    def __init__(self, d_model: int):\r\n        super().__init__()\r\n        # Input dimension is 2 * d_model because we concatenate current and next scale\r\n        self.gate_proj = nn.Linear(2 * d_model, d_model)\r\n        \r\n    def _upsample(self, tensor: torch.Tensor, target_len: int) -> torch.Tensor:\r\n        # Re-use nearest neighbor logic for consistency\r\n        tensor_p = tensor.transpose(1, 2)\r\n        upsampled = F.interpolate(tensor_p, size=target_len, mode='nearest')\r\n        return upsampled.transpose(1, 2)\r\n\r\n    def forward(self, current_scale: torch.Tensor, next_scale: torch.Tensor) -> torch.Tensor:\r\n        \"\"\"\r\n        Args:\r\n            current_scale (Tensor): Feature map at scale l (B, N_l, D)\r\n            next_scale (Tensor): Feature map at scale l+1 (Coarser) (B, N_{l+1}, D)\r\n            \r\n        Returns:\r\n            Tensor: Gated feature map at scale l (B, N_l, D)\r\n        \"\"\"\r\n        # 1. Upsample coarser scale to match current scale\r\n        target_len = current_scale.size(1)\r\n        next_scale_up = self._upsample(next_scale, target_len)\r\n        \r\n        # 2. Concatenate [C_l; U(C_{l+1})]\r\n        combined = torch.cat([current_scale, next_scale_up], dim=-1)\r\n        \r\n        # 3. Compute Gate Coefficient (z)\r\n        # z = Sigmoid(W_g * combined)\r\n        gate = torch.sigmoid(self.gate_proj(combined))\r\n        \r\n        # 4.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/layers/gating.py", "file_name": "gating.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "886", "text": "Compute Gate Coefficient (z)\r\n        # z = Sigmoid(W_g * combined)\r\n        gate = torch.sigmoid(self.gate_proj(combined))\r\n        \r\n        # 4. Apply Gating\r\n        # G_l = z * C_l + (1 - z) * U(C_{l+1})\r\n        output = gate * current_scale + (1 - gate) * next_scale_up\r\n        \r\n        return output", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/layers/gating.py", "file_name": "gating.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "888", "text": "# Hybrid Dilated-Convolutional Transformer Block\r\nimport torch\r\nimport torch.nn as nn\r\nfrom typing import Optional, Tuple\r\n\r\n# Önceki adımlarda yazdığımız modülleri import ediyoruz\r\nfrom src.layers.decomposition import HierarchicalDecomposition\r\nfrom src.layers.attention import MultiscaleAttention\r\nfrom src.layers.aggregation import OptimizationDrivenAggregation\r\n\r\nclass MAHABlock(nn.Module):\r\n    \"\"\"\r\n    Implements the Hybrid Dilated-Convolutional Transformer Block (Section 4.4).\r\n    \r\n    This block replaces the standard Self-Attention layer with the MAHA pipeline:\r\n    1. Dilated Convolution (Local Context) [cite: 142]\r\n    2. Hierarchical Decomposition [cite: 102]\r\n    3. Multiscale Attention with Shared Values [cite: 114]\r\n    4. Optimization-Driven Aggregation (Convex/Nash) [cite: 128]\r\n    5. Feed-Forward Network (Standard)\r\n    \"\"\"\r\n    \r\n    def __init__(\r\n        self,\r\n        d_model: int,\r\n        num_heads: int,\r\n        d_ff: int,\r\n        dropout: float = 0.1,\r\n        num_scales: int = 4,\r\n        compression_ratio: int = 2,\r\n        aggregation_strategy: str = 'convex'\r\n    ):\r\n        super().__init__()\r\n        \r\n        # 1. Pre-processing: Dilated Convolution to capture local context features\r\n        # Eq (11): C_l = ReLU(DilatedConv(X...))\r\n        # We apply a mild dilation here to enrich features before decomposition\r\n        self.dilated_conv = nn.Sequential(\r\n            nn.Conv1d(d_model, d_model, kernel_size=3, padding=2, dilation=2),\r\n            nn.ReLU(),\r\n            nn.Dropout(dropout)\r\n        )\r\n        \r\n        # 2.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/models/maha_block.py", "file_name": "maha_block.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "889", "text": "MAHA Components\r\n        self.decomposition = HierarchicalDecomposition(\r\n            d_model=d_model,\r\n            num_scales=num_scales,\r\n            compression_ratio=compression_ratio,\r\n            mode='conv'\r\n        )\r\n        \r\n        self.attention = MultiscaleAttention(\r\n            d_model=d_model,\r\n            num_heads=num_heads,\r\n            num_scales=num_scales,\r\n            decomposition_module=self.decomposition\r\n        )\r\n        \r\n        self.aggregation = OptimizationDrivenAggregation(\r\n            num_scales=num_scales,\r\n            d_model=d_model,\r\n            strategy=aggregation_strategy\r\n        )\r\n        \r\n        # 3. Standard Transformer Components (Norm & FFN)\r\n        self.norm1 = nn.LayerNorm(d_model)\r\n        self.norm2 = nn.LayerNorm(d_model)\r\n        \r\n        self.ffn = nn.Sequential(\r\n            nn.Linear(d_model, d_ff),\r\n            nn.GELU(),\r\n            nn.Dropout(dropout),\r\n            nn.Linear(d_ff, d_model),\r\n            nn.Dropout(dropout)\r\n        )\r\n        \r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\r\n        \"\"\"\r\n        Args:\r\n            x: Input tensor (Batch, Seq_Len, d_model)\r\n            mask: Attention mask\r\n            \r\n        Returns:\r\n            output: Tensor (Batch, Seq_Len, d_model)\r\n            aux_loss: Sparsity/Optimization loss from aggregation\r\n        \"\"\"\r\n        # Residual Connection 1 (MAHA Branch)\r\n        residual = x\r\n        \r\n        # A. Dilated Convolution (Requires Transpose for Conv1d)\r\n        # x: (B, N, D) -> (B, D, N)\r\n        x_conv = x.transpose(1, 2)\r\n        x_conv = self.dilated_conv(x_conv)\r\n        x_conv = x_conv.transpose(1, 2) # Back to (B, N, D)\r\n        \r\n        # B. Hierarchical Decomposition\r\n        # x -> [X_0, X_1, ...,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/models/maha_block.py", "file_name": "maha_block.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "890", "text": "transpose(1, 2) # Back to (B, N, D)\r\n        \r\n        # B. Hierarchical Decomposition\r\n        # x -> [X_0, X_1, ..., X_L]\r\n        scales = self.decomposition(x_conv)\r\n        \r\n        # C. Multiscale Attention\r\n        # [X_0...X_L] -> [O_0...O_L]\r\n        attn_outputs = self.attention(scales, mask)\r\n        \r\n        # D. Aggregation (Convex or Nash)\r\n        # [O_0...O_L] -> O*\r\n        maha_out, aux_loss = self.aggregation(attn_outputs)\r\n        \r\n        # Apply projection and dropout\r\n        maha_out = self.dropout(maha_out)\r\n        \r\n        # Add & Norm\r\n        x = self.norm1(residual + maha_out)\r\n        \r\n        # Residual Connection 2 (FFN Branch)\r\n        residual = x\r\n        ffn_out = self.ffn(x)\r\n        x = self.norm2(residual + ffn_out)\r\n        \r\n        return x, aux_loss", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/models/maha_block.py", "file_name": "maha_block.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "891", "text": "# Full MAHA Transformer Architecture\r\nimport torch\r\nimport torch.nn as nn\r\nfrom src.models.maha_block import MAHABlock\r\n\r\nclass MAHATransformer(nn.Module):\r\n    \"\"\"\r\n    Full MAHA Transformer Architecture.\r\n    \r\n    It stacks L MAHA Blocks to form a powerful encoder for NLP tasks.\r\n    Compatible with tasks like Text Classification (GLUE) or Masked Language Modeling.\r\n    \r\n    Ref: Section 5.1 Experimental Setup \r\n         - 12 Layers\r\n         - 768 Hidden Dimension\r\n         - 12 Attention Heads\r\n    \"\"\"\r\n    \r\n    def __init__(\r\n        self,\r\n        vocab_size: int,\r\n        max_len: int = 512,\r\n        d_model: int = 768,\r\n        num_heads: int = 12,\r\n        num_layers: int = 12,\r\n        d_ff: int = 3072,\r\n        num_scales: int = 4,\r\n        aggregation_strategy: str = 'convex',\r\n        dropout: float = 0.1\r\n    ):\r\n        super().__init__()\r\n        \r\n        self.d_model = d_model\r\n        \r\n        # Token & Position Embeddings\r\n        self.token_emb = nn.Embedding(vocab_size, d_model)\r\n        self.pos_emb = nn.Embedding(max_len, d_model)\r\n        self.emb_dropout = nn.Dropout(dropout)\r\n        \r\n        # Stack of MAHA Blocks\r\n        self.layers = nn.ModuleList([\r\n            MAHABlock(\r\n                d_model=d_model,\r\n                num_heads=num_heads,\r\n                d_ff=d_ff,\r\n                dropout=dropout,\r\n                num_scales=num_scales,\r\n                aggregation_strategy=aggregation_strategy\r\n            )\r\n            for _ in range(num_layers)\r\n        ])\r\n        \r\n        # Final Norm (Pre-classifier)\r\n        self.final_norm = nn.LayerNorm(d_model)\r\n        \r\n        # Example Classifier Head (Optional, for GLUE tasks)\r\n        self.classifier = nn.Linear(d_model, vocab_size) # Or num_classes\r\n\r\n    def forward(self, x: torch.Tensor, mask: torch.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/models/transformer.py", "file_name": "transformer.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "892", "text": "for GLUE tasks)\r\n        self.classifier = nn.Linear(d_model, vocab_size) # Or num_classes\r\n\r\n    def forward(self, x: torch.Tensor, mask: torch.Tensor = None):\r\n        \"\"\"\r\n        Args:\r\n            x: Input tokens (Batch, Seq_Len)\r\n            mask: Attention mask\r\n        \"\"\"\r\n        B, N = x.size()\r\n        \r\n        # Embeddings\r\n        positions = torch.arange(0, N, device=x.device).unsqueeze(0)\r\n        x = self.token_emb(x) + self.pos_emb(positions)\r\n        x = self.emb_dropout(x)\r\n        \r\n        total_aux_loss = 0.0\r\n        \r\n        # Pass through MAHA Layers\r\n        for layer in self.layers:\r\n            x, layer_loss = layer(x, mask)\r\n            total_aux_loss += layer_loss\r\n            \r\n        x = self.final_norm(x)\r\n        \r\n        # Return features and aggregated sparsity loss for optimization\r\n        return x, total_aux_loss", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/models/transformer.py", "file_name": "transformer.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "893", "text": "# Differentiable Convex Optimization Layer", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/optimization/convex_solver.py", "file_name": "convex_solver.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "894", "text": "# Iterative Nash Equilibrium Solver", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/optimization/game_solver.py", "file_name": "game_solver.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "896", "text": "# Attention Masking Utilities", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/utils/masking.py", "file_name": "masking.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "897", "text": "# PPL, BLEU and Efficiency Metrics", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/utils/metrics.py", "file_name": "metrics.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "898", "text": "import unittest\r\nimport torch\r\nimport sys\r\nimport os\r\n\r\n# Proje kök dizinini path'e ekle\r\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\r\n\r\nfrom src.layers.decomposition import HierarchicalDecomposition\r\nfrom src.layers.attention import MultiscaleAttention\r\nfrom src.layers.aggregation import OptimizationDrivenAggregation\r\nfrom src.models.maha_block import MAHABlock\r\nfrom src.models.transformer import MAHATransformer\r\n\r\nclass TestMAHAComponents(unittest.TestCase):\r\n    \r\n    def setUp(self):\r\n        \"\"\"Test ortamı için ortak parametreler\"\"\"\r\n        self.batch_size = 2\r\n        self.seq_len = 128  \r\n        self.d_model = 64  # Küçük bir model boyutu\r\n        self.num_heads = 4 # 64'e bölünebilir (64 % 4 == 0)\r\n        self.num_scales = 3\r\n        self.r = 2 \r\n        \r\n        self.dummy_input = torch.randn(self.batch_size, self.seq_len, self.d_model)\r\n\r\n    def test_hierarchical_decomposition_shapes(self):\r\n        \"\"\"Test: Hiyerarşik ayrıştırma doğru boyutlarda tensör üretiyor mu?\"\"\"\r\n        decomp = HierarchicalDecomposition(self.d_model, self.num_scales, self.r, mode='conv')\r\n        outputs = decomp(self.dummy_input)\r\n        \r\n        self.assertEqual(len(outputs), self.num_scales)\r\n        self.assertEqual(outputs[0].shape, (self.batch_size, self.seq_len, self.d_model))\r\n        expected_len_1 = self.seq_len // self.r\r\n        self.assertEqual(outputs[1].shape[1], expected_len_1)\r\n        print(f\"✅ Decomposition Passed. Shapes: {[t.shape for t in outputs]}\")\r\n\r\n    def test_multiscale_attention_flow(self):\r\n        \"\"\"Test: Attention katmanı Shared Value ile çalışıyor mu?\"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/tests/test_maha.py", "file_name": "test_maha.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "899", "text": "Shapes: {[t.shape for t in outputs]}\")\r\n\r\n    def test_multiscale_attention_flow(self):\r\n        \"\"\"Test: Attention katmanı Shared Value ile çalışıyor mu?\"\"\"\r\n        decomp = HierarchicalDecomposition(self.d_model, self.num_scales, self.r)\r\n        attn = MultiscaleAttention(self.d_model, self.num_heads, self.num_scales, decomp)\r\n        \r\n        scales = decomp(self.dummy_input)\r\n        attn_outputs = attn(scales)\r\n        \r\n        for i, out_tensor in enumerate(attn_outputs):\r\n            self.assertEqual(out_tensor.shape, scales[i].shape)\r\n        print(\"✅ Attention Passed.\")\r\n\r\n    def test_aggregation_convex(self):\r\n        \"\"\"Test: Convex Optimization birleştirme\"\"\"\r\n        aggregator = OptimizationDrivenAggregation(self.num_scales, self.d_model, strategy='convex')\r\n        inputs = [\r\n            torch.randn(self.batch_size, self.seq_len, self.d_model),\r\n            torch.randn(self.batch_size, self.seq_len // 2, self.d_model),\r\n            torch.randn(self.batch_size, self.seq_len // 4, self.d_model)\r\n        ]\r\n        output, loss = aggregator(inputs)\r\n        self.assertEqual(output.shape, inputs[0].shape)\r\n        self.assertTrue(torch.is_tensor(loss))\r\n        print(\"✅ Aggregation (Convex) Passed.\")\r\n\r\n    def test_aggregation_nash(self):\r\n        \"\"\"Test: Nash Equilibrium stratejisi\"\"\"\r\n        aggregator = OptimizationDrivenAggregation(self.num_scales, self.d_model, strategy='nash', nash_iterations=2)\r\n        inputs = [\r\n            torch.randn(self.batch_size, self.seq_len, self.d_model),\r\n            torch.randn(self.batch_size, self.seq_len // 2, self.d_model),\r\n            torch.randn(self.batch_size, self.seq_len // 4, self.d_model)\r\n        ]\r\n        output, _ = aggregator(inputs)\r\n        self.assertEqual(output.shape, inputs[0].shape)\r\n        print(\"✅ Aggregation (Nash) Passed.\")", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/tests/test_maha.py", "file_name": "test_maha.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "900", "text": "def test_full_maha_block(self):\r\n        \"\"\"Test: MAHABlock (End-to-End)\"\"\"\r\n        block = MAHABlock(\r\n            d_model=self.d_model,\r\n            num_heads=self.num_heads,\r\n            d_ff=self.d_model * 4,\r\n            num_scales=self.num_scales\r\n        )\r\n        output, aux_loss = block(self.dummy_input)\r\n        self.assertEqual(output.shape, self.dummy_input.shape)\r\n        print(\"✅ MAHABlock Passed.\")\r\n        \r\n    def test_transformer_integration(self):\r\n        \"\"\"Test: Tam Transformer modeli\"\"\"\r\n        vocab_size = 100\r\n        # DÜZELTME BURADA: num_heads parametresini açıkça veriyoruz\r\n        model = MAHATransformer(\r\n            vocab_size=vocab_size, \r\n            d_model=self.d_model, \r\n            num_heads=self.num_heads,  # <--- EKLENDİ (64 % 4 == 0)\r\n            num_layers=2\r\n        )\r\n        \r\n        input_ids = torch.randint(0, vocab_size, (self.batch_size, self.seq_len))\r\n        output, total_loss = model(input_ids)\r\n        \r\n        self.assertEqual(output.shape, (self.batch_size, self.seq_len, self.d_model))\r\n        print(\"✅ MAHATransformer Integration Passed.\")\r\n\r\nif __name__ == '__main__':\r\n    unittest.main()", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/tests/test_maha.py", "file_name": "test_maha.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "901", "text": "# Main Training Loop\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nfrom torch.utils.data import DataLoader, TensorDataset\r\nimport time\r\n\r\nfrom src.models.transformer import MAHATransformer\r\n\r\ndef train():\r\n    # --- 1. Hyperparameters (Section 5.1 Setup) ---\r\n    VOCAB_SIZE = 1000\r\n    SEQ_LEN = 128\r\n    BATCH_SIZE = 16\r\n    D_MODEL = 256        # Reduced for demo speed\r\n    NUM_HEADS = 4\r\n    NUM_LAYERS = 2\r\n    NUM_SCALES = 4\r\n    EPOCHS = 5\r\n    LR = 5e-5\r\n    LAMBDA_REG = 0.1     # Sparsity regularization coefficient (lambda)\r\n    \r\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n    print(f\"🚀 Training on device: {device}\")\r\n\r\n    # --- 2. Synthetic Data ---\r\n    # Random integers simulating token IDs\r\n    train_data = torch.randint(0, VOCAB_SIZE, (1000, SEQ_LEN))\r\n    # Random targets (e.g., for Masked LM or Classification)\r\n    train_labels = torch.randint(0, VOCAB_SIZE, (1000, SEQ_LEN))\r\n    \r\n    dataset = TensorDataset(train_data, train_labels)\r\n    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\r\n\r\n    # --- 3. Model Initialization ---\r\n    model = MAHATransformer(\r\n        vocab_size=VOCAB_SIZE,\r\n        max_len=SEQ_LEN,\r\n        d_model=D_MODEL,\r\n        num_heads=NUM_HEADS,\r\n        num_layers=NUM_LAYERS,\r\n        num_scales=NUM_SCALES,\r\n        aggregation_strategy='convex' # Try 'nash' as well\r\n    ).to(device)\r\n    \r\n    # Optimizer & Loss\r\n    optimizer = optim.AdamW(model.parameters(), lr=LR)\r\n    criterion = nn.CrossEntropyLoss()\r\n\r\n    print(f\"✅ Model initialized. Parameters: {sum(p.numel() for p in model.parameters()):,}\")\r\n\r\n    # --- 4. Training Loop ---\r\n    model.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/train.py", "file_name": "train.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "902", "text": "Parameters: {sum(p.numel() for p in model.parameters()):,}\")\r\n\r\n    # --- 4. Training Loop ---\r\n    model.train()\r\n    \r\n    for epoch in range(EPOCHS):\r\n        start_time = time.time()\r\n        total_loss = 0\r\n        total_task_loss = 0\r\n        total_aux_loss = 0\r\n        \r\n        for batch_idx, (inputs, targets) in enumerate(dataloader):\r\n            inputs, targets = inputs.to(device), targets.to(device)\r\n            \r\n            optimizer.zero_grad()\r\n            \r\n            # Forward Pass\r\n            # output: (B, Seq, D_model), aux_loss: Scalar (sum of L1 norms)\r\n            outputs, aux_loss = model(inputs)\r\n            \r\n            # Compute Task Loss (Flatten for CrossEntropy)\r\n            # outputs: (B*Seq, Vocab), targets: (B*Seq)\r\n            # Note: We need a projection to vocab size here if not in model\r\n            # For this demo, we use the classifier head inside MAHATransformer if it existed,\r\n            # or just project simply here:\r\n            logits = model.classifier(outputs) # (B, Seq, Vocab)\r\n            \r\n            task_loss = criterion(logits.view(-1, VOCAB_SIZE), targets.view(-1))\r\n            \r\n            # Combine Losses (Eq 9 in Paper)\r\n            loss = task_loss + (LAMBDA_REG * aux_loss)\r\n            \r\n            # Backward Pass\r\n            loss.backward()\r\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n            optimizer.step()\r\n            \r\n            total_loss += loss.item()\r\n            total_task_loss += task_loss.item()\r\n            total_aux_loss += aux_loss.item()\r\n            \r\n            if batch_idx % 10 == 0:\r\n                print(f\"Epoch {epoch+1} | Batch {batch_idx} | \"\r\n                      f\"Loss: {loss.item():.4f} (Task: {task_loss.item():.4f} + Aux: {aux_loss.item():.4f})\")\r\n        \r\n        avg_loss = total_loss / len(dataloader)\r\n        elapsed = time.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/train.py", "file_name": "train.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "903", "text": "item():.4f} (Task: {task_loss.item():.4f} + Aux: {aux_loss.item():.4f})\")\r\n        \r\n        avg_loss = total_loss / len(dataloader)\r\n        elapsed = time.time() - start_time\r\n        print(f\"🏁 End of Epoch {epoch+1} | Avg Loss: {avg_loss:.4f} | Time: {elapsed:.2f}s\")\r\n        print(\"-\" * 50)\r\n\r\nif __name__ == \"__main__\":\r\n    train()", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/train.py", "file_name": "train.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "904", "text": "\"\"\"\nNash-MHC: Deep-Equilibrium Engine for JAX/TPU.\n\nCombines:\n- mHC: Manifold-Constrained Hyper-Connections (Birkhoff polytope projection)\n- MAHA: Nash Equilibrium Multiscale Hierarchical Attention\n\"\"\"\n\n__version__ = \"0.1.0\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/__init__.py", "file_name": "__init__.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "905", "text": "\"\"\"Composed transformer blocks.\"\"\"\r\n\r\nfrom nash_mhc.blocks.decoder_block import MAHADecoderBlock\r\n\r\n__all__ = [\"MAHADecoderBlock\"]", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/blocks/__init__.py", "file_name": "__init__.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "906", "text": "\"\"\"\r\nMAHA Decoder Block with mHC residual connections.\r\n\r\nCombines hierarchical multiscale attention with manifold-constrained\r\nresidual connections for stable deep training.\r\n\r\nReference: Papers 2512.14925v2 (MAHA) and 2512.24880v1 (mHC)\r\n\"\"\"\r\n\r\nfrom typing import Literal\r\n\r\nimport jax\r\nimport jax.numpy as jnp\r\nimport equinox as eqx\r\nfrom jaxtyping import Float, Array, PRNGKeyArray\r\n\r\nfrom nash_mhc.layers.mhc import ManifoldHyperConnection\r\nfrom nash_mhc.layers.decomposition import HierarchicalDecomposition\r\nfrom nash_mhc.layers.attention import MultiscaleAttention\r\nfrom nash_mhc.layers.aggregation import OptimizationAggregation\r\nfrom nash_mhc.layers.ffn import SwiGLUFFN, RMSNorm\r\n\r\n\r\nclass MAHADecoderBlock(eqx.Module):\r\n    \"\"\"\r\n    Full MAHA decoder block with mHC residual connections.\r\n\r\n    Architecture:\r\n    1. Pre-norm (RMSNorm)\r\n    2. Hierarchical decomposition: X -> [X_0, X_1, ..., X_{L-1}]\r\n    3. Multiscale attention with shared V projection\r\n    4. Optimization-driven aggregation (Nash or Convex)\r\n    5. mHC residual connection (attention branch)\r\n    6. Pre-norm (RMSNorm)\r\n    7.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/blocks/decoder_block.py", "file_name": "decoder_block.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "907", "text": "Optimization-driven aggregation (Nash or Convex)\r\n    5. mHC residual connection (attention branch)\r\n    6. Pre-norm (RMSNorm)\r\n    7. SwiGLU FFN\r\n    8. mHC residual connection (FFN branch)\r\n\r\n    Type Parameters:\r\n        B: Batch size\r\n        N: Sequence length\r\n        D: Model dimension\r\n    \"\"\"\r\n\r\n    # Normalization layers\r\n    norm_attn: RMSNorm\r\n    norm_ffn: RMSNorm\r\n\r\n    # MAHA components\r\n    decomposition: HierarchicalDecomposition\r\n    attention: MultiscaleAttention\r\n    aggregation: OptimizationAggregation\r\n\r\n    # FFN\r\n    ffn: SwiGLUFFN\r\n\r\n    # mHC residual connections\r\n    mhc_attn: ManifoldHyperConnection\r\n    mhc_ffn: ManifoldHyperConnection\r\n\r\n    # Static configuration\r\n    d_model: int = eqx.field(static=True)\r\n    num_heads: int = eqx.field(static=True)\r\n    num_scales: int = eqx.field(static=True)\r\n\r\n    def __init__(\r\n        self,\r\n        d_model: int,\r\n        num_heads: int,\r\n        num_scales: int,\r\n        max_seq_len: int,\r\n        compression_ratio: int = 2,\r\n        ffn_multiplier: float = 2.67,\r\n        aggregation: Literal[\"nash\", \"convex\"] = \"nash\",\r\n        nash_iterations: int = 3,\r\n        sinkhorn_iterations: int = 10,\r\n        *,\r\n        key: PRNGKeyArray,\r\n    ):\r\n        \"\"\"\r\n        Args:\r\n            d_model: Model dimension (must be multiple of 128).\r\n            num_heads: Number of attention heads.\r\n            num_scales: Number of hierarchical scales (L).\r\n            max_seq_len: Maximum sequence length.\r\n            compression_ratio: Downsampling ratio between scales.\r\n            ffn_multiplier: FFN hidden dimension multiplier.\r\n            aggregation: 'nash' or 'convex' aggregation strategy.\r\n            nash_iterations: Best-response iterations for Nash.\r\n            sinkhorn_iterations: Sinkhorn-Knopp iterations for mHC.\r\n            key: PRNG key for initialization.\r\n        \"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/blocks/decoder_block.py", "file_name": "decoder_block.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "908", "text": "nash_iterations: Best-response iterations for Nash.\r\n            sinkhorn_iterations: Sinkhorn-Knopp iterations for mHC.\r\n            key: PRNG key for initialization.\r\n        \"\"\"\r\n        self.d_model = d_model\r\n        self.num_heads = num_heads\r\n        self.num_scales = num_scales\r\n\r\n        # Split key for all components\r\n        keys = jax.random.split(key, 6)\r\n\r\n        # Normalization layers\r\n        self.norm_attn = RMSNorm(d_model)\r\n        self.norm_ffn = RMSNorm(d_model)\r\n\r\n        # MAHA components\r\n        self.decomposition = HierarchicalDecomposition(\r\n            d_model=d_model,\r\n            num_scales=num_scales,\r\n            compression_ratio=compression_ratio,\r\n            key=keys[0],\r\n        )\r\n\r\n        self.attention = MultiscaleAttention(\r\n            d_model=d_model,\r\n            num_heads=num_heads,\r\n            num_scales=num_scales,\r\n            max_seq_len=max_seq_len,\r\n            compression_ratio=compression_ratio,\r\n            key=keys[1],\r\n        )\r\n\r\n        self.aggregation = OptimizationAggregation(\r\n            num_scales=num_scales,\r\n            strategy=aggregation,\r\n            nash_iterations=nash_iterations,\r\n            key=keys[2],\r\n        )\r\n\r\n        # FFN\r\n        self.ffn = SwiGLUFFN(\r\n            d_model=d_model,\r\n            ffn_multiplier=ffn_multiplier,\r\n            key=keys[3],\r\n        )\r\n\r\n        # mHC residual connections (both branches)\r\n        self.mhc_attn = ManifoldHyperConnection(\r\n            d_model=d_model,\r\n            sinkhorn_iters=sinkhorn_iterations,\r\n            key=keys[4],\r\n        )\r\n\r\n        self.mhc_ffn = ManifoldHyperConnection(\r\n            d_model=d_model,\r\n            sinkhorn_iters=sinkhorn_iterations,\r\n            key=keys[5],\r\n        )\r\n\r\n    def __call__(\r\n        self,\r\n        x: Float[Array, \"B N D\"],\r\n        causal: bool = True,\r\n    ) -> tuple[Float[Array, \"B N D\"], Float[Array, \"\"]]:\r\n        \"\"\"\r\n        Forward pass through the MAHA decoder block.\r\n\r\n        Args:\r\n            x: Input tensor [B, N, D].", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/blocks/decoder_block.py", "file_name": "decoder_block.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "909", "text": "Args:\r\n            x: Input tensor [B, N, D].\r\n            causal: Whether to apply causal masking.\r\n\r\n        Returns:\r\n            Tuple of:\r\n            - output: Block output [B, N, D]\r\n            - aux_loss: Auxiliary loss from aggregation\r\n        \"\"\"\r\n        # === Attention Branch ===\r\n        residual = x\r\n\r\n        # Pre-norm (vmap over batch and sequence)\r\n        x_normed = jax.vmap(jax.vmap(self.norm_attn))(x)\r\n\r\n        # Hierarchical decomposition\r\n        scales = self.decomposition(x_normed)\r\n\r\n        # Multiscale attention\r\n        attn_scales = self.attention(scales, self.decomposition, causal)\r\n\r\n        # Aggregation\r\n        attn_out, aux_loss = self.aggregation(attn_scales)\r\n\r\n        # mHC residual connection (manifold-constrained)\r\n        x = self.mhc_attn(residual, attn_out)\r\n\r\n        # === FFN Branch ===\r\n        residual = x\r\n\r\n        # Pre-norm\r\n        x_normed = jax.vmap(jax.vmap(self.norm_ffn))(x)\r\n\r\n        # SwiGLU FFN (vmap over batch and sequence)\r\n        ffn_out = jax.vmap(jax.vmap(self.ffn))(x_normed)\r\n\r\n        # mHC residual connection\r\n        x = self.mhc_ffn(residual, ffn_out)\r\n\r\n        return x, aux_loss\r\n\r\n\r\nclass MAHADecoderBlockLite(eqx.Module):\r\n    \"\"\"\r\n    Lightweight MAHA block with standard residual connections.\r\n\r\n    Uses standard residual addition instead of mHC for faster inference.\r\n    For ablation studies and resource-constrained settings.\r\n    \"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/blocks/decoder_block.py", "file_name": "decoder_block.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "910", "text": "Uses standard residual addition instead of mHC for faster inference.\r\n    For ablation studies and resource-constrained settings.\r\n    \"\"\"\r\n\r\n    norm_attn: RMSNorm\r\n    norm_ffn: RMSNorm\r\n    decomposition: HierarchicalDecomposition\r\n    attention: MultiscaleAttention\r\n    aggregation: OptimizationAggregation\r\n    ffn: SwiGLUFFN\r\n\r\n    d_model: int = eqx.field(static=True)\r\n\r\n    def __init__(\r\n        self,\r\n        d_model: int,\r\n        num_heads: int,\r\n        num_scales: int,\r\n        max_seq_len: int,\r\n        compression_ratio: int = 2,\r\n        ffn_multiplier: float = 2.67,\r\n        aggregation: Literal[\"nash\", \"convex\"] = \"nash\",\r\n        nash_iterations: int = 3,\r\n        *,\r\n        key: PRNGKeyArray,\r\n    ):\r\n        self.d_model = d_model\r\n\r\n        keys = jax.random.split(key, 4)\r\n\r\n        self.norm_attn = RMSNorm(d_model)\r\n        self.norm_ffn = RMSNorm(d_model)\r\n\r\n        self.decomposition = HierarchicalDecomposition(\r\n            d_model=d_model,\r\n            num_scales=num_scales,\r\n            compression_ratio=compression_ratio,\r\n            key=keys[0],\r\n        )\r\n\r\n        self.attention = MultiscaleAttention(\r\n            d_model=d_model,\r\n            num_heads=num_heads,\r\n            num_scales=num_scales,\r\n            max_seq_len=max_seq_len,\r\n            compression_ratio=compression_ratio,\r\n            key=keys[1],\r\n        )\r\n\r\n        self.aggregation = OptimizationAggregation(\r\n            num_scales=num_scales,\r\n            strategy=aggregation,\r\n            nash_iterations=nash_iterations,\r\n            key=keys[2],\r\n        )\r\n\r\n        self.ffn = SwiGLUFFN(\r\n            d_model=d_model,\r\n            ffn_multiplier=ffn_multiplier,\r\n            key=keys[3],\r\n        )\r\n\r\n    def __call__(\r\n        self,\r\n        x: Float[Array, \"B N D\"],\r\n        causal: bool = True,\r\n    ) -> tuple[Float[Array, \"B N D\"], Float[Array, \"\"]]:\r\n        \"\"\"Forward with standard residual connections.\"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/blocks/decoder_block.py", "file_name": "decoder_block.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "911", "text": "# Attention\r\n        residual = x\r\n        x_normed = jax.vmap(jax.vmap(self.norm_attn))(x)\r\n        scales = self.decomposition(x_normed)\r\n        attn_scales = self.attention(scales, self.decomposition, causal)\r\n        attn_out, aux_loss = self.aggregation(attn_scales)\r\n        x = residual + attn_out  # Standard residual\r\n\r\n        # FFN\r\n        residual = x\r\n        x_normed = jax.vmap(jax.vmap(self.norm_ffn))(x)\r\n        ffn_out = jax.vmap(jax.vmap(self.ffn))(x_normed)\r\n        x = residual + ffn_out  # Standard residual\r\n\r\n        return x, aux_loss", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/blocks/decoder_block.py", "file_name": "decoder_block.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "912", "text": "\"\"\"Data pipeline exports.\"\"\"\n\nfrom nash_mhc.data.tokenizer import TokenizerAdapter, TokenizerConfig\nfrom nash_mhc.data.datasets import DatasetConfig, load_text_dataset\nfrom nash_mhc.data.loader import (\n    LoaderConfig,\n    SequenceBatch,\n    build_sequence_dataset,\n    create_grain_dataset,\n    iterate_batches,\n)\n\n__all__ = [\n    \"TokenizerAdapter\",\n    \"TokenizerConfig\",\n    \"DatasetConfig\",\n    \"load_text_dataset\",\n    \"LoaderConfig\",\n    \"SequenceBatch\",\n    \"build_sequence_dataset\",\n    \"create_grain_dataset\",\n    \"iterate_batches\",\n]", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/data/__init__.py", "file_name": "__init__.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "913", "text": "\"\"\"HF dataset helpers for Grain-based loaders.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Iterator, Sequence\n\nimport numpy as np\nfrom datasets import Dataset, IterableDataset, load_dataset\n\nfrom nash_mhc.data.tokenizer import TokenizerAdapter, TokenizerOutput\n\n\n@dataclass(frozen=True, slots=True)\nclass DatasetConfig:\n    \"\"\"Declarative config for HF datasets.\"\"\"\n\n    path: str\n    split: str\n    name: str | None = None\n    text_field: str = \"text\"\n    streaming: bool = False\n\n\ndef load_text_dataset(config: DatasetConfig) -> Dataset | IterableDataset:\n    \"\"\"Load a HF dataset respecting streaming mode.\"\"\"\n    return load_dataset(\n        path=config.path,\n        name=config.name,\n        split=config.split,\n        streaming=config.streaming,\n    )\n\n\ndef iter_tokenized_sequences(\n    dataset: Dataset | IterableDataset,\n    tokenizer: TokenizerAdapter,\n    *,\n    text_field: str,\n) -> Iterator[TokenizerOutput]:\n    \"\"\"Yield fixed-length tokenized sequences.\"\"\"\n    for row in dataset:\n        text = row[text_field]\n        if not isinstance(text, str):\n            text = str(text)\n        yield tokenizer.encode(text)\n\n\ndef materialize_sequences(\n    dataset: Dataset | IterableDataset,\n    tokenizer: TokenizerAdapter,\n    *,\n    text_field: str,\n    max_sequences: int | None = None,\n) -> list[dict[str, np.ndarray]]:\n    \"\"\"Tokenize dataset rows eagerly to feed Grain MapDatasets.\"\"\"\n    sequences: list[dict[str, np.ndarray]] = []\n    for idx, item in enumerate(iter_tokenized_sequences(dataset, tokenizer, text_field=text_field)):\n        sequences.append(\n            {\n                \"input_ids\": np.asarray(item.input_ids, dtype=np.int32),\n                \"attention_mask\": np.asarray(item.attention_mask, dtype=np.int32),\n            }\n        )\n        if max_sequences is not None and idx + 1 >= max_sequences:\n            break\n    return sequences", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/data/datasets.py", "file_name": "datasets.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "914", "text": "\"\"\"Grain-based batching for MAHA training.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Iterable, Iterator, Sequence\n\nimport grain\nimport jax.numpy as jnp\nimport numpy as np\nfrom jaxtyping import Int, Array\n\n\nfrom nash_mhc.data.datasets import (\n    DatasetConfig,\n    load_text_dataset,\n    materialize_sequences,\n)\nfrom nash_mhc.data.tokenizer import TokenizerAdapter\n\n\nclass _SequenceMapDataset(grain.MapDataset[dict[str, np.ndarray]]):\n    def __init__(self, sequences: Sequence[dict[str, np.ndarray]]):\n        super().__init__()\n        self._sequences = tuple(sequences)\n\n    def __len__(self) -> int:\n        return len(self._sequences)\n\n    def __getitem__(self, index):  # type: ignore[override]\n        if 0 <= index < len(self._sequences):\n            return self._sequences[index]\n        return None\n\n\n@dataclass(frozen=True, slots=True)\nclass SequenceBatch:\n    token_ids: Int[Array, \"B N\"]\n    attention_mask: Int[Array, \"B N\"]\n\n\n@dataclass(frozen=True, slots=True)\nclass LoaderConfig:\n    batch_size: int\n    shuffle_seed: int = 0\n    num_epochs: int = 1\n    drop_remainder: bool = True\n\n    def __post_init__(self) -> None:\n        if self.batch_size <= 0:\n            raise ValueError(f\"batch_size must be positive, got {self.batch_size}\")\n        if self.num_epochs <= 0:\n            raise ValueError(f\"num_epochs must be positive, got {self.num_epochs}\")\n\n\ndef _to_jax_batch(batch: dict[str, np.ndarray]) -> SequenceBatch:\n    return SequenceBatch(\n        token_ids=jnp.asarray(batch[\"input_ids\"]),\n        attention_mask=jnp.asarray(batch[\"attention_mask\"]),\n    )", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/data/loader.py", "file_name": "loader.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-03"}}
{"id": "915", "text": "def _to_jax_batch(batch: dict[str, np.ndarray]) -> SequenceBatch:\n    return SequenceBatch(\n        token_ids=jnp.asarray(batch[\"input_ids\"]),\n        attention_mask=jnp.asarray(batch[\"attention_mask\"]),\n    )\n\n\ndef build_sequence_dataset(\n    dataset_config: DatasetConfig,\n    tokenizer: TokenizerAdapter,\n    *,\n    max_sequences: int | None = None,\n) -> Sequence[dict[str, np.ndarray]]:\n    \"\"\"Materialize a bounded set of tokenized sequences for training.\"\"\"\n    ds = load_text_dataset(dataset_config)\n    return materialize_sequences(\n        ds,\n        tokenizer,\n        text_field=dataset_config.text_field,\n        max_sequences=max_sequences,\n    )\n\n\ndef create_grain_dataset(\n    sequences: Sequence[dict[str, np.ndarray]],\n    *,\n    config: LoaderConfig,\n) -> grain.MapDataset[SequenceBatch]:\n    \"\"\"Create a Grain pipeline with shuffle, repeat, and batching.\"\"\"\n    dataset = _SequenceMapDataset(sequences).seed(config.shuffle_seed)\n    if len(sequences) == 0:\n        raise ValueError(\"No sequences provided to Grain dataset\")\n    dataset = dataset.shuffle(seed=config.shuffle_seed)\n    dataset = dataset.repeat(num_epochs=config.num_epochs, reseed_each_epoch=True)\n    dataset = dataset.batch(config.batch_size, drop_remainder=config.drop_remainder)\n    dataset = dataset.map(_to_jax_batch)\n    return dataset\n\n\ndef iterate_batches(\n    dataset: grain.MapDataset[SequenceBatch],\n) -> Iterator[SequenceBatch]:\n    \"\"\"Yield batches from the Grain dataset.\"\"\"\n    for batch in dataset:\n        yield batch", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/data/loader.py", "file_name": "loader.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-03"}}
{"id": "916", "text": "\"\"\"Tokenizer adapters with invariant enforcement.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Any, Protocol, Sequence\n\nimport numpy as np\nimport numpy.typing as npt\n\n\nclass TokenizerLike(Protocol):\n    \"\"\"Minimal HF tokenizer protocol.\"\"\"\n\n    def __call__(\n        self,\n        text: str | Sequence[str],\n        *,\n        max_length: int,\n        padding: str,\n        truncation: bool,\n        return_attention_mask: bool,\n        return_tensors: str | None = None,\n    ) -> Any: ...\n\n    @property\n    def pad_token_id(self) -> int | None: ...\n\n    @property\n    def eos_token_id(self) -> int | None: ...\n\n\n@dataclass(frozen=True, slots=True)\nclass TokenizerConfig:\n    \"\"\"Tokenizer hyperparameters aligned with `ModelConfig`.\"\"\"\n\n    max_length: int\n    pad_id: int | None = None\n    eos_id: int | None = None\n\n    def __post_init__(self) -> None:\n        if self.max_length <= 0:\n            raise ValueError(f\"max_length must be positive, got {self.max_length}\")\n\n\n@dataclass(frozen=True, slots=True)\nclass TokenizerOutput:\n    \"\"\"Canonical tokenized sequence representation using numpy arrays for data loading boundary.\"\"\"\n\n    input_ids: npt.NDArray[np.int32]\n    attention_mask: npt.NDArray[np.int32]\n\n\nclass TokenizerAdapter:\n    \"\"\"Wraps a Hugging Face tokenizer-like object with strict outputs.\"\"\"\n\n    def __init__(self, tokenizer: TokenizerLike, config: TokenizerConfig):\n        self._tokenizer = tokenizer\n        pad = config.pad_id or getattr(tokenizer, \"pad_token_id\", None)\n        if pad is None:\n            raise ValueError(\"Tokenizer must define pad_token_id\")\n        eos = config.eos_id or getattr(tokenizer, \"eos_token_id\", pad)\n        self._pad_id = int(pad)\n        self._eos_id = int(eos)\n        self._config = config\n\n    @property\n    def pad_id(self) -> int:\n        return self._pad_id", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/data/tokenizer.py", "file_name": "tokenizer.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-03"}}
{"id": "917", "text": "@property\n    def pad_id(self) -> int:\n        return self._pad_id\n\n    @property\n    def eos_id(self) -> int:\n        return self._eos_id\n\n    @property\n    def max_length(self) -> int:\n        return self._config.max_length\n\n    def encode(self, text: str) -> TokenizerOutput:\n        \"\"\"Tokenize a single string with padding/truncation.\"\"\"\n        encoded = self._tokenizer(\n            text,\n            max_length=self._config.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_attention_mask=True,\n        )\n        input_ids = np.asarray(encoded[\"input_ids\"], dtype=np.int32)\n        if input_ids.ndim == 2:\n            input_ids = input_ids[0]\n        attn = np.asarray(encoded[\"attention_mask\"], dtype=np.int32)\n        if attn.ndim == 2:\n            attn = attn[0]\n        if input_ids.shape[0] != self._config.max_length:\n            raise ValueError(\n                f\"Tokenized length {input_ids.shape[0]} must equal max_length {self._config.max_length}\"\n            )\n        return TokenizerOutput(input_ids=input_ids, attention_mask=attn)\n\n    def batch_encode(self, texts: Sequence[str]) -> list[TokenizerOutput]:\n        \"\"\"Tokenize multiple strings.\"\"\"\n        batch = self._tokenizer(\n            list(texts),\n            max_length=self._config.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_attention_mask=True,\n        )\n        ids = np.asarray(batch[\"input_ids\"], dtype=np.int32)\n        attn = np.asarray(batch[\"attention_mask\"], dtype=np.int32)\n        outputs: list[TokenizerOutput] = []\n        for i in range(ids.shape[0]):\n            outputs.append(\n                TokenizerOutput(\n                    input_ids=ids[i],\n                    attention_mask=attn[i],\n                )\n            )\n        return outputs", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/data/tokenizer.py", "file_name": "tokenizer.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-03"}}
{"id": "918", "text": "\"\"\"Equinox layer modules for the Deep-Equilibrium engine.\"\"\"\r\n\r\nfrom nash_mhc.layers.mhc import ManifoldHyperConnection\r\nfrom nash_mhc.layers.decomposition import HierarchicalDecomposition\r\nfrom nash_mhc.layers.attention import MultiscaleAttention\r\nfrom nash_mhc.layers.aggregation import OptimizationAggregation\r\nfrom nash_mhc.layers.ffn import SwiGLUFFN, RMSNorm\r\n\r\n__all__ = [\r\n    \"ManifoldHyperConnection\",\r\n    \"HierarchicalDecomposition\",\r\n    \"MultiscaleAttention\",\r\n    \"OptimizationAggregation\",\r\n    \"SwiGLUFFN\",\r\n    \"RMSNorm\",\r\n]", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/layers/__init__.py", "file_name": "__init__.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "919", "text": "\"\"\"\r\nOptimization-Driven Aggregation layer.\r\n\r\nAggregates outputs from multiple hierarchical scales using either\r\nNash equilibrium or convex optimization strategies.\r\n\r\nReference: Paper 2512.14925v2 (MAHA), Section 4.3\r\n\"\"\"\r\n\r\nfrom typing import Literal, cast\r\n\r\nimport jax\r\nimport jax.numpy as jnp\r\nimport equinox as eqx\r\nfrom jaxtyping import Float, Array\r\n\r\nfrom nash_mhc.primitives.nash_solver import nash_best_response, compute_sparsity_loss\r\nfrom nash_mhc.primitives.upsample import nearest_upsample\r\n\r\n\r\nclass OptimizationAggregation(eqx.Module):\r\n    \"\"\"\r\n    Optimization-driven aggregation of multiscale outputs.\r\n\r\n    Strategies:\r\n    - 'nash': Game-theoretic equilibrium via best-response dynamics\r\n    - 'convex': Learned simplex weights with L1 sparsity regularization\r\n\r\n    Nash equilibrium models each scale as a \"player\" competing to minimize\r\n    reconstruction error to the consensus. Convex uses learned weights\r\n    with sparsity penalty to focus on informative scales.\r\n    \"\"\"\r\n\r\n    strategy: Literal[\"nash\", \"convex\"] = eqx.field(static=True)\r\n    num_scales: int = eqx.field(static=True)\r\n    nash_iterations: int = eqx.field(static=True)\r\n    lambda_sparsity: float = eqx.field(static=True)\r\n\r\n    # Convex strategy parameters (only used when strategy=\"convex\")\r\n    convex_logits: Float[Array, \"L\"] | None\r\n\r\n    def __init__(\r\n        self,\r\n        num_scales: int,\r\n        strategy: Literal[\"nash\", \"convex\"] = \"nash\",\r\n        nash_iterations: int = 3,\r\n        lambda_sparsity: float = 0.1,\r\n        *,\r\n        key: jax.Array,\r\n    ):\r\n        \"\"\"\r\n        Args:\r\n            num_scales: Number of hierarchical scales (L).\r\n            strategy: Aggregation strategy ('nash' or 'convex').\r\n            nash_iterations: Number of best-response iterations for Nash.\r\n            lambda_sparsity: L1 regularization strength for convex.\r\n            key: PRNG key (unused for Nash, used for convex initialization).\r\n        \"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/layers/aggregation.py", "file_name": "aggregation.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-03"}}
{"id": "920", "text": "nash_iterations: Number of best-response iterations for Nash.\r\n            lambda_sparsity: L1 regularization strength for convex.\r\n            key: PRNG key (unused for Nash, used for convex initialization).\r\n        \"\"\"\r\n        self.strategy = strategy\r\n        self.num_scales = num_scales\r\n        self.nash_iterations = nash_iterations\r\n        self.lambda_sparsity = lambda_sparsity\r\n\r\n        if strategy == \"convex\":\r\n            # Initialize uniform (zeros -> uniform after softmax)\r\n            self.convex_logits = jnp.zeros(num_scales)\r\n        else:\r\n            self.convex_logits = None\r\n\r\n    def __call__(\r\n        self,\r\n        scale_outputs: tuple[Float[Array, \"B N_l D\"], ...],\r\n    ) -> tuple[Float[Array, \"B N D\"], Float[Array, \"\"]]:\r\n        \"\"\"\r\n        Aggregate scale outputs to original sequence length.\r\n\r\n        Args:\r\n            scale_outputs: Tuple of tensors with decreasing sequence lengths.\r\n                          scale_outputs[0] has the longest sequence.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/layers/aggregation.py", "file_name": "aggregation.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-03"}}
{"id": "921", "text": "Args:\r\n            scale_outputs: Tuple of tensors with decreasing sequence lengths.\r\n                          scale_outputs[0] has the longest sequence.\r\n\r\n        Returns:\r\n            Tuple of:\r\n            - aggregated: Combined output [B, N, D]\r\n            - aux_loss: Auxiliary loss (sparsity for convex, 0 for Nash)\r\n        \"\"\"\r\n        target_len = scale_outputs[0].shape[1]\r\n\r\n        # Upsample all scales to target length\r\n        upsampled = [scale_outputs[0]]  # First scale already at target length\r\n        for out in scale_outputs[1:]:\r\n            upsampled.append(nearest_upsample(out, target_len))\r\n\r\n        # Stack for aggregation: list of [B, N, D] -> [B, L, N, D]\r\n        stacked = jnp.stack(upsampled, axis=1)\r\n\r\n        if self.strategy == \"nash\":\r\n            result = cast(\r\n                tuple[Float[Array, \"B N D\"], Float[Array, \"B L\"]],\r\n                nash_best_response(stacked, self.nash_iterations),\r\n            )\r\n            aggregated = result[0]\r\n            weights: Float[Array, \"B L\"] | Float[Array, \"L\"] = result[1]\r\n            aux_loss = jnp.array(0.0)\r\n\r\n        else:\r\n            assert self.convex_logits is not None\r\n            weights = jax.nn.softmax(self.convex_logits)\r\n            aggregated = jnp.einsum(\"l,blnd->bnd\", weights, stacked)\r\n            aux_loss = compute_sparsity_loss(weights, self.lambda_sparsity)\r\n\r\n        return aggregated, aux_loss\r\n\r\n    def get_weights(\r\n        self,\r\n        scale_outputs: tuple[Float[Array, \"B N_l D\"], ...] | None = None,\r\n    ) -> Float[Array, \"... L\"]:\r\n        \"\"\"\r\n        Get current aggregation weights.\r\n\r\n        For Nash, requires scale_outputs to compute equilibrium.\r\n        For convex, returns learned weights.\r\n\r\n        Args:\r\n            scale_outputs: Required for Nash strategy.\r\n\r\n        Returns:\r\n            Aggregation weights.\r\n        \"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/layers/aggregation.py", "file_name": "aggregation.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-03"}}
{"id": "922", "text": "For Nash, requires scale_outputs to compute equilibrium.\r\n        For convex, returns learned weights.\r\n\r\n        Args:\r\n            scale_outputs: Required for Nash strategy.\r\n\r\n        Returns:\r\n            Aggregation weights.\r\n        \"\"\"\r\n        if self.strategy == \"convex\":\r\n            return jax.nn.softmax(self.convex_logits)\r\n\r\n        if scale_outputs is None:\r\n            raise ValueError(\"scale_outputs required for Nash strategy\")\r\n\r\n        target_len = scale_outputs[0].shape[1]\r\n        upsampled = [scale_outputs[0]]\r\n        for out in scale_outputs[1:]:\r\n            upsampled.append(nearest_upsample(out, target_len))\r\n        stacked = jnp.stack(upsampled, axis=1)\r\n\r\n        _, weights = nash_best_response(stacked, self.nash_iterations)\r\n        return weights\r\n\r\n\r\nclass AdaptiveAggregation(eqx.Module):\r\n    \"\"\"\r\n    Content-adaptive aggregation with learned gating.\r\n\r\n    Uses a learned projection to compute per-position, per-scale gates,\r\n    allowing the model to dynamically weight scales based on content.\r\n    \"\"\"\r\n\r\n    gate_proj: eqx.nn.Linear\r\n    num_scales: int = eqx.field(static=True)\r\n    d_model: int = eqx.field(static=True)\r\n\r\n    def __init__(\r\n        self,\r\n        d_model: int,\r\n        num_scales: int,\r\n        *,\r\n        key: jax.Array,\r\n    ):\r\n        self.d_model = d_model\r\n        self.num_scales = num_scales\r\n\r\n        # Project from concatenated scales to gate logits\r\n        # Input: [B, N, L * D] -> Output: [B, N, L]\r\n        self.gate_proj = eqx.nn.Linear(d_model, num_scales, key=key)\r\n\r\n    def __call__(\r\n        self,\r\n        scale_outputs: tuple[Float[Array, \"B N D\"], ...],\r\n    ) -> tuple[Float[Array, \"B N D\"], Float[Array, \"\"]]:\r\n        \"\"\"\r\n        Aggregate with content-adaptive gating.\r\n\r\n        Uses scale 0 features to compute position-dependent gates.\r\n        \"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/layers/aggregation.py", "file_name": "aggregation.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-03"}}
{"id": "923", "text": "Uses scale 0 features to compute position-dependent gates.\r\n        \"\"\"\r\n        target_len = scale_outputs[0].shape[1]\r\n\r\n        # Upsample and stack\r\n        upsampled = [scale_outputs[0]]\r\n        for out in scale_outputs[1:]:\r\n            upsampled.append(nearest_upsample(out, target_len))\r\n        stacked = jnp.stack(upsampled, axis=1)  # [B, L, N, D]\r\n\r\n        # Compute gates from scale 0 features\r\n        gate_logits = jax.vmap(jax.vmap(self.gate_proj))(scale_outputs[0])  # [B, N, L]\r\n        gates = jax.nn.softmax(gate_logits, axis=-1)\r\n\r\n        # Apply gates: [B, L, N, D] * [B, N, L] (broadcast) -> [B, N, D]\r\n        gates_expanded = gates[:, :, :, None]  # [B, N, L, 1]\r\n        stacked_transposed = jnp.transpose(stacked, (0, 2, 1, 3))  # [B, N, L, D]\r\n        aggregated = jnp.sum(stacked_transposed * gates_expanded, axis=2)  # [B, N, D]\r\n\r\n        return aggregated, jnp.array(0.0)", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/layers/aggregation.py", "file_name": "aggregation.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-03"}}
{"id": "924", "text": "\"\"\"\r\nMultiscale Attention layer with shared Value projection.\r\n\r\nImplements the MAHA attention mechanism where each scale computes\r\nattention independently with per-scale Q/K but shared V.\r\n\r\nReference: Paper 2512.14925v2 (MAHA), Section 4.2\r\n\"\"\"\r\n\r\nimport jax\r\nimport jax.numpy as jnp\r\nimport equinox as eqx\r\nfrom jaxtyping import Float, Array\r\nfrom typing import Callable\r\n\r\nfrom nash_mhc.primitives.rope import apply_rope_per_scale, compute_rope_freqs\r\nfrom nash_mhc.layers.decomposition import HierarchicalDecomposition\r\n\r\n\r\nclass MultiscaleAttention(eqx.Module):\r\n    \"\"\"\r\n    MAHA multiscale attention with shared V projection.\r\n\r\n    Key features:\r\n    - Per-scale Q, K projections (scale-specific)\r\n    - Shared V projection across all scales (parameter efficient)\r\n    - Per-scale RoPE with adjusted position indices\r\n    - Causal masking for autoregressive LM\r\n    \"\"\"\r\n\r\n    q_projs: tuple[eqx.nn.Linear, ...]\r\n    k_projs: tuple[eqx.nn.Linear, ...]\r\n    v_proj: eqx.nn.Linear\r\n    o_proj: eqx.nn.Linear\r\n\r\n    num_heads: int = eqx.field(static=True)\r\n    head_dim: int = eqx.field(static=True)\r\n    num_scales: int = eqx.field(static=True)\r\n    d_model: int = eqx.field(static=True)\r\n    compression_ratio: int = eqx.field(static=True)\r\n\r\n    # RoPE cache\r\n    rope_freqs: Float[Array, \"M K2\"]\r\n\r\n    def __init__(\r\n        self,\r\n        d_model: int,\r\n        num_heads: int,\r\n        num_scales: int,\r\n        max_seq_len: int,\r\n        compression_ratio: int = 2,\r\n        *,\r\n        key: jax.Array,\r\n    ):\r\n        \"\"\"\r\n        Args:\r\n            d_model: Model dimension.\r\n            num_heads: Number of attention heads.\r\n            num_scales: Number of hierarchical scales.\r\n            max_seq_len: Maximum sequence length (for RoPE cache).\r\n            compression_ratio: Downsampling ratio between scales.\r\n            key: PRNG key for initialization.\r\n        \"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/layers/attention.py", "file_name": "attention.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-03"}}
{"id": "925", "text": "num_scales: Number of hierarchical scales.\r\n            max_seq_len: Maximum sequence length (for RoPE cache).\r\n            compression_ratio: Downsampling ratio between scales.\r\n            key: PRNG key for initialization.\r\n        \"\"\"\r\n        if d_model % num_heads != 0:\r\n            raise ValueError(\r\n                f\"d_model ({d_model}) must be divisible by num_heads ({num_heads})\"\r\n            )\r\n\r\n        self.d_model = d_model\r\n        self.num_heads = num_heads\r\n        self.head_dim = d_model // num_heads\r\n        self.num_scales = num_scales\r\n        self.compression_ratio = compression_ratio\r\n\r\n        # Split keys for all components\r\n        keys = jax.random.split(key, 2 * num_scales + 2)\r\n\r\n        # Per-scale Q, K projections\r\n        self.q_projs = tuple(\r\n            eqx.nn.Linear(d_model, d_model, key=keys[i]) for i in range(num_scales)\r\n        )\r\n        self.k_projs = tuple(\r\n            eqx.nn.Linear(d_model, d_model, key=keys[num_scales + i])\r\n            for i in range(num_scales)\r\n        )\r\n\r\n        # Shared V and output projections\r\n        self.v_proj = eqx.nn.Linear(d_model, d_model, key=keys[-2])\r\n        self.o_proj = eqx.nn.Linear(d_model, d_model, key=keys[-1])\r\n\r\n        # Precompute RoPE frequencies\r\n        self.rope_freqs = compute_rope_freqs(self.head_dim, max_seq_len)\r\n\r\n    def _reshape_for_attention(\r\n        self,\r\n        x: Float[Array, \"B N D\"],\r\n    ) -> Float[Array, \"B N H K\"]:\r\n        \"\"\"Reshape [B, N, D] -> [B, N, H, K] for attention.\"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/layers/attention.py", "file_name": "attention.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-03"}}
{"id": "926", "text": "B, N, D = x.shape\r\n        return x.reshape(B, N, self.num_heads, self.head_dim)\r\n\r\n    def _single_scale_attention(\r\n        self,\r\n        x: Float[Array, \"B N D\"],\r\n        v: Float[Array, \"B N D\"],\r\n        scale_idx: int,\r\n        causal: bool = True,\r\n    ) -> Float[Array, \"B N D\"]:\r\n        \"\"\"\r\n        Compute attention for a single scale.\r\n\r\n        Args:\r\n            x: Input for this scale [B, N_l, D].\r\n            v: Value tensor for this scale [B, N_l, D].\r\n            scale_idx: Hierarchical scale index.\r\n            causal: Whether to apply causal masking.\r\n\r\n        Returns:\r\n            Attention output [B, N_l, D].\r\n        \"\"\"\r\n        B, N, D = x.shape\r\n\r\n        q = jax.vmap(jax.vmap(self.q_projs[scale_idx]))(x)\r\n        k = jax.vmap(jax.vmap(self.k_projs[scale_idx]))(x)\r\n\r\n        # Reshape for multi-head attention\r\n        q = self._reshape_for_attention(q)  # [B, N, H, K]\r\n        k = self._reshape_for_attention(k)\r\n        v_heads = self._reshape_for_attention(v)\r\n\r\n        # Apply per-scale RoPE\r\n        q = apply_rope_per_scale(q, self.rope_freqs, scale_idx, self.compression_ratio)\r\n        k = apply_rope_per_scale(k, self.rope_freqs, scale_idx, self.compression_ratio)\r\n\r\n        # Transpose for attention: [B, N, H, K] -> [B, H, N, K]\r\n        q = jnp.transpose(q, (0, 2, 1, 3))\r\n        k = jnp.transpose(k, (0, 2, 1, 3))\r\n        v_heads = jnp.transpose(v_heads, (0, 2, 1, 3))\r\n\r\n        scale = 1.0 / jnp.sqrt(jnp.array(self.head_dim, dtype=jnp.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/layers/attention.py", "file_name": "attention.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-03"}}
{"id": "927", "text": "transpose(v_heads, (0, 2, 1, 3))\r\n\r\n        scale = 1.0 / jnp.sqrt(jnp.array(self.head_dim, dtype=jnp.float32))\r\n        scores = jnp.einsum(\"bhqk,bhmk->bhqm\", q, k) * scale\r\n\r\n        if causal:\r\n            mask = jnp.triu(jnp.ones((N, N), dtype=jnp.bool_), k=1)\r\n            scores = jnp.where(mask, jnp.finfo(scores.dtype).min, scores)\r\n\r\n        attn_weights = jax.nn.softmax(scores, axis=-1)\r\n        context = jnp.einsum(\"bhqm,bhmk->bhqk\", attn_weights, v_heads)\r\n\r\n        # Reshape back: [B, H, N, K] -> [B, N, D]\r\n        context = jnp.transpose(context, (0, 2, 1, 3)).reshape(B, N, D)\r\n\r\n        return context\r\n\r\n    def __call__(\r\n        self,\r\n        scale_inputs: tuple[Float[Array, \"B N D\"], ...],\r\n        decomposition: HierarchicalDecomposition,\r\n        causal: bool = True,\r\n    ) -> tuple[Float[Array, \"B N D\"], ...]:\r\n        \"\"\"\r\n        Compute attention for all scales.\r\n\r\n        V is computed once from scale 0, then decomposed to match each scale.\r\n\r\n        Args:\r\n            scale_inputs: Tuple of inputs for each scale.\r\n            decomposition: Decomposition module for V projection.\r\n            causal: Whether to apply causal masking.\r\n\r\n        Returns:\r\n            Tuple of attention outputs for each scale.\r\n        \"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/layers/attention.py", "file_name": "attention.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-03"}}
{"id": "928", "text": "Args:\r\n            scale_inputs: Tuple of inputs for each scale.\r\n            decomposition: Decomposition module for V projection.\r\n            causal: Whether to apply causal masking.\r\n\r\n        Returns:\r\n            Tuple of attention outputs for each scale.\r\n        \"\"\"\r\n        v_base = jax.vmap(jax.vmap(self.v_proj))(scale_inputs[0])\r\n        v_scales = decomposition.decompose_values(v_base)\r\n\r\n        outputs = []\r\n        for i, (x_l, v_l) in enumerate(zip(scale_inputs, v_scales)):\r\n            attn_out = self._single_scale_attention(x_l, v_l, i, causal)\r\n            out_l = jax.vmap(jax.vmap(self.o_proj))(attn_out)\r\n            outputs.append(out_l)\r\n\r\n        return tuple(outputs)\r\n\r\n\r\nclass ScaledDotProductAttention(eqx.Module):\r\n    \"\"\"\r\n    Standard scaled dot-product attention (single scale).\r\n\r\n    For use in non-MAHA contexts or as a baseline comparison.\r\n    \"\"\"\r\n\r\n    num_heads: int = eqx.field(static=True)\r\n    head_dim: int = eqx.field(static=True)\r\n\r\n    def __init__(self, d_model: int, num_heads: int):\r\n        self.num_heads = num_heads\r\n        self.head_dim = d_model // num_heads\r\n\r\n    def __call__(\r\n        self,\r\n        q: Float[Array, \"B N H K\"],\r\n        k: Float[Array, \"B N H K\"],\r\n        v: Float[Array, \"B N H K\"],\r\n        causal: bool = True,\r\n    ) -> Float[Array, \"B N H K\"]:\r\n        \"\"\"Standard scaled dot-product attention.\"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/layers/attention.py", "file_name": "attention.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-03"}}
{"id": "929", "text": "B, N, H, K = q.shape\r\n\r\n        # Transpose: [B, N, H, K] -> [B, H, N, K]\r\n        q = jnp.transpose(q, (0, 2, 1, 3))\r\n        k = jnp.transpose(k, (0, 2, 1, 3))\r\n        v = jnp.transpose(v, (0, 2, 1, 3))\r\n\r\n        scale = 1.0 / jnp.sqrt(K)\r\n        scores = jnp.einsum(\"bhqk,bhmk->bhqm\", q, k) * scale\r\n\r\n        if causal:\r\n            mask = jnp.triu(jnp.ones((N, N), dtype=jnp.bool_), k=1)\r\n            scores = jnp.where(mask, -1e9, scores)\r\n\r\n        attn_weights = jax.nn.softmax(scores, axis=-1)\r\n        context = jnp.einsum(\"bhqm,bhmk->bhqk\", attn_weights, v)\r\n\r\n        # Transpose back: [B, H, N, K] -> [B, N, H, K]\r\n        return jnp.transpose(context, (0, 2, 1, 3))", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/layers/attention.py", "file_name": "attention.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-03"}}
{"id": "930", "text": "\"\"\"\r\nHierarchical Multiscale Decomposition layer.\r\n\r\nDecomposes input sequences into multiple hierarchical scales using\r\nlearnable strided convolutions.\r\n\r\nReference: Paper 2512.14925v2 (MAHA), Section 4.1\r\n\"\"\"\r\n\r\nimport jax\r\nimport jax.numpy as jnp\r\nimport equinox as eqx\r\nfrom jaxtyping import Float, Array\r\n\r\nfrom nash_mhc.primitives.strided_conv import StridedConv1d\r\n\r\n\r\nclass HierarchicalDecomposition(eqx.Module):\r\n    \"\"\"\r\n    Hierarchical multiscale decomposition via strided convolutions.\r\n\r\n    Transforms input X into L hierarchical scales:\r\n        X -> [X_0, X_1, ..., X_{L-1}]\r\n\r\n    where:\r\n        X_0 = X (original input)\r\n        X_l = D_l(X_{l-1}) = Conv1D(X_{l-1}, stride=r)\r\n        len(X_l) = len(X_{l-1}) // r\r\n\r\n    Key design decisions:\r\n    - Returns tuple of arrays (pytree-friendly, not Python list)\r\n    - All scales share the same feature dimension D\r\n    - Uses learnable strided convolutions for downsampling\r\n    \"\"\"\r\n\r\n    downsamplers: tuple[StridedConv1d, ...]\r\n    num_scales: int = eqx.field(static=True)\r\n    compression_ratio: int = eqx.field(static=True)\r\n    d_model: int = eqx.field(static=True)\r\n\r\n    def __init__(\r\n        self,\r\n        d_model: int,\r\n        num_scales: int,\r\n        compression_ratio: int = 2,\r\n        kernel_size: int = 3,\r\n        *,\r\n        key: jax.Array,\r\n    ):\r\n        \"\"\"\r\n        Args:\r\n            d_model: Model dimension (preserved across scales).\r\n            num_scales: Number of hierarchical scales (L).\r\n            compression_ratio: Downsampling stride (r).\r\n            kernel_size: Convolution kernel size.\r\n            key: PRNG key for initialization.\r\n        \"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/layers/decomposition.py", "file_name": "decomposition.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "931", "text": "num_scales: Number of hierarchical scales (L).\r\n            compression_ratio: Downsampling stride (r).\r\n            kernel_size: Convolution kernel size.\r\n            key: PRNG key for initialization.\r\n        \"\"\"\r\n        self.d_model = d_model\r\n        self.num_scales = num_scales\r\n        self.compression_ratio = compression_ratio\r\n\r\n        # Create (L-1) downsamplers (scale 0 is identity)\r\n        keys = jax.random.split(key, num_scales - 1)\r\n\r\n        self.downsamplers = tuple(\r\n            StridedConv1d(\r\n                in_channels=d_model,\r\n                out_channels=d_model,\r\n                kernel_size=kernel_size,\r\n                stride=compression_ratio,\r\n                key=keys[i],\r\n            )\r\n            for i in range(num_scales - 1)\r\n        )\r\n\r\n    def __call__(\r\n        self,\r\n        x: Float[Array, \"B N D\"],\r\n    ) -> tuple[Float[Array, \"B N D\"], ...]:\r\n        \"\"\"\r\n        Decompose input into hierarchical scales.\r\n\r\n        Args:\r\n            x: Input tensor [B, N, D].\r\n\r\n        Returns:\r\n            Tuple of scale representations:\r\n                (\r\n                    [B, N, D],          # Scale 0 (original)\r\n                    [B, N/r, D],        # Scale 1\r\n                    [B, N/r², D],       # Scale 2\r\n                    ...\r\n                )\r\n        \"\"\"\r\n        outputs = [x]  # Scale 0 is the original input\r\n        current = x\r\n\r\n        for downsampler in self.downsamplers:\r\n            current = downsampler(current)\r\n            outputs.append(current)\r\n\r\n        return tuple(outputs)\r\n\r\n    def get_scale_lengths(self, input_len: int) -> tuple[int, ...]:\r\n        \"\"\"\r\n        Compute expected sequence lengths per scale.\r\n\r\n        Args:\r\n            input_len: Input sequence length (N).\r\n\r\n        Returns:\r\n            Tuple of lengths for each scale.\r\n        \"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/layers/decomposition.py", "file_name": "decomposition.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "932", "text": "Args:\r\n            input_len: Input sequence length (N).\r\n\r\n        Returns:\r\n            Tuple of lengths for each scale.\r\n        \"\"\"\r\n        lengths = [input_len]\r\n        curr = input_len\r\n\r\n        for _ in range(self.num_scales - 1):\r\n            # Account for padding in strided conv\r\n            # With kernel_size=3, padding=1, stride=r:\r\n            # out_len = (in_len + 2*1 - 3) // r + 1 = (in_len - 1) // r + 1\r\n            # For simplicity, we use floor division\r\n            curr = curr // self.compression_ratio\r\n            lengths.append(curr)\r\n\r\n        return tuple(lengths)\r\n\r\n    def decompose_values(\r\n        self,\r\n        v: Float[Array, \"B N D\"],\r\n    ) -> tuple[Float[Array, \"B N D\"], ...]:\r\n        \"\"\"\r\n        Decompose value tensor for shared V projection pattern.\r\n\r\n        This applies the same decomposition to a pre-projected value tensor,\r\n        enabling the shared V projection pattern in MAHA attention.\r\n\r\n        Args:\r\n            v: Value tensor [B, N, D] (already projected by shared W_V).\r\n\r\n        Returns:\r\n            Tuple of decomposed values for each scale.\r\n        \"\"\"\r\n        return self(v)\r\n\r\n\r\nclass HierarchicalDecompositionPool(eqx.Module):\r\n    \"\"\"\r\n    Non-learnable hierarchical decomposition via adaptive max pooling.\r\n\r\n    Alternative to strided convolution when learnable downsampling\r\n    is not desired.\r\n    \"\"\"\r\n\r\n    num_scales: int = eqx.field(static=True)\r\n    compression_ratio: int = eqx.field(static=True)\r\n    d_model: int = eqx.field(static=True)\r\n\r\n    def __init__(\r\n        self,\r\n        d_model: int,\r\n        num_scales: int,\r\n        compression_ratio: int = 2,\r\n    ):\r\n        self.d_model = d_model\r\n        self.num_scales = num_scales\r\n        self.compression_ratio = compression_ratio\r\n\r\n    def __call__(\r\n        self,\r\n        x: Float[Array, \"B N D\"],\r\n    ) -> tuple[Float[Array, \"B N D\"], ...]:\r\n        \"\"\"Decompose using max pooling (non-learnable).\"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/layers/decomposition.py", "file_name": "decomposition.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "933", "text": "outputs = [x]\r\n        current_len = x.shape[1]\r\n\r\n        for _ in range(self.num_scales - 1):\r\n            target_len = max(1, current_len // self.compression_ratio)\r\n\r\n            # Manual max pooling via reshape and reduce\r\n            # Reshape to [B, target_len, pool_size, D] and take max\r\n            pool_size = current_len // target_len\r\n            padded_len = target_len * pool_size\r\n\r\n            # Truncate to divisible length\r\n            x_truncated = x[:, :padded_len, :]\r\n\r\n            # Reshape and pool\r\n            x_reshaped = x_truncated.reshape(\r\n                x.shape[0], target_len, pool_size, self.d_model\r\n            )\r\n            x_pooled = jnp.max(x_reshaped, axis=2)\r\n\r\n            outputs.append(x_pooled)\r\n            x = x_pooled\r\n            current_len = target_len\r\n\r\n        return tuple(outputs)", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/layers/decomposition.py", "file_name": "decomposition.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "934", "text": "\"\"\"\r\nFeed-Forward Network layers.\r\n\r\nImplements SwiGLU FFN and RMSNorm for the transformer architecture.\r\n\r\nReference: GLU Variants Improve Transformer (Shazeer, 2020)\r\n\"\"\"\r\n\r\nimport jax\r\nimport jax.numpy as jnp\r\nimport equinox as eqx\r\nfrom jaxtyping import Float, Array\r\n\r\n\r\nclass RMSNorm(eqx.Module):\r\n    \"\"\"\r\n    Root Mean Square Layer Normalization.\r\n\r\n    Normalizes activations using RMS without centering (no mean subtraction).\r\n    More efficient than LayerNorm and works well for transformers.\r\n\r\n    Reference: Zhang & Sennrich (2019)\r\n    \"\"\"\r\n\r\n    weight: Float[Array, \"D\"]\r\n    eps: float = eqx.field(static=True)\r\n    d_model: int = eqx.field(static=True)\r\n\r\n    def __init__(self, d_model: int, eps: float = 1e-6):\r\n        \"\"\"\r\n        Args:\r\n            d_model: Feature dimension.\r\n            eps: Small constant for numerical stability.\r\n        \"\"\"\r\n        self.d_model = d_model\r\n        self.eps = eps\r\n        self.weight = jnp.ones(d_model)\r\n\r\n    def __call__(self, x: Float[Array, \"... D\"]) -> Float[Array, \"... D\"]:\r\n        \"\"\"\r\n        Apply RMS normalization.\r\n\r\n        Args:\r\n            x: Input tensor [..., D].\r\n\r\n        Returns:\r\n            Normalized tensor [..., D].\r\n        \"\"\"\r\n        # Compute RMS\r\n        rms = jnp.sqrt(jnp.mean(x ** 2, axis=-1, keepdims=True) + self.eps)\r\n\r\n        # Normalize and scale\r\n        return (x / rms) * self.weight\r\n\r\n\r\nclass SwiGLUFFN(eqx.Module):\r\n    \"\"\"\r\n    SwiGLU Feed-Forward Network.\r\n\r\n    Uses the SwiGLU activation (Swish-Gated Linear Unit) which has shown\r\n    improved performance over standard FFN with ReLU/GELU.\r\n\r\n    Architecture:\r\n        FFN(x) = (Swish(x @ W_gate) * (x @ W_up)) @ W_down\r\n\r\n    The hidden dimension is computed as:\r\n        hidden = int(d_model * ffn_multiplier * 2/3)\r\n\r\n    This accounts for the gating mechanism doubling the effective params.\r\n    \"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/layers/ffn.py", "file_name": "ffn.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "935", "text": "w_gate: eqx.nn.Linear\r\n    w_up: eqx.nn.Linear\r\n    w_down: eqx.nn.Linear\r\n    d_model: int = eqx.field(static=True)\r\n    hidden_dim: int = eqx.field(static=True)\r\n\r\n    def __init__(\r\n        self,\r\n        d_model: int,\r\n        ffn_multiplier: float = 2.67,\r\n        *,\r\n        key: jax.Array,\r\n    ):\r\n        \"\"\"\r\n        Args:\r\n            d_model: Model dimension.\r\n            ffn_multiplier: Hidden dimension multiplier.\r\n                           With SwiGLU, use 2.67 for ~4x effective expansion.\r\n            key: PRNG key for initialization.\r\n        \"\"\"\r\n        self.d_model = d_model\r\n\r\n        # Compute hidden dim with 2/3 factor for SwiGLU\r\n        # This ensures parameter count matches standard 4x FFN\r\n        self.hidden_dim = int(d_model * ffn_multiplier * 2 / 3)\r\n\r\n        # Round to multiple of 128 for TPU alignment\r\n        self.hidden_dim = ((self.hidden_dim + 127) // 128) * 128\r\n\r\n        k1, k2, k3 = jax.random.split(key, 3)\r\n\r\n        # Gate and up projections (both to hidden_dim)\r\n        self.w_gate = eqx.nn.Linear(d_model, self.hidden_dim, key=k1)\r\n        self.w_up = eqx.nn.Linear(d_model, self.hidden_dim, key=k2)\r\n\r\n        # Down projection back to d_model\r\n        self.w_down = eqx.nn.Linear(self.hidden_dim, d_model, key=k3)\r\n\r\n    def __call__(self, x: Float[Array, \"D\"]) -> Float[Array, \"D\"]:\r\n        \"\"\"\r\n        Apply SwiGLU FFN.\r\n\r\n        Args:\r\n            x: Input vector [D].\r\n\r\n        Returns:\r\n            Output vector [D].\r\n        \"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/layers/ffn.py", "file_name": "ffn.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "936", "text": "Args:\r\n            x: Input vector [D].\r\n\r\n        Returns:\r\n            Output vector [D].\r\n        \"\"\"\r\n        # Gate path with Swish activation\r\n        gate = jax.nn.swish(self.w_gate(x))\r\n\r\n        # Up path (linear)\r\n        up = self.w_up(x)\r\n\r\n        # Element-wise gating\r\n        hidden = gate * up\r\n\r\n        # Down projection\r\n        return self.w_down(hidden)\r\n\r\n\r\nclass GEGLUFFN(eqx.Module):\r\n    \"\"\"\r\n    GEGLU Feed-Forward Network.\r\n\r\n    Alternative to SwiGLU using GELU activation instead of Swish.\r\n    \"\"\"\r\n\r\n    w_gate: eqx.nn.Linear\r\n    w_up: eqx.nn.Linear\r\n    w_down: eqx.nn.Linear\r\n    d_model: int = eqx.field(static=True)\r\n    hidden_dim: int = eqx.field(static=True)\r\n\r\n    def __init__(\r\n        self,\r\n        d_model: int,\r\n        ffn_multiplier: float = 2.67,\r\n        *,\r\n        key: jax.Array,\r\n    ):\r\n        self.d_model = d_model\r\n        self.hidden_dim = int(d_model * ffn_multiplier * 2 / 3)\r\n        self.hidden_dim = ((self.hidden_dim + 127) // 128) * 128\r\n\r\n        k1, k2, k3 = jax.random.split(key, 3)\r\n        self.w_gate = eqx.nn.Linear(d_model, self.hidden_dim, key=k1)\r\n        self.w_up = eqx.nn.Linear(d_model, self.hidden_dim, key=k2)\r\n        self.w_down = eqx.nn.Linear(self.hidden_dim, d_model, key=k3)\r\n\r\n    def __call__(self, x: Float[Array, \"D\"]) -> Float[Array, \"D\"]:\r\n        gate = jax.nn.gelu(self.w_gate(x))\r\n        up = self.w_up(x)\r\n        hidden = gate * up\r\n        return self.w_down(hidden)\r\n\r\n\r\nclass StandardFFN(eqx.Module):\r\n    \"\"\"\r\n    Standard transformer FFN with GELU activation.\r\n\r\n    For comparison with SwiGLU variants.\r\n    \"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/layers/ffn.py", "file_name": "ffn.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "937", "text": "For comparison with SwiGLU variants.\r\n    \"\"\"\r\n\r\n    w_up: eqx.nn.Linear\r\n    w_down: eqx.nn.Linear\r\n    d_model: int = eqx.field(static=True)\r\n    hidden_dim: int = eqx.field(static=True)\r\n\r\n    def __init__(\r\n        self,\r\n        d_model: int,\r\n        ffn_multiplier: float = 4.0,\r\n        *,\r\n        key: jax.Array,\r\n    ):\r\n        self.d_model = d_model\r\n        self.hidden_dim = int(d_model * ffn_multiplier)\r\n        self.hidden_dim = ((self.hidden_dim + 127) // 128) * 128\r\n\r\n        k1, k2 = jax.random.split(key)\r\n        self.w_up = eqx.nn.Linear(d_model, self.hidden_dim, key=k1)\r\n        self.w_down = eqx.nn.Linear(self.hidden_dim, d_model, key=k2)\r\n\r\n    def __call__(self, x: Float[Array, \"D\"]) -> Float[Array, \"D\"]:\r\n        return self.w_down(jax.nn.gelu(self.w_up(x)))", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/layers/ffn.py", "file_name": "ffn.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "938", "text": "\"\"\"\r\nManifold-Constrained Hyper-Connection (mHC) layer.\r\n\r\nProjects residual connection weight matrices onto the Birkhoff polytope\r\nto ensure signal mean conservation and bounded spectral norm.\r\n\r\nReference: Paper 2512.24880v1\r\n\"\"\"\r\n\r\nimport jax\r\nimport jax.numpy as jnp\r\nimport equinox as eqx\r\nfrom jaxtyping import Float, Array\r\n\r\nfrom nash_mhc.primitives.sinkhorn import sinkhorn_knopp\r\n\r\n\r\nclass ManifoldHyperConnection(eqx.Module):\r\n    \"\"\"\r\n    Manifold-constrained hyper-connection layer.\r\n\r\n    Replaces standard residual connections with a manifold-constrained\r\n    mixing operation that preserves signal mean across layers.\r\n\r\n    The residual connection becomes:\r\n        output = H @ residual + (I - H) @ block_output\r\n\r\n    where H is projected onto the Birkhoff polytope (doubly stochastic matrices)\r\n    via Sinkhorn-Knopp algorithm.\r\n\r\n    Invariants:\r\n    - H is doubly stochastic: H @ 1 = 1, H.T @ 1 = 1, H >= 0\r\n    - ||H||_2 <= 1 (spectral norm bounded, non-expansive)\r\n    - Signal mean is conserved across layers\r\n    \"\"\"\r\n\r\n    log_alpha: Float[Array, \"D D\"]\r\n    layer_scale: Float[Array, \"D\"]\r\n    sinkhorn_iters: int = eqx.field(static=True)\r\n    d_model: int = eqx.field(static=True)\r\n\r\n    def __init__(\r\n        self,\r\n        d_model: int,\r\n        sinkhorn_iters: int = 10,\r\n        *,\r\n        key: jax.Array,\r\n    ):\r\n        \"\"\"\r\n        Args:\r\n            d_model: Model dimension.\r\n            sinkhorn_iters: Number of Sinkhorn-Knopp iterations.\r\n            key: PRNG key for initialization.\r\n        \"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/layers/mhc.py", "file_name": "mhc.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "939", "text": "sinkhorn_iters: Number of Sinkhorn-Knopp iterations.\r\n            key: PRNG key for initialization.\r\n        \"\"\"\r\n        self.d_model = d_model\r\n        self.sinkhorn_iters = sinkhorn_iters\r\n\r\n        k1, k2 = jax.random.split(key)\r\n\r\n        # Initialize log_alpha near zero (identity-like after Sinkhorn)\r\n        # Small noise breaks symmetry\r\n        noise = jax.random.normal(k1, (d_model, d_model)) * 0.01\r\n        self.log_alpha = noise\r\n\r\n        # Layer scale initialized to 1 (identity behavior)\r\n        self.layer_scale = jnp.ones(d_model)\r\n\r\n    def __call__(\r\n        self,\r\n        residual: Float[Array, \"B N D\"],\r\n        block_output: Float[Array, \"B N D\"],\r\n    ) -> Float[Array, \"B N D\"]:\r\n        \"\"\"\r\n        Apply manifold-constrained residual connection.\r\n\r\n        The operation computes:\r\n            H = sinkhorn(log_alpha)  # Doubly stochastic projection\r\n            output = (H @ residual + (I - H) @ block_output) * layer_scale\r\n\r\n        This can be rewritten as:\r\n            output = H @ (residual - block_output) + block_output\r\n            output = output * layer_scale\r\n\r\n        Args:\r\n            residual: Skip connection input [B, N, D].\r\n            block_output: Output from attention/FFN block [B, N, D].\r\n\r\n        Returns:\r\n            Mixed output [B, N, D] on the manifold.\r\n        \"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/layers/mhc.py", "file_name": "mhc.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "940", "text": "block_output: Output from attention/FFN block [B, N, D].\r\n\r\n        Returns:\r\n            Mixed output [B, N, D] on the manifold.\r\n        \"\"\"\r\n        # Project log_alpha to doubly stochastic matrix\r\n        H = sinkhorn_knopp(self.log_alpha, self.sinkhorn_iters)\r\n\r\n        # Compute residual contribution\r\n        # H @ residual: [D, D] @ [B, N, D] -> need einsum for batch handling\r\n        # einsum \"de,bne->bnd\" applies H to the D dimension\r\n        res_contrib = jnp.einsum(\"de,bne->bnd\", H, residual)\r\n\r\n        # Compute (I - H) @ block_output\r\n        I_minus_H = jnp.eye(self.d_model, dtype=H.dtype) - H\r\n        block_contrib = jnp.einsum(\"de,bne->bnd\", I_minus_H, block_output)\r\n\r\n        # Combine with learned layer scale\r\n        output = (res_contrib + block_contrib) * self.layer_scale\r\n\r\n        return output\r\n\r\n    def get_mixing_matrix(self) -> Float[Array, \"D D\"]:\r\n        \"\"\"Return the current doubly stochastic mixing matrix.\"\"\"\r\n        return sinkhorn_knopp(self.log_alpha, self.sinkhorn_iters)\r\n\r\n\r\nclass ManifoldHyperConnectionLite(eqx.Module):\r\n    \"\"\"\r\n    Lightweight mHC variant with per-channel mixing instead of full matrix.\r\n\r\n    Uses diagonal mixing weights (element-wise) instead of full matrix\r\n    for O(D) instead of O(D²) parameters and compute.\r\n\r\n    The residual connection becomes:\r\n        output = alpha * residual + (1 - alpha) * block_output\r\n\r\n    where alpha is constrained to [0, 1] via sigmoid.\r\n    \"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/layers/mhc.py", "file_name": "mhc.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "941", "text": "The residual connection becomes:\r\n        output = alpha * residual + (1 - alpha) * block_output\r\n\r\n    where alpha is constrained to [0, 1] via sigmoid.\r\n    \"\"\"\r\n\r\n    logit_alpha: Float[Array, \"D\"]\r\n    layer_scale: Float[Array, \"D\"]\r\n    d_model: int = eqx.field(static=True)\r\n\r\n    def __init__(self, d_model: int, *, key: jax.Array):\r\n        self.d_model = d_model\r\n\r\n        k1, k2 = jax.random.split(key)\r\n\r\n        # Initialize near 0.5 mixing (logit = 0)\r\n        self.logit_alpha = jax.random.normal(k1, (d_model,)) * 0.01\r\n        self.layer_scale = jnp.ones(d_model)\r\n\r\n    def __call__(\r\n        self,\r\n        residual: Float[Array, \"B N D\"],\r\n        block_output: Float[Array, \"B N D\"],\r\n    ) -> Float[Array, \"B N D\"]:\r\n        \"\"\"Apply element-wise manifold-constrained mixing.\"\"\"\r\n        # Constrain alpha to [0, 1]\r\n        alpha = jax.nn.sigmoid(self.logit_alpha)\r\n\r\n        # Element-wise mixing\r\n        output = alpha * residual + (1 - alpha) * block_output\r\n\r\n        return output * self.layer_scale", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/layers/mhc.py", "file_name": "mhc.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "942", "text": "\"\"\"Full model architectures.\"\"\"\r\n\r\nfrom nash_mhc.models.backbone import MAHALanguageModel\r\n\r\n__all__ = [\"MAHALanguageModel\"]", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/models/__init__.py", "file_name": "__init__.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "943", "text": "\"\"\"\r\nMAHA Language Model backbone.\r\n\r\nFull transformer architecture with hierarchical multiscale attention\r\nand manifold-constrained residual connections.\r\n\"\"\"\r\n\r\nfrom typing import Literal\r\n\r\nimport jax\r\nimport jax.numpy as jnp\r\nimport equinox as eqx\r\nfrom jaxtyping import Float, Int, Array, PRNGKeyArray\r\n\r\nfrom nash_mhc.types.configs import ModelConfig\r\nfrom nash_mhc.blocks.decoder_block import MAHADecoderBlock\r\nfrom nash_mhc.layers.ffn import RMSNorm\r\n\r\n\r\nclass MAHALanguageModel(eqx.Module):\r\n    \"\"\"\r\n    Full MAHA language model for autoregressive generation.\r\n\r\n    Architecture:\r\n    - Token embedding\r\n    - N × MAHA decoder blocks\r\n    - Final RMSNorm\r\n    - LM head (tied to embeddings optional)\r\n\r\n    Type Parameters:\r\n        B: Batch size\r\n        N: Sequence length\r\n        D: Model dimension\r\n        V: Vocabulary size\r\n    \"\"\"\r\n\r\n    # Embeddings\r\n    token_embedding: eqx.nn.Embedding\r\n\r\n    # Decoder blocks\r\n    blocks: tuple[MAHADecoderBlock, ...]\r\n\r\n    # Output\r\n    final_norm: RMSNorm\r\n    lm_head: eqx.nn.Linear\r\n\r\n    # Configuration (static)\r\n    config: ModelConfig = eqx.field(static=True)\r\n\r\n    def __init__(\r\n        self,\r\n        config: ModelConfig,\r\n        *,\r\n        key: PRNGKeyArray,\r\n    ):\r\n        \"\"\"\r\n        Args:\r\n            config: Model configuration.\r\n            key: PRNG key for initialization.\r\n        \"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/models/backbone.py", "file_name": "backbone.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "944", "text": "key: PRNG key for initialization.\r\n        \"\"\"\r\n        self.config = config\r\n\r\n        # Split keys\r\n        keys = jax.random.split(key, config.num_layers + 3)\r\n\r\n        # Token embedding\r\n        self.token_embedding = eqx.nn.Embedding(\r\n            num_embeddings=config.vocab_size,\r\n            embedding_size=config.d_model,\r\n            key=keys[0],\r\n        )\r\n\r\n        # Decoder blocks\r\n        self.blocks = tuple(\r\n            MAHADecoderBlock(\r\n                d_model=config.d_model,\r\n                num_heads=config.num_heads,\r\n                num_scales=config.num_scales,\r\n                max_seq_len=config.max_seq_len,\r\n                compression_ratio=config.compression_ratio,\r\n                ffn_multiplier=config.ffn_multiplier,\r\n                aggregation=config.aggregation,\r\n                nash_iterations=config.nash_iterations,\r\n                sinkhorn_iterations=config.sinkhorn_iterations,\r\n                key=keys[i + 1],\r\n            )\r\n            for i in range(config.num_layers)\r\n        )\r\n\r\n        # Output layers\r\n        self.final_norm = RMSNorm(config.d_model)\r\n        self.lm_head = eqx.nn.Linear(\r\n            config.d_model,\r\n            config.vocab_size,\r\n            use_bias=False,\r\n            key=keys[-1],\r\n        )\r\n\r\n    def __call__(\r\n        self,\r\n        input_ids: Int[Array, \"B N\"],\r\n        causal: bool = True,\r\n    ) -> tuple[Float[Array, \"B N V\"], Float[Array, \"\"]]:\r\n        \"\"\"\r\n        Forward pass for language modeling.\r\n\r\n        Args:\r\n            input_ids: Token indices [B, N].\r\n            causal: Whether to apply causal masking.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/models/backbone.py", "file_name": "backbone.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "945", "text": "Args:\r\n            input_ids: Token indices [B, N].\r\n            causal: Whether to apply causal masking.\r\n\r\n        Returns:\r\n            Tuple of:\r\n            - logits: Vocabulary logits [B, N, V]\r\n            - total_aux_loss: Sum of aggregation auxiliary losses\r\n        \"\"\"\r\n        # Embed tokens\r\n        x = jax.vmap(jax.vmap(self.token_embedding))(input_ids)\r\n\r\n        # Process through decoder blocks\r\n        total_aux_loss = jnp.array(0.0)\r\n\r\n        for block in self.blocks:\r\n            x, aux_loss = block(x, causal=causal)\r\n            total_aux_loss = total_aux_loss + aux_loss\r\n\r\n        # Final norm\r\n        x = jax.vmap(jax.vmap(self.final_norm))(x)\r\n\r\n        # LM head\r\n        logits = jax.vmap(jax.vmap(self.lm_head))(x)\r\n\r\n        return logits, total_aux_loss\r\n\r\n    def generate(\r\n        self,\r\n        input_ids: Int[Array, \"B N\"],\r\n        max_new_tokens: int,\r\n        *,\r\n        key: PRNGKeyArray,\r\n        temperature: float = 1.0,\r\n        top_k: int | None = None,\r\n    ) -> Int[Array, \"B M\"]:\r\n        \"\"\"\r\n        Autoregressive text generation.\r\n\r\n        Args:\r\n            input_ids: Initial token indices [B, N].\r\n            max_new_tokens: Number of tokens to generate.\r\n            key: PRNG key for sampling.\r\n            temperature: Sampling temperature.\r\n            top_k: If set, sample from top-k tokens only.\r\n\r\n        Returns:\r\n            Generated token indices [B, N + max_new_tokens].\r\n        \"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/models/backbone.py", "file_name": "backbone.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "946", "text": "key: PRNG key for sampling.\r\n            temperature: Sampling temperature.\r\n            top_k: If set, sample from top-k tokens only.\r\n\r\n        Returns:\r\n            Generated token indices [B, N + max_new_tokens].\r\n        \"\"\"\r\n        B, N = input_ids.shape\r\n        generated = input_ids\r\n\r\n        for i in range(max_new_tokens):\r\n            # Get logits for last position\r\n            logits, _ = self(generated, causal=True)\r\n            next_logits = logits[:, -1, :]  # [B, V]\r\n\r\n            # Apply temperature\r\n            next_logits = next_logits / temperature\r\n\r\n            # Optional top-k filtering\r\n            if top_k is not None:\r\n                # Get top-k values and indices\r\n                top_values, top_indices = jax.lax.top_k(next_logits, top_k)\r\n                # Create mask for non-top-k values\r\n                mask = jnp.ones_like(next_logits) * (-1e9)\r\n                # Scatter top-k values back\r\n                next_logits = mask.at[jnp.arange(B)[:, None], top_indices].set(top_values)\r\n\r\n            # Sample\r\n            key, subkey = jax.random.split(key)\r\n            next_token = jax.random.categorical(subkey, next_logits, axis=-1)\r\n\r\n            # Append\r\n            generated = jnp.concatenate(\r\n                [generated, next_token[:, None]],\r\n                axis=1,\r\n            )\r\n\r\n        return generated\r\n\r\n    def count_params(self) -> int:\r\n        \"\"\"Count total parameters in the model.\"\"\"\r\n        return sum(\r\n            x.size for x in jax.tree_util.tree_leaves(eqx.filter(self, eqx.is_array))\r\n        )\r\n\r\n\r\nclass MAHALanguageModelTied(eqx.Module):\r\n    \"\"\"\r\n    MAHA Language Model with tied embedding weights.\r\n\r\n    The LM head shares weights with the token embedding,\r\n    reducing parameter count.\r\n    \"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/models/backbone.py", "file_name": "backbone.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "947", "text": "The LM head shares weights with the token embedding,\r\n    reducing parameter count.\r\n    \"\"\"\r\n\r\n    token_embedding: eqx.nn.Embedding\r\n    blocks: tuple[MAHADecoderBlock, ...]\r\n    final_norm: RMSNorm\r\n    config: ModelConfig = eqx.field(static=True)\r\n\r\n    def __init__(\r\n        self,\r\n        config: ModelConfig,\r\n        *,\r\n        key: PRNGKeyArray,\r\n    ):\r\n        self.config = config\r\n\r\n        keys = jax.random.split(key, config.num_layers + 2)\r\n\r\n        self.token_embedding = eqx.nn.Embedding(\r\n            num_embeddings=config.vocab_size,\r\n            embedding_size=config.d_model,\r\n            key=keys[0],\r\n        )\r\n\r\n        self.blocks = tuple(\r\n            MAHADecoderBlock(\r\n                d_model=config.d_model,\r\n                num_heads=config.num_heads,\r\n                num_scales=config.num_scales,\r\n                max_seq_len=config.max_seq_len,\r\n                compression_ratio=config.compression_ratio,\r\n                ffn_multiplier=config.ffn_multiplier,\r\n                aggregation=config.aggregation,\r\n                nash_iterations=config.nash_iterations,\r\n                sinkhorn_iterations=config.sinkhorn_iterations,\r\n                key=keys[i + 1],\r\n            )\r\n            for i in range(config.num_layers)\r\n        )\r\n\r\n        self.final_norm = RMSNorm(config.d_model)\r\n\r\n    def __call__(\r\n        self,\r\n        input_ids: Int[Array, \"B N\"],\r\n        causal: bool = True,\r\n    ) -> tuple[Float[Array, \"B N V\"], Float[Array, \"\"]]:\r\n        \"\"\"Forward pass with tied output weights.\"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/models/backbone.py", "file_name": "backbone.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "948", "text": "x = jax.vmap(jax.vmap(self.token_embedding))(input_ids)\r\n\r\n        total_aux_loss = jnp.array(0.0)\r\n        for block in self.blocks:\r\n            x, aux_loss = block(x, causal=causal)\r\n            total_aux_loss = total_aux_loss + aux_loss\r\n\r\n        x = jax.vmap(jax.vmap(self.final_norm))(x)\r\n\r\n        # Tied output: x @ embedding.T\r\n        # embedding.weight: [V, D]\r\n        # x: [B, N, D]\r\n        # logits: [B, N, V]\r\n        logits = jnp.einsum(\"bnd,vd->bnv\", x, self.token_embedding.weight)\r\n\r\n        return logits, total_aux_loss", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/models/backbone.py", "file_name": "backbone.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "949", "text": "\"\"\"Hardware-aligned primitive operations.\"\"\"\r\n\r\nfrom nash_mhc.primitives.sinkhorn import sinkhorn_knopp, sinkhorn_knopp_regularized\r\nfrom nash_mhc.primitives.nash_solver import nash_best_response, convex_aggregation\r\nfrom nash_mhc.primitives.upsample import nearest_upsample, upsample_scale_outputs\r\nfrom nash_mhc.primitives.rope import compute_rope_freqs, apply_rope, apply_rope_per_scale\r\nfrom nash_mhc.primitives.strided_conv import StridedConv1d, create_downsampler_stack\r\n\r\n__all__ = [\r\n    \"sinkhorn_knopp\",\r\n    \"sinkhorn_knopp_regularized\",\r\n    \"nash_best_response\",\r\n    \"convex_aggregation\",\r\n    \"nearest_upsample\",\r\n    \"upsample_scale_outputs\",\r\n    \"compute_rope_freqs\",\r\n    \"apply_rope\",\r\n    \"apply_rope_per_scale\",\r\n    \"StridedConv1d\",\r\n    \"create_downsampler_stack\",\r\n]", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/primitives/__init__.py", "file_name": "__init__.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "950", "text": "\"\"\"\r\nNash equilibrium solver via best-response dynamics.\r\n\r\nImplements game-theoretic aggregation where each scale is a player\r\ncompeting to minimize reconstruction error to the consensus.\r\n\r\nReference: Paper 2512.14925v2 (MAHA), Section 4.3, Equation 10\r\n\"\"\"\r\n\r\nfrom functools import partial\r\n\r\nimport jax\r\nimport jax.numpy as jnp\r\nfrom jax import lax\r\nfrom jaxtyping import Float, Array\r\n\r\n\r\n@partial(jax.jit, static_argnames=[\"num_iterations\"])\r\ndef nash_best_response(\r\n    scale_outputs: Float[Array, \"B L N D\"],\r\n    num_iterations: int = 3,\r\n) -> tuple[Float[Array, \"B N D\"], Float[Array, \"B L\"]]:\r\n    \"\"\"\r\n    Nash equilibrium aggregation via iterated best-response dynamics.\r\n\r\n    Models each scale as a \"player\" in a non-cooperative game where:\r\n    - Each player's strategy is their weight w_l\r\n    - Each player minimizes reconstruction error ||O_l - O*||²\r\n    - O* is the consensus (weighted sum of all scale outputs)\r\n\r\n    The algorithm iterates:\r\n    1. Compute consensus: O* = sum(w_l * O_l)\r\n    2. Compute per-scale error: error_l = ||O_l - O*||_2\r\n    3. Update weights: w_l = softmax(-error_l)\r\n\r\n    At equilibrium, scales with lower reconstruction error receive higher weights.\r\n\r\n    Args:\r\n        scale_outputs: Stacked outputs from all scales [B, L, N, D].\r\n                      All scales must already be upsampled to the same length N.\r\n        num_iterations: Number of best-response iterations (typically 3-5).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/primitives/nash_solver.py", "file_name": "nash_solver.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-03"}}
{"id": "951", "text": "Returns:\r\n        Tuple of:\r\n        - aggregated: Final consensus output [B, N, D]\r\n        - weights: Equilibrium weights [B, L]\r\n\r\n    Invariants (post-condition):\r\n        - sum(weights, axis=-1) = 1 (simplex constraint)\r\n        - weights >= 0 (non-negative)\r\n    \"\"\"\r\n    B, L, N, D = scale_outputs.shape\r\n\r\n    # Initialize uniform weights: each scale starts with equal weight\r\n    init_weights = jnp.ones((B, L), dtype=scale_outputs.dtype) / L\r\n\r\n    def iteration_body(\r\n        i: int,\r\n        weights: Float[Array, \"B L\"],\r\n    ) -> Float[Array, \"B L\"]:\r\n        w_expanded = weights[:, :, None, None]\r\n        consensus = jnp.sum(scale_outputs * w_expanded, axis=1)\r\n        diffs = scale_outputs - consensus[:, None, :, :]\r\n\r\n        # L2 norm with epsilon to prevent sqrt(0) gradient explosion\r\n        errors = jnp.sqrt(jnp.sum(diffs**2, axis=(-2, -1)) + 1e-8)\r\n\r\n        new_weights = jax.nn.softmax(-errors, axis=-1)\r\n\r\n        # Stop gradient through iterates - gradients flow only through final aggregation\r\n        return lax.stop_gradient(new_weights)\r\n\r\n    # Run fixed number of iterations using fori_loop (JIT-friendly)\r\n    final_weights = lax.fori_loop(\r\n        0,\r\n        num_iterations,\r\n        iteration_body,\r\n        init_weights,\r\n    )\r\n\r\n    # Compute final aggregated output using equilibrium weights\r\n    w_expanded = final_weights[:, :, None, None]\r\n    aggregated = jnp.sum(scale_outputs * w_expanded, axis=1)\r\n\r\n    return aggregated, final_weights\r\n\r\n\r\n@partial(jax.jit, static_argnames=[\"num_iterations\"])\r\ndef nash_best_response_with_temperature(\r\n    scale_outputs: Float[Array, \"B L N D\"],\r\n    num_iterations: int = 3,\r\n    temperature: float = 1.0,\r\n) -> tuple[Float[Array, \"B N D\"], Float[Array, \"B L\"]]:\r\n    \"\"\"\r\n    Nash best-response with temperature-controlled weight sharpness.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/primitives/nash_solver.py", "file_name": "nash_solver.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-03"}}
{"id": "952", "text": "Lower temperature -> sharper weights (more confidence in best scale)\r\n    Higher temperature -> softer weights (more uniform mixing)\r\n\r\n    Args:\r\n        scale_outputs: Stacked outputs from all scales [B, L, N, D].\r\n        num_iterations: Number of best-response iterations.\r\n        temperature: Softmax temperature (default 1.0).\r\n\r\n    Returns:\r\n        Tuple of (aggregated [B, N, D], weights [B, L]).\r\n    \"\"\"\r\n    B, L, N, D = scale_outputs.shape\r\n    init_weights = jnp.ones((B, L), dtype=scale_outputs.dtype) / L\r\n\r\n    def iteration_body(i: int, weights: Float[Array, \"B L\"]) -> Float[Array, \"B L\"]:\r\n        w_expanded = weights[:, :, None, None]\r\n        consensus = jnp.sum(scale_outputs * w_expanded, axis=1)\r\n        diffs = scale_outputs - consensus[:, None, :, :]\r\n        errors = jnp.sqrt(jnp.sum(diffs**2, axis=(-2, -1)))\r\n\r\n        # Temperature-scaled softmax\r\n        new_weights = jax.nn.softmax(-errors / temperature, axis=-1)\r\n        return new_weights\r\n\r\n    final_weights = lax.fori_loop(0, num_iterations, iteration_body, init_weights)\r\n    w_expanded = final_weights[:, :, None, None]\r\n    aggregated = jnp.sum(scale_outputs * w_expanded, axis=1)\r\n\r\n    return aggregated, final_weights\r\n\r\n\r\ndef convex_aggregation(\r\n    scale_outputs: Float[Array, \"B L N D\"],\r\n    logits: Float[Array, \"L\"],\r\n) -> tuple[Float[Array, \"B N D\"], Float[Array, \"L\"]]:\r\n    \"\"\"\r\n    Convex optimization aggregation with learned weights.\r\n\r\n    Uses softmax to enforce simplex constraint: sum(w) = 1, w >= 0.\r\n\r\n    Args:\r\n        scale_outputs: Stacked outputs from all scales [B, L, N, D].\r\n        logits: Learnable weight logits [L] (before softmax).\r\n\r\n    Returns:\r\n        Tuple of (aggregated [B, N, D], weights [L]).\r\n    \"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/primitives/nash_solver.py", "file_name": "nash_solver.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-03"}}
{"id": "953", "text": "logits: Learnable weight logits [L] (before softmax).\r\n\r\n    Returns:\r\n        Tuple of (aggregated [B, N, D], weights [L]).\r\n    \"\"\"\r\n    # Enforce simplex constraint via softmax\r\n    weights = jax.nn.softmax(logits)\r\n\r\n    # Weighted sum: einsum is cleaner for this pattern\r\n    aggregated = jnp.einsum(\"l,blnd->bnd\", weights, scale_outputs)\r\n\r\n    return aggregated, weights\r\n\r\n\r\ndef compute_sparsity_loss(\r\n    weights: Float[Array, \"... L\"], lambda_sparsity: float\r\n) -> Float[Array, \"\"]:\r\n    \"\"\"\r\n    Compute L1 sparsity penalty for aggregation weights.\r\n\r\n    Encourages some weights to go to zero, focusing on fewer scales.\r\n\r\n    Args:\r\n        weights: Simplex weights [..., L].\r\n        lambda_sparsity: Regularization strength.\r\n\r\n    Returns:\r\n        Scalar sparsity loss.\r\n    \"\"\"\r\n    return lambda_sparsity * jnp.sum(jnp.abs(weights))", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/primitives/nash_solver.py", "file_name": "nash_solver.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-03"}}
{"id": "954", "text": "\"\"\"\r\nRotary Position Embedding (RoPE) with per-scale adjustment.\r\n\r\nImplements RoPE where position indices are scaled based on the hierarchical\r\nlevel, maintaining consistent positional relationships across scales.\r\n\r\nReference: RoFormer (Su et al., 2021)\r\n\"\"\"\r\n\r\nimport jax\r\nimport jax.numpy as jnp\r\nfrom jaxtyping import Float, Int, Array\r\nfrom functools import partial\r\n\r\n\r\ndef compute_rope_freqs(\r\n    dim: int,\r\n    max_seq_len: int,\r\n    base: float = 10000.0,\r\n    dtype: jnp.dtype = jnp.float32,\r\n) -> Float[Array, \"N D2\"]:\r\n    \"\"\"\r\n    Precompute rotary position embedding frequencies.\r\n\r\n    Args:\r\n        dim: Head dimension (must be even).\r\n        max_seq_len: Maximum sequence length.\r\n        base: Base for geometric progression (default 10000).\r\n        dtype: Output dtype.\r\n\r\n    Returns:\r\n        Frequency matrix [max_seq_len, dim // 2].\r\n    \"\"\"\r\n    if dim % 2 != 0:\r\n        raise ValueError(f\"dim must be even, got {dim}\")\r\n\r\n    # Inverse frequencies: 1 / (base^(2i/dim)) for i in [0, dim/2)\r\n    inv_freq = 1.0 / (base ** (jnp.arange(0, dim, 2, dtype=dtype) / dim))\r\n\r\n    # Position indices\r\n    positions = jnp.arange(max_seq_len, dtype=dtype)\r\n\r\n    # Outer product: [max_seq_len, dim/2]\r\n    freqs = jnp.outer(positions, inv_freq)\r\n\r\n    return freqs\r\n\r\n\r\n@jax.jit\r\ndef apply_rope(\r\n    x: Float[Array, \"B N H K\"],\r\n    freqs: Float[Array, \"N K2\"],\r\n) -> Float[Array, \"B N H K\"]:\r\n    \"\"\"\r\n    Apply rotary position embedding to input tensor.\r\n\r\n    The rotation is applied in pairs: (x[0], x[1]), (x[2], x[3]), ...\r\n    Each pair is rotated by the corresponding frequency.\r\n\r\n    Args:\r\n        x: Input tensor [B, N, H, K] where K = head dimension.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/primitives/rope.py", "file_name": "rope.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "955", "text": "Args:\r\n        x: Input tensor [B, N, H, K] where K = head dimension.\r\n        freqs: Precomputed frequencies [N, K//2].\r\n\r\n    Returns:\r\n        Rotated tensor [B, N, H, K].\r\n    \"\"\"\r\n    # Split into pairs\r\n    x1, x2 = x[..., ::2], x[..., 1::2]  # [B, N, H, K/2] each\r\n\r\n    # Compute sin and cos\r\n    # freqs is [N, K/2], need to broadcast to [1, N, 1, K/2]\r\n    cos = jnp.cos(freqs)[None, :, None, :]\r\n    sin = jnp.sin(freqs)[None, :, None, :]\r\n\r\n    # Apply rotation\r\n    # (x1, x2) -> (x1 * cos - x2 * sin, x1 * sin + x2 * cos)\r\n    y1 = x1 * cos - x2 * sin\r\n    y2 = x1 * sin + x2 * cos\r\n\r\n    # Interleave back\r\n    # Stack and reshape: [B, N, H, K/2, 2] -> [B, N, H, K]\r\n    return jnp.stack([y1, y2], axis=-1).reshape(x.shape)\r\n\r\n\r\n@partial(jax.jit, static_argnames=[\"scale_idx\", \"compression_ratio\"])\r\ndef apply_rope_per_scale(\r\n    x: Float[Array, \"B N H K\"],\r\n    freqs: Float[Array, \"M K2\"],\r\n    scale_idx: int,\r\n    compression_ratio: int,\r\n) -> Float[Array, \"B N H K\"]:\r\n    \"\"\"\r\n    Apply RoPE with position indices adjusted for hierarchical scale.\r\n\r\n    At scale l, position i corresponds to original position i * r^l.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/primitives/rope.py", "file_name": "rope.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "956", "text": "At scale l, position i corresponds to original position i * r^l.\r\n    This maintains consistent positional relationships across scales:\r\n    - Scale 0: positions [0, 1, 2, 3, ...]\r\n    - Scale 1 (r=2): positions [0, 2, 4, 6, ...] (every other position)\r\n    - Scale 2 (r=2): positions [0, 4, 8, 12, ...] (every 4th position)\r\n\r\n    Args:\r\n        x: Input tensor [B, N, H, K].\r\n        freqs: Precomputed frequencies [M, K//2] where M >= N * r^scale_idx.\r\n        scale_idx: Hierarchical scale index (0 = finest, L-1 = coarsest).\r\n        compression_ratio: Downsampling ratio between scales.\r\n\r\n    Returns:\r\n        Rotated tensor [B, N, H, K].\r\n    \"\"\"\r\n    seq_len = x.shape[1]\r\n\r\n    # Compute adjusted position indices\r\n    # At scale l, position i maps to original position i * r^l\r\n    scale_factor = compression_ratio ** scale_idx\r\n    adjusted_positions = jnp.arange(seq_len) * scale_factor\r\n\r\n    # Clip to valid frequency range\r\n    max_pos = freqs.shape[0] - 1\r\n    adjusted_positions = jnp.clip(adjusted_positions, 0, max_pos).astype(jnp.int32)\r\n\r\n    # Gather frequencies for adjusted positions\r\n    scaled_freqs = freqs[adjusted_positions]  # [N, K/2]\r\n\r\n    # Apply standard RoPE with scaled frequencies\r\n    return apply_rope(x, scaled_freqs)\r\n\r\n\r\ndef create_rope_cache(\r\n    max_seq_len: int,\r\n    head_dim: int,\r\n    num_scales: int,\r\n    compression_ratio: int,\r\n    base: float = 10000.0,\r\n    dtype: jnp.dtype = jnp.float32,\r\n) -> Float[Array, \"M K2\"]:\r\n    \"\"\"\r\n    Create RoPE frequency cache for all scales.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/primitives/rope.py", "file_name": "rope.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "957", "text": "The cache must be large enough to cover the maximum position\r\n    at the finest scale: max_seq_len * r^(num_scales - 1).\r\n\r\n    Args:\r\n        max_seq_len: Maximum sequence length at scale 0.\r\n        head_dim: Dimension per attention head.\r\n        num_scales: Number of hierarchical scales.\r\n        compression_ratio: Downsampling ratio between scales.\r\n        base: Base for geometric progression.\r\n        dtype: Output dtype.\r\n\r\n    Returns:\r\n        Frequency cache [M, head_dim // 2] where M accounts for all scales.\r\n    \"\"\"\r\n    # Maximum position needed (at coarsest scale, positions are multiplied by r^(L-1))\r\n    # But wait - coarsest scale has FEWER positions, not more\r\n    # Actually we need freqs for scale 0 positions which go up to max_seq_len\r\n    # At coarser scales, we use subset of those frequencies\r\n    return compute_rope_freqs(head_dim, max_seq_len, base, dtype)\r\n\r\n\r\nclass RoPECache:\r\n    \"\"\"\r\n    Cached RoPE frequencies for efficient inference.\r\n\r\n    Precomputes and stores cos/sin values for faster application.\r\n    \"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        max_seq_len: int,\r\n        head_dim: int,\r\n        base: float = 10000.0,\r\n        dtype: jnp.dtype = jnp.float32,\r\n    ):\r\n        self.freqs = compute_rope_freqs(head_dim, max_seq_len, base, dtype)\r\n        self._cos_cached = jnp.cos(self.freqs)\r\n        self._sin_cached = jnp.sin(self.freqs)\r\n\r\n    @property\r\n    def cos(self) -> Float[Array, \"N K2\"]:\r\n        return self._cos_cached\r\n\r\n    @property\r\n    def sin(self) -> Float[Array, \"N K2\"]:\r\n        return self._sin_cached\r\n\r\n    def apply(\r\n        self,\r\n        x: Float[Array, \"B N H K\"],\r\n        start_pos: int = 0,\r\n    ) -> Float[Array, \"B N H K\"]:\r\n        \"\"\"Apply cached RoPE starting from given position.\"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/primitives/rope.py", "file_name": "rope.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "958", "text": "seq_len = x.shape[1]\r\n        cos = self._cos_cached[start_pos : start_pos + seq_len]\r\n        sin = self._sin_cached[start_pos : start_pos + seq_len]\r\n\r\n        x1, x2 = x[..., ::2], x[..., 1::2]\r\n        cos = cos[None, :, None, :]\r\n        sin = sin[None, :, None, :]\r\n\r\n        y1 = x1 * cos - x2 * sin\r\n        y2 = x1 * sin + x2 * cos\r\n\r\n        return jnp.stack([y1, y2], axis=-1).reshape(x.shape)", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/primitives/rope.py", "file_name": "rope.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "959", "text": "\"\"\"\r\nSinkhorn-Knopp algorithm for Birkhoff polytope projection.\r\n\r\nProjects arbitrary matrices onto the set of doubly stochastic matrices\r\nusing iterative row/column normalization in log-space.\r\n\r\nReference: Paper 2512.24880v1 (mHC)\r\n\"\"\"\r\n\r\nfrom functools import partial\r\n\r\nimport jax\r\nimport jax.numpy as jnp\r\nfrom jax import lax\r\nfrom jaxtyping import Float, Array\r\n\r\n\r\n@partial(jax.jit, static_argnames=[\"num_iterations\"])\r\ndef sinkhorn_knopp(\r\n    log_alpha: Float[Array, \"... n n\"],\r\n    num_iterations: int = 10,\r\n) -> Float[Array, \"... n n\"]:\r\n    \"\"\"\r\n    Sinkhorn-Knopp algorithm for Birkhoff polytope projection.\r\n\r\n    Computes a doubly stochastic matrix via iterative row/column normalization\r\n    in log-space for numerical stability.\r\n\r\n    The algorithm alternates between:\r\n    1. Row normalization: u <- -logsumexp(log_alpha + v, axis=-1)\r\n    2. Column normalization: v <- -logsumexp(log_alpha + u, axis=-2)\r\n\r\n    After convergence, P = exp(log_alpha + u[:, None] + v[None, :]) is doubly stochastic.\r\n\r\n    Args:\r\n        log_alpha: Log-space input matrix [..., n, n]. Can be any real-valued matrix.\r\n        num_iterations: Number of Sinkhorn iterations. 10-20 typically sufficient.\r\n\r\n    Returns:\r\n        Doubly stochastic matrix [..., n, n] on the Birkhoff polytope.\r\n\r\n    Invariants (post-condition):\r\n        - P @ 1 = 1 (rows sum to 1)\r\n        - P.T @ 1 = 1 (columns sum to 1)\r\n        - P >= 0 (non-negative)\r\n\r\n    Note:\r\n        - Uses float32 for logsumexp operations to prevent manifold drift\r\n        - Returns in the same dtype as input\r\n    \"\"\"\r\n    original_dtype = log_alpha.dtype\r\n    # Promote to f32 for stability in logsumexp\r\n    log_alpha_f32 = log_alpha.astype(jnp.float32)\r\n\r\n    # Initialize dual variables\r\n    n = log_alpha.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/primitives/sinkhorn.py", "file_name": "sinkhorn.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-03"}}
{"id": "960", "text": "dtype\r\n    # Promote to f32 for stability in logsumexp\r\n    log_alpha_f32 = log_alpha.astype(jnp.float32)\r\n\r\n    # Initialize dual variables\r\n    n = log_alpha.shape[-1]\r\n    u = jnp.zeros(log_alpha.shape[:-1], dtype=jnp.float32)\r\n    v = jnp.zeros(log_alpha.shape[:-1], dtype=jnp.float32)\r\n\r\n    def scan_body(\r\n        carry: tuple[Float[Array, \"...\"], Float[Array, \"...\"]],\r\n        _: None,\r\n    ) -> tuple[tuple[Float[Array, \"...\"], Float[Array, \"...\"]], None]:\r\n        u_prev, v_prev = carry\r\n\r\n        # Row normalization: make rows sum to 1\r\n        # u_new[i] = -log(sum_j exp(log_alpha[i,j] + v[j]))\r\n        u_new = -jax.nn.logsumexp(log_alpha_f32 + v_prev[..., None, :], axis=-1)\r\n\r\n        # Column normalization: make columns sum to 1\r\n        # v_new[j] = -log(sum_i exp(log_alpha[i,j] + u[i]))\r\n        v_new = -jax.nn.logsumexp(log_alpha_f32 + u_new[..., :, None], axis=-2)\r\n\r\n        return (u_new, v_new), None\r\n\r\n    (u_final, v_final), _ = lax.scan(\r\n        scan_body,\r\n        (u, v),\r\n        xs=None,\r\n        length=num_iterations,\r\n    )\r\n\r\n    log_P = log_alpha_f32 + u_final[..., :, None] + v_final[..., None, :]\r\n    log_P = jnp.clip(log_P, -20.0, 20.0)\r\n    P = jnp.exp(log_P)\r\n    P = jnp.clip(P, 1e-8, 1.0)\r\n\r\n    return P.astype(original_dtype)\r\n\r\n\r\n@partial(jax.jit, static_argnames=[\"num_iterations\",", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/primitives/sinkhorn.py", "file_name": "sinkhorn.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-03"}}
{"id": "961", "text": "exp(log_P)\r\n    P = jnp.clip(P, 1e-8, 1.0)\r\n\r\n    return P.astype(original_dtype)\r\n\r\n\r\n@partial(jax.jit, static_argnames=[\"num_iterations\", \"epsilon\"])\r\ndef sinkhorn_knopp_regularized(\r\n    cost: Float[Array, \"... n n\"],\r\n    num_iterations: int = 10,\r\n    epsilon: float = 0.1,\r\n) -> Float[Array, \"... n n\"]:\r\n    \"\"\"\r\n    Entropy-regularized Sinkhorn for optimal transport.\r\n\r\n    Solves the regularized OT problem:\r\n        min_P <C, P> - epsilon * H(P)\r\n        s.t. P @ 1 = a, P.T @ 1 = b\r\n\r\n    With uniform marginals a = b = 1/n.\r\n\r\n    Args:\r\n        cost: Cost matrix [..., n, n].\r\n        num_iterations: Number of Sinkhorn iterations.\r\n        epsilon: Entropy regularization strength. Smaller = sharper transport.\r\n\r\n    Returns:\r\n        Transport plan [..., n, n] (doubly stochastic with uniform marginals).\r\n    \"\"\"\r\n    log_alpha = -cost / epsilon\r\n    return sinkhorn_knopp(log_alpha, num_iterations)\r\n\r\n\r\ndef sinkhorn_knopp_simple(\r\n    M: Float[Array, \"n n\"],\r\n    num_iterations: int = 10,\r\n) -> Float[Array, \"n n\"]:\r\n    \"\"\"\r\n    Simple Sinkhorn-Knopp without log-space (for small matrices).\r\n\r\n    Directly alternates between row and column normalization.\r\n    Less numerically stable but faster for small matrices.\r\n\r\n    Args:\r\n        M: Non-negative input matrix [n, n].\r\n        num_iterations: Number of iterations.\r\n\r\n    Returns:\r\n        Doubly stochastic matrix [n, n].\r\n    \"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/primitives/sinkhorn.py", "file_name": "sinkhorn.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-03"}}
{"id": "962", "text": "Args:\r\n        M: Non-negative input matrix [n, n].\r\n        num_iterations: Number of iterations.\r\n\r\n    Returns:\r\n        Doubly stochastic matrix [n, n].\r\n    \"\"\"\r\n    P = jnp.abs(M) + 1e-10  # Ensure positive\r\n\r\n    def body(_, P: Float[Array, \"n n\"]) -> Float[Array, \"n n\"]:\r\n        # Row normalize\r\n        P = P / jnp.sum(P, axis=-1, keepdims=True)\r\n        # Column normalize\r\n        P = P / jnp.sum(P, axis=-2, keepdims=True)\r\n        return P\r\n\r\n    return lax.fori_loop(0, num_iterations, body, P)", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/primitives/sinkhorn.py", "file_name": "sinkhorn.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-03"}}
{"id": "963", "text": "\"\"\"\r\nTPU-aligned strided 1D convolution for hierarchical downsampling.\r\n\r\nImplements learnable downsampling via strided convolution, optimized\r\nfor TPU's 128x128 systolic array.\r\n\r\nReference: Paper 2512.14925v2 (MAHA), Section 4.1, Equations 5, 109\r\n\"\"\"\r\n\r\nimport jax\r\nimport jax.numpy as jnp\r\nfrom jax import lax\r\nimport equinox as eqx\r\nfrom jaxtyping import Float, Array\r\n\r\n\r\nclass StridedConv1d(eqx.Module):\r\n    \"\"\"\r\n    1D strided convolution for hierarchical downsampling.\r\n\r\n    Implements D_l(X) = Conv1D(X, W_l, stride=r) where r is the compression ratio.\r\n\r\n    Key differences from PyTorch:\r\n    - Explicit padding calculation for TPU alignment\r\n    - Uses lax.conv_general_dilated for XLA optimization\r\n    - bf16 weights with f32 accumulation on TPU\r\n    \"\"\"\r\n\r\n    weight: Float[Array, \"out_c in_c k\"]\r\n    bias: Float[Array, \"out_c\"] | None\r\n    stride: int = eqx.field(static=True)\r\n    padding: int = eqx.field(static=True)\r\n    kernel_size: int = eqx.field(static=True)\r\n\r\n    def __init__(\r\n        self,\r\n        in_channels: int,\r\n        out_channels: int,\r\n        kernel_size: int = 3,\r\n        stride: int = 2,\r\n        *,\r\n        key: jax.Array,\r\n        use_bias: bool = True,\r\n    ):\r\n        \"\"\"\r\n        Args:\r\n            in_channels: Number of input channels (d_model).\r\n            out_channels: Number of output channels (usually same as in_channels).\r\n            kernel_size: Convolution kernel size (default 3).\r\n            stride: Downsampling stride (compression_ratio, default 2).\r\n            key: PRNG key for initialization.\r\n            use_bias: Whether to include bias term.\r\n        \"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/primitives/strided_conv.py", "file_name": "strided_conv.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "964", "text": "kernel_size: Convolution kernel size (default 3).\r\n            stride: Downsampling stride (compression_ratio, default 2).\r\n            key: PRNG key for initialization.\r\n            use_bias: Whether to include bias term.\r\n        \"\"\"\r\n        self.stride = stride\r\n        self.kernel_size = kernel_size\r\n        self.padding = kernel_size // 2  # Same padding for alignment\r\n\r\n        # Initialize with truncated normal (similar to PyTorch default)\r\n        wkey, bkey = jax.random.split(key)\r\n        fan_in = in_channels * kernel_size\r\n        scale = 1.0 / jnp.sqrt(fan_in)\r\n\r\n        self.weight = jax.random.truncated_normal(\r\n            wkey,\r\n            lower=-2.0,\r\n            upper=2.0,\r\n            shape=(out_channels, in_channels, kernel_size),\r\n        ) * scale\r\n\r\n        self.bias = jnp.zeros(out_channels) if use_bias else None\r\n\r\n    def __call__(self, x: Float[Array, \"B N D\"]) -> Float[Array, \"B N2 D\"]:\r\n        \"\"\"\r\n        Apply strided convolution for downsampling.\r\n\r\n        Args:\r\n            x: Input tensor [B, N, D] where D = in_channels.\r\n\r\n        Returns:\r\n            Downsampled tensor [B, N // stride, D] where D = out_channels.\r\n        \"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/primitives/strided_conv.py", "file_name": "strided_conv.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "965", "text": "Args:\r\n            x: Input tensor [B, N, D] where D = in_channels.\r\n\r\n        Returns:\r\n            Downsampled tensor [B, N // stride, D] where D = out_channels.\r\n        \"\"\"\r\n        # Transpose for conv: [B, N, D] -> [B, D, N]\r\n        x = jnp.swapaxes(x, -1, -2)\r\n\r\n        # Apply conv1d via lax.conv_general_dilated\r\n        # dimension_numbers: (batch, channel, spatial) for input, kernel, output\r\n        y = lax.conv_general_dilated(\r\n            x,\r\n            self.weight,\r\n            window_strides=(self.stride,),\r\n            padding=((self.padding, self.padding),),\r\n            dimension_numbers=(\"NCH\", \"OIH\", \"NCH\"),\r\n            precision=lax.Precision.DEFAULT,  # TPU uses bf16->f32 automatically\r\n        )\r\n\r\n        if self.bias is not None:\r\n            y = y + self.bias[:, None]\r\n\r\n        # Transpose back: [B, D, N] -> [B, N, D]\r\n        return jnp.swapaxes(y, -1, -2)\r\n\r\n\r\nclass StridedConv1dTPUAligned(eqx.Module):\r\n    \"\"\"\r\n    TPU-optimized strided convolution with 128-aligned dimensions.\r\n\r\n    Pads channel dimensions to multiples of 128 for optimal MXU utilization.\r\n    \"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/primitives/strided_conv.py", "file_name": "strided_conv.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "966", "text": "Pads channel dimensions to multiples of 128 for optimal MXU utilization.\r\n    \"\"\"\r\n\r\n    inner_conv: StridedConv1d\r\n    in_channels: int = eqx.field(static=True)\r\n    out_channels: int = eqx.field(static=True)\r\n    padded_in: int = eqx.field(static=True)\r\n    padded_out: int = eqx.field(static=True)\r\n\r\n    def __init__(\r\n        self,\r\n        in_channels: int,\r\n        out_channels: int,\r\n        kernel_size: int = 3,\r\n        stride: int = 2,\r\n        *,\r\n        key: jax.Array,\r\n        use_bias: bool = True,\r\n    ):\r\n        self.in_channels = in_channels\r\n        self.out_channels = out_channels\r\n\r\n        # Pad to 128 alignment\r\n        self.padded_in = ((in_channels + 127) // 128) * 128\r\n        self.padded_out = ((out_channels + 127) // 128) * 128\r\n\r\n        self.inner_conv = StridedConv1d(\r\n            in_channels=self.padded_in,\r\n            out_channels=self.padded_out,\r\n            kernel_size=kernel_size,\r\n            stride=stride,\r\n            key=key,\r\n            use_bias=use_bias,\r\n        )\r\n\r\n    def __call__(self, x: Float[Array, \"B N D\"]) -> Float[Array, \"B N2 D\"]:\r\n        \"\"\"Apply with padding/unpadding for TPU alignment.\"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/primitives/strided_conv.py", "file_name": "strided_conv.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "967", "text": "# Pad input channels\r\n        if self.padded_in > self.in_channels:\r\n            pad_width = self.padded_in - self.in_channels\r\n            x = jnp.pad(x, ((0, 0), (0, 0), (0, pad_width)))\r\n\r\n        # Apply convolution\r\n        y = self.inner_conv(x)\r\n\r\n        # Unpad output channels\r\n        if self.padded_out > self.out_channels:\r\n            y = y[..., : self.out_channels]\r\n\r\n        return y\r\n\r\n\r\ndef create_downsampler_stack(\r\n    d_model: int,\r\n    num_scales: int,\r\n    compression_ratio: int = 2,\r\n    kernel_size: int = 3,\r\n    *,\r\n    key: jax.Array,\r\n) -> tuple[StridedConv1d, ...]:\r\n    \"\"\"\r\n    Create a stack of strided convolutions for hierarchical decomposition.\r\n\r\n    Returns (num_scales - 1) convolutions since scale 0 is the original input.\r\n\r\n    Args:\r\n        d_model: Model dimension (in and out channels).\r\n        num_scales: Number of hierarchical scales.\r\n        compression_ratio: Downsampling stride.\r\n        kernel_size: Convolution kernel size.\r\n        key: PRNG key.\r\n\r\n    Returns:\r\n        Tuple of StridedConv1d modules.\r\n    \"\"\"\r\n    keys = jax.random.split(key, num_scales - 1)\r\n\r\n    return tuple(\r\n        StridedConv1d(\r\n            in_channels=d_model,\r\n            out_channels=d_model,\r\n            kernel_size=kernel_size,\r\n            stride=compression_ratio,\r\n            key=keys[i],\r\n        )\r\n        for i in range(num_scales - 1)\r\n    )", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/primitives/strided_conv.py", "file_name": "strided_conv.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "968", "text": "\"\"\"\r\nNearest-neighbor upsampling primitive.\r\n\r\nImplements manual upsampling via index gathering since JAX lacks F.interpolate.\r\nOptimized for TPU by avoiding dynamic control flow.\r\n\r\nReference: Paper 2512.14925v2 (MAHA), Equation 13\r\n\"\"\"\r\n\r\nimport jax\r\nimport jax.numpy as jnp\r\nfrom jaxtyping import Float, Array\r\n\r\n\r\nfrom functools import partial\r\n\r\n\r\n@partial(jax.jit, static_argnames=[\"target_len\"])\r\ndef nearest_upsample(\r\n    x: Float[Array, \"B N D\"],\r\n    target_len: int,\r\n) -> Float[Array, \"B target_len D\"]:\r\n    \"\"\"\r\n    Nearest-neighbor upsampling without interpolation.\r\n\r\n    Each target position is filled with the value from the nearest source position.\r\n    Uses index gathering for TPU efficiency (no dynamic control flow).\r\n\r\n    Args:\r\n        x: Input tensor [B, N, D] where N < target_len.\r\n        target_len: Desired output sequence length.\r\n\r\n    Returns:\r\n        Upsampled tensor [B, target_len, D].", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/primitives/upsample.py", "file_name": "upsample.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-03"}}
{"id": "969", "text": "Args:\r\n        x: Input tensor [B, N, D] where N < target_len.\r\n        target_len: Desired output sequence length.\r\n\r\n    Returns:\r\n        Upsampled tensor [B, target_len, D].\r\n\r\n    Example:\r\n        x of length 4: [a, b, c, d]\r\n        target_len 8:  [a, a, b, b, c, c, d, d]\r\n    \"\"\"\r\n    B, N, D = x.shape\r\n\r\n    # Compute source index for each target position\r\n    # target_idx * (N / target_len) gives the fractional source position\r\n    # floor() gives the nearest source index (nearest-neighbor behavior)\r\n    scale = N / target_len\r\n    target_indices = jnp.arange(target_len)\r\n    source_indices = jnp.floor(target_indices * scale).astype(jnp.int32)\r\n\r\n    # Clip to valid range (handles edge cases)\r\n    source_indices = jnp.clip(source_indices, 0, N - 1)\r\n\r\n    # Gather along sequence dimension\r\n    # x[:, source_indices, :] broadcasts over batch\r\n    return x[:, source_indices, :]\r\n\r\n\r\n@partial(jax.jit, static_argnames=[\"target_len\"])\r\ndef nearest_downsample(\r\n    x: Float[Array, \"B N D\"],\r\n    target_len: int,\r\n) -> Float[Array, \"B target_len D\"]:\r\n    \"\"\"\r\n    Nearest-neighbor downsampling (subsampling).\r\n\r\n    Selects evenly-spaced positions from the source.\r\n\r\n    Args:\r\n        x: Input tensor [B, N, D] where N > target_len.\r\n        target_len: Desired output sequence length.\r\n\r\n    Returns:\r\n        Downsampled tensor [B, target_len, D].\r\n    \"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/primitives/upsample.py", "file_name": "upsample.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-03"}}
{"id": "970", "text": "Args:\r\n        x: Input tensor [B, N, D] where N > target_len.\r\n        target_len: Desired output sequence length.\r\n\r\n    Returns:\r\n        Downsampled tensor [B, target_len, D].\r\n    \"\"\"\r\n    B, N, D = x.shape\r\n\r\n    # Compute which source positions to sample\r\n    scale = N / target_len\r\n    target_indices = jnp.arange(target_len)\r\n    source_indices = jnp.floor(target_indices * scale).astype(jnp.int32)\r\n    source_indices = jnp.clip(source_indices, 0, N - 1)\r\n\r\n    return x[:, source_indices, :]\r\n\r\n\r\ndef upsample_scale_outputs(\r\n    scale_outputs: tuple[Float[Array, \"B N_l D\"], ...],\r\n) -> Float[Array, \"B L N D\"]:\r\n    \"\"\"\r\n    Upsample all scale outputs to match the first scale's length, then stack.\r\n\r\n    Args:\r\n        scale_outputs: Tuple of tensors with decreasing sequence lengths.\r\n                      scale_outputs[0] has the longest sequence.\r\n\r\n    Returns:\r\n        Stacked tensor [B, L, N, D] where N = scale_outputs[0].shape[1].\r\n    \"\"\"\r\n    target_len = scale_outputs[0].shape[1]\r\n    L = len(scale_outputs)\r\n\r\n    # Upsample each scale and collect\r\n    upsampled = [scale_outputs[0]]  # First scale already at target length\r\n\r\n    for i in range(1, L):\r\n        upsampled.append(nearest_upsample(scale_outputs[i], target_len))\r\n\r\n    # Stack along new axis: list of [B, N, D] -> [B, L, N, D]\r\n    return jnp.stack(upsampled, axis=1)", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/primitives/upsample.py", "file_name": "upsample.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-03"}}
{"id": "971", "text": "\"\"\"Sharding helpers.\"\"\"\n\nfrom nash_mhc.sharding.mesh import MeshConfig, create_mesh, mesh_context, with_named_sharding\nfrom nash_mhc.sharding.specs import activation_specs, parameter_spec_from_name, SpecLayout\n\n__all__ = [\n    \"MeshConfig\",\n    \"create_mesh\",\n    \"mesh_context\",\n    \"with_named_sharding\",\n    \"SpecLayout\",\n    \"parameter_spec_from_name\",\n    \"activation_specs\",\n]", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/sharding/__init__.py", "file_name": "__init__.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "972", "text": "\"\"\"TPU mesh helpers and sharding utilities.\"\"\"\n\nfrom __future__ import annotations\n\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass\nfrom typing import Iterable, Sequence\n\nimport jax\nfrom jax.experimental import mesh_utils\nfrom jax.sharding import Mesh, NamedSharding\n\n\nAxisNames = tuple[str, ...]\n\n\n@dataclass(frozen=True, slots=True)\nclass MeshConfig:\n    \"\"\"Declarative TPU mesh description.\"\"\"\n\n    axis_lengths: tuple[int, ...]\n    axis_names: AxisNames = (\"data\", \"fsdp\", \"tp\")\n\n    def __post_init__(self) -> None:\n        if len(self.axis_lengths) != len(self.axis_names):\n            raise ValueError(\n                f\"axis_lengths ({self.axis_lengths}) and axis_names \"\n                f\"({self.axis_names}) must have equal rank\"\n            )\n        if any(length <= 0 for length in self.axis_lengths):\n            raise ValueError(f\"All axis lengths must be positive, got {self.axis_lengths}\")\n\n    @property\n    def total_devices(self) -> int:\n        \"\"\"Total devices implied by the mesh.\"\"\"\n        prod = 1\n        for length in self.axis_lengths:\n            prod *= length\n        return prod\n\n\ndef _select_devices(num_devices: int) -> Sequence[jax.Device]:\n    \"\"\"Select the first `num_devices` devices from the global pool.\"\"\"\n    available = jax.devices()\n    if num_devices > len(available):\n        raise ValueError(\n            f\"Requested {num_devices} devices for mesh, only {len(available)} available\"\n        )\n    return available[:num_devices]\n\n\ndef create_mesh(config: MeshConfig) -> Mesh:\n    \"\"\"Instantiate a `Mesh` aligned with the provided configuration.\"\"\"\n    devices = _select_devices(config.total_devices)\n    mesh_array = mesh_utils.create_device_mesh(config.axis_lengths, devices=devices)\n    return Mesh(mesh_array, config.axis_names)\n\n\n@contextmanager\ndef mesh_context(config: MeshConfig) -> Iterable[Mesh]:\n    \"\"\"Context manager that installs a mesh as the default sharding scope.\"\"\"\n    mesh = create_mesh(config)\n    with mesh:\n        yield mesh", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/sharding/mesh.py", "file_name": "mesh.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "973", "text": "@contextmanager\ndef mesh_context(config: MeshConfig) -> Iterable[Mesh]:\n    \"\"\"Context manager that installs a mesh as the default sharding scope.\"\"\"\n    mesh = create_mesh(config)\n    with mesh:\n        yield mesh\n\n\ndef with_named_sharding(mesh: Mesh, partition_spec) -> NamedSharding:\n    \"\"\"Create `NamedSharding` for a given mesh and `PartitionSpec`.\"\"\"\n    return NamedSharding(mesh, partition_spec)", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/sharding/mesh.py", "file_name": "mesh.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "974", "text": "\"\"\"PartitionSpec helpers aligned with TPU mesh axes.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Mapping\n\nfrom jax.sharding import PartitionSpec as PS\n\n\nAxis = str\n\n\n@dataclass(frozen=True, slots=True)\nclass SpecLayout:\n    \"\"\"Canonical PartitionSpecs for MAHA parameters and activations.\"\"\"\n\n    data_axis: Axis = \"data\"\n    fsdp_axis: Axis = \"fsdp\"\n    tp_axis: Axis = \"tp\"\n\n    def embeddings(self) -> PS:\n        \"\"\"Embedding tables replicated across data, sharded over fsdp×tp.\"\"\"\n        return PS((self.fsdp_axis, self.tp_axis), None)\n\n    def qkv_projection(self) -> PS:\n        \"\"\"Attention projections shard along fsdp (rows) and tp (cols).\"\"\"\n        return PS(self.fsdp_axis, self.tp_axis)\n\n    def attn_output(self) -> PS:\n        \"\"\"Attention output projection shares tp axis on columns.\"\"\"\n        return PS(self.fsdp_axis, self.tp_axis)\n\n    def ffn_up(self) -> PS:\n        return PS(self.fsdp_axis, self.tp_axis)\n\n    def ffn_down(self) -> PS:\n        return PS(self.fsdp_axis, None)\n\n    def layer_norm(self) -> PS:\n        return PS(self.fsdp_axis, None)\n\n    def activations(self) -> PS:\n        \"\"\"Runtime activations are sharded across data and tensor axes.\"\"\"\n        return PS(self.data_axis, None, self.tp_axis)\n\n    def logits(self) -> PS:\n        return PS(self.data_axis, None, self.tp_axis)\n\n\ndef parameter_spec_from_name(param_name: str, layout: SpecLayout | None = None) -> PS:\n    \"\"\"Heuristic PartitionSpec assignment based on parameter name.\"\"\"\n    layout = layout or SpecLayout()\n    name = param_name.lower()", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/sharding/specs.py", "file_name": "specs.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "975", "text": "def parameter_spec_from_name(param_name: str, layout: SpecLayout | None = None) -> PS:\n    \"\"\"Heuristic PartitionSpec assignment based on parameter name.\"\"\"\n    layout = layout or SpecLayout()\n    name = param_name.lower()\n\n    if \"embedding\" in name:\n        return layout.embeddings()\n    if \"q_proj\" in name or \"k_proj\" in name or \"v_proj\" in name:\n        return layout.qkv_projection()\n    if \"o_proj\" in name:\n        return layout.attn_output()\n    if \"ffn\" in name or \"w_gate\" in name or \"w_up\" in name:\n        return layout.ffn_up()\n    if \"w_down\" in name:\n        return layout.ffn_down()\n    if \"norm\" in name or \"rms\" in name:\n        return layout.layer_norm()\n    return PS()\n\n\ndef activation_specs(layout: SpecLayout | None = None) -> Mapping[str, PS]:\n    \"\"\"PartitionSpec mapping for common runtime tensors.\"\"\"\n    layout = layout or SpecLayout()\n    return {\n        \"hidden\": layout.activations(),\n        \"attention_context\": layout.activations(),\n        \"logits\": layout.logits(),\n    }", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/sharding/specs.py", "file_name": "specs.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "976", "text": "\"\"\"Training loop utilities.\"\"\"\n\nfrom nash_mhc.training.loop import TrainState, init_train_state, train_epoch, train_step\nfrom nash_mhc.training.loss import LossComponents, cross_entropy_loss\nfrom nash_mhc.training.metrics import MetricState, aggregate_metrics\n\n__all__ = [\n    \"TrainState\",\n    \"init_train_state\",\n    \"train_epoch\",\n    \"train_step\",\n    \"LossComponents\",\n    \"cross_entropy_loss\",\n    \"MetricState\",\n    \"aggregate_metrics\",\n]", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/training/__init__.py", "file_name": "__init__.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "977", "text": "\"\"\"Training loop for MAHA language model.\"\"\"\n\nfrom __future__ import annotations\n\nimport time\nfrom dataclasses import dataclass, replace\nfrom typing import Iterable\n\nimport equinox as eqx\nimport jax\nimport optax\n\nfrom nash_mhc.data.loader import SequenceBatch\nfrom nash_mhc.models.backbone import MAHALanguageModel\nfrom nash_mhc.training.loss import LossComponents, cross_entropy_loss\nfrom nash_mhc.training.metrics import MetricState, aggregate_metrics\nfrom nash_mhc.types.configs import TrainingConfig\n\n\n@dataclass\nclass TrainState:\n    model: MAHALanguageModel\n    optimizer: optax.GradientTransformation\n    opt_state: optax.OptState\n    step: int\n    pad_token_id: int\n\n\ndef create_optimizer(config: TrainingConfig) -> optax.GradientTransformation:\n    \"\"\"AdamW schedule with linear warmup and cosine decay.\"\"\"\n    warmup = optax.linear_schedule(\n        init_value=0.0,\n        end_value=config.learning_rate,\n        transition_steps=config.warmup_steps,\n    )\n    decay = optax.cosine_decay_schedule(\n        init_value=config.learning_rate,\n        decay_steps=max(config.total_steps - config.warmup_steps, 1),\n    )\n    lr_schedule = optax.join_schedules(\n        schedules=[warmup, decay],\n        boundaries=[config.warmup_steps],\n    )\n    return optax.adamw(\n        learning_rate=lr_schedule,\n        weight_decay=config.weight_decay,\n    )\n\n\ndef init_train_state(\n    model: MAHALanguageModel,\n    training_config: TrainingConfig,\n    *,\n    pad_token_id: int,\n) -> TrainState:\n    optimizer = create_optimizer(training_config)\n    params, _ = eqx.partition(model, eqx.is_array)\n    opt_state = optimizer.init(params)\n    return TrainState(\n        model=model,\n        optimizer=optimizer,\n        opt_state=opt_state,\n        step=0,\n        pad_token_id=pad_token_id,\n    )\n\n\ndef _prepare_inputs(batch: SequenceBatch) -> tuple:\n    inputs = batch.token_ids[:, :-1]\n    labels = batch.token_ids[:, 1:]\n    return inputs, labels", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/training/loop.py", "file_name": "loop.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "978", "text": "def _prepare_inputs(batch: SequenceBatch) -> tuple:\n    inputs = batch.token_ids[:, :-1]\n    labels = batch.token_ids[:, 1:]\n    return inputs, labels\n\n\ndef train_step(state: TrainState, batch: SequenceBatch) -> tuple[TrainState, LossComponents]:\n    \"\"\"Single JIT-able training step.\"\"\"\n    inputs, labels = _prepare_inputs(batch)\n    params, static = eqx.partition(state.model, eqx.is_array)\n\n    def loss_fn(filtered_params):\n        model = eqx.combine(filtered_params, static)\n        logits, aux_loss = model(inputs, causal=True)\n        logits = logits[:, :-1, :]\n        components = cross_entropy_loss(\n            logits,\n            labels,\n            padding_id=state.pad_token_id,\n            aux_loss=aux_loss,\n        )\n        return components.total, components\n\n    (loss_value, components), grads = eqx.filter_value_and_grad(loss_fn, has_aux=True)(params)\n    updates, new_opt_state = state.optimizer.update(grads, state.opt_state, params)\n    new_params = optax.apply_updates(params, updates)\n    new_model = eqx.combine(new_params, static)\n    new_state = replace(state, model=new_model, opt_state=new_opt_state, step=state.step + 1)\n    return new_state, components", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/training/loop.py", "file_name": "loop.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "979", "text": "def train_epoch(\n    state: TrainState,\n    batches: Iterable[SequenceBatch],\n    *,\n    max_steps: int | None = None,\n) -> tuple[TrainState, MetricState]:\n    \"\"\"Iterate over an epoch of data.\"\"\"\n    total_loss = 0.0\n    total_tokens = 0\n    start = time.time()\n    for idx, batch in enumerate(batches):\n        if max_steps is not None and idx >= max_steps:\n            break\n        state, metrics = train_step(state, batch)\n        total_loss += float(metrics.total)\n        total_tokens += int(batch.token_ids.size)\n    elapsed = time.time() - start\n    average_loss = total_loss / max(idx + 1, 1)\n    summary = aggregate_metrics(\n        total_loss=average_loss,\n        total_tokens=total_tokens,\n        elapsed_time_s=elapsed,\n        step=state.step,\n    )\n    return state, summary", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/training/loop.py", "file_name": "loop.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "980", "text": "\"\"\"Loss functions for MAHA training.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\n\nimport jax\nimport jax.numpy as jnp\nfrom jaxtyping import Float, Int, Array\n\n\n@jax.tree_util.register_pytree_node_class\n@dataclass(frozen=True, slots=True)\nclass LossComponents:\n    total: Float[Array, \"\"]\n    cross_entropy: Float[Array, \"\"]\n    aux_loss: Float[Array, \"\"]\n\n    def tree_flatten(self):\n        return ((self.total, self.cross_entropy, self.aux_loss), None)\n\n    @classmethod\n    def tree_unflatten(cls, aux_data, children):\n        total, cross_entropy, aux_loss = children\n        return cls(total=total, cross_entropy=cross_entropy, aux_loss=aux_loss)\n\n\ndef cross_entropy_loss(\n    logits: Float[Array, \"B N V\"],\n    labels: Int[Array, \"B N\"],\n    *,\n    padding_id: int,\n    aux_loss: Float[Array, \"\"] | None = None,\n    label_smoothing: float = 0.0,\n) -> LossComponents:\n    \"\"\"Compute cross-entropy with padding mask.\"\"\"\n    log_probs = jax.nn.log_softmax(logits, axis=-1)\n    vocab = logits.shape[-1]\n    targets = jax.nn.one_hot(labels, vocab)\n    if label_smoothing > 0:\n        smooth = label_smoothing / vocab\n        targets = (1.0 - label_smoothing) * targets + smooth\n    nll = -jnp.sum(targets * log_probs, axis=-1)\n    mask = (labels != padding_id).astype(logits.dtype)\n    denom = jnp.maximum(jnp.sum(mask), 1.0)\n    ce_loss = jnp.sum(nll * mask) / denom\n    aux = aux_loss if aux_loss is not None else jnp.array(0.0, dtype=logits.dtype)\n    total = ce_loss + aux\n    return LossComponents(total=total, cross_entropy=ce_loss, aux_loss=aux)", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/training/loss.py", "file_name": "loss.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "981", "text": "\"\"\"Training metrics helpers.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\n\nimport jax.numpy as jnp\nfrom jaxtyping import Float, Array\n\n\n@dataclass(frozen=True, slots=True)\nclass MetricState:\n    step: int\n    loss: float\n    tokens: int\n    tokens_per_second: float\n    perplexity: float\n\n\ndef compute_perplexity(loss: Float[Array, \"\"]) -> Float[Array, \"\"]:\n    return jnp.exp(loss)\n\n\ndef aggregate_metrics(\n    total_loss: float,\n    total_tokens: int,\n    elapsed_time_s: float,\n    step: int,\n) -> MetricState:\n    token_rate = total_tokens / max(elapsed_time_s, 1e-6)\n    ppl = float(jnp.exp(jnp.array(total_loss)))\n    return MetricState(\n        step=step,\n        loss=total_loss,\n        tokens=total_tokens,\n        tokens_per_second=token_rate,\n        perplexity=ppl,\n    )", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/training/metrics.py", "file_name": "metrics.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "982", "text": "\"\"\"Type definitions for nash-mhc.\"\"\"\r\n\r\nfrom nash_mhc.types.configs import ModelConfig, TrainingConfig\r\nfrom nash_mhc.types.arrays import (\r\n    DoublyStochastic,\r\n    SimplexWeights,\r\n    BoundedResidual,\r\n    AttentionWeights,\r\n)\r\nfrom nash_mhc.types.invariants import (\r\n    assert_doubly_stochastic,\r\n    assert_simplex,\r\n    assert_spectral_norm_bounded,\r\n)\r\n\r\n__all__ = [\r\n    \"ModelConfig\",\r\n    \"TrainingConfig\",\r\n    \"DoublyStochastic\",\r\n    \"SimplexWeights\",\r\n    \"BoundedResidual\",\r\n    \"AttentionWeights\",\r\n    \"assert_doubly_stochastic\",\r\n    \"assert_simplex\",\r\n    \"assert_spectral_norm_bounded\",\r\n]", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/types/__init__.py", "file_name": "__init__.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "983", "text": "\"\"\"Semantic array types for compile-time documentation and runtime validation.\"\"\"\r\n\r\nfrom typing import NewType, TypeVar\r\n\r\nfrom jaxtyping import Float, Int, Array\r\n\r\n\r\n# === Dimension type variables (for documentation) ===\r\nB = TypeVar(\"B\")  # Batch\r\nN = TypeVar(\"N\")  # Sequence length\r\nD = TypeVar(\"D\")  # Model dimension\r\nH = TypeVar(\"H\")  # Number of heads\r\nK = TypeVar(\"K\")  # Head dimension (D // H)\r\nL = TypeVar(\"L\")  # Number of scales\r\nV = TypeVar(\"V\")  # Vocabulary size\r\n\r\n\r\n# === Semantic newtypes ===\r\n# These provide compile-time documentation about array invariants.\r\n# Runtime enforcement is handled by the invariants module.\r\n\r\nDoublyStochastic = NewType(\r\n    \"DoublyStochastic\",\r\n    Float[Array, \"... n n\"],\r\n)\r\n\"\"\"\r\nMatrix on the Birkhoff Polytope.\r\n\r\nInvariants:\r\n- M @ 1 = 1 (rows sum to 1)\r\n- M.T @ 1 = 1 (columns sum to 1)\r\n- M >= 0 (non-negative entries)\r\n\"\"\"\r\n\r\nSimplexWeights = NewType(\r\n    \"SimplexWeights\",\r\n    Float[Array, \"... L\"],\r\n)\r\n\"\"\"\r\nWeights on the probability simplex.\r\n\r\nInvariants:\r\n- sum(w) = 1\r\n- w >= 0\r\n\"\"\"\r\n\r\nBoundedResidual = NewType(\r\n    \"BoundedResidual\",\r\n    Float[Array, \"B N D\"],\r\n)\r\n\"\"\"\r\nResidual connection output with bounded spectral norm.\r\n\r\nInvariant:\r\n- ||H||_2 <= 1 (non-expansive)\r\n\"\"\"\r\n\r\nAttentionWeights = NewType(\r\n    \"AttentionWeights\",\r\n    Float[Array, \"B H N N\"],\r\n)\r\n\"\"\"\r\nAttention probability matrix (row-stochastic).\r\n\r\nInvariant:\r\n- Each row sums to 1 (after softmax)\r\n\"\"\"\r\n\r\nRawLogits = NewType(\r\n    \"RawLogits\",\r\n    Float[Array, \"B H N N\"],\r\n)\r\n\"\"\"Pre-softmax attention scores (unbounded).\"\"\"\r\n\r\nTokenIds = NewType(\r\n    \"TokenIds\",\r\n    Int[Array, \"B N\"],\r\n)\r\n\"\"\"Integer token indices.\"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/types/arrays.py", "file_name": "arrays.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "984", "text": "TokenIds = NewType(\r\n    \"TokenIds\",\r\n    Int[Array, \"B N\"],\r\n)\r\n\"\"\"Integer token indices.\"\"\"\r\n\r\nEmbeddings = NewType(\r\n    \"Embeddings\",\r\n    Float[Array, \"B N D\"],\r\n)\r\n\"\"\"Token embeddings.\"\"\"\r\n\r\nScaleOutputs = NewType(\r\n    \"ScaleOutputs\",\r\n    Float[Array, \"B L N D\"],\r\n)\r\n\"\"\"Stacked outputs from all hierarchical scales.\"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/types/arrays.py", "file_name": "arrays.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "985", "text": "\"\"\"Configuration types with compile-time invariant enforcement.\"\"\"\r\n\r\nfrom dataclasses import dataclass\r\nfrom typing import Literal\r\n\r\n\r\n@dataclass(frozen=True, slots=True)\r\nclass ModelConfig:\r\n    \"\"\"\r\n    Immutable model hyperparameters.\r\n\r\n    All dimensions are TPU-aligned (multiples of 128) for optimal MXU utilization.\r\n\r\n    Invariants enforced at construction:\r\n    - vocab_size % 128 == 0\r\n    - max_seq_len % 128 == 0\r\n    - d_model % 128 == 0\r\n    - d_model % num_heads == 0\r\n    - max_seq_len % (compression_ratio ^ (num_scales - 1)) == 0\r\n    \"\"\"\r\n\r\n    vocab_size: int\r\n    max_seq_len: int\r\n    d_model: int\r\n    num_heads: int\r\n    num_layers: int\r\n    num_scales: int\r\n    compression_ratio: int\r\n    ffn_multiplier: float\r\n    sinkhorn_iterations: int\r\n    nash_iterations: int\r\n    aggregation: Literal[\"nash\", \"convex\"]\r\n    dtype: Literal[\"bfloat16\", \"float32\"] = \"bfloat16\"\r\n\r\n    def __post_init__(self) -> None:\r\n        # TPU alignment invariants\r\n        if self.vocab_size % 128 != 0:\r\n            raise ValueError(f\"vocab_size must be multiple of 128, got {self.vocab_size}\")\r\n        if self.max_seq_len % 128 != 0:\r\n            raise ValueError(f\"max_seq_len must be multiple of 128, got {self.max_seq_len}\")\r\n        if self.d_model % 128 != 0:\r\n            raise ValueError(f\"d_model must be multiple of 128, got {self.d_model}\")\r\n\r\n        # Attention head invariant\r\n        if self.d_model % self.num_heads != 0:\r\n            raise ValueError(\r\n                f\"d_model ({self.d_model}) must be divisible by num_heads ({self.num_heads})\"\r\n            )\r\n\r\n        # Scale hierarchy invariant\r\n        if self.num_scales < 2:\r\n            raise ValueError(f\"num_scales must be >= 2, got {self.num_scales}\")\r\n        if self.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/types/configs.py", "file_name": "configs.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "986", "text": "num_heads})\"\r\n            )\r\n\r\n        # Scale hierarchy invariant\r\n        if self.num_scales < 2:\r\n            raise ValueError(f\"num_scales must be >= 2, got {self.num_scales}\")\r\n        if self.compression_ratio < 2:\r\n            raise ValueError(f\"compression_ratio must be >= 2, got {self.compression_ratio}\")\r\n\r\n        # Sequence alignment invariant\r\n        scale_factor = self.compression_ratio ** (self.num_scales - 1)\r\n        if self.max_seq_len % scale_factor != 0:\r\n            raise ValueError(\r\n                f\"max_seq_len ({self.max_seq_len}) must be divisible by \"\r\n                f\"compression_ratio^(num_scales-1) = {scale_factor}\"\r\n            )\r\n\r\n        min_scale_len = self.max_seq_len // scale_factor\r\n        if min_scale_len < 1:\r\n            raise ValueError(\r\n                f\"max_seq_len too small: coarsest scale would have length {min_scale_len}\"\r\n            )\r\n\r\n    @property\r\n    def head_dim(self) -> int:\r\n        \"\"\"Dimension per attention head.\"\"\"\r\n        return self.d_model // self.num_heads\r\n\r\n    @property\r\n    def ffn_hidden(self) -> int:\r\n        \"\"\"Hidden dimension for FFN (SwiGLU uses 2/3 factor internally).\"\"\"\r\n        return int(self.d_model * self.ffn_multiplier)\r\n\r\n    def scale_lengths(self) -> tuple[int, ...]:\r\n        \"\"\"Compute sequence lengths at each scale.\"\"\"\r\n        lengths = [self.max_seq_len]\r\n        for _ in range(self.num_scales - 1):\r\n            lengths.append(lengths[-1] // self.compression_ratio)\r\n        return tuple(lengths)\r\n\r\n\r\n@dataclass(frozen=True, slots=True)\r\nclass TrainingConfig:\r\n    \"\"\"Training hyperparameters.\"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/types/configs.py", "file_name": "configs.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "987", "text": "batch_size: int\r\n    gradient_accumulation_steps: int\r\n    learning_rate: float\r\n    warmup_steps: int\r\n    total_steps: int\r\n    weight_decay: float\r\n    max_grad_norm: float\r\n    lambda_sparsity: float\r\n    seed: int = 42\r\n\r\n    def __post_init__(self) -> None:\r\n        if self.batch_size <= 0:\r\n            raise ValueError(f\"batch_size must be positive, got {self.batch_size}\")\r\n        if self.gradient_accumulation_steps <= 0:\r\n            raise ValueError(\r\n                f\"gradient_accumulation_steps must be positive, got {self.gradient_accumulation_steps}\"\r\n            )\r\n        if self.learning_rate <= 0:\r\n            raise ValueError(f\"learning_rate must be positive, got {self.learning_rate}\")\r\n        if self.warmup_steps < 0:\r\n            raise ValueError(f\"warmup_steps must be non-negative, got {self.warmup_steps}\")\r\n        if self.total_steps <= 0:\r\n            raise ValueError(f\"total_steps must be positive, got {self.total_steps}\")\r\n        if self.warmup_steps >= self.total_steps:\r\n            raise ValueError(\r\n                f\"warmup_steps ({self.warmup_steps}) must be < total_steps ({self.total_steps})\"\r\n            )\r\n\r\n    @property\r\n    def effective_batch_size(self) -> int:\r\n        \"\"\"Batch size accounting for gradient accumulation.\"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/types/configs.py", "file_name": "configs.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "988", "text": "return self.batch_size * self.gradient_accumulation_steps\r\n\r\n\r\n# Default configurations for development\r\nDEFAULT_MODEL_CONFIG = ModelConfig(\r\n    vocab_size=32768,\r\n    max_seq_len=2048,\r\n    d_model=2048,\r\n    num_heads=16,\r\n    num_layers=24,\r\n    num_scales=4,\r\n    compression_ratio=2,\r\n    ffn_multiplier=2.67,\r\n    sinkhorn_iterations=10,\r\n    nash_iterations=3,\r\n    aggregation=\"nash\",\r\n    dtype=\"bfloat16\",\r\n)\r\n\r\nSMALL_MODEL_CONFIG = ModelConfig(\r\n    vocab_size=32768,\r\n    max_seq_len=512,\r\n    d_model=512,\r\n    num_heads=8,\r\n    num_layers=6,\r\n    num_scales=3,\r\n    compression_ratio=2,\r\n    ffn_multiplier=2.67,\r\n    sinkhorn_iterations=10,\r\n    nash_iterations=3,\r\n    aggregation=\"nash\",\r\n    dtype=\"float32\",\r\n)", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/types/configs.py", "file_name": "configs.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "989", "text": "\"\"\"Runtime invariant assertions for mathematical properties.\"\"\"\r\n\r\nimport jax.numpy as jnp\r\nfrom jaxtyping import Float, Array\r\n\r\n\r\ndef assert_doubly_stochastic(\r\n    m: Float[Array, \"... n n\"],\r\n    rtol: float = 1e-4,\r\n    atol: float = 1e-6,\r\n) -> None:\r\n    \"\"\"\r\n    Assert matrix is doubly stochastic (Birkhoff polytope member).\r\n\r\n    Checks:\r\n    - Rows sum to 1\r\n    - Columns sum to 1\r\n    - All entries non-negative\r\n\r\n    Raises:\r\n        AssertionError: If any invariant is violated.\r\n    \"\"\"\r\n    row_sums = jnp.sum(m, axis=-1)\r\n    col_sums = jnp.sum(m, axis=-2)\r\n    ones = jnp.ones_like(row_sums)\r\n\r\n    row_ok = jnp.allclose(row_sums, ones, rtol=rtol, atol=atol)\r\n    col_ok = jnp.allclose(col_sums, ones, rtol=rtol, atol=atol)\r\n    nonneg_ok = jnp.all(m >= -atol)\r\n\r\n    if not row_ok:\r\n        max_row_err = jnp.max(jnp.abs(row_sums - 1.0))\r\n        raise AssertionError(f\"Rows must sum to 1, max error: {max_row_err}\")\r\n    if not col_ok:\r\n        max_col_err = jnp.max(jnp.abs(col_sums - 1.0))\r\n        raise AssertionError(f\"Columns must sum to 1, max error: {max_col_err}\")\r\n    if not nonneg_ok:\r\n        min_val = jnp.min(m)\r\n        raise AssertionError(f\"All entries must be non-negative, min value: {min_val}\")\r\n\r\n\r\ndef assert_simplex(\r\n    w: Float[Array, \"... L\"],\r\n    rtol: float = 1e-5,\r\n    atol: float = 1e-7,\r\n) -> None:\r\n    \"\"\"\r\n    Assert weights are on the probability simplex.\r\n\r\n    Checks:\r\n    - Weights sum to 1\r\n    - All weights non-negative\r\n\r\n    Raises:\r\n        AssertionError: If any invariant is violated.\r\n    \"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/types/invariants.py", "file_name": "invariants.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "990", "text": "Checks:\r\n    - Weights sum to 1\r\n    - All weights non-negative\r\n\r\n    Raises:\r\n        AssertionError: If any invariant is violated.\r\n    \"\"\"\r\n    sums = jnp.sum(w, axis=-1)\r\n    ones = jnp.ones_like(sums)\r\n\r\n    sum_ok = jnp.allclose(sums, ones, rtol=rtol, atol=atol)\r\n    nonneg_ok = jnp.all(w >= -atol)\r\n\r\n    if not sum_ok:\r\n        max_err = jnp.max(jnp.abs(sums - 1.0))\r\n        raise AssertionError(f\"Weights must sum to 1, max error: {max_err}\")\r\n    if not nonneg_ok:\r\n        min_val = jnp.min(w)\r\n        raise AssertionError(f\"Weights must be non-negative, min value: {min_val}\")\r\n\r\n\r\ndef assert_spectral_norm_bounded(\r\n    m: Float[Array, \"... n d\"],\r\n    bound: float = 1.0,\r\n    rtol: float = 1e-4,\r\n) -> None:\r\n    \"\"\"\r\n    Assert matrix has spectral norm <= bound (non-expansive).\r\n\r\n    Uses SVD to compute the largest singular value.\r\n\r\n    Note: This is expensive for large matrices. Use sparingly in production.\r\n\r\n    Raises:\r\n        AssertionError: If spectral norm exceeds bound.\r\n    \"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/types/invariants.py", "file_name": "invariants.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "991", "text": "Uses SVD to compute the largest singular value.\r\n\r\n    Note: This is expensive for large matrices. Use sparingly in production.\r\n\r\n    Raises:\r\n        AssertionError: If spectral norm exceeds bound.\r\n    \"\"\"\r\n    # Flatten batch dimensions for SVD\r\n    original_shape = m.shape\r\n    m_2d = m.reshape(-1, original_shape[-2], original_shape[-1])\r\n\r\n    # Compute singular values for each batch element\r\n    max_sv = jnp.array(0.0)\r\n    for i in range(m_2d.shape[0]):\r\n        s = jnp.linalg.svd(m_2d[i], compute_uv=False, full_matrices=False)\r\n        max_sv = jnp.maximum(max_sv, jnp.max(s))\r\n\r\n    if max_sv > bound * (1 + rtol):\r\n        raise AssertionError(f\"Spectral norm {max_sv} exceeds bound {bound}\")\r\n\r\n\r\ndef assert_row_stochastic(\r\n    m: Float[Array, \"... n n\"],\r\n    rtol: float = 1e-4,\r\n    atol: float = 1e-6,\r\n) -> None:\r\n    \"\"\"\r\n    Assert matrix is row-stochastic (each row sums to 1).\r\n\r\n    Used for attention weight matrices after softmax.\r\n\r\n    Raises:\r\n        AssertionError: If any invariant is violated.\r\n    \"\"\"\r\n    row_sums = jnp.sum(m, axis=-1)\r\n    ones = jnp.ones_like(row_sums)\r\n\r\n    row_ok = jnp.allclose(row_sums, ones, rtol=rtol, atol=atol)\r\n    nonneg_ok = jnp.all(m >= -atol)\r\n\r\n    if not row_ok:\r\n        max_err = jnp.max(jnp.abs(row_sums - 1.0))\r\n        raise AssertionError(f\"Rows must sum to 1, max error: {max_err}\")\r\n    if not nonneg_ok:\r\n        min_val = jnp.min(m)\r\n        raise AssertionError(f\"All entries must be non-negative, min value: {min_val}\")\r\n\r\n\r\ndef check_sequence_alignment(\r\n    seq_len: int,\r\n    num_scales: int,\r\n    compression_ratio: int,\r\n) -> bool:\r\n    \"\"\"\r\n    Check if sequence length is valid for hierarchical decomposition.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/types/invariants.py", "file_name": "invariants.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "992", "text": "Returns True if seq_len is divisible by compression_ratio^(num_scales-1).\r\n    \"\"\"\r\n    scale_factor = compression_ratio ** (num_scales - 1)\r\n    return seq_len % scale_factor == 0", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/src/nash_mhc/types/invariants.py", "file_name": "invariants.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "994", "text": "\"\"\"Pytest configuration and fixtures.\"\"\"\r\n\r\nimport pytest\r\nimport jax\r\n\r\n\r\n@pytest.fixture\r\ndef key():\r\n    \"\"\"Provide a PRNG key for tests.\"\"\"\r\n    return jax.random.PRNGKey(42)\r\n\r\n\r\n@pytest.fixture\r\ndef small_config():\r\n    \"\"\"Provide a small model config for testing.\"\"\"\r\n    from nash_mhc.types.configs import SMALL_MODEL_CONFIG\r\n    return SMALL_MODEL_CONFIG", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/tests/conftest.py", "file_name": "conftest.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "996", "text": "\"\"\"Tests for Nash equilibrium solver invariants.\"\"\"\r\n\r\nimport jax\r\nimport jax.numpy as jnp\r\nimport pytest\r\nfrom hypothesis import given, settings\r\nfrom hypothesis import strategies as st\r\n\r\nfrom nash_mhc.primitives.nash_solver import nash_best_response\r\nfrom nash_mhc.types.invariants import assert_simplex\r\n\r\n\r\nclass TestNashInvariants:\r\n    \"\"\"Test that Nash solver output satisfies simplex invariants.\"\"\"\r\n\r\n    def test_weights_simplex(self, key):\r\n        \"\"\"Weights form a valid probability distribution.\"\"\"\r\n        scale_outputs = jax.random.normal(key, (2, 4, 32, 64))\r\n        aggregated, weights = nash_best_response(scale_outputs, num_iterations=5)\r\n\r\n        # Weights sum to 1\r\n        weight_sums = jnp.sum(weights, axis=-1)\r\n        assert jnp.allclose(weight_sums, 1.0, rtol=1e-5)\r\n\r\n        # Weights are non-negative\r\n        assert jnp.all(weights >= 0)\r\n\r\n    def test_output_shape_preserved(self, key):\r\n        \"\"\"Aggregated output has correct shape.\"\"\"\r\n        B, L, N, D = 2, 4, 32, 64\r\n        scale_outputs = jax.random.normal(key, (B, L, N, D))\r\n        aggregated, weights = nash_best_response(scale_outputs, num_iterations=3)\r\n\r\n        assert aggregated.shape == (B, N, D)\r\n        assert weights.shape == (B, L)\r\n\r\n    def test_more_iterations_stabilizes(self, key):\r\n        \"\"\"More iterations lead to more stable weights.\"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/tests/invariants/test_nash.py", "file_name": "test_nash.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "997", "text": "scale_outputs = jax.random.normal(key, (2, 4, 32, 64))\r\n\r\n        _, weights_1 = nash_best_response(scale_outputs, num_iterations=1)\r\n        _, weights_5 = nash_best_response(scale_outputs, num_iterations=5)\r\n        _, weights_10 = nash_best_response(scale_outputs, num_iterations=10)\r\n\r\n        # Weights should remain on simplex regardless of iterations\r\n        for w in [weights_1, weights_5, weights_10]:\r\n            assert_simplex(w, rtol=1e-5)\r\n\r\n    def test_uniform_input_uniform_weights(self, key):\r\n        \"\"\"If all scales are identical, weights should be uniform.\"\"\"\r\n        B, L, N, D = 2, 4, 32, 64\r\n        single_scale = jax.random.normal(key, (B, N, D))\r\n\r\n        # Stack identical scales\r\n        scale_outputs = jnp.stack([single_scale] * L, axis=1)\r\n\r\n        _, weights = nash_best_response(scale_outputs, num_iterations=10)\r\n\r\n        # Weights should be approximately uniform (1/L each)\r\n        expected = jnp.ones((B, L)) / L\r\n        assert jnp.allclose(weights, expected, rtol=1e-3)\r\n\r\n    def test_gradient_exists(self, key):\r\n        \"\"\"Nash solver is differentiable.\"\"\"\r\n        scale_outputs = jax.random.normal(key, (2, 4, 32, 64))\r\n\r\n        def loss_fn(scale_outputs):\r\n            aggregated, _ = nash_best_response(scale_outputs, num_iterations=3)\r\n            return jnp.sum(aggregated ** 2)\r\n\r\n        grad = jax.grad(loss_fn)(scale_outputs)\r\n\r\n        assert not jnp.any(jnp.isnan(grad))\r\n        assert not jnp.any(jnp.isinf(grad))\r\n\r\n    def test_aggregation_is_weighted_sum(self, key):\r\n        \"\"\"Aggregated output is weighted sum of inputs.\"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/tests/invariants/test_nash.py", "file_name": "test_nash.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "998", "text": "B, L, N, D = 2, 4, 32, 64\r\n        scale_outputs = jax.random.normal(key, (B, L, N, D))\r\n\r\n        aggregated, weights = nash_best_response(scale_outputs, num_iterations=5)\r\n\r\n        # Recompute expected aggregation\r\n        w_expanded = weights[:, :, None, None]\r\n        expected = jnp.sum(scale_outputs * w_expanded, axis=1)\r\n\r\n        assert jnp.allclose(aggregated, expected, rtol=1e-5)\r\n\r\n\r\n@settings(max_examples=20, deadline=5000)\r\n@given(\r\n    B=st.integers(min_value=1, max_value=4),\r\n    L=st.integers(min_value=2, max_value=6),\r\n    N=st.integers(min_value=8, max_value=32),\r\n    D=st.integers(min_value=16, max_value=64),\r\n    seed=st.integers(min_value=0, max_value=2**16),\r\n)\r\ndef test_nash_property_simplex(B: int, L: int, N: int, D: int, seed: int):\r\n    \"\"\"Property test: Nash weights are always on simplex.\"\"\"\r\n    key = jax.random.PRNGKey(seed)\r\n    scale_outputs = jax.random.normal(key, (B, L, N, D))\r\n\r\n    _, weights = nash_best_response(scale_outputs, num_iterations=3)\r\n\r\n    # Sum to 1\r\n    weight_sums = jnp.sum(weights, axis=-1)\r\n    assert jnp.allclose(weight_sums, 1.0, rtol=1e-4)\r\n\r\n    # Non-negative\r\n    assert jnp.all(weights >= -1e-6)", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/tests/invariants/test_nash.py", "file_name": "test_nash.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "999", "text": "\"\"\"Tests for Sinkhorn-Knopp invariants.\"\"\"\r\n\r\nimport jax\r\nimport jax.numpy as jnp\r\nimport pytest\r\nfrom hypothesis import given, settings\r\nfrom hypothesis import strategies as st\r\n\r\nfrom nash_mhc.primitives.sinkhorn import sinkhorn_knopp\r\nfrom nash_mhc.types.invariants import assert_doubly_stochastic\r\n\r\n\r\nclass TestSinkhornInvariants:\r\n    \"\"\"Test that Sinkhorn output satisfies doubly stochastic invariants.\"\"\"\r\n\r\n    def test_doubly_stochastic_small(self, key):\r\n        \"\"\"Output is doubly stochastic for small matrices.\"\"\"\r\n        log_alpha = jax.random.normal(key, (8, 8))\r\n        P = sinkhorn_knopp(log_alpha, num_iterations=20)\r\n\r\n        # Rows sum to 1\r\n        row_sums = jnp.sum(P, axis=-1)\r\n        assert jnp.allclose(row_sums, 1.0, rtol=1e-4)\r\n\r\n        # Columns sum to 1\r\n        col_sums = jnp.sum(P, axis=-2)\r\n        assert jnp.allclose(col_sums, 1.0, rtol=1e-4)\r\n\r\n        # All entries non-negative\r\n        assert jnp.all(P >= -1e-6)\r\n\r\n    def test_doubly_stochastic_larger(self, key):\r\n        \"\"\"Output is doubly stochastic for larger matrices.\"\"\"\r\n        log_alpha = jax.random.normal(key, (64, 64))\r\n        P = sinkhorn_knopp(log_alpha, num_iterations=20)\r\n\r\n        assert_doubly_stochastic(P, rtol=1e-3, atol=1e-5)\r\n\r\n    def test_doubly_stochastic_batched(self, key):\r\n        \"\"\"Output is doubly stochastic for batched input.\"\"\"\r\n        log_alpha = jax.random.normal(key, (4, 16, 16))\r\n        P = sinkhorn_knopp(log_alpha, num_iterations=15)\r\n\r\n        # Check each batch element\r\n        for i in range(4):\r\n            assert_doubly_stochastic(P[i], rtol=1e-3, atol=1e-5)\r\n\r\n    def test_iteration_convergence(self, key):\r\n        \"\"\"More iterations improve convergence.\"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/tests/invariants/test_sinkhorn.py", "file_name": "test_sinkhorn.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "1000", "text": "log_alpha = jax.random.normal(key, (32, 32))\r\n\r\n        P_5 = sinkhorn_knopp(log_alpha, num_iterations=5)\r\n        P_20 = sinkhorn_knopp(log_alpha, num_iterations=20)\r\n\r\n        # Compute error from doubly stochastic\r\n        err_5 = jnp.max(jnp.abs(jnp.sum(P_5, axis=-1) - 1.0))\r\n        err_20 = jnp.max(jnp.abs(jnp.sum(P_20, axis=-1) - 1.0))\r\n\r\n        # More iterations should give lower error\r\n        assert err_20 <= err_5\r\n\r\n    def test_gradient_exists(self, key):\r\n        \"\"\"Sinkhorn is differentiable.\"\"\"\r\n        log_alpha = jax.random.normal(key, (8, 8))\r\n\r\n        def loss_fn(log_alpha):\r\n            P = sinkhorn_knopp(log_alpha, num_iterations=10)\r\n            return jnp.sum(P ** 2)\r\n\r\n        grad = jax.grad(loss_fn)(log_alpha)\r\n\r\n        # Gradient should exist and be finite\r\n        assert not jnp.any(jnp.isnan(grad))\r\n        assert not jnp.any(jnp.isinf(grad))\r\n\r\n    def test_preserves_dtype(self, key):\r\n        \"\"\"Output dtype matches input dtype.\"\"\"\r\n        log_alpha_f32 = jax.random.normal(key, (8, 8), dtype=jnp.float32)\r\n        log_alpha_bf16 = log_alpha_f32.astype(jnp.bfloat16)\r\n\r\n        P_f32 = sinkhorn_knopp(log_alpha_f32, num_iterations=10)\r\n        P_bf16 = sinkhorn_knopp(log_alpha_bf16, num_iterations=10)\r\n\r\n        assert P_f32.dtype == jnp.float32\r\n        assert P_bf16.dtype == jnp.bfloat16\r\n\r\n\r\n@settings(max_examples=20, deadline=5000)\r\n@given(\r\n    n=st.integers(min_value=4, max_value=32),\r\n    seed=st.integers(min_value=0, max_value=2**16),\r\n)\r\ndef test_sinkhorn_property_doubly_stochastic(n: int, seed: int):\r\n    \"\"\"Property test: Sinkhorn output is always doubly stochastic.\"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/tests/invariants/test_sinkhorn.py", "file_name": "test_sinkhorn.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "1001", "text": "key = jax.random.PRNGKey(seed)\r\n    log_alpha = jax.random.normal(key, (n, n))\r\n\r\n    P = sinkhorn_knopp(log_alpha, num_iterations=20)\r\n\r\n    row_sums = jnp.sum(P, axis=-1)\r\n    col_sums = jnp.sum(P, axis=-2)\r\n\r\n    assert jnp.allclose(row_sums, 1.0, rtol=1e-3, atol=1e-4)\r\n    assert jnp.allclose(col_sums, 1.0, rtol=1e-3, atol=1e-4)\r\n    assert jnp.all(P >= -1e-5)", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/tests/invariants/test_sinkhorn.py", "file_name": "test_sinkhorn.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "1003", "text": "[environment]\npython = \".venv\"\npython-version = \"3.13\"\nroot = [\"./src\"]\n\n[rules]\n# JAX JIT/vmap/pmap decorators transform function signatures in ways ty can't track.\n# These cause false positives; demote to warnings until JAX stubs improve.\npossibly-unresolved-reference = \"warn\"\ninvalid-argument-type = \"warn\"\ntoo-many-positional-arguments = \"warn\"\nmissing-argument = \"warn\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/ty.toml", "file_name": "ty.toml", "creation_date": "2026-01-02", "last_modified_date": "2026-01-03"}}
