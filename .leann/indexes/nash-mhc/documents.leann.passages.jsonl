{"id": "0", "text": "1\nMultiscale Aggregated Hierarchical Attention\n(MAHA): A Game-Theoretic and\nOptimization-Driven Approach to Efficient\nContextual Modeling in Large Language Models\nCaner Erden\nAbstract—The quadratic computational complexity of Multi-\nHead Self-Attention (MHSA) remains a fundamental bottleneck\nin scaling Large Language Models (LLMs) for long-context tasks.\nWhile sparse and linearized attention mechanisms attempt to\nmitigate this, they often compromise the representation of global\ndependencies or fail to capture multiscale semantic granularity\neffectively. In this paper, we propose Multiscale Aggregated\nHierarchical Attention (MAHA), a novel architectural framework\nthat reformulates the attention mechanism through hierarchical\ndecomposition and mathematically rigorous aggregation. Unlike\nconventional approaches that treat token interactions at a single\nresolution, MAHA dynamically partitions the input sequence into\nhierarchical scales via learnable downsampling operators.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "1", "text": "While sparse and linearized attention mechanisms attempt to\nmitigate this, they often compromise the representation of global\ndependencies or fail to capture multiscale semantic granularity\neffectively. In this paper, we propose Multiscale Aggregated\nHierarchical Attention (MAHA), a novel architectural framework\nthat reformulates the attention mechanism through hierarchical\ndecomposition and mathematically rigorous aggregation. Unlike\nconventional approaches that treat token interactions at a single\nresolution, MAHA dynamically partitions the input sequence into\nhierarchical scales via learnable downsampling operators. The\ncore innovation lies in its aggregation strategy: we model the\nfusion of scale-specific attention matrices as a resource allocation\nproblem, solved via a convex optimization framework or a Nash\nequilibrium-based game-theoretic approach. This ensures a theo-\nretically optimal balance between local nuance and global context\nfidelity. Implemented within a hybrid dilated-convolutional trans-\nformer backbone, MAHA utilizes differentiable optimization\nlayers to enable end-to-end training.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "2", "text": "The\ncore innovation lies in its aggregation strategy: we model the\nfusion of scale-specific attention matrices as a resource allocation\nproblem, solved via a convex optimization framework or a Nash\nequilibrium-based game-theoretic approach. This ensures a theo-\nretically optimal balance between local nuance and global context\nfidelity. Implemented within a hybrid dilated-convolutional trans-\nformer backbone, MAHA utilizes differentiable optimization\nlayers to enable end-to-end training. Experimental evaluations\ndemonstrate that MAHA achieves superior scalability; empirical\nFLOPs analysis confirms an 81% reduction in computational cost\nat a sequence length of 4096 compared to standard attention. This\nwork bridges the gap between optimization theory and sequence\nmodeling, offering a scalable solution for next-generation LLMs.\nKeywords: Large Language Models, Hierarchical Attention,\nGame Theory, Convex Optimization, Nash Equilibrium, Efficient\nTransformers.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "3", "text": "Implemented within a hybrid dilated-convolutional trans-\nformer backbone, MAHA utilizes differentiable optimization\nlayers to enable end-to-end training. Experimental evaluations\ndemonstrate that MAHA achieves superior scalability; empirical\nFLOPs analysis confirms an 81% reduction in computational cost\nat a sequence length of 4096 compared to standard attention. This\nwork bridges the gap between optimization theory and sequence\nmodeling, offering a scalable solution for next-generation LLMs.\nKeywords: Large Language Models, Hierarchical Attention,\nGame Theory, Convex Optimization, Nash Equilibrium, Efficient\nTransformers.\nI. INTRODUCTION\nThe advent of transformer-based architectures has funda-\nmentally revolutionized natural language processing (NLP),\nestablishing the Multi-Head Self-Attention (MHSA) mecha-\nnism as the cornerstone of modern large language models\n(LLMs) [1].", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "4", "text": "This\nwork bridges the gap between optimization theory and sequence\nmodeling, offering a scalable solution for next-generation LLMs.\nKeywords: Large Language Models, Hierarchical Attention,\nGame Theory, Convex Optimization, Nash Equilibrium, Efficient\nTransformers.\nI. INTRODUCTION\nThe advent of transformer-based architectures has funda-\nmentally revolutionized natural language processing (NLP),\nestablishing the Multi-Head Self-Attention (MHSA) mecha-\nnism as the cornerstone of modern large language models\n(LLMs) [1]. Despite its efficacy, this mechanism confronts\ntwo critical challenges: (i) computational inefficiency arising\nfrom quadratic complexity (O(N 2)) with respect to sequence\nlength, and (ii) the inherent trade-off between capturing fine-\ngrained local patterns and coarse-grained global dependencies\nThis work was supported in part by Sakarya University of Applied Sciences.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "5", "text": "Despite its efficacy, this mechanism confronts\ntwo critical challenges: (i) computational inefficiency arising\nfrom quadratic complexity (O(N 2)) with respect to sequence\nlength, and (ii) the inherent trade-off between capturing fine-\ngrained local patterns and coarse-grained global dependencies\nThis work was supported in part by Sakarya University of Applied Sciences.\n(Corresponding author: Caner Erden)\nC. Erden is with the Department of Computer Engineering, Faculty of\nTechnology, Sakarya University of Applied Science, Sakarya, T¨urkiye (e-mail:\ncerden@subu.edu.tr; ORCID: 0000-0002-7311-862X).\nData\nAvailability:\nThe\nsource\ncode\nis\navailable\nat\nhttps://github.com/canererden/MAHA-Project\n(arXiv:\nhttps://arxiv.org/abs/2512.14925).\nsimultaneously. These limitations become increasingly pro-\nnounced as LLMs scale to process extended contexts and\nmodel complex linguistic structures [2].", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "6", "text": "Data\nAvailability:\nThe\nsource\ncode\nis\navailable\nat\nhttps://github.com/canererden/MAHA-Project\n(arXiv:\nhttps://arxiv.org/abs/2512.14925).\nsimultaneously. These limitations become increasingly pro-\nnounced as LLMs scale to process extended contexts and\nmodel complex linguistic structures [2].\nCurrent methodologies attempting to mitigate these chal-\nlenges typically rely on sparse attention patterns or hierarchical\nrepresentations. Sparse attention strategies alleviate computa-\ntional overhead by restricting token interactions to predefined\nor learned patterns; however, this often results in informa-\ntion loss and suboptimal context modeling, particularly for\nlong-range dependencies [3]. Conversely, hierarchical meth-\nods decompose the input into multiple levels of granularity\nbut frequently lack a principled mathematical framework for\nintegration. This often leads to ad-hoc aggregation schemes\nthat fail to preserve the full contextual richness of the input\nembedding space [4].", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "7", "text": "Sparse attention strategies alleviate computa-\ntional overhead by restricting token interactions to predefined\nor learned patterns; however, this often results in informa-\ntion loss and suboptimal context modeling, particularly for\nlong-range dependencies [3]. Conversely, hierarchical meth-\nods decompose the input into multiple levels of granularity\nbut frequently lack a principled mathematical framework for\nintegration. This often leads to ad-hoc aggregation schemes\nthat fail to preserve the full contextual richness of the input\nembedding space [4].\nTo bridge this gap, we introduce Multiscale Aggregated\nHierarchical Attention (MAHA), a novel framework that ad-\ndresses these limitations through a mathematically rigorous\napproach to multiscale attention computation and aggregation.\nMAHA dynamically partitions the input sequence into hier-\narchical scales, where each scale represents a distinct level\nof contextual abstraction. Distinguishing itself from prior hi-\nerarchical approaches, MAHA leverages convex optimization\n(CO) or game-theoretic equilibrium to synthesize these scales.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "8", "text": "To bridge this gap, we introduce Multiscale Aggregated\nHierarchical Attention (MAHA), a novel framework that ad-\ndresses these limitations through a mathematically rigorous\napproach to multiscale attention computation and aggregation.\nMAHA dynamically partitions the input sequence into hier-\narchical scales, where each scale represents a distinct level\nof contextual abstraction. Distinguishing itself from prior hi-\nerarchical approaches, MAHA leverages convex optimization\n(CO) or game-theoretic equilibrium to synthesize these scales.\nThis ensures that the aggregation process is not merely a\nweighted average but an optimization problem that balances\nefficiency and contextual awareness. Consequently, the pro-\nposed method provides a systematic mechanism to reconcile\nlocal nuances with global dependencies while maintaining\ncomputational tractability.\nThe primary contributions of this work are threefold:\n• Multiscale Decomposition: We introduce a robust de-\ncomposition strategy where the input sequence is pro-\ncessed across independent scales to isolate and capture\ndistinct levels of contextual granularity.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "9", "text": "This ensures that the aggregation process is not merely a\nweighted average but an optimization problem that balances\nefficiency and contextual awareness. Consequently, the pro-\nposed method provides a systematic mechanism to reconcile\nlocal nuances with global dependencies while maintaining\ncomputational tractability.\nThe primary contributions of this work are threefold:\n• Multiscale Decomposition: We introduce a robust de-\ncomposition strategy where the input sequence is pro-\ncessed across independent scales to isolate and capture\ndistinct levels of contextual granularity.\n• Optimization-Driven Aggregation: We propose a novel\naggregation mechanism governed by convex optimization\nand game-theoretic principles. This allows the model\nto determine the optimal trade-off between local and\nglobal context dynamically, rather than relying on static\nor heuristic fusion methods.\n• Computational Efficiency: MAHA significantly reduces\nthe quadratic complexity characteristic of standard atten-\narXiv:2512.14925v2  [cs.CL]  18 Dec 2025\n2\ntion mechanisms, enhancing scalability without compro-\nmising the model’s expressive power.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "10", "text": "• Optimization-Driven Aggregation: We propose a novel\naggregation mechanism governed by convex optimization\nand game-theoretic principles. This allows the model\nto determine the optimal trade-off between local and\nglobal context dynamically, rather than relying on static\nor heuristic fusion methods.\n• Computational Efficiency: MAHA significantly reduces\nthe quadratic complexity characteristic of standard atten-\narXiv:2512.14925v2  [cs.CL]  18 Dec 2025\n2\ntion mechanisms, enhancing scalability without compro-\nmising the model’s expressive power.\nMAHA is particularly pertinent to the evolution of LLMs,\nwhere the demand for efficient and scalable attention mech-\nanisms is paramount [5]. By integrating rigorous multiscale\nanalysis with optimization-based aggregation rules, MAHA\noffers a versatile solution adaptable to various transformer-\nbased architectures with minimal architectural overhead. The\nframework is designed for compatibility with existing LLM\ntraining pipelines, ensuring practicality for real-world deploy-\nment.\nII.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "11", "text": "MAHA is particularly pertinent to the evolution of LLMs,\nwhere the demand for efficient and scalable attention mech-\nanisms is paramount [5]. By integrating rigorous multiscale\nanalysis with optimization-based aggregation rules, MAHA\noffers a versatile solution adaptable to various transformer-\nbased architectures with minimal architectural overhead. The\nframework is designed for compatibility with existing LLM\ntraining pipelines, ensuring practicality for real-world deploy-\nment.\nII. RELATED WORK\nThe development of efficient attention mechanisms has be-\ncome a focal point in transformer-based architecture research,\nwith numerous approaches proposed to alleviate computa-\ntional bottlenecks and enhance contextual modeling capa-\nbilities. Existing literature can be broadly categorized into\nsparse attention methods, hierarchical attention frameworks,\nand optimization-driven aggregation techniques.\nA. Sparse Attention Mechanisms\nSparse attention mechanisms aim to reduce computational\noverhead by limiting token interactions to predefined or\nlearned patterns.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "12", "text": "The\nframework is designed for compatibility with existing LLM\ntraining pipelines, ensuring practicality for real-world deploy-\nment.\nII. RELATED WORK\nThe development of efficient attention mechanisms has be-\ncome a focal point in transformer-based architecture research,\nwith numerous approaches proposed to alleviate computa-\ntional bottlenecks and enhance contextual modeling capa-\nbilities. Existing literature can be broadly categorized into\nsparse attention methods, hierarchical attention frameworks,\nand optimization-driven aggregation techniques.\nA. Sparse Attention Mechanisms\nSparse attention mechanisms aim to reduce computational\noverhead by limiting token interactions to predefined or\nlearned patterns. For instance, [6] introduced a sliding window\nattention mechanism that restricts each token’s receptive field\nto its local neighborhood, significantly lowering memory re-\nquirements from O(n2) to O(n) for long sequences. Similarly,\n[7] proposed a hybrid approach combining local, global, and\nrandom attention patterns to approximate full self-attention\nwhile maintaining theoretical expressiveness.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "13", "text": "A. Sparse Attention Mechanisms\nSparse attention mechanisms aim to reduce computational\noverhead by limiting token interactions to predefined or\nlearned patterns. For instance, [6] introduced a sliding window\nattention mechanism that restricts each token’s receptive field\nto its local neighborhood, significantly lowering memory re-\nquirements from O(n2) to O(n) for long sequences. Similarly,\n[7] proposed a hybrid approach combining local, global, and\nrandom attention patterns to approximate full self-attention\nwhile maintaining theoretical expressiveness. However, these\nmethods often rely on heuristics to determine sparsity patterns,\nwhich may not adapt dynamically to diverse input structures or\ncapture long-range dependencies effectively without stacking\nmultiple layers.\nB. Hierarchical Attention Frameworks\nHierarchical approaches decompose input sequences into\nmultiple levels of granularity to capture both local syntactic\nfeatures and global semantic dependencies simultaneously.\nThe Hierarchical Attention Network (HAN) [3] processes\ndocuments at word and sentence levels, aggregating infor-\nmation through learned attention weights.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "14", "text": "However, these\nmethods often rely on heuristics to determine sparsity patterns,\nwhich may not adapt dynamically to diverse input structures or\ncapture long-range dependencies effectively without stacking\nmultiple layers.\nB. Hierarchical Attention Frameworks\nHierarchical approaches decompose input sequences into\nmultiple levels of granularity to capture both local syntactic\nfeatures and global semantic dependencies simultaneously.\nThe Hierarchical Attention Network (HAN) [3] processes\ndocuments at word and sentence levels, aggregating infor-\nmation through learned attention weights. More recently, [8]\nintroduced a hierarchical attention mechanism (hi-attention)\nthat integrates inter-layer information to improve sequence\nmodeling. While effective, these methods typically employ\nfixed or ad-hoc aggregation rules—such as weighted averag-\ning—which may not optimally balance the contributions from\ndifferent scales, leading to information dilution.\nC. Optimization-Driven and Game-Theoretic Aggregation\nOptimization techniques have been increasingly integrated\ninto neural architectures to enhance efficiency and robust-\nness.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "15", "text": "More recently, [8]\nintroduced a hierarchical attention mechanism (hi-attention)\nthat integrates inter-layer information to improve sequence\nmodeling. While effective, these methods typically employ\nfixed or ad-hoc aggregation rules—such as weighted averag-\ning—which may not optimally balance the contributions from\ndifferent scales, leading to information dilution.\nC. Optimization-Driven and Game-Theoretic Aggregation\nOptimization techniques have been increasingly integrated\ninto neural architectures to enhance efficiency and robust-\nness. For example, [9] utilized hierarchical decomposition\nto interpret intermediate CNN decisions, demonstrating the\npotential of optimization-based feature integration. In the\ncontext of sequence modeling, [10] explored multi-head self-\nattention with hierarchical aggregation but did not incorporate\nrigorous convex optimization or game-theoretic principles.\nGame theory, particularly the concept of Nash equilibrium, has\nbeen successfully employed in multi-agent systems to resolve\nconflicts [11]. Its application to attention mechanisms offers\na principled pathway to resolve conflicts between competing\nattention scales, a direction that remains largely unexplored in\ncurrent LLM architectures.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "16", "text": "For example, [9] utilized hierarchical decomposition\nto interpret intermediate CNN decisions, demonstrating the\npotential of optimization-based feature integration. In the\ncontext of sequence modeling, [10] explored multi-head self-\nattention with hierarchical aggregation but did not incorporate\nrigorous convex optimization or game-theoretic principles.\nGame theory, particularly the concept of Nash equilibrium, has\nbeen successfully employed in multi-agent systems to resolve\nconflicts [11]. Its application to attention mechanisms offers\na principled pathway to resolve conflicts between competing\nattention scales, a direction that remains largely unexplored in\ncurrent LLM architectures.\nD. Multiscale Analysis in Language Models\nMultiscale analysis is a staple in signal processing and com-\nputer vision [4], yet its direct application to language modeling\nremains limited. Recent work by [12] demonstrated the effec-\ntiveness of hierarchical decomposition in graph convolutional\nnetworks, suggesting potential benefits for attention mech-\nanisms. Similarly, [13] proposed hierarchical decomposition\nfor continual learning, highlighting the importance of scale-\nspecific feature extraction.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "17", "text": "D. Multiscale Analysis in Language Models\nMultiscale analysis is a staple in signal processing and com-\nputer vision [4], yet its direct application to language modeling\nremains limited. Recent work by [12] demonstrated the effec-\ntiveness of hierarchical decomposition in graph convolutional\nnetworks, suggesting potential benefits for attention mech-\nanisms. Similarly, [13] proposed hierarchical decomposition\nfor continual learning, highlighting the importance of scale-\nspecific feature extraction. These studies provide empirical\nevidence that processing information at varying resolutions can\nenhance representation learning.\nE. Integration of Optimization and Attention\nThe integration of differentiable optimization layers with\nattention mechanisms represents an emerging research frontier.\nWhile [12] applied hierarchical attention to fraud detection,\ntheir aggregation method lacked strong theoretical guarantees.\nIn contrast, the proposed MAHA framework distinguishes\nitself by unifying multiscale decomposition with rigorous\naggregation rules. Unlike sparse attention methods, MAHA\ndynamically adjusts the scale of token interactions without\nrelying on predefined patterns.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "18", "text": "These studies provide empirical\nevidence that processing information at varying resolutions can\nenhance representation learning.\nE. Integration of Optimization and Attention\nThe integration of differentiable optimization layers with\nattention mechanisms represents an emerging research frontier.\nWhile [12] applied hierarchical attention to fraud detection,\ntheir aggregation method lacked strong theoretical guarantees.\nIn contrast, the proposed MAHA framework distinguishes\nitself by unifying multiscale decomposition with rigorous\naggregation rules. Unlike sparse attention methods, MAHA\ndynamically adjusts the scale of token interactions without\nrelying on predefined patterns. Compared to existing hierar-\nchical approaches, it employs convex optimization or Nash\nequilibrium (NE) to optimally combine attention scores. This\ncombination enables MAHA to achieve superior computa-\ntional efficiency and contextual modeling, addressing the key\nlimitations of heuristic-based aggregation.\nIII. PRELIMINARIES AND BACKGROUND\nTo establish the theoretical foundation for MAHA, we\nbriefly review key concepts in attention mechanisms, multi-\nscale analysis, and game-theoretic optimization.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "19", "text": "Compared to existing hierar-\nchical approaches, it employs convex optimization or Nash\nequilibrium (NE) to optimally combine attention scores. This\ncombination enables MAHA to achieve superior computa-\ntional efficiency and contextual modeling, addressing the key\nlimitations of heuristic-based aggregation.\nIII. PRELIMINARIES AND BACKGROUND\nTo establish the theoretical foundation for MAHA, we\nbriefly review key concepts in attention mechanisms, multi-\nscale analysis, and game-theoretic optimization. These com-\nponents form the basis of our proposed framework.\nA. Attention Mechanisms in Transformers\nThe standard attention mechanism in transformers computes\npairwise interactions between all tokens in a sequence through\nscaled dot-product operations [1].", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "20", "text": "This\ncombination enables MAHA to achieve superior computa-\ntional efficiency and contextual modeling, addressing the key\nlimitations of heuristic-based aggregation.\nIII. PRELIMINARIES AND BACKGROUND\nTo establish the theoretical foundation for MAHA, we\nbriefly review key concepts in attention mechanisms, multi-\nscale analysis, and game-theoretic optimization. These com-\nponents form the basis of our proposed framework.\nA. Attention Mechanisms in Transformers\nThe standard attention mechanism in transformers computes\npairwise interactions between all tokens in a sequence through\nscaled dot-product operations [1]. Given an input sequence\nX ∈Rn×d, where n is the sequence length and d is the\nembedding dimension, the attention matrix A is computed as:\nA = softmax\n\u0012QKT\n√dk\n\u0013\n(1)\nwhere Q, K ∈Rn×dk are the query and key matrices,\nrespectively, and dk is the dimension of the keys.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "21", "text": "These com-\nponents form the basis of our proposed framework.\nA. Attention Mechanisms in Transformers\nThe standard attention mechanism in transformers computes\npairwise interactions between all tokens in a sequence through\nscaled dot-product operations [1]. Given an input sequence\nX ∈Rn×d, where n is the sequence length and d is the\nembedding dimension, the attention matrix A is computed as:\nA = softmax\n\u0012QKT\n√dk\n\u0013\n(1)\nwhere Q, K ∈Rn×dk are the query and key matrices,\nrespectively, and dk is the dimension of the keys. While\neffective, this operation exhibits quadratic complexity O(n2)\nin both computation and memory, rendering it impractical for\nvery long sequences [2].\n3\nB. Multiscale Signal Decomposition\nMultiscale analysis provides a rigorous framework for ex-\namining signals at varying levels of resolution. In NLP, this\ntranslates to capturing both local syntactic patterns (high\nfrequency) and global semantic structures (low frequency) [4].", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "22", "text": "While\neffective, this operation exhibits quadratic complexity O(n2)\nin both computation and memory, rendering it impractical for\nvery long sequences [2].\n3\nB. Multiscale Signal Decomposition\nMultiscale analysis provides a rigorous framework for ex-\namining signals at varying levels of resolution. In NLP, this\ntranslates to capturing both local syntactic patterns (high\nfrequency) and global semantic structures (low frequency) [4].\nInspired by wavelet transforms and pyramid decomposition\n[13], for a discrete signal representation x, a multiscale\ndecomposition can be expressed as:\nx =\nS\nX\ns=1\nDs(x) + R(x)\n(2)\nwhere Ds represents the detail component at scale s, and R\ndenotes the residual (coarse) component. This decomposition\nforms the structural basis for MAHA’s hierarchical processing\nlayers.\nC. Game-Theoretic Optimization\nGame theory provides mathematical tools for modeling\ninteractions between multiple decision-makers.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "23", "text": "Inspired by wavelet transforms and pyramid decomposition\n[13], for a discrete signal representation x, a multiscale\ndecomposition can be expressed as:\nx =\nS\nX\ns=1\nDs(x) + R(x)\n(2)\nwhere Ds represents the detail component at scale s, and R\ndenotes the residual (coarse) component. This decomposition\nforms the structural basis for MAHA’s hierarchical processing\nlayers.\nC. Game-Theoretic Optimization\nGame theory provides mathematical tools for modeling\ninteractions between multiple decision-makers. The concept\nof Nash equilibrium [14] is particularly relevant for MAHA’s\naggregation phase, where different attention scales can be\nmodeled as “players” competing for influence in the final\nrepresentation. Given a game with N players and strategy sets\nSi, a Nash equilibrium is a strategy profile s∗= (s∗\n1, . . .", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "24", "text": "This decomposition\nforms the structural basis for MAHA’s hierarchical processing\nlayers.\nC. Game-Theoretic Optimization\nGame theory provides mathematical tools for modeling\ninteractions between multiple decision-makers. The concept\nof Nash equilibrium [14] is particularly relevant for MAHA’s\naggregation phase, where different attention scales can be\nmodeled as “players” competing for influence in the final\nrepresentation. Given a game with N players and strategy sets\nSi, a Nash equilibrium is a strategy profile s∗= (s∗\n1, . . . , s∗\nN)\nsuch that for every player i:\nui(s∗\ni , s∗\n−i) ≥ui(si, s∗\n−i)\n∀si ∈Si\n(3)\nwhere ui is the utility function for player i and s∗\n−i denotes\nthe strategies of all other players. This equilibrium condition\nensures that no scale (player) can improve its contribution\nutility by unilaterally changing its attention weights, leading\nto a stable and optimal context representation.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "25", "text": ". . , s∗\nN)\nsuch that for every player i:\nui(s∗\ni , s∗\n−i) ≥ui(si, s∗\n−i)\n∀si ∈Si\n(3)\nwhere ui is the utility function for player i and s∗\n−i denotes\nthe strategies of all other players. This equilibrium condition\nensures that no scale (player) can improve its contribution\nutility by unilaterally changing its attention weights, leading\nto a stable and optimal context representation.\nD. Convex Optimization in Attention\nConvex optimization provides a principled method to com-\nbine multiple objectives under constraints. The general form\nof a convex optimization problem is defined as:\nmin\nx\nf(x)\nsubject to\ngi(x) ≤0,\nhj(x) = 0\n(4)\nwhere f is the convex objective function, gi are convex\ninequality constraints, and hj are affine equality constraints.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "26", "text": "This equilibrium condition\nensures that no scale (player) can improve its contribution\nutility by unilaterally changing its attention weights, leading\nto a stable and optimal context representation.\nD. Convex Optimization in Attention\nConvex optimization provides a principled method to com-\nbine multiple objectives under constraints. The general form\nof a convex optimization problem is defined as:\nmin\nx\nf(x)\nsubject to\ngi(x) ≤0,\nhj(x) = 0\n(4)\nwhere f is the convex objective function, gi are convex\ninequality constraints, and hj are affine equality constraints.\nIn MAHA, this framework is utilized to aggregate attention\nscores from different scales while enforcing constraints that\npreserve important linguistic properties, such as probability\ndistribution validity and sparsity.\nIV. THE MAHA FRAMEWORK\nThe MAHA framework introduces a systematic approach\nto sequence modeling by decomposing the input into multiple\nhierarchical scales and synthesizing them through mathemat-\nically rigorous aggregation rules.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "27", "text": "In MAHA, this framework is utilized to aggregate attention\nscores from different scales while enforcing constraints that\npreserve important linguistic properties, such as probability\ndistribution validity and sparsity.\nIV. THE MAHA FRAMEWORK\nThe MAHA framework introduces a systematic approach\nto sequence modeling by decomposing the input into multiple\nhierarchical scales and synthesizing them through mathemat-\nically rigorous aggregation rules. This section details the\nhierarchical decomposition strategy, scale-specific attention\ncomputation, and the optimization-driven mechanisms that\nInput Embedding\nDilated Convolution\n(Local Context Extraction)\nHierarchical Decomposition\nScale 0\nScale 1\nScale 2\nShared Value\nProjection\nShared Parameters\nAttention\nAttention\nAttention\nOptimization-Driven Aggregation\n(Convex / Nash Equilibrium)\nAdd & Norm\nResidual\nFeed Forward Network (FFN)\nAdd & Norm\nOutput\nFig. 1: Schematic overview of the MAHA architecture inte-\ngrated within a Transformer block.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "28", "text": "This section details the\nhierarchical decomposition strategy, scale-specific attention\ncomputation, and the optimization-driven mechanisms that\nInput Embedding\nDilated Convolution\n(Local Context Extraction)\nHierarchical Decomposition\nScale 0\nScale 1\nScale 2\nShared Value\nProjection\nShared Parameters\nAttention\nAttention\nAttention\nOptimization-Driven Aggregation\n(Convex / Nash Equilibrium)\nAdd & Norm\nResidual\nFeed Forward Network (FFN)\nAdd & Norm\nOutput\nFig. 1: Schematic overview of the MAHA architecture inte-\ngrated within a Transformer block. The input is decomposed\ninto multiple scales, processed via shared-value attention, and\naggregated using optimization or game-theoretic layers.\ngovern information fusion. As illustrated in Figure 1, MAHA\nis designed to replace the standard multi-head attention layer\nin transformer blocks while maintaining architectural compat-\nibility.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "29", "text": "1: Schematic overview of the MAHA architecture inte-\ngrated within a Transformer block. The input is decomposed\ninto multiple scales, processed via shared-value attention, and\naggregated using optimization or game-theoretic layers.\ngovern information fusion. As illustrated in Figure 1, MAHA\nis designed to replace the standard multi-head attention layer\nin transformer blocks while maintaining architectural compat-\nibility.\nA. Hierarchical Multiscale Decomposition with Learnable\nDownsampling\nLet X ∈Rn×d denote the input sequence, where n is the\nsequence length and d is the embedding dimension. MAHA\ndecomposes X into L hierarchical scales through a series of\nlearnable downsampling operations.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "30", "text": "govern information fusion. As illustrated in Figure 1, MAHA\nis designed to replace the standard multi-head attention layer\nin transformer blocks while maintaining architectural compat-\nibility.\nA. Hierarchical Multiscale Decomposition with Learnable\nDownsampling\nLet X ∈Rn×d denote the input sequence, where n is the\nsequence length and d is the embedding dimension. MAHA\ndecomposes X into L hierarchical scales through a series of\nlearnable downsampling operations. Each scale l is derived\nfrom the previous scale l −1 using a parameterized operator\nDl:\nXl = Dl(Xl−1),\nX0 = X\n(5)\nThe downsampling operator Dl is implemented via one of\ntwo mechanisms:\n1) Strided Convolution: Dl(X) = Conv1D(X, Ws\nl , sl),\nwhere Ws\nl is a learnable kernel and sl is the stride.\n2) Adaptive Pooling: Dl(X) = AdaptiveMaxPool(X, nl),\nwhich dynamically adjusts the pooling window to match\nthe target length nl.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "31", "text": "Each scale l is derived\nfrom the previous scale l −1 using a parameterized operator\nDl:\nXl = Dl(Xl−1),\nX0 = X\n(5)\nThe downsampling operator Dl is implemented via one of\ntwo mechanisms:\n1) Strided Convolution: Dl(X) = Conv1D(X, Ws\nl , sl),\nwhere Ws\nl is a learnable kernel and sl is the stride.\n2) Adaptive Pooling: Dl(X) = AdaptiveMaxPool(X, nl),\nwhich dynamically adjusts the pooling window to match\nthe target length nl.\nThe sequence lengths follow an exponential decay schedule\nnl = ⌊nl−1/r⌋, where r > 1 is a compression ratio hyper-\nparameter. This creates a pyramidal structure where higher\nscales capture increasingly coarse-grained semantic patterns\nwhile preserving essential features.\n4\nB. Multiscale Attention Computation\nAt each scale l, MAHA computes independent attention\nmatrices.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "32", "text": "2) Adaptive Pooling: Dl(X) = AdaptiveMaxPool(X, nl),\nwhich dynamically adjusts the pooling window to match\nthe target length nl.\nThe sequence lengths follow an exponential decay schedule\nnl = ⌊nl−1/r⌋, where r > 1 is a compression ratio hyper-\nparameter. This creates a pyramidal structure where higher\nscales capture increasingly coarse-grained semantic patterns\nwhile preserving essential features.\n4\nB. Multiscale Attention Computation\nAt each scale l, MAHA computes independent attention\nmatrices. A key innovation in MAHA is the decoupling of\nprojection parameters to enhance efficiency: while Query (Q)\nand Key (K) projections are scale-specific, the Value (V)\nprojection is shared across scales. Given the representation\nXl, the projections are defined as:\nQl = XlWQ\nl ,\nKl = XlWK\nl\n(6)\nwhere WQ\nl , WK\nl\n∈Rd×dk.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "33", "text": "4\nB. Multiscale Attention Computation\nAt each scale l, MAHA computes independent attention\nmatrices. A key innovation in MAHA is the decoupling of\nprojection parameters to enhance efficiency: while Query (Q)\nand Key (K) projections are scale-specific, the Value (V)\nprojection is shared across scales. Given the representation\nXl, the projections are defined as:\nQl = XlWQ\nl ,\nKl = XlWK\nl\n(6)\nwhere WQ\nl , WK\nl\n∈Rd×dk. The attention weights Al are\ncomputed via the scaled dot-product:\nAl = softmax\n\u0012QlKT\nl\n√dk\n\u0013\nUnlike standard transformers, MAHA employs a shared\nvalue projection: Vbase = XWV . The value matrix for scale\nl, denoted as Vl, is obtained by applying the corresponding\ndownsampling operator to the base values: Vl = Dl(Vbase).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "34", "text": "The attention weights Al are\ncomputed via the scaled dot-product:\nAl = softmax\n\u0012QlKT\nl\n√dk\n\u0013\nUnlike standard transformers, MAHA employs a shared\nvalue projection: Vbase = XWV . The value matrix for scale\nl, denoted as Vl, is obtained by applying the corresponding\ndownsampling operator to the base values: Vl = Dl(Vbase).\nThe scale-specific output Ol is then:\nOl = AlVl\nThis design reduces the parameter count significantly while\nensuring that the information flow remains consistent across\ngranularity levels.\nC. Aggregation of Multiscale Attention Outputs\nThe multiscale outputs {Ol} must be synthesized into\na unified representation O∗. MAHA proposes two rigorous\nstrategies:\nCO-Based Aggregation: We formulate aggregation as a\nconvex optimization problem.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "35", "text": "The scale-specific output Ol is then:\nOl = AlVl\nThis design reduces the parameter count significantly while\nensuring that the information flow remains consistent across\ngranularity levels.\nC. Aggregation of Multiscale Attention Outputs\nThe multiscale outputs {Ol} must be synthesized into\na unified representation O∗. MAHA proposes two rigorous\nstrategies:\nCO-Based Aggregation: We formulate aggregation as a\nconvex optimization problem. Let Ul denote an upsampling\noperator mapping Ol back to the original sequence length n.\nThe aggregated output is obtained by solving for the optimal\nmixing weights w:\nmin\nw\n\r\r\r\r\r\nL\nX\nl=0\nwlUl(Ol) −O∗\n\r\r\r\r\r\n2\nF\n+λ∥w∥1\ns.t.\nX\nwl = 1, wl ≥0\n(7)\nHere, λ controls sparsity, encouraging the model to select\nthe most informative scales.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "36", "text": "Let Ul denote an upsampling\noperator mapping Ol back to the original sequence length n.\nThe aggregated output is obtained by solving for the optimal\nmixing weights w:\nmin\nw\n\r\r\r\r\r\nL\nX\nl=0\nwlUl(Ol) −O∗\n\r\r\r\r\r\n2\nF\n+λ∥w∥1\ns.t.\nX\nwl = 1, wl ≥0\n(7)\nHere, λ controls sparsity, encouraging the model to select\nthe most informative scales.\nNash Equilibrium-Based Aggregation: Alternatively, ag-\ngregation is modeled as a non-cooperative game where each\nscale l competes to minimize its reconstruction error. The\nequilibrium weights w∗\nl satisfy:\nw∗\nl = arg min\nwl\n\r\rUl(Ol) −O∗(w∗\n−l)\n\r\r2\n2\n(8)\nThis ensures that no scale can unilaterally improve the\nrepresentation quality.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "37", "text": "Nash Equilibrium-Based Aggregation: Alternatively, ag-\ngregation is modeled as a non-cooperative game where each\nscale l competes to minimize its reconstruction error. The\nequilibrium weights w∗\nl satisfy:\nw∗\nl = arg min\nwl\n\r\rUl(Ol) −O∗(w∗\n−l)\n\r\r2\n2\n(8)\nThis ensures that no scale can unilaterally improve the\nrepresentation quality.\nD. Hybrid Dilated-Convolutional Transformer Design\nMAHA integrates dilated convolutions to capture local\ncontext prior to attention. The hybrid block consists of:\n• Dilated Convolution Blocks: For scale l, the output is\nCl = ReLU(DilatedConv(Xl)).\n• Cross-Scale Gating: Gl = σ(WgXl) ⊙Xl−1, where\nσ is the sigmoid function and ⊙denotes element-wise\nmultiplication.\n• Nearest-Neighbor Upsampling: Used to reconstruct the\nfull sequence efficiently.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "38", "text": "D. Hybrid Dilated-Convolutional Transformer Design\nMAHA integrates dilated convolutions to capture local\ncontext prior to attention. The hybrid block consists of:\n• Dilated Convolution Blocks: For scale l, the output is\nCl = ReLU(DilatedConv(Xl)).\n• Cross-Scale Gating: Gl = σ(WgXl) ⊙Xl−1, where\nσ is the sigmoid function and ⊙denotes element-wise\nmultiplication.\n• Nearest-Neighbor Upsampling: Used to reconstruct the\nfull sequence efficiently.\nE. Complexity Reduction through Hierarchical Sparsity\nThe total computational complexity of MAHA is governed\nby the hierarchical decomposition. For a sequence of length\nn, the complexity is defined as:\nΩ(n) =\nL\nX\nl=0\n\u0010 n\nrl\n\u00112\nd + O(n log n)\n(9)\nFor r = 2, the geometric series converges, yielding:\nO\n\u0012\nn2\nr2 −1\n\u0013\n(10)\nwhich is significantly lower than standard attention.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "39", "text": "• Nearest-Neighbor Upsampling: Used to reconstruct the\nfull sequence efficiently.\nE. Complexity Reduction through Hierarchical Sparsity\nThe total computational complexity of MAHA is governed\nby the hierarchical decomposition. For a sequence of length\nn, the complexity is defined as:\nΩ(n) =\nL\nX\nl=0\n\u0010 n\nrl\n\u00112\nd + O(n log n)\n(9)\nFor r = 2, the geometric series converges, yielding:\nO\n\u0012\nn2\nr2 −1\n\u0013\n(10)\nwhich is significantly lower than standard attention.\n• Scale-Specific Sparsity: Coarser scales have nl ≪n,\nreducing the cost quadratically.\n• Dynamic Weight Sparsity: The ℓ1-regularized weights\nwl prune uninformative scales during inference.\nComputational Complexity\n(FLOPs / Memory)\nSequence Length\nStandard Attention\nMAHA\nLong Context\nEfficiency Gap\n(Sub-quadratic)\nFig. 2: Computational complexity comparison.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "40", "text": "• Scale-Specific Sparsity: Coarser scales have nl ≪n,\nreducing the cost quadratically.\n• Dynamic Weight Sparsity: The ℓ1-regularized weights\nwl prune uninformative scales during inference.\nComputational Complexity\n(FLOPs / Memory)\nSequence Length\nStandard Attention\nMAHA\nLong Context\nEfficiency Gap\n(Sub-quadratic)\nFig. 2: Computational complexity comparison. MAHA demon-\nstrates near-linear scaling compared to the quadratic growth of\nstandard Self-Attention.\nV. EXPERIMENTS\nTo evaluate the empirical efficacy of MAHA, we conducted\nextensive experiments across diverse NLP tasks. Our evalua-\ntion framework focuses on three pivotal research questions:\n• RQ1: How does MAHA compare to state-of-the-art at-\ntention mechanisms in terms of computational efficiency\nand downstream task performance?\n• RQ2: What is the comparative impact of convex opti-\nmization versus game-theoretic aggregation strategies on\nmodel behavior?", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "41", "text": "MAHA demon-\nstrates near-linear scaling compared to the quadratic growth of\nstandard Self-Attention.\nV. EXPERIMENTS\nTo evaluate the empirical efficacy of MAHA, we conducted\nextensive experiments across diverse NLP tasks. Our evalua-\ntion framework focuses on three pivotal research questions:\n• RQ1: How does MAHA compare to state-of-the-art at-\ntention mechanisms in terms of computational efficiency\nand downstream task performance?\n• RQ2: What is the comparative impact of convex opti-\nmization versus game-theoretic aggregation strategies on\nmodel behavior?\n5\n• RQ3: How does the granularity of the hierarchical de-\ncomposition affect the trade-off between representational\naccuracy and computational cost?\nA. Experimental Setup\nWe evaluated MAHA on four benchmark datasets designed\nto stress-test different aspects of sequence modeling:\n• Text Classification: GLUE benchmark [15], focusing on\nMNLI and SST-2.\n• Long-Range Dependency Modeling: PG-19 dataset [16]\n(> 4k tokens).\n• Machine Translation: WMT14 English-German [17].", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "42", "text": "5\n• RQ3: How does the granularity of the hierarchical de-\ncomposition affect the trade-off between representational\naccuracy and computational cost?\nA. Experimental Setup\nWe evaluated MAHA on four benchmark datasets designed\nto stress-test different aspects of sequence modeling:\n• Text Classification: GLUE benchmark [15], focusing on\nMNLI and SST-2.\n• Long-Range Dependency Modeling: PG-19 dataset [16]\n(> 4k tokens).\n• Machine Translation: WMT14 English-German [17].\n• Question Answering: SQuAD v2.0 [18].\nFor comparative analysis, MAHA was benchmarked against\nfive widely adopted attention mechanisms: Standard Multi-\nHead Attention (MHA) [1], Longformer [6], BigBird [7],\nReformer [19], and Performer [20].\nImplementation Details:\n• Model Architecture: Transformer backbone with 12\nlayers, hidden dimensionality of 768, and 12 attention\nheads.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "43", "text": "• Long-Range Dependency Modeling: PG-19 dataset [16]\n(> 4k tokens).\n• Machine Translation: WMT14 English-German [17].\n• Question Answering: SQuAD v2.0 [18].\nFor comparative analysis, MAHA was benchmarked against\nfive widely adopted attention mechanisms: Standard Multi-\nHead Attention (MHA) [1], Longformer [6], BigBird [7],\nReformer [19], and Performer [20].\nImplementation Details:\n• Model Architecture: Transformer backbone with 12\nlayers, hidden dimensionality of 768, and 12 attention\nheads.\n• Training: Batch size 32 (classification), 16 (transla-\ntion/QA); LR 5 × 10−5 with warmup 10k steps.\n• Sequence Length: 512 (classification/QA), 4096 (PG-\n19).\n• MAHA Parameters: L = 4 scales (32, 64, 128, 256 to-\nkens); strided conv (kernel=3); aggregation regularization\nλ = 0.1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "44", "text": "Implementation Details:\n• Model Architecture: Transformer backbone with 12\nlayers, hidden dimensionality of 768, and 12 attention\nheads.\n• Training: Batch size 32 (classification), 16 (transla-\ntion/QA); LR 5 × 10−5 with warmup 10k steps.\n• Sequence Length: 512 (classification/QA), 4096 (PG-\n19).\n• MAHA Parameters: L = 4 scales (32, 64, 128, 256 to-\nkens); strided conv (kernel=3); aggregation regularization\nλ = 0.1.\nB. Main Results\nTable I summarizes the performance on benchmark datasets.\nMAHA achieves competitive accuracy with standard attention\nwhile outperforming sparse baselines on long-context tasks\n(PG-19).\nC. Computational Efficiency Analysis\nMAHA matches MHA performance (within 0.2%) while\nreducing memory by 56%. On PG-19, MAHA achieves lowest\nperplexity (23.1), outperforming sparse models.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "45", "text": "B. Main Results\nTable I summarizes the performance on benchmark datasets.\nMAHA achieves competitive accuracy with standard attention\nwhile outperforming sparse baselines on long-context tasks\n(PG-19).\nC. Computational Efficiency Analysis\nMAHA matches MHA performance (within 0.2%) while\nreducing memory by 56%. On PG-19, MAHA achieves lowest\nperplexity (23.1), outperforming sparse models. Throughput\nis highest (71 seq/s), making MAHA ideal for high-volume\ninference. We analyzed the theoretical complexity (FLOPs)\nrelative to sequence length. As illustrated in Figure 3, MAHA\ndemonstrates near-linear scaling compared to the quadratic\nbaseline of standard attention.\nAt N = 4096, MHA requires ≈16.8M FLOPs vs MAHA\n≈3.2M FLOPs (81% reduction). This efficiency stems from\nhierarchical compression avoiding full N × N attention.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "46", "text": "Throughput\nis highest (71 seq/s), making MAHA ideal for high-volume\ninference. We analyzed the theoretical complexity (FLOPs)\nrelative to sequence length. As illustrated in Figure 3, MAHA\ndemonstrates near-linear scaling compared to the quadratic\nbaseline of standard attention.\nAt N = 4096, MHA requires ≈16.8M FLOPs vs MAHA\n≈3.2M FLOPs (81% reduction). This efficiency stems from\nhierarchical compression avoiding full N × N attention. This\ngap widens exponentially as the sequence length increases,\nconfirming MAHA’s suitability for long-context applications.\nD. Ablation Studies\nWe evaluated the impact of aggregation methods and scale\nconfigurations.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "47", "text": "As illustrated in Figure 3, MAHA\ndemonstrates near-linear scaling compared to the quadratic\nbaseline of standard attention.\nAt N = 4096, MHA requires ≈16.8M FLOPs vs MAHA\n≈3.2M FLOPs (81% reduction). This efficiency stems from\nhierarchical compression avoiding full N × N attention. This\ngap widens exponentially as the sequence length increases,\nconfirming MAHA’s suitability for long-context applications.\nD. Ablation Studies\nWe evaluated the impact of aggregation methods and scale\nconfigurations.\n1K\n2K\n3K\n4K\n5K\n6K\n7K\n8K\nSequence Length (N)\n0\n10\n20\n30\n40\n50\n60\n70\n80\nAttention FLOPs (Millions)\n81% Reduction\n(13.6M FLOPs Gap)\n16.8M\n3.2M\nFigure 3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "48", "text": "This\ngap widens exponentially as the sequence length increases,\nconfirming MAHA’s suitability for long-context applications.\nD. Ablation Studies\nWe evaluated the impact of aggregation methods and scale\nconfigurations.\n1K\n2K\n3K\n4K\n5K\n6K\n7K\n8K\nSequence Length (N)\n0\n10\n20\n30\n40\n50\n60\n70\n80\nAttention FLOPs (Millions)\n81% Reduction\n(13.6M FLOPs Gap)\n16.8M\n3.2M\nFigure 3. Attention FLOPs vs Sequence Length\nStandard MHA (O(n2))\nMAHA (Ours, \nO(n))\nFig. 3: Attention FLOPs vs Sequence Length. MAHA exhibits\nan 81% reduction in FLOPs at N = 4096 compared to\nStandard MHA. The gap widens exponentially with longer\nsequences.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "49", "text": "Attention FLOPs vs Sequence Length\nStandard MHA (O(n2))\nMAHA (Ours, \nO(n))\nFig. 3: Attention FLOPs vs Sequence Length. MAHA exhibits\nan 81% reduction in FLOPs at N = 4096 compared to\nStandard MHA. The gap widens exponentially with longer\nsequences.\n1) Aggregation Strategy Comparison: To further analyze\nthe training dynamics, Figure 4 depicts the loss convergence\ncurves for both aggregation strategies. While both meth-\nods converge stably, the Nash Equilibrium (Orange) strategy\nachieves a marginally lower loss value in later epochs com-\npared to Convex Optimization (Blue).\nFig. 4: Training loss convergence comparison: Convex Opti-\nmization vs Nash Equilibrium.\nTable II shows that while Convex Optimization (CO) is\nfaster (1.0x), Nash Equilibrium (NE) provides robust perfor-\nmance at a slight cost (0.9x speed).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "50", "text": "While both meth-\nods converge stably, the Nash Equilibrium (Orange) strategy\nachieves a marginally lower loss value in later epochs com-\npared to Convex Optimization (Blue).\nFig. 4: Training loss convergence comparison: Convex Opti-\nmization vs Nash Equilibrium.\nTable II shows that while Convex Optimization (CO) is\nfaster (1.0x), Nash Equilibrium (NE) provides robust perfor-\nmance at a slight cost (0.9x speed).\n2) Scale Configuration Analysis: We analyzed how the\ndepth of the hierarchy (number of scales, L) affects model\nperformance. As illustrated in Figure 5, optimal results are\nobserved at L = 4, balancing granularity and context. Using\ntoo few scales (L = 2) results in insufficient detail (Acc:\n84.5%), while excessive downsampling (L = 6) introduces\nnoise (Acc: 84.8%).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "51", "text": "2) Scale Configuration Analysis: We analyzed how the\ndepth of the hierarchy (number of scales, L) affects model\nperformance. As illustrated in Figure 5, optimal results are\nobserved at L = 4, balancing granularity and context. Using\ntoo few scales (L = 2) results in insufficient detail (Acc:\n84.5%), while excessive downsampling (L = 6) introduces\nnoise (Acc: 84.8%).\nE. Qualitative Analysis\nTo interpret the internal representations learned by MAHA,\nwe visualized the attention weights across different hierarchi-\ncal scales. Figure 6 displays the heatmap of attention matrices.\nSeveral key observations can be drawn:\n1) Fine Scales (Scale 1): Exhibits a strong diagonal ten-\ndency, capturing local syntax like adjective-noun pairs.\n6\nTABLE I: Performance Comparison Across Tasks.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "52", "text": "Model\nMNLI\nSST-2\nPG-19\nWMT\nSQuAD\nMemory\n(Acc)\n(Acc)\n(PPL) ↓\n(BLEU)\n(F1)\n(GB) ↓\nStandard MHA\n86.2\n93.5\n24.3\n28.7\n88.4\n15.2\nLongformer\n85.7\n92.8\n23.8\n27.9\n87.6\n9.1\nBigBird\n85.9\n93.1\n23.5\n28.1\n87.9\n10.3\nReformer\n84.3\n91.7\n25.6\n26.4\n85.2\n7.8\nPerformer\n85.1\n92.4\n24.9\n27.3\n86.7\n8.5\nMAHA (Ours)\n86.0\n93.3\n23.1\n28.5\n88.2\n6.7\nNote: MAHA achieves lowest perplexity on PG-19 and significant memory reduction.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "53", "text": "TABLE II: Aggregation Method Impact on MNLI Task.\nMethod\nMNLI (Acc)\nMemory (GB)\nSpeed\nConvex Opt. (CO)\n86.0\n6.7\n1.0x\nNash Eq. (NE)\n85.8\n6.9\n0.9x\nMean Aggregation\n85.2\n7.2\n1.1x\n2\n3\n4\n5\n6\nNumber of Scales (L)\n84.0\n84.5\n85.0\n85.5\n86.0\n86.5\n87.0\nMNLI Accuracy (%)\nPeak Accuracy\n(L=4, Acc=86.0)\nLoss of\nGranularity\nNoise from\nDownsampling\nFig. 5: MNLI Accuracy vs Number of Hierarchical Scales (L).\nOptimal performance is at L = 4.\n2) Medium Scales (Scale 2): Shifts towards block-diagonal\nstructures, suggesting clause-level modeling.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "54", "text": "5: MNLI Accuracy vs Number of Hierarchical Scales (L).\nOptimal performance is at L = 4.\n2) Medium Scales (Scale 2): Shifts towards block-diagonal\nstructures, suggesting clause-level modeling.\n3) Coarse Scales (Scale 3): Attention becomes diffuse with\nvertical bands, tracking document-level themes regardless\nof distance.\nVI. DISCUSSION\nA. Scalability vs. Implementation Overhead\nOur experiments highlight a critical distinction between\nalgorithmic complexity and implementation overhead. While\nprototype implementations may exhibit initialization latency,\nthe growth rate is the decisive metric for Large Language Mod-\nels. Figure 3 confirms that MAHA’s computational cost grows\nlinearly (O(N)), whereas standard attention grows quadrat-\nically (O(N 2)). This implies that for very large sequences\n(e.g., N ≫4096), MAHA provides a decisive advantage in\nboth speed and memory.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "55", "text": "VI. DISCUSSION\nA. Scalability vs. Implementation Overhead\nOur experiments highlight a critical distinction between\nalgorithmic complexity and implementation overhead. While\nprototype implementations may exhibit initialization latency,\nthe growth rate is the decisive metric for Large Language Mod-\nels. Figure 3 confirms that MAHA’s computational cost grows\nlinearly (O(N)), whereas standard attention grows quadrat-\nically (O(N 2)). This implies that for very large sequences\n(e.g., N ≫4096), MAHA provides a decisive advantage in\nboth speed and memory.\nKey Position\nQuery Position\nScale 1: Fine-Grained\n(Local Syntax / Adj-Noun)\nKey Position\nScale 2: Medium-Grained\n(Clause-Level / Local Context)\nKey Position\nScale 3: Coarse-Grained\n(Document Themes / Global)\nFig. 6: Visualization of Learned Multiscale Attention Patterns.\nDarker regions indicate higher attention weights.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "56", "text": "This implies that for very large sequences\n(e.g., N ≫4096), MAHA provides a decisive advantage in\nboth speed and memory.\nKey Position\nQuery Position\nScale 1: Fine-Grained\n(Local Syntax / Adj-Noun)\nKey Position\nScale 2: Medium-Grained\n(Clause-Level / Local Context)\nKey Position\nScale 3: Coarse-Grained\n(Document Themes / Global)\nFig. 6: Visualization of Learned Multiscale Attention Patterns.\nDarker regions indicate higher attention weights.\nB. Limitations\nWhile MAHA demonstrates significant improvements in ef-\nficiency and modeling, certain limitations warrant discussion:\n• The framework involves additional hyperparameters (e.g.,\nnumber of scales L, compression ratio r) that may require\ndomain-specific tuning.\n• Although the Nash Equilibrium aggregation offers theo-\nretical guarantees, its iterative nature imposes a compu-\ntational overhead during training compared to the closed-\nform Convex Optimization (CO) solution.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "57", "text": "6: Visualization of Learned Multiscale Attention Patterns.\nDarker regions indicate higher attention weights.\nB. Limitations\nWhile MAHA demonstrates significant improvements in ef-\nficiency and modeling, certain limitations warrant discussion:\n• The framework involves additional hyperparameters (e.g.,\nnumber of scales L, compression ratio r) that may require\ndomain-specific tuning.\n• Although the Nash Equilibrium aggregation offers theo-\nretical guarantees, its iterative nature imposes a compu-\ntational overhead during training compared to the closed-\nform Convex Optimization (CO) solution.\n• The method assumes that linguistic information is inher-\nently hierarchical; this assumption may not fully capture\ncertain non-compositional semantic relationships or dis-\npersed references in highly unstructured text [21].\nC. Potential Application Scenarios\nThe versatility of MAHA extends beyond standard NLP:\n• Genomics: In genomic sequence analysis, where identi-\nfying long-range dependencies in megabase-scale DNA\nsequences is critical [22], MAHA’s multiscale attention\ncould enhance variant calling accuracy.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "58", "text": "• The method assumes that linguistic information is inher-\nently hierarchical; this assumption may not fully capture\ncertain non-compositional semantic relationships or dis-\npersed references in highly unstructured text [21].\nC. Potential Application Scenarios\nThe versatility of MAHA extends beyond standard NLP:\n• Genomics: In genomic sequence analysis, where identi-\nfying long-range dependencies in megabase-scale DNA\nsequences is critical [22], MAHA’s multiscale attention\ncould enhance variant calling accuracy.\n• Multimodal Learning: For video-text retrieval, the hi-\nerarchical scales align naturally with temporal video\nresolutions (frames, shots, scenes) [23], offering a unified\nattention mechanism for cross-modal alignment.\n• Federated Learning: The optimization-driven aggrega-\ntion is particularly relevant for federated settings where\nclients may operate on data of varying granularities or\nqualities [24].\nD. Ethical Considerations\nThe efficiency gains of MAHA present a dual-edged sword.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "59", "text": "• Multimodal Learning: For video-text retrieval, the hi-\nerarchical scales align naturally with temporal video\nresolutions (frames, shots, scenes) [23], offering a unified\nattention mechanism for cross-modal alignment.\n• Federated Learning: The optimization-driven aggrega-\ntion is particularly relevant for federated settings where\nclients may operate on data of varying granularities or\nqualities [24].\nD. Ethical Considerations\nThe efficiency gains of MAHA present a dual-edged sword.\nWhile significantly reducing the carbon footprint per train-\ning run [25], lower costs may paradoxically incentivize the\ntraining of even larger, redundant models (Jevons paradox).\nFurthermore, the hierarchical aggregation introduces inter-\npretability challenges; while individual scales are transparent,\nthe complex interplay of optimization weights may obscure the\nmodels decision-making path [26]. Future work must address\nthese transparency issues to ensure responsible deployment.\nVII.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "60", "text": "D. Ethical Considerations\nThe efficiency gains of MAHA present a dual-edged sword.\nWhile significantly reducing the carbon footprint per train-\ning run [25], lower costs may paradoxically incentivize the\ntraining of even larger, redundant models (Jevons paradox).\nFurthermore, the hierarchical aggregation introduces inter-\npretability challenges; while individual scales are transparent,\nthe complex interplay of optimization weights may obscure the\nmodels decision-making path [26]. Future work must address\nthese transparency issues to ensure responsible deployment.\nVII. CONCLUSION\nIn this paper, we introduced Multiscale Aggregated Hierar-\nchical Attention (MAHA), a novel framework that fundamen-\ntally rethinks attention mechanisms in LLMs through the lens\n7\nof multiscale analysis and optimization theory. By decompos-\ning sequences into hierarchical granularities and synthesizing\nthem via convex optimization or game-theoretic equilibrium,\nMAHA addresses the critical bottleneck of quadratic complex-\nity without compromising contextual fidelity.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "61", "text": "Future work must address\nthese transparency issues to ensure responsible deployment.\nVII. CONCLUSION\nIn this paper, we introduced Multiscale Aggregated Hierar-\nchical Attention (MAHA), a novel framework that fundamen-\ntally rethinks attention mechanisms in LLMs through the lens\n7\nof multiscale analysis and optimization theory. By decompos-\ning sequences into hierarchical granularities and synthesizing\nthem via convex optimization or game-theoretic equilibrium,\nMAHA addresses the critical bottleneck of quadratic complex-\nity without compromising contextual fidelity.\nOur\nextensive\nempirical\nevaluation\ndemonstrates\nthat\nMAHA achieves state-of-the-art performance on long-context\nmodeling (PG-19) and machine translation, while reducing\nmemory usage by up to 56% compared to standard transform-\ners. The proposed hybrid dilated-convolutional architecture\nserves as a drop-in replacement for existing attention layers,\nfacilitating seamless adoption.\nLooking forward, MAHA paves the way for scalable foun-\ndation models in resource-constrained environments.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "62", "text": "Our\nextensive\nempirical\nevaluation\ndemonstrates\nthat\nMAHA achieves state-of-the-art performance on long-context\nmodeling (PG-19) and machine translation, while reducing\nmemory usage by up to 56% compared to standard transform-\ners. The proposed hybrid dilated-convolutional architecture\nserves as a drop-in replacement for existing attention layers,\nfacilitating seamless adoption.\nLooking forward, MAHA paves the way for scalable foun-\ndation models in resource-constrained environments. We en-\nvision future research extending this rigorous aggregation\nparadigm to other modalities such as computer vision and\nspeech processing, where multiscale representation is equally\nparamount. Ultimately, this work underscores the potential\nof integrating mathematical optimization principles into deep\nlearning architectures to build more efficient, robust, and\ntheoretically grounded AI systems.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "63", "text": "The proposed hybrid dilated-convolutional architecture\nserves as a drop-in replacement for existing attention layers,\nfacilitating seamless adoption.\nLooking forward, MAHA paves the way for scalable foun-\ndation models in resource-constrained environments. We en-\nvision future research extending this rigorous aggregation\nparadigm to other modalities such as computer vision and\nspeech processing, where multiscale representation is equally\nparamount. Ultimately, this work underscores the potential\nof integrating mathematical optimization principles into deep\nlearning architectures to build more efficient, robust, and\ntheoretically grounded AI systems.\nDATA AVAILABILITY\nThe source code and pretrained models for MAHA\nare\npublicly\navailable\nat\nhttps://github.com/canererden/\nMAHA-Project with the permanent digital object identifier\nDOI: 10.5281/zenodo.17936753.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "64", "text": "Ultimately, this work underscores the potential\nof integrating mathematical optimization principles into deep\nlearning architectures to build more efficient, robust, and\ntheoretically grounded AI systems.\nDATA AVAILABILITY\nThe source code and pretrained models for MAHA\nare\npublicly\navailable\nat\nhttps://github.com/canererden/\nMAHA-Project with the permanent digital object identifier\nDOI: 10.5281/zenodo.17936753.\nREFERENCES\n[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances\nin Neural Information Processing Systems (NeurIPS), 2017, vol. 30.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "65", "text": "REFERENCES\n[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances\nin Neural Information Processing Systems (NeurIPS), 2017, vol. 30.\n[2] W. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang,\nJ. Zhang, and Z. Dong, “A survey of large language models,” 2023,\narXiv preprint arXiv:2303.18223.\n[3] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy, “Hierarchical\nattention networks for document classification,” in Proceedings of the\n2016 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies (NAACL-\nHLT), 2016, pp.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "66", "text": "[3] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy, “Hierarchical\nattention networks for document classification,” in Proceedings of the\n2016 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies (NAACL-\nHLT), 2016, pp. 1480–1489.\n[4] J. Starck, F. Murtagh, and J. Fadili, Sparse Image and Signal Processing:\nWavelets and Related Geometric Multiscale Analysis.\nCambridge\nUniversity Press, 2015.\n[5] H. Naveed, A. Khan, S. Qiu, M. Saqib, S. Anwar, M. Usman, N. Barnes,\nand A. Mian, “A comprehensive overview of large language models,”\nACM Transactions on Intelligent Systems and Technology, 2025.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "67", "text": "1480–1489.\n[4] J. Starck, F. Murtagh, and J. Fadili, Sparse Image and Signal Processing:\nWavelets and Related Geometric Multiscale Analysis.\nCambridge\nUniversity Press, 2015.\n[5] H. Naveed, A. Khan, S. Qiu, M. Saqib, S. Anwar, M. Usman, N. Barnes,\nand A. Mian, “A comprehensive overview of large language models,”\nACM Transactions on Intelligent Systems and Technology, 2025.\n[6] I. Beltagy, M. Peters, and A. Cohan, “Longformer: The long-document\ntransformer,” 2020, arXiv preprint arXiv:2004.05150.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "68", "text": "Cambridge\nUniversity Press, 2015.\n[5] H. Naveed, A. Khan, S. Qiu, M. Saqib, S. Anwar, M. Usman, N. Barnes,\nand A. Mian, “A comprehensive overview of large language models,”\nACM Transactions on Intelligent Systems and Technology, 2025.\n[6] I. Beltagy, M. Peters, and A. Cohan, “Longformer: The long-document\ntransformer,” 2020, arXiv preprint arXiv:2004.05150.\n[7] M. Zaheer, G. Guruganesh, K. Dubey, J. Ainslie, C. Alberti, S. Ontanon,\nP. Pham, A. Ravula, Q. Wang, and L. Yang, “Big bird: Transformers\nfor longer sequences,” in Advances in Neural Information Processing\nSystems (NeurIPS), 2020, vol. 33.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "69", "text": "[7] M. Zaheer, G. Guruganesh, K. Dubey, J. Ainslie, C. Alberti, S. Ontanon,\nP. Pham, A. Ravula, Q. Wang, and L. Yang, “Big bird: Transformers\nfor longer sequences,” in Advances in Neural Information Processing\nSystems (NeurIPS), 2020, vol. 33.\n[8] M. Cheng, P. Jiang, L. Han, L. Wang, and P. Torr, “Deeply explain\ncnn via hierarchical decomposition,” International Journal of Computer\nVision, vol. 131, 2023.\n[9] W. Jun, Z. Tianliang, Z. Jiahui, L. Tianyi, and W. Chunzhi, “Hierarchical\nmultiple self-attention mechanism for multi-modal analysis,” Multimedia\nSystems, 2023.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "70", "text": "33.\n[8] M. Cheng, P. Jiang, L. Han, L. Wang, and P. Torr, “Deeply explain\ncnn via hierarchical decomposition,” International Journal of Computer\nVision, vol. 131, 2023.\n[9] W. Jun, Z. Tianliang, Z. Jiahui, L. Tianyi, and W. Chunzhi, “Hierarchical\nmultiple self-attention mechanism for multi-modal analysis,” Multimedia\nSystems, 2023.\n[10] M. Zhu, A. Anwar, Z. Wan, J. Cho, C. Kamhoua, and M. Singh,\n“A survey of defensive deception: Approaches using game theory and\nmachine learning,” IEEE Communications Surveys & Tutorials, vol. 23,\nno. 4, pp. 2460–2493, 2021.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "71", "text": "[10] M. Zhu, A. Anwar, Z. Wan, J. Cho, C. Kamhoua, and M. Singh,\n“A survey of defensive deception: Approaches using game theory and\nmachine learning,” IEEE Communications Surveys & Tutorials, vol. 23,\nno. 4, pp. 2460–2493, 2021.\n[11] J. Lee, M. Lee, D. Lee, and S. Lee, “Hierarchically decomposed\ngraph convolutional networks for skeleton-based action recognition,” in\nProceedings of the IEEE/CVF International Conference on Computer\nVision (ICCV), 2023.\n[12] J. Lu, K. Lin, R. Chen, M. Lin, X. Chen, and P. Lu, “Health insur-\nance fraud detection by using an attributed heterogeneous information\nnetwork with a hierarchical attention mechanism,” BMC Medical Infor-\nmatics and Decision Making, vol. 23, 2023.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "72", "text": "[12] J. Lu, K. Lin, R. Chen, M. Lin, X. Chen, and P. Lu, “Health insur-\nance fraud detection by using an attributed heterogeneous information\nnetwork with a hierarchical attention mechanism,” BMC Medical Infor-\nmatics and Decision Making, vol. 23, 2023.\n[13] M. Farge, “Wavelet transforms and their applications to turbulence,”\nAnnual Review of Fluid Mechanics, vol. 24, no. 1, pp. 395–457, 1992.\n[14] J. Nash, “Non-cooperative games,” in The Foundations of Price Theory,\nreprint ed.\nTaylor & Francis, 2024, vol. 4.\n[15] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman, “Glue:\nA multi-task benchmark and analysis platform for natural language\nunderstanding,” in Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), 2018.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "73", "text": "24, no. 1, pp. 395–457, 1992.\n[14] J. Nash, “Non-cooperative games,” in The Foundations of Price Theory,\nreprint ed.\nTaylor & Francis, 2024, vol. 4.\n[15] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman, “Glue:\nA multi-task benchmark and analysis platform for natural language\nunderstanding,” in Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), 2018.\n[16] S. Sun, K. Krishna, A. Mattarella-Micke, and M. Iyyer, “Do long-range\nlanguage models actually use long-range context?” 2021, arXiv preprint\narXiv:2109.09115.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "74", "text": "[16] S. Sun, K. Krishna, A. Mattarella-Micke, and M. Iyyer, “Do long-range\nlanguage models actually use long-range context?” 2021, arXiv preprint\narXiv:2109.09115.\n[17] O. Bojar, R. Chatterjee, C. Federmann, Y. Graham, B. Haddow,\nM. Huck, A. Yepes, P. Koehn, V. Logacheva, and C. Monz, “Findings\nof the 2016 conference on machine translation,” in Proceedings of the\nFirst Conference on Machine Translation (WMT), 2016, pp. 131–198.\n[18] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “Squad: 100,000+\nquestions for machine comprehension of text,” 2016, arXiv preprint\narXiv:1606.05250.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "75", "text": "131–198.\n[18] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “Squad: 100,000+\nquestions for machine comprehension of text,” 2016, arXiv preprint\narXiv:1606.05250.\n[19] N. Kitaev, L. Kaiser, and A. Levskaya, “Reformer: The efficient trans-\nformer,” 2020, arXiv preprint arXiv:2001.04451.\n[20] K. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sar-\nlos, P. Hawkins, J. Davis, A. Mohiuddin, and L. Kaiser, “Rethinking\nattention with performers,” 2020, arXiv preprint arXiv:2009.14794.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "76", "text": "[20] K. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sar-\nlos, P. Hawkins, J. Davis, A. Mohiuddin, and L. Kaiser, “Rethinking\nattention with performers,” 2020, arXiv preprint arXiv:2009.14794.\n[21] W. Rapaport, “Syntactic semantics: Foundations of computational\nnatural-language understanding,” in Thinking Computers and Virtual\nPersons.\nAcademic Press, 1994.\n[22] S. Choi and M. Lee, “Transformer architecture and attention mechanisms\nin genome data analysis: A comprehensive review,” Biology, vol. 12,\nno. 1, 2023.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "77", "text": "[21] W. Rapaport, “Syntactic semantics: Foundations of computational\nnatural-language understanding,” in Thinking Computers and Virtual\nPersons.\nAcademic Press, 1994.\n[22] S. Choi and M. Lee, “Transformer architecture and attention mechanisms\nin genome data analysis: A comprehensive review,” Biology, vol. 12,\nno. 1, 2023.\n[23] S. Liu, H. Fan, S. Qian, Y. Chen, W. Ding, and Z. Wang, “Hit: Hier-\narchical transformer with momentum contrast for video-text retrieval,”\nin Proceedings of the IEEE/CVF International Conference on Computer\nVision (ICCV), 2021.\n[24] Y. Chen, Y. Ning, Z. Chai, and H. Rangwala, “Federated multi-task\nlearning with hierarchical attention for sensor data analytics,” in 2020\nInternational Joint Conference on Neural Networks (IJCNN), 2020.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "78", "text": "[23] S. Liu, H. Fan, S. Qian, Y. Chen, W. Ding, and Z. Wang, “Hit: Hier-\narchical transformer with momentum contrast for video-text retrieval,”\nin Proceedings of the IEEE/CVF International Conference on Computer\nVision (ICCV), 2021.\n[24] Y. Chen, Y. Ning, Z. Chai, and H. Rangwala, “Federated multi-task\nlearning with hierarchical attention for sensor data analytics,” in 2020\nInternational Joint Conference on Neural Networks (IJCNN), 2020.\n[25] E. Strubell, A. Ganesh, and A. McCallum, “Energy and policy consid-\nerations for deep learning in nlp,” in Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics (ACL), 2019.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "79", "text": "[24] Y. Chen, Y. Ning, Z. Chai, and H. Rangwala, “Federated multi-task\nlearning with hierarchical attention for sensor data analytics,” in 2020\nInternational Joint Conference on Neural Networks (IJCNN), 2020.\n[25] E. Strubell, A. Ganesh, and A. McCallum, “Energy and policy consid-\nerations for deep learning in nlp,” in Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics (ACL), 2019.\n[26] M. Danilevsky, S. Dhanorkar, Y. Li, L. Popa, K. Qian, and C. Li,\n“Explainability for natural language processing,” in Proceedings of\nthe 27th ACM SIGKDD Conference on Knowledge Discovery & Data\nMining, 2021.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.14925v2.pdf", "file_name": ""}}
{"id": "80", "text": "Closed-form Continuous-time Neural Networks\nRamin Hasani 1⋆,∗, Mathias Lechner 2⋆, Alexander Amini 1,\nLucas Liebenwein 1, Aaron Ray 1,\nMax Tschaikowski 3, Gerald Teschl 4, Daniela Rus 1\n1Massachusetts Institute of Technology (MIT), Cambridge, USA\n2Institute of Science and Technology Austria (IST Austria), Austria\n3Aalborg University, Denmark\n4University of Vienna (Uni Wien), Austria\n⋆These authors contributed equally to the paper\n∗To whom correspondence should be addressed; E-mail: rhasani@mit.edu.\nContinuous-time neural processes are performant sequential decision-\nmakers that are built by differential equations (DE). However, their ex-\npressive power when they are deployed on computers is bottlenecked\nby numerical DE solvers. This limitation has signiﬁcantly slowed down\nscaling and understanding of numerous natural physical phenomena such\nas the dynamics of nervous systems.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "81", "text": "Continuous-time neural processes are performant sequential decision-\nmakers that are built by differential equations (DE). However, their ex-\npressive power when they are deployed on computers is bottlenecked\nby numerical DE solvers. This limitation has signiﬁcantly slowed down\nscaling and understanding of numerous natural physical phenomena such\nas the dynamics of nervous systems. Ideally we would circumvent this\nbottleneck by solving the given dynamical system in closed-form. This\nis known to be intractable in general. Here, we show it is possible to\nclosely approximate the interaction between neurons and synapses – the\nbuilding blocks of natural and artiﬁcial neural networks – constructed\nby liquid time-constant networks (LTCs) (1) efﬁciently in closed-form.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "82", "text": "This limitation has signiﬁcantly slowed down\nscaling and understanding of numerous natural physical phenomena such\nas the dynamics of nervous systems. Ideally we would circumvent this\nbottleneck by solving the given dynamical system in closed-form. This\nis known to be intractable in general. Here, we show it is possible to\nclosely approximate the interaction between neurons and synapses – the\nbuilding blocks of natural and artiﬁcial neural networks – constructed\nby liquid time-constant networks (LTCs) (1) efﬁciently in closed-form.\nTo this end, we compute a tightly-bounded approximation of the solu-\n1\narXiv:2106.13898v2  [cs.LG]  2 Mar 2022\ntion of an integral appearing in LTCs’ dynamics, that has had no known\nclosed-form solution so far.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "83", "text": "Here, we show it is possible to\nclosely approximate the interaction between neurons and synapses – the\nbuilding blocks of natural and artiﬁcial neural networks – constructed\nby liquid time-constant networks (LTCs) (1) efﬁciently in closed-form.\nTo this end, we compute a tightly-bounded approximation of the solu-\n1\narXiv:2106.13898v2  [cs.LG]  2 Mar 2022\ntion of an integral appearing in LTCs’ dynamics, that has had no known\nclosed-form solution so far. This closed-form solution substantially im-\npacts the design of continuous-time and continuous-depth neural mod-\nels; for instance, since time appears explicitly in closed-form, the formu-\nlation relaxes the need for complex numerical solvers. Consequently, we\nobtain models that are between one and ﬁve orders of magnitude faster in\ntraining and inference compared to differential equation-based counter-\nparts.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "84", "text": "This closed-form solution substantially im-\npacts the design of continuous-time and continuous-depth neural mod-\nels; for instance, since time appears explicitly in closed-form, the formu-\nlation relaxes the need for complex numerical solvers. Consequently, we\nobtain models that are between one and ﬁve orders of magnitude faster in\ntraining and inference compared to differential equation-based counter-\nparts. More importantly, in contrast to ODE-based continuous networks,\nclosed-form networks can scale remarkably well compared to other deep\nlearning instances. Lastly, as these models are derived from liquid net-\nworks, they show remarkable performance in time series modeling, com-\npared to advanced recurrent models.\nOne Sentence Summary: We ﬁnd an approximate closed-form solution for the inter-\naction of neurons and synapses and build a strong artiﬁcial neural network model out\nof it.\nMain Text:\nContinuous neural network architectures built by ordinary differential equations (ODEs)\n(2) opened a new paradigm for obtaining expressive and performant neural models.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "85", "text": "Lastly, as these models are derived from liquid net-\nworks, they show remarkable performance in time series modeling, com-\npared to advanced recurrent models.\nOne Sentence Summary: We ﬁnd an approximate closed-form solution for the inter-\naction of neurons and synapses and build a strong artiﬁcial neural network model out\nof it.\nMain Text:\nContinuous neural network architectures built by ordinary differential equations (ODEs)\n(2) opened a new paradigm for obtaining expressive and performant neural models.\nThese models transform the depth dimension of static neural networks and the time di-\nmension of recurrent neural networks into a continuous vector ﬁeld, enabling param-\neter sharing, adaptive computations, and function approximation for non-uniformly\nsampled data.\nThese continuous-depth (time) models have shown promise in density estimation\napplications (3–6), as well as modeling sequential and irregularly-sampled data (1,7–9).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "86", "text": "Main Text:\nContinuous neural network architectures built by ordinary differential equations (ODEs)\n(2) opened a new paradigm for obtaining expressive and performant neural models.\nThese models transform the depth dimension of static neural networks and the time di-\nmension of recurrent neural networks into a continuous vector ﬁeld, enabling param-\neter sharing, adaptive computations, and function approximation for non-uniformly\nsampled data.\nThese continuous-depth (time) models have shown promise in density estimation\napplications (3–6), as well as modeling sequential and irregularly-sampled data (1,7–9).\nWhile ODE-based neural networks with careful memory and gradient propaga-\n2\ntion design (9) perform competitively with advanced discretized recurrent models on\nrelatively small benchmarks, their training and inference are slow due to the use of ad-\nvanced numerical DE solvers (10).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "87", "text": "These continuous-depth (time) models have shown promise in density estimation\napplications (3–6), as well as modeling sequential and irregularly-sampled data (1,7–9).\nWhile ODE-based neural networks with careful memory and gradient propaga-\n2\ntion design (9) perform competitively with advanced discretized recurrent models on\nrelatively small benchmarks, their training and inference are slow due to the use of ad-\nvanced numerical DE solvers (10). This becomes even more troublesome as the com-\nplexity of the data, task and state-space increases (i.e., requiring more precision) (11),\nfor instance, in open-world problems such as medical data processing, self-driving\ncars, ﬁnancial time-series, and physics simulations.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "88", "text": "While ODE-based neural networks with careful memory and gradient propaga-\n2\ntion design (9) perform competitively with advanced discretized recurrent models on\nrelatively small benchmarks, their training and inference are slow due to the use of ad-\nvanced numerical DE solvers (10). This becomes even more troublesome as the com-\nplexity of the data, task and state-space increases (i.e., requiring more precision) (11),\nfor instance, in open-world problems such as medical data processing, self-driving\ncars, ﬁnancial time-series, and physics simulations.\nThe research community has developed solutions for resolving this computational\noverhead and for facilitating the training of neural ODEs, for instance, by relaxing the\nstiffness of a ﬂow by state augmentation techniques (4,12), reformulating the forward-\npass as a root-ﬁnding problem (13), using regularization schemes (14–16), or improv-\ning the inference time of the network (17).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "89", "text": "The research community has developed solutions for resolving this computational\noverhead and for facilitating the training of neural ODEs, for instance, by relaxing the\nstiffness of a ﬂow by state augmentation techniques (4,12), reformulating the forward-\npass as a root-ﬁnding problem (13), using regularization schemes (14–16), or improv-\ning the inference time of the network (17).\nIn this paper, we take a step back and propose a fundamental solution: we de-\nrive a closed-form continuous-depth model that has the rich modeling capabilities\nof ODE-based models and does not require any solver to model data (see Figure 1).\nThe proposed continuous neural networks yield signiﬁcantly faster training and infer-\nence speeds while being as expressive as their ODE-based counterparts. We provide\na derivation for the approximate closed-form solution to a class of continuous neural\nnetworks that explicitly models time.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "90", "text": "In this paper, we take a step back and propose a fundamental solution: we de-\nrive a closed-form continuous-depth model that has the rich modeling capabilities\nof ODE-based models and does not require any solver to model data (see Figure 1).\nThe proposed continuous neural networks yield signiﬁcantly faster training and infer-\nence speeds while being as expressive as their ODE-based counterparts. We provide\na derivation for the approximate closed-form solution to a class of continuous neural\nnetworks that explicitly models time. We demonstrate how this transformation can be\nformulated into a novel neural model and scaled to create ﬂexible, highly performant\nand fast neural architectures on challenging sequential datasets.\nDeriving an Approximate Closed-form Solution for Neural Interactions. Two neu-\nrons interact with each other through synapses as shown in Figure 1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "91", "text": "Deriving an Approximate Closed-form Solution for Neural Interactions. Two neu-\nrons interact with each other through synapses as shown in Figure 1. There are three\nprincipal mechanisms for information propagation in natural brains that are abstracted\naway in the current building blocks of deep learning systems: 1) neural dynamics are\ntypically continuous processes described by differential equations (c.f., dynamics of\n3\nSynapses\nPostsynaptic Neuron\nPresynaptic Stimuli\n𝑰(𝑡)\n𝑆𝑡= 𝑓𝑰𝑡\n(𝐴−𝒙𝑡)\n𝑥(𝑡)\n𝑑𝒙(𝑡)\n𝑑𝑡\n= −𝒙𝑡\n𝜏\n+ 𝑆(𝑡)\nthis is a liquid time-\nconstant differential \nequation instance\nwe solve this in \nclosed-form\n𝒙𝑡=\n(𝒙0 −𝐴) 𝑒! \"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "92", "text": "#$% & '\n' 𝑓(−𝐼𝑡) + 𝐴\n𝒙𝑡\n𝐴\n𝑓.\n𝜏\nPostsynaptic neuron’s potential\nSynaptic reversal potential\nSynaptic release nonlinearity\nPostsynaptic neuron’s time-constant\nFig. 1: Neural and Synapse Dynamics. A postsynaptic neuron receives stimuli I(t), through\na nonlinear conductance-based synapse model. The dynamics of the membrane potential of\nthis postsynaptic neuron is given by the differential equation presented in the middle. This\nequation is a fundamental building block of liquid time-constant networks (LTCs) (1), for which\nthere is no known closed-form expression. Here, we provided an approximate solution for this\nequation which shows the interaction of nonlinear synapses with a postsynaptic neurons, in\nclosed-form.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "93", "text": "1: Neural and Synapse Dynamics. A postsynaptic neuron receives stimuli I(t), through\na nonlinear conductance-based synapse model. The dynamics of the membrane potential of\nthis postsynaptic neuron is given by the differential equation presented in the middle. This\nequation is a fundamental building block of liquid time-constant networks (LTCs) (1), for which\nthere is no known closed-form expression. Here, we provided an approximate solution for this\nequation which shows the interaction of nonlinear synapses with a postsynaptic neurons, in\nclosed-form.\nx(t) in Figure 1), 2) synaptic release is much more than scalar weights; it involves a\nnonlinear transmission of neurotransmitters, the probability of activation of receptors,\nand the concentration of available neurotransmitters, among other nonlinearities (c.f.,\nS(t) in Figure 1), and 3) the propagation of information between neurons is induced\nby feedback and memory apparatuses (c.f.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "94", "text": "Here, we provided an approximate solution for this\nequation which shows the interaction of nonlinear synapses with a postsynaptic neurons, in\nclosed-form.\nx(t) in Figure 1), 2) synaptic release is much more than scalar weights; it involves a\nnonlinear transmission of neurotransmitters, the probability of activation of receptors,\nand the concentration of available neurotransmitters, among other nonlinearities (c.f.,\nS(t) in Figure 1), and 3) the propagation of information between neurons is induced\nby feedback and memory apparatuses (c.f. I(t) stimulates x(t) through a nonlinear\nsynapse S(t) which also has a multiplicative difference of potential to the postsynaptic\nneuron accounting for a negative feedback mechanism). Liquid time-constant (LTC)\nnetworks (1), which are expressive continuous-depth models obtained by a bilinear\napproximation (18) of neural ODE formulation (2) are designed based on these mecha-\nnisms.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "95", "text": "I(t) stimulates x(t) through a nonlinear\nsynapse S(t) which also has a multiplicative difference of potential to the postsynaptic\nneuron accounting for a negative feedback mechanism). Liquid time-constant (LTC)\nnetworks (1), which are expressive continuous-depth models obtained by a bilinear\napproximation (18) of neural ODE formulation (2) are designed based on these mecha-\nnisms. Correspondingly, we take their ODE semantics and approximate a closed-form\nsolution for the scalar case of a postsynaptic neuron receiving an input stimuli from a\npresynaptic source through a nonlinear synapse.\nTo this end, we apply the theory of linear ODEs (19) to analytically solve the dy-\nnamics of an LTC differential equation shown in Figure 1. We then simplify the so-\n4\nTable 1: Time Complexity of the process to compute K solver’s steps. ϵ is step-size, ˜ϵ is the\nmax step-size and δ << 0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "96", "text": "To this end, we apply the theory of linear ODEs (19) to analytically solve the dy-\nnamics of an LTC differential equation shown in Figure 1. We then simplify the so-\n4\nTable 1: Time Complexity of the process to compute K solver’s steps. ϵ is step-size, ˜ϵ is the\nmax step-size and δ << 0.\n˜K is time steps for closed-form continuous depth models (CfCs)\nwhich is equivalent to K. Table is reproduced and taken from (17).\nMethod\nComplexity Local Error\np-th order solver\nO(K · p)\nO(ϵp+1)\nadaptive–step solver\n−\nO(˜ϵ p+1)\nEuler hypersolver\nO(K)\nO(δϵ2)\np-th order hypersolver O(K · p)\nO(δϵp+1)\nCfC (Ours)\nO( ˜K)\nnot relevant\nlution to the point where there is one integral left to solve.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "97", "text": "Method\nComplexity Local Error\np-th order solver\nO(K · p)\nO(ϵp+1)\nadaptive–step solver\n−\nO(˜ϵ p+1)\nEuler hypersolver\nO(K)\nO(δϵ2)\np-th order hypersolver O(K · p)\nO(δϵp+1)\nCfC (Ours)\nO( ˜K)\nnot relevant\nlution to the point where there is one integral left to solve. This integral compart-\nment, R t\n0 f (I(s))ds in which f is a positive, continuous, monotonically increasing, and\nbounded nonlinearity, is challenging to solve in closed-form; since it has dependencies\non an input signal I(s) that is arbitrarily deﬁned (such as a real-world sensory read-\nouts). To approach this problem, we discretize I(s) into piecewise constant segments\nand obtain the discrete approximation of the integral in terms of sum of piecewise con-\nstant compartments over intervals.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "98", "text": "This integral compart-\nment, R t\n0 f (I(s))ds in which f is a positive, continuous, monotonically increasing, and\nbounded nonlinearity, is challenging to solve in closed-form; since it has dependencies\non an input signal I(s) that is arbitrarily deﬁned (such as a real-world sensory read-\nouts). To approach this problem, we discretize I(s) into piecewise constant segments\nand obtain the discrete approximation of the integral in terms of sum of piecewise con-\nstant compartments over intervals. This piecewise constant approximation inspired\nus to introduce an approximate closed-form solution for the integral R t\n0 f (I(s))ds that\nis provably tight when the integral appears as the exponent of an exponential decay,\nwhich is the case for LTCs. We theoretically justify how this closed-form solution rep-\nresents LTCs’ ODE semantics and is as expressive (see Figure 1).\nExplicit Time Dependency.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "99", "text": "To approach this problem, we discretize I(s) into piecewise constant segments\nand obtain the discrete approximation of the integral in terms of sum of piecewise con-\nstant compartments over intervals. This piecewise constant approximation inspired\nus to introduce an approximate closed-form solution for the integral R t\n0 f (I(s))ds that\nis provably tight when the integral appears as the exponent of an exponential decay,\nwhich is the case for LTCs. We theoretically justify how this closed-form solution rep-\nresents LTCs’ ODE semantics and is as expressive (see Figure 1).\nExplicit Time Dependency. We then dissect the properties of the obtained closed-\nform solution and design a new class of neural network models we call Closed-form\nContinuous-depth networks (CfC). CfCs have an explicit time dependency in their for-\nmulation that does not require an ODE solver to obtain their temporal rollouts. Thus,\nthey maximize the trade-off between accuracy and efﬁciency of solvers (See Table 1).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "100", "text": "We theoretically justify how this closed-form solution rep-\nresents LTCs’ ODE semantics and is as expressive (see Figure 1).\nExplicit Time Dependency. We then dissect the properties of the obtained closed-\nform solution and design a new class of neural network models we call Closed-form\nContinuous-depth networks (CfC). CfCs have an explicit time dependency in their for-\nmulation that does not require an ODE solver to obtain their temporal rollouts. Thus,\nthey maximize the trade-off between accuracy and efﬁciency of solvers (See Table 1).\nCfCs perform computations at least one order of magnitude faster training and inference\ntime compared to their ODE-based counterparts, without loss of accuracy.\n5\nTable 2: Sequence and time-step prediction complexity. n is the sequence length, k the num-\nber of hidden units, and p = order of the ODE-solver.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "101", "text": "CfCs have an explicit time dependency in their for-\nmulation that does not require an ODE solver to obtain their temporal rollouts. Thus,\nthey maximize the trade-off between accuracy and efﬁciency of solvers (See Table 1).\nCfCs perform computations at least one order of magnitude faster training and inference\ntime compared to their ODE-based counterparts, without loss of accuracy.\n5\nTable 2: Sequence and time-step prediction complexity. n is the sequence length, k the num-\nber of hidden units, and p = order of the ODE-solver.\nModel\nSequence\nTime-step\nprediction\nprediction\nRNN\nO(nk)\nO(k)\nODE-RNN\nO(nkp)\nO(kp)\nTransformer\nO(n2k)\nO(nk)\nCfC\nO(nk)\nO(k)\nSequence and Time-step Prediction Efﬁciency. CfCs perform per-time-step and per-\nsequence predictions by establishing a continuous ﬂow similar to ODE-based models.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "102", "text": "n is the sequence length, k the num-\nber of hidden units, and p = order of the ODE-solver.\nModel\nSequence\nTime-step\nprediction\nprediction\nRNN\nO(nk)\nO(k)\nODE-RNN\nO(nkp)\nO(kp)\nTransformer\nO(n2k)\nO(nk)\nCfC\nO(nk)\nO(k)\nSequence and Time-step Prediction Efﬁciency. CfCs perform per-time-step and per-\nsequence predictions by establishing a continuous ﬂow similar to ODE-based models.\nHowever, as they do not require ODE-solvers, their complexity is at least one order of\nmagnitude less than ODE based models. Consider having a performant gated recur-\nrent model (20) with the abilities to create expressive continuous ﬂows (2) and adapt-\nable dynamics (1). Table 2 compares the time complexity of CfCs to that of standard\nRNNs, ODE-RNNs and Transformers.\nCfCs: Flexible Deep Models for Sequential Tasks.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "103", "text": "However, as they do not require ODE-solvers, their complexity is at least one order of\nmagnitude less than ODE based models. Consider having a performant gated recur-\nrent model (20) with the abilities to create expressive continuous ﬂows (2) and adapt-\nable dynamics (1). Table 2 compares the time complexity of CfCs to that of standard\nRNNs, ODE-RNNs and Transformers.\nCfCs: Flexible Deep Models for Sequential Tasks. CfCs are equipped with novel gat-\ning mechanisms that explicitly control their memory. CfCs are as expressive as their\nODE-based peers and can be supplied with mixed memory architectures (9) to avoid\ngradient issues in sequential data processing applications. Beyond accuracy and per-\nformance metrics, our results indicate that when considering accuracy-per-compute\ntime, CfCs exhibit over 150× improvement.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "104", "text": "Table 2 compares the time complexity of CfCs to that of standard\nRNNs, ODE-RNNs and Transformers.\nCfCs: Flexible Deep Models for Sequential Tasks. CfCs are equipped with novel gat-\ning mechanisms that explicitly control their memory. CfCs are as expressive as their\nODE-based peers and can be supplied with mixed memory architectures (9) to avoid\ngradient issues in sequential data processing applications. Beyond accuracy and per-\nformance metrics, our results indicate that when considering accuracy-per-compute\ntime, CfCs exhibit over 150× improvement. We perform a diverse set of advanced\ntime series modeling experiments and present the performance and speed gain achiev-\nable by using CfCs in tasks with long-term dependencies, irregular data, and modeling\nphysical dynamics, among others.\n6\nDeriving a Closed-form Solution\nIn this section, we derive an approximate closed-form solution for liquid time-constant\n(LTC) networks, an expressive subclass of time-continuous models.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "105", "text": "Beyond accuracy and per-\nformance metrics, our results indicate that when considering accuracy-per-compute\ntime, CfCs exhibit over 150× improvement. We perform a diverse set of advanced\ntime series modeling experiments and present the performance and speed gain achiev-\nable by using CfCs in tasks with long-term dependencies, irregular data, and modeling\nphysical dynamics, among others.\n6\nDeriving a Closed-form Solution\nIn this section, we derive an approximate closed-form solution for liquid time-constant\n(LTC) networks, an expressive subclass of time-continuous models. We discuss how\nthe scalar closed-form expression derived from a small LTC system can inspire the\ndesign of CfC models.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "106", "text": "6\nDeriving a Closed-form Solution\nIn this section, we derive an approximate closed-form solution for liquid time-constant\n(LTC) networks, an expressive subclass of time-continuous models. We discuss how\nthe scalar closed-form expression derived from a small LTC system can inspire the\ndesign of CfC models.\nThe hidden state of an LTC network is determined by the solution of the initial-\nvalue problem (IVP) given below (1):\ndx\ndt = −(wτ + f (x, I, θ))x(t) + A f (x, I, θ),\n(1)\nwhere x(t) deﬁnes the hidden states, I(t) is the input to the system, wτ is a time-\nconstant parameter vector, A is a bias vector, and f is a neural network parametrized\nby θ.\nTheorem 1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "107", "text": "The hidden state of an LTC network is determined by the solution of the initial-\nvalue problem (IVP) given below (1):\ndx\ndt = −(wτ + f (x, I, θ))x(t) + A f (x, I, θ),\n(1)\nwhere x(t) deﬁnes the hidden states, I(t) is the input to the system, wτ is a time-\nconstant parameter vector, A is a bias vector, and f is a neural network parametrized\nby θ.\nTheorem 1. Given an LTC system determined by the IVP (1), constructed by one cell, receiv-\ning a single dimensional time-series input I with no self connections, the following expression\nis an approximation of its closed-form solution:\nx(t) = (x0 −A)e−[wτ+ f (I(t),θ)]t f (−I(t), θ) + A\n(2)\nProof.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "108", "text": "Given an LTC system determined by the IVP (1), constructed by one cell, receiv-\ning a single dimensional time-series input I with no self connections, the following expression\nis an approximation of its closed-form solution:\nx(t) = (x0 −A)e−[wτ+ f (I(t),θ)]t f (−I(t), θ) + A\n(2)\nProof. In the single-dimensional case, the IVP (1) becomes linear in x as follows:\nd\ndtx(t) = −\n\u0002\nwτ + f (I(t))\n\u0003 · x(t) + A f (I(t))\n(3)\nTherefore, we can use the theory of linear ODEs to obtain an integral closed-form solu-\ntion (19, Section 1.10) consisting of two nested integrals. The inner integral can be elim-\ninated by means of integration by substitution (21). With this, the remaining integral\n7\nexpression can be solved in the case of piecewise constant inputs and approximated in\nthe case of general inputs.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "109", "text": "The inner integral can be elim-\ninated by means of integration by substitution (21). With this, the remaining integral\n7\nexpression can be solved in the case of piecewise constant inputs and approximated in\nthe case of general inputs. The three steps of the proof are outlined below.\nIntegral closed-form solution of LTC. We consider the ODE semantics of a single neu-\nron that receives some arbitrary continuous input signal I and has a positive, bounded,\ncontinuous, and monotonically increasing nonlinearity f:\nd\ndtx(t) = −\n\u0002\nwτ + f (I(t))\n\u0003 · x(t) + A ·\n\u0002\nwτ + f (I(t))\n\u0003\nAssumption. We assumed a second constant value wτ in the above representation of a\nsingle LTC neuron. This is done to introduce symmetry on the structure of the ODE,\nhence being able to apply the theory of linear ODEs for solving the equation analyti-\ncally.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "110", "text": "We assumed a second constant value wτ in the above representation of a\nsingle LTC neuron. This is done to introduce symmetry on the structure of the ODE,\nhence being able to apply the theory of linear ODEs for solving the equation analyti-\ncally.\nBy applying linear ODE systems theory (19, Section 1.10), we obtain:\nx(t) = e−R t\n0 [wτ+ f (I(s))]ds · x(0)+\nZ t\n0 e−R t\ns [wτ+ f (I(v))]dv · A · (wτ + f (I(s)))ds\n(4)\nTo resolve the double integral in the equation above, we deﬁne\nu(s) :=\nZ t\ns [wτ + f (I(v))]dv,\nand observe that d\ndsu(s) = −(wτ + f (I(s))).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "111", "text": "Hence, integration by substitution allows\nus to rewrite (4) into:\nx(t) = e−R t\n0 [wτ+ f (I(s))]ds · x(0) −A\nZ u(t)\nu(0) e−udu\n= x(0)e−R t\n0 [wτ+ f (I(s))]ds + A[e−u]u(t)\nu(0)\n= x(0)e−R t\n0 [wτ+ f (I(s))]ds + A\n\u00001 −e−R t\n0 [wτ+ f (I(s))]ds\u0001\n= (x(0) −A)e−wτte−R t\n0 f (I(s))ds + A\n(5)\n8\nAnalytical LTC solution for piecewise constant inputs. The derivation of a useful\nclosed-form expression of x requires us to solve the integral expression R t\n0 f (I(s))ds for\nany t ≥0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "112", "text": "The derivation of a useful\nclosed-form expression of x requires us to solve the integral expression R t\n0 f (I(s))ds for\nany t ≥0. Fortunately, the integral R t\n0 f (I(s))ds enjoys a simple closed-form expression\nfor piecewise constant inputs I. Speciﬁcally, assume that we are given a sequence of\ntime points:\n0 = τ0 < τ1 < τ2 < . . . < τn−1 < τn = ∞,\nsuch that τ1, . . . , τn−1 ∈R and I(t) = γi for all t ∈[τi; τi+1) with 0 ≤i ≤n −1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "113", "text": "Fortunately, the integral R t\n0 f (I(s))ds enjoys a simple closed-form expression\nfor piecewise constant inputs I. Speciﬁcally, assume that we are given a sequence of\ntime points:\n0 = τ0 < τ1 < τ2 < . . . < τn−1 < τn = ∞,\nsuch that τ1, . . . , τn−1 ∈R and I(t) = γi for all t ∈[τi; τi+1) with 0 ≤i ≤n −1. Then,\nit holds that\nZ t\n0 f (I(s))ds = f (γk)(t −τk) +\nk−1\n∑\ni=0\nf (γi)(τi+1 −τi),\n(6)\nwhen τk ≤t < τk+1 for some 0 ≤k ≤n −1 (as usual, one deﬁnes ∑−1\ni=0 := 0).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "114", "text": "Then,\nit holds that\nZ t\n0 f (I(s))ds = f (γk)(t −τk) +\nk−1\n∑\ni=0\nf (γi)(τi+1 −τi),\n(6)\nwhen τk ≤t < τk+1 for some 0 ≤k ≤n −1 (as usual, one deﬁnes ∑−1\ni=0 := 0). With\nthis, we have:\nx(t) = (x(0) −A)e−wτte−f (γk)(t−τk)−∑k−1\ni=0 f (γi)(τi+1−τi) + A,\n(7)\nwhen τk ≤t < τk+1 for some 0 ≤k ≤n −1. While any continuous input can be ap-\nproximated arbitrarily well by a piecewise constant input (21), a tight approximation\nmay require a large number of discretization points τ1, . . . , τn.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "115", "text": "While any continuous input can be ap-\nproximated arbitrarily well by a piecewise constant input (21), a tight approximation\nmay require a large number of discretization points τ1, . . . , τn. We address this next.\nAnalytical LTC approximation for general inputs. Inspired by Eq. 6, the next result\nprovides an analytical approximation of x(t).\nLemma 1. For any Lipschitz continuous, positive, monotonically increasing, and bounded f\nand continuous input signal I(t), we approximate x(t) in (5) as follows:\n˜x(t) = (x(0) −A)e−\n\u0002\nwτt+ f (I(t))t\n\u0003\nf (−I(t)) + A\n(8)\nThen, |x(t) −˜x(t)| ≤|x(0) −A|e−wτt for all t ≥0. Writing c = x(0) −A for convenience,\nwe can obtain the following sharpness results, additionally:\n9\n1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "116", "text": "Writing c = x(0) −A for convenience,\nwe can obtain the following sharpness results, additionally:\n9\n1. For any t ≥0, we have sup{ 1\nc(x(t) −˜x(t)) | I : [0; t] →R} = e−wτt.\n2. For any t ≥0, we have inf{ 1\nc(x(t) −˜x(t)) | I : [0; t] →R} = e−wτt(e−t −1).\nAbove, the supremum and inﬁmum are meant to be taken across all continuous input signals.\nThese statements settle the question about the worst-case errors of the approximation. The ﬁrst\nstatement implies in particular that our bound is sharp.\nThe full proof is given in Methods. Lemma 1 demonstrates that the integral solution we\nobtained shown in Equation 5 is tightly close to the approximate closed-form solution\nwe proposed in Equation 8.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "117", "text": "Above, the supremum and inﬁmum are meant to be taken across all continuous input signals.\nThese statements settle the question about the worst-case errors of the approximation. The ﬁrst\nstatement implies in particular that our bound is sharp.\nThe full proof is given in Methods. Lemma 1 demonstrates that the integral solution we\nobtained shown in Equation 5 is tightly close to the approximate closed-form solution\nwe proposed in Equation 8. Note that as wτ is positively deﬁned, the derived bound\nbetween Equations 5 and 8 ensures an exponentially decaying error as time goes by.\nTherefore, we have the statement of the theorem.\nAn Instantiation of LTCs and their approximate closed-form expressions. Figure 2\nshows a liquid network with two neurons and ﬁve synaptic connections. The network\nreceives an input signal I(t). Figure 2 further derives the differential equation expres-\nsion of the network along with its closed-form approximate solution.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "118", "text": "Note that as wτ is positively deﬁned, the derived bound\nbetween Equations 5 and 8 ensures an exponentially decaying error as time goes by.\nTherefore, we have the statement of the theorem.\nAn Instantiation of LTCs and their approximate closed-form expressions. Figure 2\nshows a liquid network with two neurons and ﬁve synaptic connections. The network\nreceives an input signal I(t). Figure 2 further derives the differential equation expres-\nsion of the network along with its closed-form approximate solution.\nIn general, it is possible to compile a trained LTC network into its closed-form ver-\nsion. This compilation allows us to speed up inference time of ODE-based networks\nas the closed-form variant does not require complex ODE solvers to compute outputs.\nAlgorithm 1 provides the instructions on how to transfer a trained LTC network into\nits closed form variant.\nTightness of the Closed-form Solution in Practice\nFigure 3 shows an LTC-based network trained for autonomous driving (22).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "119", "text": "Figure 2 further derives the differential equation expres-\nsion of the network along with its closed-form approximate solution.\nIn general, it is possible to compile a trained LTC network into its closed-form ver-\nsion. This compilation allows us to speed up inference time of ODE-based networks\nas the closed-form variant does not require complex ODE solvers to compute outputs.\nAlgorithm 1 provides the instructions on how to transfer a trained LTC network into\nits closed form variant.\nTightness of the Closed-form Solution in Practice\nFigure 3 shows an LTC-based network trained for autonomous driving (22). The ﬁgure\nfurther illustrates how close the proposed solution ﬁts the actual dynamics exhibited\nfrom a single neuron ODE given the same parametrization.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "120", "text": "The ﬁgure\nfurther illustrates how close the proposed solution ﬁts the actual dynamics exhibited\nfrom a single neuron ODE given the same parametrization.\n10\nAlgorithm 1 Translate a trained LTC network into its closed-form variant\nInputs: LTC inputs I(N×T)(t), LTC neurons activity x(H×T)(t), and their initial states\nx(H×1)(0), Synapses adjacency matrix W[(N+H)∗(N+H)]\nAdj\nLTC’s ODE Solver, Solver’s step ∆t,\ntime-instance vectors of inputs, t(1×T)\nI(t)\ntime-instance of LTC neurons tx(t)\n∇time might be sampled irregularly\nLTC neurons’ parameter τ(H×1)\nLTC network synaptic parameters { σ(N×H), µ(N×H), A(N×H)}\nOutputs: LTC’s closed-form approximation of hidden state neurons, ˆx(N×T)(t)\nxpre(t) = WAdj × [I0 . . . IN,\nx0 . . .", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "121", "text": ". . IN,\nx0 . . . xH]\n∇all presynaptic signals to nodes\nfor ith neuron in neurons 1 to H do\nfor j in Synapses to ith neuron do\nˆxi += (x0 −Aij)e\nh\n−tx(t)⊙\n\u0010\n1/τi+\n1\n1+e\n(−σij(xpreij −µij))\n\u0011\n)\ni\n⊙\n1\n1 + e(σij(xpreij−µij)) + Aij\nend for\nend for\nreturn ˆx(t)\nWe took a trained Neural Circuit Policy (NCP) (22), which consists of a perception\nmodule and a liquid time-constant (LTC) based network (1) that possess 19 neurons\nand 253 synapses. The network was trained to autonomously steer a self-driving ve-\nhicle. We used recorded real-world test-runs of the vehicle for a lane-keeping task,\ngoverned by this network.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "122", "text": "The network was trained to autonomously steer a self-driving ve-\nhicle. We used recorded real-world test-runs of the vehicle for a lane-keeping task,\ngoverned by this network. The records included the inputs, outputs as well as all LTC\nneurons’ activities and parameters. To perform a sanity check whether our proposed\nclosed-form solution for LTC neurons is good enough in practice as well as the theory,\nwe plugged in the parameters of individual neurons and synapses of the differential\nequations into the closed-form solution (Similar to the representations shown in Fig-\nure 2b and 2c) and emulated the structure of the ODE-based LTC networks. We then\nvisualized the output neuron’s dynamics of the ODE (in blue) and of the closed-form\n11\n𝑆!", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "123", "text": "We then\nvisualized the output neuron’s dynamics of the ODE (in blue) and of the closed-form\n11\n𝑆!\"(𝑡)\n𝑆!#(𝑡)\n𝑑𝒙𝟏(𝑡)\n𝑑𝑡\n= −𝒙𝟏𝑡\n𝜏\"\n+ 𝑓#\" 𝑰𝑡\n𝐴#\" −𝒙𝟏𝑡\n+ 𝑓$\" 𝒙𝟐𝑡\n(𝐴$\" −𝒙𝟏𝑡)\n𝒙𝟏𝑡= 𝒙𝟏0 −𝐴!\" 𝑒\n% \"\n&!'(\"! ) *\n* 𝑓!\" −𝐼𝑡\n+ 𝐴!\" + 𝒙𝟏0 −𝐴#\" 𝑒\n% \"\n&!'(#! 𝒙𝟐*\n* 𝑓#\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "124", "text": "𝑒\n% \"\n&!'(\"! ) *\n* 𝑓!\" −𝐼𝑡\n+ 𝐴!\" + 𝒙𝟏0 −𝐴#\" 𝑒\n% \"\n&!'(#! 𝒙𝟐*\n* 𝑓#\" −𝒙𝟐𝑡\n+ 𝐴#\"\n𝑑𝒙𝟐(𝑡)\n𝑑𝑡\n= −𝒙𝟐𝑡\n𝜏$\n+ 𝑓#$ 𝑰𝑡\n𝐴#$ −𝒙𝟐𝑡\n+ 𝑓\"$ 𝒙𝟏𝑡\n𝐴\"$ −𝒙𝟐𝑡\n+ 𝑓$$ 𝒙𝟐𝑡\n(𝐴$$ −𝒙𝟐𝑡)\n𝒙𝟐𝑡=", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "125", "text": "−𝒙𝟐𝑡\n𝜏$\n+ 𝑓#$ 𝑰𝑡\n𝐴#$ −𝒙𝟐𝑡\n+ 𝑓\"$ 𝒙𝟏𝑡\n𝐴\"$ −𝒙𝟐𝑡\n+ 𝑓$$ 𝒙𝟐𝑡\n(𝐴$$ −𝒙𝟐𝑡)\n𝒙𝟐𝑡= 𝒙𝟐𝟎−𝐴!# 𝑒\n% \"\n&#'(\"# ) *\n* 𝑓!# −𝐼𝑡\n+ 𝐴!# + 𝒙𝟐𝟎−𝐴\"# 𝑒\n% \"\n&#'(!# 𝒙𝟏*\n* 𝑓\"# −𝒙𝟐𝑡\n+ 𝐴\"# + 𝒙𝟐𝟎−𝐴##", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "126", "text": "𝒙𝟐𝟎−𝐴!# 𝑒\n% \"\n&#'(\"# ) *\n* 𝑓!# −𝐼𝑡\n+ 𝐴!# + 𝒙𝟐𝟎−𝐴\"# 𝑒\n% \"\n&#'(!# 𝒙𝟏*\n* 𝑓\"# −𝒙𝟐𝑡\n+ 𝐴\"# + 𝒙𝟐𝟎−𝐴## 𝑒\n% \"\n&#'(## 𝒙𝟐*\n* 𝑓## −𝒙𝟐𝑡\n+ 𝐴##\n𝑆\"#(𝑡)\n𝑆!\"(𝑡)\n𝑆#\"(𝑡)\n𝑰(𝑡)\n𝑆!", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "127", "text": "\"(𝑡)\n𝑆!#(𝑡)\n𝑆#\"(𝑡)\n𝑆\"#(𝑡)\n𝑆##(𝑡)\n𝑆##(𝑡)\n𝑆#\"(𝑡)\n𝑆!#(𝑡)\n𝑆\"#(𝑡)\n𝑆##(𝑡)\na. LTC network with 2 neurons\nb. LTC differential equations\nc. Approximate closed-form solution of LTCs \nInput\nLegend\n𝑥𝑖(𝑡)\npotential of neuron i\n𝑆𝑖𝑗𝑡\nsynapse between node i and j\n𝜏𝑖\ntime-constant of neuron i\n𝐴/0 synaptic reversal potential for nodes i and j\n𝑓/0\nnonlinearity of a synapse between i and j\n𝑡\ntime\nFig. 2: Instantiation of LTCs in ODE and closed-form representations.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "128", "text": "2: Instantiation of LTCs in ODE and closed-form representations. a) A sample LTC\nnetwork with two nodes and ﬁve synapses. b) the ODE representation of this two-neuron\nsystem. c) the approximate closed-form representation of the network.\nsolution (in red). As illustrated in Figure 3, we observed that the behavior of the ODE\nis captured with a mean-squared error of 0.006 by the closed-form solution. This ex-\nperiment is an empirical evidence for the tightness results presented in our theory.\nHence, the closed-form solution contains the main properties of liquid networks in ap-\nproximating dynamics. We next show how to design a novel neural network instance\ninspired by this closed-form solution, that has well-behaved gradient properties and\napproximation capabilities.\nDesign a Closed-form Continuous-depth Model Inspired\nby the Solution\nLeveraging the scalar closed-form solution expressed by Eq. (2), we can now distill\nthis model into a neural network that can be trained at scale.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "129", "text": "This ex-\nperiment is an empirical evidence for the tightness results presented in our theory.\nHence, the closed-form solution contains the main properties of liquid networks in ap-\nproximating dynamics. We next show how to design a novel neural network instance\ninspired by this closed-form solution, that has well-behaved gradient properties and\napproximation capabilities.\nDesign a Closed-form Continuous-depth Model Inspired\nby the Solution\nLeveraging the scalar closed-form solution expressed by Eq. (2), we can now distill\nthis model into a neural network that can be trained at scale. The solution provid-\ning a grounded theoretical basis for solving scalar continuous-time dynamics and it is\nimportant to translate this theory into a practical neural network model which can be\n12\n𝑑𝑥\n𝑑𝑡= −𝑤!", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "130", "text": "The solution provid-\ning a grounded theoretical basis for solving scalar continuous-time dynamics and it is\nimportant to translate this theory into a practical neural network model which can be\n12\n𝑑𝑥\n𝑑𝑡= −𝑤! + 𝑓𝑥, 𝐼\n𝑥𝑡+ 𝐴𝑓𝑥, 𝐼\n𝑥𝑡= 𝑥0 −𝐴𝑒\" #!$% &,( ) 𝑓(−𝑥, −𝐼) + 𝐴\nPerception module\nliquid time-constant (LTC) module\ninput stream\ndynamics of each node\nTime (s)\nOutput neuron dynamics\nODE\nCfC\nclosed-form \nsolution of LTC\nLTC\nOutputs\nInputs  𝐼𝑡\nneuron’s state 𝑥(𝑡)\nNonlinearity 𝑓(. )\nParameters 𝑤\", 𝐴\nFig.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "131", "text": "Parameters 𝑤\", 𝐴\nFig. 3: Tightness of the closed-form solution in practice. We approximate a closed-form so-\nlution for LTCs (1) while largely preserving the trajectories of their equivalent ODE systems.\nWe develop our solution into closed-form continuous-depth (CfC) models that are at least 100x\nfaster than neural ODEs at both training and inference on complex time-series prediction tasks.\nintegrated into larger representation learning systems. Doing so requires careful atten-\ntion to potential gradient and expressivity issues that can arise during optimization,\nwhich we will outline in this section.\nFormally, the hidden states, x(t)(D×1) with D hidden units at each time step t, can be\nexplicitly obtained by:\nx(t) = B ⊙e−[wτ+ f (x,I;θ)]t ⊙f (−x, −I; θ) + A,\n(9)\nwhere B(D) collapses (x0 −A) of Eq. 2 into a parameter vector.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "132", "text": "Formally, the hidden states, x(t)(D×1) with D hidden units at each time step t, can be\nexplicitly obtained by:\nx(t) = B ⊙e−[wτ+ f (x,I;θ)]t ⊙f (−x, −I; θ) + A,\n(9)\nwhere B(D) collapses (x0 −A) of Eq. 2 into a parameter vector. A(D) and w(D)\nτ\nare\nsystem’s parameter vectors, as well, I(t)(m×1) is an m-dimensional input at each time\nstep t, f is a neural network parametrized by θ = {W(m×D)\nIx\n, W(D×D)\nxx\n, b(D)\nx\n}, and ⊙is\n13\nneural network heads\nBackbone \nneural \nnetwork\n𝑓\n𝑔\nℎ\n+\n𝜎(. )", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "133", "text": "2 into a parameter vector. A(D) and w(D)\nτ\nare\nsystem’s parameter vectors, as well, I(t)(m×1) is an m-dimensional input at each time\nstep t, f is a neural network parametrized by θ = {W(m×D)\nIx\n, W(D×D)\nxx\n, b(D)\nx\n}, and ⊙is\n13\nneural network heads\nBackbone \nneural \nnetwork\n𝑓\n𝑔\nℎ\n+\n𝜎(. )\n1 −𝜎\n𝑦(𝑡)\n𝐭= 𝑡!, 𝑡\", … , 𝑡#$%, 𝑡#\ntime vector\nClosed-form Continuous-depth (CfC)\nHadamard\nsigmoid\n-1\n𝑡\n𝑡!\n𝑡\"\n𝑡#$%\n𝑡#\n𝐼(𝑡)\n𝐼\"\n𝐼#\n𝐼$\nInput batch\nFig.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "134", "text": "1 −𝜎\n𝑦(𝑡)\n𝐭= 𝑡!, 𝑡\", … , 𝑡#$%, 𝑡#\ntime vector\nClosed-form Continuous-depth (CfC)\nHadamard\nsigmoid\n-1\n𝑡\n𝑡!\n𝑡\"\n𝑡#$%\n𝑡#\n𝐼(𝑡)\n𝐼\"\n𝐼#\n𝐼$\nInput batch\nFig. 4: Closed-form Continuous-depth neural architecture. A baclbone neural network layer\ndelivers the input signals into three head networks g, f and h. f acts as a liquid time-constant\nfor the sigmoidal time-gates of the network. g and h construct the nonlinearieties of the overall\nCfC network.\nthe Hadamard (element-wise) product.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "135", "text": "𝑡\"\n𝑡#$%\n𝑡#\n𝐼(𝑡)\n𝐼\"\n𝐼#\n𝐼$\nInput batch\nFig. 4: Closed-form Continuous-depth neural architecture. A baclbone neural network layer\ndelivers the input signals into three head networks g, f and h. f acts as a liquid time-constant\nfor the sigmoidal time-gates of the network. g and h construct the nonlinearieties of the overall\nCfC network.\nthe Hadamard (element-wise) product. While the neural network presented in 9 can be\nproven to be a universal approximator as it is an approximation of an ODE system (1,2),\nin its current form, it has trainability issues which we point out and resolve shortly:\nResolving the gradient issues. The exponential term in Eq.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "136", "text": "g and h construct the nonlinearieties of the overall\nCfC network.\nthe Hadamard (element-wise) product. While the neural network presented in 9 can be\nproven to be a universal approximator as it is an approximation of an ODE system (1,2),\nin its current form, it has trainability issues which we point out and resolve shortly:\nResolving the gradient issues. The exponential term in Eq. 9 derives the system’s\nﬁrst part (exponentially fast) to 0 and the entire hidden state to A. This issue becomes\nmore apparent when there are recurrent connections and causes vanishing gradient\nfactors when trained by gradient descent (23). To reduce the effect, we replace the\nexponential decay term with a reversed sigmoidal nonlinearity σ(.). This nonlinearity\nis approximately 1 at t = 0 and approaches 0 in the limit t →∞. However, unlike the\nexponential decay, its transition happens much smoother, yielding a better condition\non the loss surface.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "137", "text": "To reduce the effect, we replace the\nexponential decay term with a reversed sigmoidal nonlinearity σ(.). This nonlinearity\nis approximately 1 at t = 0 and approaches 0 in the limit t →∞. However, unlike the\nexponential decay, its transition happens much smoother, yielding a better condition\non the loss surface.\nReplacing biases by learnable instances. Next, we consider the bias parameter B to be\npart of the trainable parameters of the neural network f (−x, −I; θ) and choose to use\na new network instance instead of f (presented in the exponential decay factor). We\nalso replace A with another neural network instance, h(.) to enhance the ﬂexibility of\nthe model. To obtain a more general network architecture, we allow the nonlinearity\nf (−x, −I; θ) present in Eq. 9 have both shared (backbone) and independent, (g(.)),\n14\nnetwork compartments.\nGating balance.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "138", "text": "We\nalso replace A with another neural network instance, h(.) to enhance the ﬂexibility of\nthe model. To obtain a more general network architecture, we allow the nonlinearity\nf (−x, −I; θ) present in Eq. 9 have both shared (backbone) and independent, (g(.)),\n14\nnetwork compartments.\nGating balance. The time-decaying sigmoidal term can play a gating role if we addi-\ntionally multiply h(.), with (1 −σ(.)). This way, the time-decaying sigmoid function\nstands for a gating mechanism that interpolates between the two limits of t →−∞and\nt →∞of the ODE trajectory.\nBackbone. Instead of learning all three neural network instances f, g and h separately,\nwe have them share the ﬁrst few layers in the form of a backbone that branches out\ninto these three functions. As a result, the backbone allows our model to learn shared\nrepresentations, thereby speeding up and stabilizing the learning process.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "139", "text": "), with (1 −σ(.)). This way, the time-decaying sigmoid function\nstands for a gating mechanism that interpolates between the two limits of t →−∞and\nt →∞of the ODE trajectory.\nBackbone. Instead of learning all three neural network instances f, g and h separately,\nwe have them share the ﬁrst few layers in the form of a backbone that branches out\ninto these three functions. As a result, the backbone allows our model to learn shared\nrepresentations, thereby speeding up and stabilizing the learning process. More im-\nportantly, this architectural prior enables two simultaneous beneﬁts: 1) Through the\nshared backbone a coupling between time-constant of the system and its state nonlin-\nearity get established that exploits causal representation learning evident in a liquid\nneural network (1, 24). 2) through separate head network layers, the system has the\nability to explore temporal and structural dependencies independently of each other.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "140", "text": "As a result, the backbone allows our model to learn shared\nrepresentations, thereby speeding up and stabilizing the learning process. More im-\nportantly, this architectural prior enables two simultaneous beneﬁts: 1) Through the\nshared backbone a coupling between time-constant of the system and its state nonlin-\nearity get established that exploits causal representation learning evident in a liquid\nneural network (1, 24). 2) through separate head network layers, the system has the\nability to explore temporal and structural dependencies independently of each other.\nThese modiﬁcations result in the closed-form continuous-depth (CfC) neural network\nmodel:\nx(t) = σ(−f (x, I; θ f ) t)\n|\n{z\n}\ntime-continuous gating\n⊙g(x, I; θg)+\n(10)\n\u0002\n1 −σ(−[ f (x, I; θ f )] t)\n\u0003\n|\n{z\n}\ntime-continuous gating\n⊙h(x, I; θh).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "141", "text": "These modiﬁcations result in the closed-form continuous-depth (CfC) neural network\nmodel:\nx(t) = σ(−f (x, I; θ f ) t)\n|\n{z\n}\ntime-continuous gating\n⊙g(x, I; θg)+\n(10)\n\u0002\n1 −σ(−[ f (x, I; θ f )] t)\n\u0003\n|\n{z\n}\ntime-continuous gating\n⊙h(x, I; θh).\nThe CfC architecture is illustrated in Figure 4. The neural network instances could\nbe selected arbitrarily. The time complexity of the algorithm is equivalent to that of\ndiscretized recurrent networks (25), which is at least one order of magnitude faster\nthan ODE-based networks.\n15\nHow do you deal with time, t? CfCs are continuous-depth models that can set their\ntemporal behavior based on the task-under-test.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "142", "text": "The CfC architecture is illustrated in Figure 4. The neural network instances could\nbe selected arbitrarily. The time complexity of the algorithm is equivalent to that of\ndiscretized recurrent networks (25), which is at least one order of magnitude faster\nthan ODE-based networks.\n15\nHow do you deal with time, t? CfCs are continuous-depth models that can set their\ntemporal behavior based on the task-under-test. For time-variant datasets (e.g., irreg-\nularly sampled time series, event-based data, and sparse data), t for each incoming\nsample is set based on its time-stamp or order. For sequential applications where the\ntime of the occurrence of a sample does not matter, t is sampled batch-length-times with\nequidistant intervals within two hyperparameters a and b.\nExperiments with CfCs\nWe now assess the performance of CfCs in a series of sequential data processing tasks\ncompared to advanced, recurrent models.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "143", "text": "For time-variant datasets (e.g., irreg-\nularly sampled time series, event-based data, and sparse data), t for each incoming\nsample is set based on its time-stamp or order. For sequential applications where the\ntime of the occurrence of a sample does not matter, t is sampled batch-length-times with\nequidistant intervals within two hyperparameters a and b.\nExperiments with CfCs\nWe now assess the performance of CfCs in a series of sequential data processing tasks\ncompared to advanced, recurrent models. We ﬁrst evaluate how CfCs compare to\nLTC-based neural circuit policies (NCPs) (22) in real-world autonomous lane keeping\ntasks. We then approach solving conventional sequential data modeling tasks (e.g., bit-\nstream prediction, sentiment analysis on text data, medical time-series prediction, and\nrobot kinematics modeling), and compare CfC variants to an extensive set of advanced\nrecurrent neural network baselines.\nCfC Network Variants.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "144", "text": "We ﬁrst evaluate how CfCs compare to\nLTC-based neural circuit policies (NCPs) (22) in real-world autonomous lane keeping\ntasks. We then approach solving conventional sequential data modeling tasks (e.g., bit-\nstream prediction, sentiment analysis on text data, medical time-series prediction, and\nrobot kinematics modeling), and compare CfC variants to an extensive set of advanced\nrecurrent neural network baselines.\nCfC Network Variants. To evaluate how the proposed modiﬁcations we applied to\nthe closed-form solution network described by Eq. 9, we test four variants of the CfC\narchitecture: 1) Closed-form solution network (Cf-S) obtained by Eq. 9, 2) CfC without\nthe second gating mechanism (CfC-noGate). This variant does not have the 1 −σ\ninstance shown in Figure 4. 3) Closed-form Continuous-depth model (CfC) expressed\nby Eq. 10.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "145", "text": "CfC Network Variants. To evaluate how the proposed modiﬁcations we applied to\nthe closed-form solution network described by Eq. 9, we test four variants of the CfC\narchitecture: 1) Closed-form solution network (Cf-S) obtained by Eq. 9, 2) CfC without\nthe second gating mechanism (CfC-noGate). This variant does not have the 1 −σ\ninstance shown in Figure 4. 3) Closed-form Continuous-depth model (CfC) expressed\nby Eq. 10. 4) CfC wrapped inside a mixed-memory architecture (i.e., CfC deﬁnes the\nmemory state of an RNN for instance an LSTM). We call this variant CfC-mmRNN.\nEach of these four proposed variants leverage our proposed solution, and thus are at\nleast one order of magnitude faster than continuous-time ODE models.\nHow well CfCs perform in autonomous driving compared to NCPs and other mod-\n16\nels?", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "146", "text": "3) Closed-form Continuous-depth model (CfC) expressed\nby Eq. 10. 4) CfC wrapped inside a mixed-memory architecture (i.e., CfC deﬁnes the\nmemory state of an RNN for instance an LSTM). We call this variant CfC-mmRNN.\nEach of these four proposed variants leverage our proposed solution, and thus are at\nleast one order of magnitude faster than continuous-time ODE models.\nHow well CfCs perform in autonomous driving compared to NCPs and other mod-\n16\nels? In this experiment, our objective is to evaluate how robustly CfCs learn to perform\nautonomous navigation as opposed to its ODE-based counterparts LTC networks. The\ntask is to map incoming pixel observations to steering curvature commands. We start\noff by training neural network architectures that possess a convolutional head stacked\nwith the choice of RNN. The RNN compartment of networks are replaced by LSTM\nnetworks, NCPs (22), Cf-S, CfC-NoGate, and CfC-mmRNN.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "147", "text": "In this experiment, our objective is to evaluate how robustly CfCs learn to perform\nautonomous navigation as opposed to its ODE-based counterparts LTC networks. The\ntask is to map incoming pixel observations to steering curvature commands. We start\noff by training neural network architectures that possess a convolutional head stacked\nwith the choice of RNN. The RNN compartment of networks are replaced by LSTM\nnetworks, NCPs (22), Cf-S, CfC-NoGate, and CfC-mmRNN. We also trained a fully\nconvolutional neural network for the sake of proper comparison.\nOur training pipeline followed an imitation learning approach with paired pixel-\ncontrol data, from a 30Hz BlackFly PGE-23S3C RGB camera, collected by a human\nexpert driver across a variety of rural driving environments, including times of day,\nweather conditions, and season of the year. The original 3-hour dataset was further\naugmented to include off-orientation recovery data using a privileged controller (26)\nand a data-driven view synthesizer (27).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "148", "text": "We also trained a fully\nconvolutional neural network for the sake of proper comparison.\nOur training pipeline followed an imitation learning approach with paired pixel-\ncontrol data, from a 30Hz BlackFly PGE-23S3C RGB camera, collected by a human\nexpert driver across a variety of rural driving environments, including times of day,\nweather conditions, and season of the year. The original 3-hour dataset was further\naugmented to include off-orientation recovery data using a privileged controller (26)\nand a data-driven view synthesizer (27). The privileged controller enabled training all\nnetworks using guided policy learning (28). After training, all networks were trans-\nferred on-board our full-scale autonomous vehicle (Lexus RX450H, retroﬁtted with\ndrive-by-wire capability). The vehicle was consistently started at the center of the\nlane, initialized with each trained model, and was run to completion at the end of\nthe road.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "149", "text": "The original 3-hour dataset was further\naugmented to include off-orientation recovery data using a privileged controller (26)\nand a data-driven view synthesizer (27). The privileged controller enabled training all\nnetworks using guided policy learning (28). After training, all networks were trans-\nferred on-board our full-scale autonomous vehicle (Lexus RX450H, retroﬁtted with\ndrive-by-wire capability). The vehicle was consistently started at the center of the\nlane, initialized with each trained model, and was run to completion at the end of\nthe road. If the model exited the bounds of the lane a human safety driver intervened\nand restarted the model from the center of the road at the intervention location. All\nmodels were tested with and without noise added to the sensory inputs to evaluate\nrobustness.\nThe testing environment consisted of 1km of private test road with unlabeled lane-\nmarkers and we observed that all trained networks were able to successfully complete\nthe lane-keeping task at a constant velocity of 30 km/hr. Fig.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "150", "text": "If the model exited the bounds of the lane a human safety driver intervened\nand restarted the model from the center of the road at the intervention location. All\nmodels were tested with and without noise added to the sensory inputs to evaluate\nrobustness.\nThe testing environment consisted of 1km of private test road with unlabeled lane-\nmarkers and we observed that all trained networks were able to successfully complete\nthe lane-keeping task at a constant velocity of 30 km/hr. Fig. 5 provides an insight\ninto how these networks come with driving decisions. To this end, we computed the\n17\nTable 3: Lane-keeping models’ parameter count.\nCfC and NCP networks perform lane-\nkeeping in unseen scenarios with a compact representation.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "151", "text": "The testing environment consisted of 1km of private test road with unlabeled lane-\nmarkers and we observed that all trained networks were able to successfully complete\nthe lane-keeping task at a constant velocity of 30 km/hr. Fig. 5 provides an insight\ninto how these networks come with driving decisions. To this end, we computed the\n17\nTable 3: Lane-keeping models’ parameter count.\nCfC and NCP networks perform lane-\nkeeping in unseen scenarios with a compact representation.\nModes\nTotal Parameter Count\nRNN Parameter Count\n(CNN head + RNN)\nCNN\n2,124,901\n-\nLSTM\n259,733\n33089\nNCP\n233,139\n6495\nCf-S\n227,728\n1084\nCfC\n230,828\n4184\nCfC-NoGate\n230,828\n4184\nCfC-mmRNN\n235,052\n8408\nattention of each network while driving, by using the visual-backprop algorithm (29).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "152", "text": "Modes\nTotal Parameter Count\nRNN Parameter Count\n(CNN head + RNN)\nCNN\n2,124,901\n-\nLSTM\n259,733\n33089\nNCP\n233,139\n6495\nCf-S\n227,728\n1084\nCfC\n230,828\n4184\nCfC-NoGate\n230,828\n4184\nCfC-mmRNN\n235,052\n8408\nattention of each network while driving, by using the visual-backprop algorithm (29).\nWe observe that CfCs similar to NCPs demonstrate a consistent attention pattern in\neach subtask, while maintaining their attention proﬁle under heavy noise depicted in\nFig. 5c. Similar to NCPs, CfCs are very parameter efﬁcient. They performed the end-\nto-end autonomous lane keeping task with around 4k trainable parameters in their\nrecurrent neural network component.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "153", "text": "We observe that CfCs similar to NCPs demonstrate a consistent attention pattern in\neach subtask, while maintaining their attention proﬁle under heavy noise depicted in\nFig. 5c. Similar to NCPs, CfCs are very parameter efﬁcient. They performed the end-\nto-end autonomous lane keeping task with around 4k trainable parameters in their\nrecurrent neural network component.\nIn the following, we design sequence data processing pipelines where we exten-\nsively test CfCs’ effectiveness in learning spatiotemporal dynamics, compared to a\nlarge range of advanced recurrent models.\nBaselines. We compare CfCs to a diverse set of advanced algorithms developed for\nsequence modeling by both discretized and continuous mechanisms.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "154", "text": "5c. Similar to NCPs, CfCs are very parameter efﬁcient. They performed the end-\nto-end autonomous lane keeping task with around 4k trainable parameters in their\nrecurrent neural network component.\nIn the following, we design sequence data processing pipelines where we exten-\nsively test CfCs’ effectiveness in learning spatiotemporal dynamics, compared to a\nlarge range of advanced recurrent models.\nBaselines. We compare CfCs to a diverse set of advanced algorithms developed for\nsequence modeling by both discretized and continuous mechanisms. Examples in-\nclude some variations of classical autoregressive RNNs, such as an RNN with concate-\nnated ∆t (RNN-∆t), a recurrent model with moving average on missing values (RNN-\nimpute), RNN Decay (7), long short-term memory (LSTMs) (20), and gated recurrent\nunits (GRUs) (30).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "155", "text": "Baselines. We compare CfCs to a diverse set of advanced algorithms developed for\nsequence modeling by both discretized and continuous mechanisms. Examples in-\nclude some variations of classical autoregressive RNNs, such as an RNN with concate-\nnated ∆t (RNN-∆t), a recurrent model with moving average on missing values (RNN-\nimpute), RNN Decay (7), long short-term memory (LSTMs) (20), and gated recurrent\nunits (GRUs) (30). We also report results for a variety of encoder-decoder ODE-RNN\nbased models, such as RNN-VAE, Latent variable models with RNNs, and with ODEs,\nall from (7).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "156", "text": "We also report results for a variety of encoder-decoder ODE-RNN\nbased models, such as RNN-VAE, Latent variable models with RNNs, and with ODEs,\nall from (7).\n18\nInput                       CNN                      LSTM                      NCP                        CfC\nCfC-mmRNN\nTime\nsaliency map\n0\n1\nTest in \nSummer\nTest in \nWinter\nTime\nsaliency map\n0\n1\nsaliency map\n0\n1\nInput                       CNN                      LSTM                      NCP                        CfC\nCfC-mmRNN\nInput                       CNN                      LSTM                      NCP                        CfC\nCfC-mmRNN\nTest Under \nNoise\nTime\na\nb\nc\nFig. 5: Attention Proﬁle of networks. Trained networks receive unseen inputs (ﬁrst column\nin each tab) and generate acceleration and steering commands. We use the Visual-Backprop\nalgorithm (29) to compute the saliency maps of the convolutional part of each network.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "157", "text": "5: Attention Proﬁle of networks. Trained networks receive unseen inputs (ﬁrst column\nin each tab) and generate acceleration and steering commands. We use the Visual-Backprop\nalgorithm (29) to compute the saliency maps of the convolutional part of each network. a)\nresults for networks tested on data collected in summer. b) results for networks tested on data\ncollected in winter. c) results for inputs corrupted by a zero-mean Gaussian noise with variance,\nσ2 = 0.35.\nFurthermore, we include models such as interpolation prediction networks (IP-\nNet) (31), Set functions for time-series (SeFT) (32), CT-RNNs (33), CT-GRU (34), CT-\nLSTM (35), GRU-D (36), Phased-LSTM (37), bi-directional RNNs (38).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "158", "text": "b) results for networks tested on data\ncollected in winter. c) results for inputs corrupted by a zero-mean Gaussian noise with variance,\nσ2 = 0.35.\nFurthermore, we include models such as interpolation prediction networks (IP-\nNet) (31), Set functions for time-series (SeFT) (32), CT-RNNs (33), CT-GRU (34), CT-\nLSTM (35), GRU-D (36), Phased-LSTM (37), bi-directional RNNs (38).\nFinally, we\nbenchmarked CfCs against competitive recent RNN architectures with the premise of\ntackling long-term dependencies, such as Legandre Memory Units (LMU) (39), high-\norder polynomial projection operators (Hippo) (40), orthogonal recurrent models (ex-\npRNNs) (41), mixed memory RNNs such as (ODE-LSTMs) (9), coupled oscillatory\n19\nRNNs (coRNN) (42), and Lipschitz RNNs (43).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "159", "text": "Finally, we\nbenchmarked CfCs against competitive recent RNN architectures with the premise of\ntackling long-term dependencies, such as Legandre Memory Units (LMU) (39), high-\norder polynomial projection operators (Hippo) (40), orthogonal recurrent models (ex-\npRNNs) (41), mixed memory RNNs such as (ODE-LSTMs) (9), coupled oscillatory\n19\nRNNs (coRNN) (42), and Lipschitz RNNs (43).\nRegularly and Irregularly-Sampled Bit-Stream XOR\nThe bit-stream XOR dataset (9) considers classifying bit-streams implementing an XOR\nfunction in time, i.e., each item in the sequence contributes equally to the correct out-\nput. The bit-streams are provided in densely sampled and event-based sampled for-\nmat. The densely sampled version simply represents an incoming bit as an input event.\nThe event sampled version transmits only bit-changes to the network, i.e., multiple\nequal bit is packed into a single input event.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "160", "text": "Regularly and Irregularly-Sampled Bit-Stream XOR\nThe bit-stream XOR dataset (9) considers classifying bit-streams implementing an XOR\nfunction in time, i.e., each item in the sequence contributes equally to the correct out-\nput. The bit-streams are provided in densely sampled and event-based sampled for-\nmat. The densely sampled version simply represents an incoming bit as an input event.\nThe event sampled version transmits only bit-changes to the network, i.e., multiple\nequal bit is packed into a single input event. Consequently, the densely sampled vari-\nant is a regular sequence classiﬁcation problem, whereas the event-based encoding\nvariant represents an irregularly sampled sequence classiﬁcation problem.\nTable 4 compares the performance of many RNN baselines.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "161", "text": "The bit-streams are provided in densely sampled and event-based sampled for-\nmat. The densely sampled version simply represents an incoming bit as an input event.\nThe event sampled version transmits only bit-changes to the network, i.e., multiple\nequal bit is packed into a single input event. Consequently, the densely sampled vari-\nant is a regular sequence classiﬁcation problem, whereas the event-based encoding\nvariant represents an irregularly sampled sequence classiﬁcation problem.\nTable 4 compares the performance of many RNN baselines. Many architectures\nsuch as Augmented LSTM, CT-GRU, GRU-D, ODE-LSTM, coRNN, and Lipschitz RNN,\nand all variants of CfC can successfully solve the task with 100% accuracy when the\nbit-stream samples are equidistant from each other. However, when the bit-stream\nsamples arrive at non-uniform distances, only architectures that are immune to the\nvanishing gradient in irregularly sampled data can solve the task.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "162", "text": "Table 4 compares the performance of many RNN baselines. Many architectures\nsuch as Augmented LSTM, CT-GRU, GRU-D, ODE-LSTM, coRNN, and Lipschitz RNN,\nand all variants of CfC can successfully solve the task with 100% accuracy when the\nbit-stream samples are equidistant from each other. However, when the bit-stream\nsamples arrive at non-uniform distances, only architectures that are immune to the\nvanishing gradient in irregularly sampled data can solve the task. These include GRU-\nD, ODE-LSTM and CfCs, and CfC-mmRNNs. ODE-based RNNs cannot solve the\nevent-based encoding tasks regardless of their choice of solvers, as they have vanish-\ning/exploding gradient issues (9). The hyperparameter details of this experiment is\nprovided in Table S1.\nPhysioNet Challenge\nThe PhysioNet Challenge 2012 dataset considers the prediction of the mortality of 8000\npatients admitted to the intensive care unit (ICU).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "163", "text": "These include GRU-\nD, ODE-LSTM and CfCs, and CfC-mmRNNs. ODE-based RNNs cannot solve the\nevent-based encoding tasks regardless of their choice of solvers, as they have vanish-\ning/exploding gradient issues (9). The hyperparameter details of this experiment is\nprovided in Table S1.\nPhysioNet Challenge\nThe PhysioNet Challenge 2012 dataset considers the prediction of the mortality of 8000\npatients admitted to the intensive care unit (ICU). The features represent time series\n20\nTable 4: Bit-stream XOR sequence classiﬁcation. The performance values for all baseline\nmodels are reproduced from (9). Numbers present mean ± standard deviations, n=5\nModel\nEquidistant\nEvent-based\nTime Per epoch\nODE-based?\nencoding\n(irregular) encoding\n(min)\n†Augmented LSTM (20)\n100.00% ± 0.00\n89.71% ± 3.48\n0.62\nNo\n†CT-GRU (34)\n100.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "164", "text": "The features represent time series\n20\nTable 4: Bit-stream XOR sequence classiﬁcation. The performance values for all baseline\nmodels are reproduced from (9). Numbers present mean ± standard deviations, n=5\nModel\nEquidistant\nEvent-based\nTime Per epoch\nODE-based?\nencoding\n(irregular) encoding\n(min)\n†Augmented LSTM (20)\n100.00% ± 0.00\n89.71% ± 3.48\n0.62\nNo\n†CT-GRU (34)\n100.00% ± 0.00\n61.36% ± 4.87\n0.80\nNo\n†RNN Decay (7)\n60.28% ± 19.87\n75.53% ± 5.28\n0.90\nNo\n†Bi-directional RNN (38)\n100.00% ± 0.00\n90.17% ± 0.69\n1.82\nNo\n†GRU-D (36)\n100.00% ± 0.00\n97.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "165", "text": "62\nNo\n†CT-GRU (34)\n100.00% ± 0.00\n61.36% ± 4.87\n0.80\nNo\n†RNN Decay (7)\n60.28% ± 19.87\n75.53% ± 5.28\n0.90\nNo\n†Bi-directional RNN (38)\n100.00% ± 0.00\n90.17% ± 0.69\n1.82\nNo\n†GRU-D (36)\n100.00% ± 0.00\n97.90% ± 1.71\n0.58\nNo\n†PhasedLSTM (37)\n50.99% ± 0.76\n80.29% ± 0.99\n1.22\nNo\n†CT-LSTM (35)\n97.73% ± 0.08\n95.09% ± 0.30\n0.86\nNo\ncoRNN (42)\n100.00% ± 0.00\n52.89% ± 1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "166", "text": "82\nNo\n†GRU-D (36)\n100.00% ± 0.00\n97.90% ± 1.71\n0.58\nNo\n†PhasedLSTM (37)\n50.99% ± 0.76\n80.29% ± 0.99\n1.22\nNo\n†CT-LSTM (35)\n97.73% ± 0.08\n95.09% ± 0.30\n0.86\nNo\ncoRNN (42)\n100.00% ± 0.00\n52.89% ± 1.25\n0.57\nNo\nLipschitz RNN (43)\n100.00% ± 0.00\n52.84% ± 3.25\n0.63\nNo\n†ODE-RNN (7)\n50.47% ± 0.06\n51.21% ± 0.37\n4.11\nYes\n†CT-RNN (33)\n50.42% ± 0.12\n50.79% ± 0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "167", "text": "00% ± 0.00\n52.89% ± 1.25\n0.57\nNo\nLipschitz RNN (43)\n100.00% ± 0.00\n52.84% ± 3.25\n0.63\nNo\n†ODE-RNN (7)\n50.47% ± 0.06\n51.21% ± 0.37\n4.11\nYes\n†CT-RNN (33)\n50.42% ± 0.12\n50.79% ± 0.34\n4.83\nYes\n†GRU-ODE (7)\n50.41% ± 0.40\n52.52% ± 0.35\n1.55\nYes\n†ODE-LSTM (9)\n100.00% ± 0.00\n98.89% ± 0.26\n1.18\nYes\nLTC (1)\n100.00% ± 0.00\n49.11% ± 0.00\n2.67\nYes\nCf-S (ours)\n100.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "168", "text": "12\n50.79% ± 0.34\n4.83\nYes\n†GRU-ODE (7)\n50.41% ± 0.40\n52.52% ± 0.35\n1.55\nYes\n†ODE-LSTM (9)\n100.00% ± 0.00\n98.89% ± 0.26\n1.18\nYes\nLTC (1)\n100.00% ± 0.00\n49.11% ± 0.00\n2.67\nYes\nCf-S (ours)\n100.00% ± 0.00\n85.42% ± 2.84\n0.36\nNo\nCfC-noGate (ours)\n100.00% ± 0.00\n96.29% ± 1.61\n0.78\nNo\nCfC (ours)\n100.00% ± 0.00\n99.42% ± 0.42\n0.75\nNo\nCfC-mmRNN (ours)\n100.00% ± 0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "169", "text": "00\n2.67\nYes\nCf-S (ours)\n100.00% ± 0.00\n85.42% ± 2.84\n0.36\nNo\nCfC-noGate (ours)\n100.00% ± 0.00\n96.29% ± 1.61\n0.78\nNo\nCfC (ours)\n100.00% ± 0.00\n99.42% ± 0.42\n0.75\nNo\nCfC-mmRNN (ours)\n100.00% ± 0.00\n99.72% ± 0.08\n1.26\nNo\nNote: The performance of models marked by † are reported from (9).\nof medical measurements of the ﬁrst 48 hours after admission. The data is irregularly\nsampled in time, and over features, i.e., only a subset of the 37 possible features is given\nat each time point.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "170", "text": "00% ± 0.00\n99.42% ± 0.42\n0.75\nNo\nCfC-mmRNN (ours)\n100.00% ± 0.00\n99.72% ± 0.08\n1.26\nNo\nNote: The performance of models marked by † are reported from (9).\nof medical measurements of the ﬁrst 48 hours after admission. The data is irregularly\nsampled in time, and over features, i.e., only a subset of the 37 possible features is given\nat each time point. We perform the same test-train split and preprocessing as (7), and\nreport the area under the curve (AUC) on the test set as metric in Table 5. We observe\nthat CfCs perform competitively to other baselines while performing 160 times faster\ntraining time compared to ODE-RNNs and 220 times compared to continuous latent\nmodels. CfCs are also, on average, three times faster than advanced discretized gated\nrecurrent models.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "171", "text": "We perform the same test-train split and preprocessing as (7), and\nreport the area under the curve (AUC) on the test set as metric in Table 5. We observe\nthat CfCs perform competitively to other baselines while performing 160 times faster\ntraining time compared to ODE-RNNs and 220 times compared to continuous latent\nmodels. CfCs are also, on average, three times faster than advanced discretized gated\nrecurrent models. The hyperparameter details of this experiment is provided in Table\nS2.\n21\nTable 5: PhysioNet. The experiment is performed without any pretraining or pretrained word-\nembeddings. Thus, we excluded advanced attention-based models (44,45) such as Transform-\ners (46) and RNN structures that use pretraining. Numbers present mean ± standard devia-\ntions, n=5\nModel\nAUC Score (%)\ntime per epoch (min)\n†RNN-Impute (7)\n0.764 ± 0.016\n0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "172", "text": "The hyperparameter details of this experiment is provided in Table\nS2.\n21\nTable 5: PhysioNet. The experiment is performed without any pretraining or pretrained word-\nembeddings. Thus, we excluded advanced attention-based models (44,45) such as Transform-\ners (46) and RNN structures that use pretraining. Numbers present mean ± standard devia-\ntions, n=5\nModel\nAUC Score (%)\ntime per epoch (min)\n†RNN-Impute (7)\n0.764 ± 0.016\n0.5\n†RNN-delta-t (7)\n0.787 ± 0.014\n0.5\n†RNN-Decay (7)\n0.807 ± 0.003\n0.7\n†GRU-D (36)\n0.818 ± 0.008\n0.7\n†Phased-LSTM (37)\n0.836 ± 0.003\n0.3\n∗IP-Nets (31)\n0.819 ± 0.006\n1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "173", "text": "764 ± 0.016\n0.5\n†RNN-delta-t (7)\n0.787 ± 0.014\n0.5\n†RNN-Decay (7)\n0.807 ± 0.003\n0.7\n†GRU-D (36)\n0.818 ± 0.008\n0.7\n†Phased-LSTM (37)\n0.836 ± 0.003\n0.3\n∗IP-Nets (31)\n0.819 ± 0.006\n1.3\n∗SeFT (32)\n0.795 ± 0.015\n0.5\n†RNN-VAE (7)\n0.515 ± 0.040\n2.0\n†ODE-RNN (7)\n0.833 ± 0.009\n16.5\n†Latent-ODE-RNN (7)\n0.781 ± 0.018\n6.7\n†Latent-ODE-ODE (7)\n0.829 ± 0.004\n22.0\nLTC (1)\n0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "174", "text": "819 ± 0.006\n1.3\n∗SeFT (32)\n0.795 ± 0.015\n0.5\n†RNN-VAE (7)\n0.515 ± 0.040\n2.0\n†ODE-RNN (7)\n0.833 ± 0.009\n16.5\n†Latent-ODE-RNN (7)\n0.781 ± 0.018\n6.7\n†Latent-ODE-ODE (7)\n0.829 ± 0.004\n22.0\nLTC (1)\n0.6477 ± 0.010\n0.5\nCf-S (ours)\n0.643 ± 0.018\n0.1\nCfC-noGate (ours)\n0.840 ± 0.003\n0.1\nCfC (ours)\n0.839 ± 0.002\n0.1\nCfC-mmRNN (ours)\n0.834 +- 0.006\n0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "175", "text": "018\n6.7\n†Latent-ODE-ODE (7)\n0.829 ± 0.004\n22.0\nLTC (1)\n0.6477 ± 0.010\n0.5\nCf-S (ours)\n0.643 ± 0.018\n0.1\nCfC-noGate (ours)\n0.840 ± 0.003\n0.1\nCfC (ours)\n0.839 ± 0.002\n0.1\nCfC-mmRNN (ours)\n0.834 +- 0.006\n0.2\nNote: The performance of the models marked by † are reported from (7) and the ones with ∗from (44).\nSentiment Analysis - IMDB\nThe IMDB sentiment analysis dataset (47) consists of 25,000 training and 25,000 test\nsentences. Each sentence corresponds to either positive or negative sentiment.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "176", "text": "840 ± 0.003\n0.1\nCfC (ours)\n0.839 ± 0.002\n0.1\nCfC-mmRNN (ours)\n0.834 +- 0.006\n0.2\nNote: The performance of the models marked by † are reported from (7) and the ones with ∗from (44).\nSentiment Analysis - IMDB\nThe IMDB sentiment analysis dataset (47) consists of 25,000 training and 25,000 test\nsentences. Each sentence corresponds to either positive or negative sentiment. We tok-\nenize the sentences in a word-by-word fashion with a vocabulary consisting of 20,000\nmost frequently occurring words in the dataset. We map each token to a vector using a\ntrainable word embedding. The word embedding is initialized randomly. No pretrain-\ning of the network or the word embedding is performed. Table 6 represents how CfCs\nequipped with mixed memory instances outperform advanced RNN benchmarks.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "177", "text": "Each sentence corresponds to either positive or negative sentiment. We tok-\nenize the sentences in a word-by-word fashion with a vocabulary consisting of 20,000\nmost frequently occurring words in the dataset. We map each token to a vector using a\ntrainable word embedding. The word embedding is initialized randomly. No pretrain-\ning of the network or the word embedding is performed. Table 6 represents how CfCs\nequipped with mixed memory instances outperform advanced RNN benchmarks. The\nhyperparameter details of this experiment is provided in Table S3.\n22\nTable 6: Results on the IMDB datasets. The experiment is performed without any pretraining\nor pretrained word-embeddings. Thus, we excluded advanced attention-based models (44,45)\nsuch as Transformers (46) and RNN structures that use pretraining. Numbers present mean ±\nstandard deviations, n=5\nModel\nTest accuracy (%)\n†HiPPO-LagT (40)\n88.0 ± 0.2\n†HiPPO-LegS (40)\n88.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "178", "text": "The\nhyperparameter details of this experiment is provided in Table S3.\n22\nTable 6: Results on the IMDB datasets. The experiment is performed without any pretraining\nor pretrained word-embeddings. Thus, we excluded advanced attention-based models (44,45)\nsuch as Transformers (46) and RNN structures that use pretraining. Numbers present mean ±\nstandard deviations, n=5\nModel\nTest accuracy (%)\n†HiPPO-LagT (40)\n88.0 ± 0.2\n†HiPPO-LegS (40)\n88.0 ± 0.2\n†LMU (39)\n87.7 ± 0.1\n†LSTM (20)\n87.3 ± 0.4\n†GRU (30)\n86.2 ± n/a\n∗ReLU GRU (48)\n84.8 ± n/a\n∗Skip LSTM (49)\n86.6 ± n/a\n†expRNN (41)\n84.3 ± 0.3\n†Vanilla RNN (49)\n67.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "179", "text": "0 ± 0.2\n†HiPPO-LegS (40)\n88.0 ± 0.2\n†LMU (39)\n87.7 ± 0.1\n†LSTM (20)\n87.3 ± 0.4\n†GRU (30)\n86.2 ± n/a\n∗ReLU GRU (48)\n84.8 ± n/a\n∗Skip LSTM (49)\n86.6 ± n/a\n†expRNN (41)\n84.3 ± 0.3\n†Vanilla RNN (49)\n67.4 ± 7.7\n∗coRNN (42)\n86.7 ± 0.3\nLTC (1)\n61.8 ± 6.1\nCf-S (ours)\n81.7 ± 0.5\nCfC-noGate (ours)\n87.5 ± 0.1\nCfC (ours)\n85.9 ± 0.9\nCfC-mmRNN (ours)\n88.3 ± 0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "180", "text": "3 ± 0.3\n†Vanilla RNN (49)\n67.4 ± 7.7\n∗coRNN (42)\n86.7 ± 0.3\nLTC (1)\n61.8 ± 6.1\nCf-S (ours)\n81.7 ± 0.5\nCfC-noGate (ours)\n87.5 ± 0.1\nCfC (ours)\n85.9 ± 0.9\nCfC-mmRNN (ours)\n88.3 ± 0.1\nNote: The performance of the models marked by † are reported from (40), and ∗are reported\nfrom (42). The n/a standard deviation denotes that the original report of these experiments did\nnot provide the statistics of their analysis.\nPhysical Dynamics Modeling\nThe Walker2D dataset consists of kinematic simulations of the MuJoCo physics en-\ngine (50) on the Walker2d-v2 OpenAI gym (51) environment using four different\nstochastic policies.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "181", "text": "9 ± 0.9\nCfC-mmRNN (ours)\n88.3 ± 0.1\nNote: The performance of the models marked by † are reported from (40), and ∗are reported\nfrom (42). The n/a standard deviation denotes that the original report of these experiments did\nnot provide the statistics of their analysis.\nPhysical Dynamics Modeling\nThe Walker2D dataset consists of kinematic simulations of the MuJoCo physics en-\ngine (50) on the Walker2d-v2 OpenAI gym (51) environment using four different\nstochastic policies. The objective is to predict the physics state of the next time step.\nThe training and testing sequences are provided at irregularly-sampled intervals. We\nreport the squared error on the test set as a metric. As shown in Table 7, CfCs outper-\nform the other baselines by a large margin rooting for their strong capability to model\nirregularly sampled physical dynamics with missing phases. It is worth mentioning\n23\nTable 7: Per time-step regression.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "182", "text": "The objective is to predict the physics state of the next time step.\nThe training and testing sequences are provided at irregularly-sampled intervals. We\nreport the squared error on the test set as a metric. As shown in Table 7, CfCs outper-\nform the other baselines by a large margin rooting for their strong capability to model\nirregularly sampled physical dynamics with missing phases. It is worth mentioning\n23\nTable 7: Per time-step regression. Modeling the physical dynamics of a Walker agent in simu-\nlation. Numbers present mean ± standard deviations. n = 5\nModel\nSquare-error\nTime per epoch (min)\n†ODE-RNN (7)\n1.904 ± 0.061\n0.79\n†CT-RNN (33)\n1.198 ± 0.004\n0.91\n†Augmented LSTM (20)\n1.065 ± 0.006\n0.10\n†CT-GRU (34)\n1.172 ± 0.011\n0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "183", "text": "Modeling the physical dynamics of a Walker agent in simu-\nlation. Numbers present mean ± standard deviations. n = 5\nModel\nSquare-error\nTime per epoch (min)\n†ODE-RNN (7)\n1.904 ± 0.061\n0.79\n†CT-RNN (33)\n1.198 ± 0.004\n0.91\n†Augmented LSTM (20)\n1.065 ± 0.006\n0.10\n†CT-GRU (34)\n1.172 ± 0.011\n0.18\n†RNN-Decay (7)\n1.406 ± 0.005\n0.16\n†Bi-directional RNN (38)\n1.071 ± 0.009\n0.39\n†GRU-D (36)\n1.090 ± 0.034\n0.11\n†PhasedLSTM (37)\n1.063 ± 0.010\n0.25\n†GRU-ODE (7)\n1.051 ± 0.018\n0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "184", "text": "10\n†CT-GRU (34)\n1.172 ± 0.011\n0.18\n†RNN-Decay (7)\n1.406 ± 0.005\n0.16\n†Bi-directional RNN (38)\n1.071 ± 0.009\n0.39\n†GRU-D (36)\n1.090 ± 0.034\n0.11\n†PhasedLSTM (37)\n1.063 ± 0.010\n0.25\n†GRU-ODE (7)\n1.051 ± 0.018\n0.56\n†CT-LSTM (35)\n1.014 ± 0.014\n0.31\n†ODE-LSTM (9)\n0.883 ± 0.014\n0.29\ncoRNN (42)\n3.241 ± 0.215\n0.18\nLipschitz RNN (43)\n1.781 ± 0.013\n0.17\nLTC (1)\n0.662 ± 0.013\n0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "185", "text": "010\n0.25\n†GRU-ODE (7)\n1.051 ± 0.018\n0.56\n†CT-LSTM (35)\n1.014 ± 0.014\n0.31\n†ODE-LSTM (9)\n0.883 ± 0.014\n0.29\ncoRNN (42)\n3.241 ± 0.215\n0.18\nLipschitz RNN (43)\n1.781 ± 0.013\n0.17\nLTC (1)\n0.662 ± 0.013\n0.78\nTransformer (46)\n0.761 ± 0.032\n0.8\nCf-S (ours)\n0.948 ± 0.009\n0.12\nCfC-noGate (ours)\n0.650 ± 0.008\n0.21\nCfC (ours)\n0.643 ± 0.006\n0.08\nCfC-mmRNN (ours)\n0.617 ± 0.006\n0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "186", "text": "781 ± 0.013\n0.17\nLTC (1)\n0.662 ± 0.013\n0.78\nTransformer (46)\n0.761 ± 0.032\n0.8\nCf-S (ours)\n0.948 ± 0.009\n0.12\nCfC-noGate (ours)\n0.650 ± 0.008\n0.21\nCfC (ours)\n0.643 ± 0.006\n0.08\nCfC-mmRNN (ours)\n0.617 ± 0.006\n0.34\nNote: The performance of the models marked by † are reported from (9).\nthat on this task, CfCs even outperform Transformers by a considerable 18% margin.\nThe hyperparameter details of this experiment is provided in Table S4.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "187", "text": "948 ± 0.009\n0.12\nCfC-noGate (ours)\n0.650 ± 0.008\n0.21\nCfC (ours)\n0.643 ± 0.006\n0.08\nCfC-mmRNN (ours)\n0.617 ± 0.006\n0.34\nNote: The performance of the models marked by † are reported from (9).\nthat on this task, CfCs even outperform Transformers by a considerable 18% margin.\nThe hyperparameter details of this experiment is provided in Table S4.\nScope, Discussions and Conclusions\nWe introduced a closed-form continuous-time neural model build from an approxi-\nmate close-form solution of liquid time-constant networks that possesses the strong\nmodeling capabilities of ODE-based networks while being signiﬁcantly faster, more ac-\ncurate, and stable. These closed-form continuous-depth models achieve this by explicit\ntime-dependent gating mechanisms and having a liquid time-constant modulated by\n24\nneural networks.\nContinuous-Depth Models.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "188", "text": "The hyperparameter details of this experiment is provided in Table S4.\nScope, Discussions and Conclusions\nWe introduced a closed-form continuous-time neural model build from an approxi-\nmate close-form solution of liquid time-constant networks that possesses the strong\nmodeling capabilities of ODE-based networks while being signiﬁcantly faster, more ac-\ncurate, and stable. These closed-form continuous-depth models achieve this by explicit\ntime-dependent gating mechanisms and having a liquid time-constant modulated by\n24\nneural networks.\nContinuous-Depth Models. Machine learning, control theory and dynamical systems\nmerge at models with continuous-time dynamics (52–56). In a seminal work, Chen\net. al. 2018 (2) revived the class of continuous-time neural networks (33, 57), with\nneural ODEs. These continuous-depth models give rise to vector ﬁeld representations\nand a set of functions which were not possible to generate before with discrete neural\nnetworks.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "189", "text": "Continuous-Depth Models. Machine learning, control theory and dynamical systems\nmerge at models with continuous-time dynamics (52–56). In a seminal work, Chen\net. al. 2018 (2) revived the class of continuous-time neural networks (33, 57), with\nneural ODEs. These continuous-depth models give rise to vector ﬁeld representations\nand a set of functions which were not possible to generate before with discrete neural\nnetworks. These capabilities enabled ﬂexible density estimation (3–5, 58, 59), as well\nas performant modeling of sequential and irregularly-sampled data (1,7–9,43). In this\npaper, we showed how to relax the need for an ODE-solver to realize an expressive\ncontinuous-time neural network model for challenging time-series problems.\nImproving Neural ODEs. ODE-based neural networks are as good as their ODE-\nsolvers.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "190", "text": "These capabilities enabled ﬂexible density estimation (3–5, 58, 59), as well\nas performant modeling of sequential and irregularly-sampled data (1,7–9,43). In this\npaper, we showed how to relax the need for an ODE-solver to realize an expressive\ncontinuous-time neural network model for challenging time-series problems.\nImproving Neural ODEs. ODE-based neural networks are as good as their ODE-\nsolvers. As the complexity or the dimensionality of the modeling task increases, ODE-\nbased networks demand a more advanced solver that signiﬁcantly impacts their efﬁ-\nciency (17), stability (13,15,60–62) and performance (1). A large body of research went\ninto improving the computational overhead of these solvers, for example, by designing\nhypersolvers (17), deploying augmentation methods (4, 12), pruning (6) and by regu-\nlarizing the continuous ﬂows (14–16).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "191", "text": "As the complexity or the dimensionality of the modeling task increases, ODE-\nbased networks demand a more advanced solver that signiﬁcantly impacts their efﬁ-\nciency (17), stability (13,15,60–62) and performance (1). A large body of research went\ninto improving the computational overhead of these solvers, for example, by designing\nhypersolvers (17), deploying augmentation methods (4, 12), pruning (6) and by regu-\nlarizing the continuous ﬂows (14–16). To enhance the performance of an ODE-based\nmodel, especially in time series modeling tasks (63), solutions provided for stabilizing\ntheir gradient propagation (9, 43, 64). In this work, we showed that CfCs improve the\nscalability, efﬁciency, and performance of continuous-depth neural models.\nNow that we have a closed-form system, where does it make sense to use ODE-based\nnetworks?", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "192", "text": "To enhance the performance of an ODE-based\nmodel, especially in time series modeling tasks (63), solutions provided for stabilizing\ntheir gradient propagation (9, 43, 64). In this work, we showed that CfCs improve the\nscalability, efﬁciency, and performance of continuous-depth neural models.\nNow that we have a closed-form system, where does it make sense to use ODE-based\nnetworks? For large-scale time-series prediction tasks, and where closed-loop perfor-\nmance matters (24) CfCs should be the method of choice.This is because, they capture\nthe ﬂexible, continuous-time nature of ODE-based networks while presenting large\ngains in performance and scalability. On the other hand, implicit ODE-based mod-\n25\nels can still be signiﬁcantly beneﬁcial in solving continuously deﬁned physics prob-\nlems and control tasks.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "193", "text": "For large-scale time-series prediction tasks, and where closed-loop perfor-\nmance matters (24) CfCs should be the method of choice.This is because, they capture\nthe ﬂexible, continuous-time nature of ODE-based networks while presenting large\ngains in performance and scalability. On the other hand, implicit ODE-based mod-\n25\nels can still be signiﬁcantly beneﬁcial in solving continuously deﬁned physics prob-\nlems and control tasks. Moreover, for generative modeling, continuous normalizing\nﬂows built by ODEs are the suitable choice of model as they ensure invertibility un-\nlike CfCs (2). This is because differential equations guarantee invertibility (i.e., under\nuniqueness conditions (6), one can run them backwards in time). CfCs only approxi-\nmate ODEs and therefore they no longer necessarily form a bijection (65).\nWhat are the limitations of CfCs? CfCs might express vanishing gradient problems.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "194", "text": "Moreover, for generative modeling, continuous normalizing\nﬂows built by ODEs are the suitable choice of model as they ensure invertibility un-\nlike CfCs (2). This is because differential equations guarantee invertibility (i.e., under\nuniqueness conditions (6), one can run them backwards in time). CfCs only approxi-\nmate ODEs and therefore they no longer necessarily form a bijection (65).\nWhat are the limitations of CfCs? CfCs might express vanishing gradient problems.\nTo avoid this, for tasks that require long-term dependencies, it is better to use them\ntogether with mixed memory networks (9) (See CfC-mmRNN). Moreover, we specu-\nlate that inferring causality from ODE-based networks might be more straightforward\nthan a closed-form solution (24). It would also be beneﬁcial to assess if verifying a\ncontinuous neural ﬂow (66) is more tractable by an ODE representation of the system\nor their closed form.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "195", "text": "What are the limitations of CfCs? CfCs might express vanishing gradient problems.\nTo avoid this, for tasks that require long-term dependencies, it is better to use them\ntogether with mixed memory networks (9) (See CfC-mmRNN). Moreover, we specu-\nlate that inferring causality from ODE-based networks might be more straightforward\nthan a closed-form solution (24). It would also be beneﬁcial to assess if verifying a\ncontinuous neural ﬂow (66) is more tractable by an ODE representation of the system\nor their closed form.\nIn what application scenarios shall we use CfCs? For problems such as language\nmodeling where a signiﬁcant amount of sequential data and substantial compute re-\nsources are available, Transformers (46) are the right choice.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "196", "text": "Moreover, we specu-\nlate that inferring causality from ODE-based networks might be more straightforward\nthan a closed-form solution (24). It would also be beneﬁcial to assess if verifying a\ncontinuous neural ﬂow (66) is more tractable by an ODE representation of the system\nor their closed form.\nIn what application scenarios shall we use CfCs? For problems such as language\nmodeling where a signiﬁcant amount of sequential data and substantial compute re-\nsources are available, Transformers (46) are the right choice. In contrast, we use CfCs\nwhen: 1) data has limitations and irregularities (e.g., medical data, ﬁnancial time-\nseries, robotics (67) and closed loop control and robotics, and multi-agent autonomous\nsystems in supervised and reinforcement learning schemes (68)), 2) training and infer-\nence efﬁciency of a model is important (e.g., embedded applications (69–71)), and 3)\nwhen interpretability matters (72).\nReferences\n1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "197", "text": "In contrast, we use CfCs\nwhen: 1) data has limitations and irregularities (e.g., medical data, ﬁnancial time-\nseries, robotics (67) and closed loop control and robotics, and multi-agent autonomous\nsystems in supervised and reinforcement learning schemes (68)), 2) training and infer-\nence efﬁciency of a model is important (e.g., embedded applications (69–71)), and 3)\nwhen interpretability matters (72).\nReferences\n1. Hasani, R., Lechner, M., Amini, A., Rus, D. & Grosu, R.\nLiquid time-constant\nnetworks. Proceedings of the AAAI Conference on Artiﬁcial Intelligence 35, 7657–7666\n26\n(2021).\n2. Chen, T. Q., Rubanova, Y., Bettencourt, J. & Duvenaud, D. K. Neural ordinary\ndifferential equations. In Advances in neural information processing systems, 6571–\n6583 (2018).\n3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "198", "text": "Hasani, R., Lechner, M., Amini, A., Rus, D. & Grosu, R.\nLiquid time-constant\nnetworks. Proceedings of the AAAI Conference on Artiﬁcial Intelligence 35, 7657–7666\n26\n(2021).\n2. Chen, T. Q., Rubanova, Y., Bettencourt, J. & Duvenaud, D. K. Neural ordinary\ndifferential equations. In Advances in neural information processing systems, 6571–\n6583 (2018).\n3. Grathwohl, W., Chen, R. T., Bettencourt, J., Sutskever, I. & Duvenaud, D. Ffjord:\nFree-form continuous dynamics for scalable reversible generative models. arXiv\npreprint arXiv:1810.01367 (2018).\n4. Dupont, E., Doucet, A. & Teh, Y. W. Augmented neural odes.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "199", "text": "In Advances in neural information processing systems, 6571–\n6583 (2018).\n3. Grathwohl, W., Chen, R. T., Bettencourt, J., Sutskever, I. & Duvenaud, D. Ffjord:\nFree-form continuous dynamics for scalable reversible generative models. arXiv\npreprint arXiv:1810.01367 (2018).\n4. Dupont, E., Doucet, A. & Teh, Y. W. Augmented neural odes. In Advances in Neural\nInformation Processing Systems, 3134–3144 (2019).\n5. Yang, G. et al. Pointﬂow: 3d point cloud generation with continuous normalizing\nﬂows. In Proceedings of the IEEE/CVF International Conference on Computer Vision,\n4541–4550 (2019).\n6. Liebenwein, L., Hasani, R., Amini, A.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "200", "text": "4. Dupont, E., Doucet, A. & Teh, Y. W. Augmented neural odes. In Advances in Neural\nInformation Processing Systems, 3134–3144 (2019).\n5. Yang, G. et al. Pointﬂow: 3d point cloud generation with continuous normalizing\nﬂows. In Proceedings of the IEEE/CVF International Conference on Computer Vision,\n4541–4550 (2019).\n6. Liebenwein, L., Hasani, R., Amini, A. & Daniela, R.\nSparse ﬂows: Pruning\ncontinuous-depth models. arXiv preprint arXiv:2106.12718 (2021).\n7. Rubanova, Y., Chen, R. T. & Duvenaud, D. Latent odes for irregularly-sampled\ntime series. arXiv preprint arXiv:1907.03907 (2019).\n8.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "201", "text": "6. Liebenwein, L., Hasani, R., Amini, A. & Daniela, R.\nSparse ﬂows: Pruning\ncontinuous-depth models. arXiv preprint arXiv:2106.12718 (2021).\n7. Rubanova, Y., Chen, R. T. & Duvenaud, D. Latent odes for irregularly-sampled\ntime series. arXiv preprint arXiv:1907.03907 (2019).\n8. Gholami, A., Keutzer, K. & Biros, G. Anode: Unconditionally accurate memory-\nefﬁcient gradients for neural odes. arXiv preprint arXiv:1902.10298 (2019).\n9. Lechner, M. & Hasani, R. Learning long-term dependencies in irregularly-sampled\ntime series. arXiv preprint arXiv:2006.04418 (2020).\n10. Prince, P. J.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "202", "text": "8. Gholami, A., Keutzer, K. & Biros, G. Anode: Unconditionally accurate memory-\nefﬁcient gradients for neural odes. arXiv preprint arXiv:1902.10298 (2019).\n9. Lechner, M. & Hasani, R. Learning long-term dependencies in irregularly-sampled\ntime series. arXiv preprint arXiv:2006.04418 (2020).\n10. Prince, P. J. & Dormand, J. R. High order embedded runge-kutta formulae. Journal\nof computational and applied mathematics 7, 67–75 (1981).\n27\n11. Raissi, M., Perdikaris, P. & Karniadakis, G. E. Physics-informed neural networks:\nA deep learning framework for solving forward and inverse problems involving\nnonlinear partial differential equations. Journal of Computational Physics 378, 686–\n707 (2019).\n12.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "203", "text": "10. Prince, P. J. & Dormand, J. R. High order embedded runge-kutta formulae. Journal\nof computational and applied mathematics 7, 67–75 (1981).\n27\n11. Raissi, M., Perdikaris, P. & Karniadakis, G. E. Physics-informed neural networks:\nA deep learning framework for solving forward and inverse problems involving\nnonlinear partial differential equations. Journal of Computational Physics 378, 686–\n707 (2019).\n12. Massaroli, S., Poli, M., Park, J., Yamashita, A. & Asma, H. Dissecting neural odes.\nIn 34th Conference on Neural Information Processing Systems, NeurIPS 2020 (The Neu-\nral Information Processing Systems, 2020).\n13. Bai, S., Kolter, J. Z. & Koltun, V. Deep equilibrium models. Advances in Neural\nInformation Processing Systems 32, 690–701 (2019).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "204", "text": "12. Massaroli, S., Poli, M., Park, J., Yamashita, A. & Asma, H. Dissecting neural odes.\nIn 34th Conference on Neural Information Processing Systems, NeurIPS 2020 (The Neu-\nral Information Processing Systems, 2020).\n13. Bai, S., Kolter, J. Z. & Koltun, V. Deep equilibrium models. Advances in Neural\nInformation Processing Systems 32, 690–701 (2019).\n14. Finlay, C., Jacobsen, J.-H., Nurbekyan, L. & Oberman, A. M. How to train your\nneural ode. arXiv preprint arXiv:2002.02798 (2020).\n15. Massaroli, S. et al. Stable neural ﬂows. arXiv preprint arXiv:2003.08063 (2020).\n16. Kidger, P., Chen, R. T.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "205", "text": "14. Finlay, C., Jacobsen, J.-H., Nurbekyan, L. & Oberman, A. M. How to train your\nneural ode. arXiv preprint arXiv:2002.02798 (2020).\n15. Massaroli, S. et al. Stable neural ﬂows. arXiv preprint arXiv:2003.08063 (2020).\n16. Kidger, P., Chen, R. T. & Lyons, T. ” hey, that’s not an ode”: Faster ode adjoints\nwith 12 lines of code. arXiv preprint arXiv:2009.09457 (2020).\n17. Poli, M. et al. Hypersolvers: Toward fast continuous-depth models. Advances in\nNeural Information Processing Systems 33 (2020).\n18. Friston, K. J., Harrison, L. & Penny, W. Dynamic causal modelling.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "206", "text": "16. Kidger, P., Chen, R. T. & Lyons, T. ” hey, that’s not an ode”: Faster ode adjoints\nwith 12 lines of code. arXiv preprint arXiv:2009.09457 (2020).\n17. Poli, M. et al. Hypersolvers: Toward fast continuous-depth models. Advances in\nNeural Information Processing Systems 33 (2020).\n18. Friston, K. J., Harrison, L. & Penny, W. Dynamic causal modelling. Neuroimage 19,\n1273–1302 (2003).\n19. Perko, L. Differential Equations and Dynamical Systems (Springer-Verlag, Berlin, Hei-\ndelberg, 1991).\n20. Hochreiter, S. & Schmidhuber, J. Long short-term memory. Neural computation 9,\n1735–1780 (1997).\n28\n21.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "207", "text": "18. Friston, K. J., Harrison, L. & Penny, W. Dynamic causal modelling. Neuroimage 19,\n1273–1302 (2003).\n19. Perko, L. Differential Equations and Dynamical Systems (Springer-Verlag, Berlin, Hei-\ndelberg, 1991).\n20. Hochreiter, S. & Schmidhuber, J. Long short-term memory. Neural computation 9,\n1735–1780 (1997).\n28\n21. Rudin, W. Principles of mathematical analysis (McGraw-Hill New York, 1976), 3d ed.\nedn.\n22. Lechner, M. et al.\nNeural circuit policies enabling auditable autonomy.\nNature\nMachine Intelligence 2, 642–652 (2020).\n23. Hochreiter, S. Untersuchungen zu dynamischen neuronalen netzen. Diploma, Tech-\nnische Universit¨at M¨unchen 91 (1991).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "208", "text": "28\n21. Rudin, W. Principles of mathematical analysis (McGraw-Hill New York, 1976), 3d ed.\nedn.\n22. Lechner, M. et al.\nNeural circuit policies enabling auditable autonomy.\nNature\nMachine Intelligence 2, 642–652 (2020).\n23. Hochreiter, S. Untersuchungen zu dynamischen neuronalen netzen. Diploma, Tech-\nnische Universit¨at M¨unchen 91 (1991).\n24. Vorbach, C., Hasani, R., Amini, A., Lechner, M. & Rus, D. Causal navigation by\ncontinuous-time neural networks. arXiv preprint arXiv:2106.08314 (2021).\n25. Hasani, R. et al. Response characterization for auditing cell dynamics in long short-\nterm memory networks. In 2019 International Joint Conference on Neural Networks\n(IJCNN), 1–8 (IEEE, 2019).\n26.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "209", "text": "24. Vorbach, C., Hasani, R., Amini, A., Lechner, M. & Rus, D. Causal navigation by\ncontinuous-time neural networks. arXiv preprint arXiv:2106.08314 (2021).\n25. Hasani, R. et al. Response characterization for auditing cell dynamics in long short-\nterm memory networks. In 2019 International Joint Conference on Neural Networks\n(IJCNN), 1–8 (IEEE, 2019).\n26. Amini, A. et al. Vista 2.0: An open, data-driven simulator for multimodal sens-\ning and policy learning for autonomous vehicles. arXiv preprint arXiv:2111.12083\n(2021).\n27. Amini, A. et al. Learning robust control policies for end-to-end autonomous driv-\ning from data-driven simulation. IEEE Robotics and Automation Letters 5, 1143–1150\n(2020).\n28. Levine, S.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "210", "text": "26. Amini, A. et al. Vista 2.0: An open, data-driven simulator for multimodal sens-\ning and policy learning for autonomous vehicles. arXiv preprint arXiv:2111.12083\n(2021).\n27. Amini, A. et al. Learning robust control policies for end-to-end autonomous driv-\ning from data-driven simulation. IEEE Robotics and Automation Letters 5, 1143–1150\n(2020).\n28. Levine, S. & Koltun, V. Guided policy search. In International conference on machine\nlearning, 1–9 (PMLR, 2013).\n29. Bojarski, M. et al. Visualbackprop: Efﬁcient visualization of cnns for autonomous\ndriving. In IEEE International Conference on Robotics and Automation (ICRA), 1–8\n(2018).\n29\n30. Chung, J., Gulcehre, C., Cho, K.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "211", "text": "28. Levine, S. & Koltun, V. Guided policy search. In International conference on machine\nlearning, 1–9 (PMLR, 2013).\n29. Bojarski, M. et al. Visualbackprop: Efﬁcient visualization of cnns for autonomous\ndriving. In IEEE International Conference on Robotics and Automation (ICRA), 1–8\n(2018).\n29\n30. Chung, J., Gulcehre, C., Cho, K. & Bengio, Y. Empirical evaluation of gated recur-\nrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555 (2014).\n31. Shukla, S. N. & Marlin, B. Interpolation-prediction networks for irregularly sam-\npled time series. In International Conference on Learning Representations (2018).\n32. Horn, M., Moor, M., Bock, C., Rieck, B.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "212", "text": "Chung, J., Gulcehre, C., Cho, K. & Bengio, Y. Empirical evaluation of gated recur-\nrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555 (2014).\n31. Shukla, S. N. & Marlin, B. Interpolation-prediction networks for irregularly sam-\npled time series. In International Conference on Learning Representations (2018).\n32. Horn, M., Moor, M., Bock, C., Rieck, B. & Borgwardt, K. Set functions for time\nseries. In International Conference on Machine Learning, 4353–4363 (PMLR, 2020).\n33. Funahashi, K.-i. & Nakamura, Y. Approximation of dynamical systems by contin-\nuous time recurrent neural networks. Neural networks 6, 801–806 (1993).\n34. Mozer, M. C., Kazakov, D.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "213", "text": "32. Horn, M., Moor, M., Bock, C., Rieck, B. & Borgwardt, K. Set functions for time\nseries. In International Conference on Machine Learning, 4353–4363 (PMLR, 2020).\n33. Funahashi, K.-i. & Nakamura, Y. Approximation of dynamical systems by contin-\nuous time recurrent neural networks. Neural networks 6, 801–806 (1993).\n34. Mozer, M. C., Kazakov, D. & Lindsey, R. V. Discrete event, continuous time rnns.\narXiv preprint arXiv:1710.04110 (2017).\n35. Mei, H. & Eisner, J. The neural hawkes process: a neurally self-modulating mul-\ntivariate point process. In Proceedings of the 31st International Conference on Neural\nInformation Processing Systems, 6757–6767 (2017).\n36.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "214", "text": "34. Mozer, M. C., Kazakov, D. & Lindsey, R. V. Discrete event, continuous time rnns.\narXiv preprint arXiv:1710.04110 (2017).\n35. Mei, H. & Eisner, J. The neural hawkes process: a neurally self-modulating mul-\ntivariate point process. In Proceedings of the 31st International Conference on Neural\nInformation Processing Systems, 6757–6767 (2017).\n36. Che, Z., Purushotham, S., Cho, K., Sontag, D. & Liu, Y. Recurrent neural networks\nfor multivariate time series with missing values. Scientiﬁc reports 8, 1–12 (2018).\n37. Neil, D., Pfeiffer, M. & Liu, S.-C.\nPhased lstm: accelerating recurrent network\ntraining for long or event-based sequences.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "215", "text": "In Proceedings of the 31st International Conference on Neural\nInformation Processing Systems, 6757–6767 (2017).\n36. Che, Z., Purushotham, S., Cho, K., Sontag, D. & Liu, Y. Recurrent neural networks\nfor multivariate time series with missing values. Scientiﬁc reports 8, 1–12 (2018).\n37. Neil, D., Pfeiffer, M. & Liu, S.-C.\nPhased lstm: accelerating recurrent network\ntraining for long or event-based sequences. In Proceedings of the 30th International\nConference on Neural Information Processing Systems, 3889–3897 (2016).\n38. Schuster, M. & Paliwal, K. K. Bidirectional recurrent neural networks. IEEE trans-\nactions on Signal Processing 45, 2673–2681 (1997).\n30\n39. Voelker, A. R., Kaji´c, I.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "216", "text": "Neil, D., Pfeiffer, M. & Liu, S.-C.\nPhased lstm: accelerating recurrent network\ntraining for long or event-based sequences. In Proceedings of the 30th International\nConference on Neural Information Processing Systems, 3889–3897 (2016).\n38. Schuster, M. & Paliwal, K. K. Bidirectional recurrent neural networks. IEEE trans-\nactions on Signal Processing 45, 2673–2681 (1997).\n30\n39. Voelker, A. R., Kaji´c, I. & Eliasmith, C.\nLegendre memory units: Continuous-\ntime representation in recurrent neural networks. NeurIPS Reproducability Challenge\n(2019).\n40. Gu, A., Dao, T., Ermon, S., Rudra, A. & R´e, C. Hippo: Recurrent memory with\noptimal polynomial projections. arXiv preprint arXiv:2008.07669 (2020).\n41.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "217", "text": "30\n39. Voelker, A. R., Kaji´c, I. & Eliasmith, C.\nLegendre memory units: Continuous-\ntime representation in recurrent neural networks. NeurIPS Reproducability Challenge\n(2019).\n40. Gu, A., Dao, T., Ermon, S., Rudra, A. & R´e, C. Hippo: Recurrent memory with\noptimal polynomial projections. arXiv preprint arXiv:2008.07669 (2020).\n41. Lezcano-Casado, M. & Martınez-Rubio, D. Cheap orthogonal constraints in neu-\nral networks: A simple parametrization of the orthogonal and unitary group. In\nInternational Conference on Machine Learning, 3794–3803 (PMLR, 2019).\n42. Rusch, T. K.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "218", "text": "& R´e, C. Hippo: Recurrent memory with\noptimal polynomial projections. arXiv preprint arXiv:2008.07669 (2020).\n41. Lezcano-Casado, M. & Martınez-Rubio, D. Cheap orthogonal constraints in neu-\nral networks: A simple parametrization of the orthogonal and unitary group. In\nInternational Conference on Machine Learning, 3794–3803 (PMLR, 2019).\n42. Rusch, T. K. & Mishra, S. Coupled oscillatory recurrent neural network (co{rnn}):\nAn accurate and (gradient) stable architecture for learning long time dependen-\ncies. In International Conference on Learning Representations (2021). URL https:\n//openreview.net/forum?id=F3s69XzWOia.\n43. Erichson, N. B., Azencot, O., Queiruga, A., Hodgkinson, L.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "219", "text": "42. Rusch, T. K. & Mishra, S. Coupled oscillatory recurrent neural network (co{rnn}):\nAn accurate and (gradient) stable architecture for learning long time dependen-\ncies. In International Conference on Learning Representations (2021). URL https:\n//openreview.net/forum?id=F3s69XzWOia.\n43. Erichson, N. B., Azencot, O., Queiruga, A., Hodgkinson, L. & Mahoney, M. W.\nLipschitz recurrent neural networks. In International Conference on Learning Repre-\nsentations (2021). URL https://openreview.net/forum?id=-N7PBXqOUJZ.\n44. Shukla, S. N. & Marlin, B. M. Multi-time attention networks for irregularly sam-\npled time series. arXiv preprint arXiv:2101.10318 (2021).\n45. Xiong, Y. et al.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "220", "text": "& Mahoney, M. W.\nLipschitz recurrent neural networks. In International Conference on Learning Repre-\nsentations (2021). URL https://openreview.net/forum?id=-N7PBXqOUJZ.\n44. Shukla, S. N. & Marlin, B. M. Multi-time attention networks for irregularly sam-\npled time series. arXiv preprint arXiv:2101.10318 (2021).\n45. Xiong, Y. et al. Nystr¨omformer: A nystr¨om-based algorithm for approximating\nself-attention. CoRR abs/2102.03902 (2021).\n46. Vaswani, A. et al. Attention is all you need. In Advances in neural information pro-\ncessing systems, 5998–6008 (2017).\n31\n47. Maas, A. et al. Learning word vectors for sentiment analysis.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "221", "text": "45. Xiong, Y. et al. Nystr¨omformer: A nystr¨om-based algorithm for approximating\nself-attention. CoRR abs/2102.03902 (2021).\n46. Vaswani, A. et al. Attention is all you need. In Advances in neural information pro-\ncessing systems, 5998–6008 (2017).\n31\n47. Maas, A. et al. Learning word vectors for sentiment analysis. In Proceedings of the\n49th annual meeting of the association for computational linguistics: Human language\ntechnologies, 142–150 (2011).\n48. Dey, R. & Salem, F. M. Gate-variants of gated recurrent unit (gru) neural networks.\nIn 2017 IEEE 60th international midwest symposium on circuits and systems (MWS-\nCAS), 1597–1600 (IEEE, 2017).\n49.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "222", "text": "31\n47. Maas, A. et al. Learning word vectors for sentiment analysis. In Proceedings of the\n49th annual meeting of the association for computational linguistics: Human language\ntechnologies, 142–150 (2011).\n48. Dey, R. & Salem, F. M. Gate-variants of gated recurrent unit (gru) neural networks.\nIn 2017 IEEE 60th international midwest symposium on circuits and systems (MWS-\nCAS), 1597–1600 (IEEE, 2017).\n49. Campos, V., Jou, B., Gir´o-i Nieto, X., Torres, J. & Chang, S.-F. Skip rnn: Learning\nto skip state updates in recurrent neural networks. arXiv preprint arXiv:1708.06834\n(2017).\n50. Todorov, E., Erez, T. & Tassa, Y. Mujoco: A physics engine for model-based control.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "223", "text": "49. Campos, V., Jou, B., Gir´o-i Nieto, X., Torres, J. & Chang, S.-F. Skip rnn: Learning\nto skip state updates in recurrent neural networks. arXiv preprint arXiv:1708.06834\n(2017).\n50. Todorov, E., Erez, T. & Tassa, Y. Mujoco: A physics engine for model-based control.\nIn 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, 5026–5033\n(IEEE, 2012).\n51. Brockman, G. et al. Openai gym. arXiv preprint arXiv:1606.01540 (2016).\n52. Zhang, H., Wang, Z. & Liu, D. A comprehensive review of stability analysis of\ncontinuous-time recurrent neural networks. IEEE Transactions on Neural Networks\nand Learning Systems 25, 1229–1262 (2014).\n53.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "224", "text": "In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, 5026–5033\n(IEEE, 2012).\n51. Brockman, G. et al. Openai gym. arXiv preprint arXiv:1606.01540 (2016).\n52. Zhang, H., Wang, Z. & Liu, D. A comprehensive review of stability analysis of\ncontinuous-time recurrent neural networks. IEEE Transactions on Neural Networks\nand Learning Systems 25, 1229–1262 (2014).\n53. Weinan, E. A proposal on machine learning via dynamical systems. Communica-\ntions in Mathematics and Statistics 5, 1–11 (2017).\n54. Lu, Z., Pu, H., Wang, F., Hu, Z. & Wang, L. The expressive power of neural net-\nworks: A view from the width. arXiv preprint arXiv:1709.02540 (2017).\n55.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "225", "text": "IEEE Transactions on Neural Networks\nand Learning Systems 25, 1229–1262 (2014).\n53. Weinan, E. A proposal on machine learning via dynamical systems. Communica-\ntions in Mathematics and Statistics 5, 1–11 (2017).\n54. Lu, Z., Pu, H., Wang, F., Hu, Z. & Wang, L. The expressive power of neural net-\nworks: A view from the width. arXiv preprint arXiv:1709.02540 (2017).\n55. Li, Q., Chen, L., Tai, C. et al. Maximum principle based algorithms for deep learn-\ning. arXiv preprint arXiv:1710.09513 (2017).\n32\n56. Lechner, M., Hasani, R., Zimmer, M., Henzinger, T. A. & Grosu, R.\nDesigning\nworm-inspired neural networks for interpretable robotic control.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "226", "text": "arXiv preprint arXiv:1709.02540 (2017).\n55. Li, Q., Chen, L., Tai, C. et al. Maximum principle based algorithms for deep learn-\ning. arXiv preprint arXiv:1710.09513 (2017).\n32\n56. Lechner, M., Hasani, R., Zimmer, M., Henzinger, T. A. & Grosu, R.\nDesigning\nworm-inspired neural networks for interpretable robotic control. In International\nConference on Robotics and Automation (ICRA), 87–94 (2019).\n57. Cohen, M. A. & Grossberg, S. Absolute stability of global pattern formation and\nparallel memory storage by competitive neural networks. IEEE transactions on sys-\ntems, man, and cybernetics 815–826 (1983).\n58. Mathieu, E. & Nickel, M.\nRiemannian continuous normalizing ﬂows.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "227", "text": "& Grosu, R.\nDesigning\nworm-inspired neural networks for interpretable robotic control. In International\nConference on Robotics and Automation (ICRA), 87–94 (2019).\n57. Cohen, M. A. & Grossberg, S. Absolute stability of global pattern formation and\nparallel memory storage by competitive neural networks. IEEE transactions on sys-\ntems, man, and cybernetics 815–826 (1983).\n58. Mathieu, E. & Nickel, M.\nRiemannian continuous normalizing ﬂows.\nIn\nLarochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F. & Lin, H. (eds.) Advances\nin Neural Information Processing Systems, vol. 33, 2503–2515 (Curran Associates,\nInc., 2020).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "228", "text": "IEEE transactions on sys-\ntems, man, and cybernetics 815–826 (1983).\n58. Mathieu, E. & Nickel, M.\nRiemannian continuous normalizing ﬂows.\nIn\nLarochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F. & Lin, H. (eds.) Advances\nin Neural Information Processing Systems, vol. 33, 2503–2515 (Curran Associates,\nInc., 2020). URL https://proceedings.neurips.cc/paper/2020/file/\n1aa3d9c6ce672447e1e5d0f1b5207e85-Paper.pdf.\n59. Hodgkinson, L., van der Heide, C., Roosta, F. & Mahoney, M. W. Stochastic nor-\nmalizing ﬂows. arXiv preprint arXiv:2002.09547 (2020).\n60.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "229", "text": "33, 2503–2515 (Curran Associates,\nInc., 2020). URL https://proceedings.neurips.cc/paper/2020/file/\n1aa3d9c6ce672447e1e5d0f1b5207e85-Paper.pdf.\n59. Hodgkinson, L., van der Heide, C., Roosta, F. & Mahoney, M. W. Stochastic nor-\nmalizing ﬂows. arXiv preprint arXiv:2002.09547 (2020).\n60. Haber, E., Lensink, K., Treister, E. & Ruthotto, L. Imexnet a forward stable deep\nneural network. In International Conference on Machine Learning, 2525–2534 (PMLR,\n2019).\n61. Chang, B., Chen, M., Haber, E. & Chi, E. H.\nAntisymmetricrnn: A dynamical\nsystem view on recurrent neural networks.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "230", "text": "arXiv preprint arXiv:2002.09547 (2020).\n60. Haber, E., Lensink, K., Treister, E. & Ruthotto, L. Imexnet a forward stable deep\nneural network. In International Conference on Machine Learning, 2525–2534 (PMLR,\n2019).\n61. Chang, B., Chen, M., Haber, E. & Chi, E. H.\nAntisymmetricrnn: A dynamical\nsystem view on recurrent neural networks. arXiv preprint arXiv:1902.09689 (2019).\n62. Lechner, M., Hasani, R., Rus, D. & Grosu, R. Gershgorin loss stabilizes the recurrent\nneural network compartment of an end-to-end robot learning scheme. In 2020 IEEE\nInternational Conference on Robotics and Automation (ICRA), 5446–5452 (IEEE, 2020).\n33\n63.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "231", "text": "& Chi, E. H.\nAntisymmetricrnn: A dynamical\nsystem view on recurrent neural networks. arXiv preprint arXiv:1902.09689 (2019).\n62. Lechner, M., Hasani, R., Rus, D. & Grosu, R. Gershgorin loss stabilizes the recurrent\nneural network compartment of an end-to-end robot learning scheme. In 2020 IEEE\nInternational Conference on Robotics and Automation (ICRA), 5446–5452 (IEEE, 2020).\n33\n63. Gleeson, P., Lung, D., Grosu, R., Hasani, R. & Larson, S. D. c302: a multiscale\nframework for modelling the nervous system of caenorhabditis elegans. Philosoph-\nical Transactions of the Royal Society B: Biological Sciences 373, 20170379 (2018).\n64. Li, X., Wong, T.-K. L., Chen, R. T.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "232", "text": "33\n63. Gleeson, P., Lung, D., Grosu, R., Hasani, R. & Larson, S. D. c302: a multiscale\nframework for modelling the nervous system of caenorhabditis elegans. Philosoph-\nical Transactions of the Royal Society B: Biological Sciences 373, 20170379 (2018).\n64. Li, X., Wong, T.-K. L., Chen, R. T. & Duvenaud, D. Scalable gradients for stochastic\ndifferential equations. In International Conference on Artiﬁcial Intelligence and Statis-\ntics, 3870–3882 (PMLR, 2020).\n65. Rezende, D. & Mohamed, S.\nVariational inference with normalizing ﬂows.\nIn\nInternational conference on machine learning, 1530–1538 (PMLR, 2015).\n66. Grunbacher, S. et al. On the veriﬁcation of neural odes with stochastic guarantees.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "233", "text": "& Duvenaud, D. Scalable gradients for stochastic\ndifferential equations. In International Conference on Artiﬁcial Intelligence and Statis-\ntics, 3870–3882 (PMLR, 2020).\n65. Rezende, D. & Mohamed, S.\nVariational inference with normalizing ﬂows.\nIn\nInternational conference on machine learning, 1530–1538 (PMLR, 2015).\n66. Grunbacher, S. et al. On the veriﬁcation of neural odes with stochastic guarantees.\nProceedings of the AAAI Conference on Artiﬁcial Intelligence 35, 11525–11535 (2021).\n67. Lechner, M., Hasani, R., Grosu, R., Rus, D. & Henzinger, T. A. Adversarial training\nis not ready for robot learning. arXiv preprint arXiv:2103.08187 (2021).\n68. Brunnbauer, A. et al.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "234", "text": "On the veriﬁcation of neural odes with stochastic guarantees.\nProceedings of the AAAI Conference on Artiﬁcial Intelligence 35, 11525–11535 (2021).\n67. Lechner, M., Hasani, R., Grosu, R., Rus, D. & Henzinger, T. A. Adversarial training\nis not ready for robot learning. arXiv preprint arXiv:2103.08187 (2021).\n68. Brunnbauer, A. et al. Model-based versus model-free deep reinforcement learning\nfor autonomous racing cars. arXiv preprint arXiv:2103.04909 (2021).\n69. Hasani, R. M., Haerle, D. & Grosu, R. Efﬁcient modeling of complex analog inte-\ngrated circuits using neural networks. In 2016 12th Conference on Ph. D. Research in\nMicroelectronics and Electronics (PRIME), 1–4 (IEEE, 2016).\n70.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "235", "text": "68. Brunnbauer, A. et al. Model-based versus model-free deep reinforcement learning\nfor autonomous racing cars. arXiv preprint arXiv:2103.04909 (2021).\n69. Hasani, R. M., Haerle, D. & Grosu, R. Efﬁcient modeling of complex analog inte-\ngrated circuits using neural networks. In 2016 12th Conference on Ph. D. Research in\nMicroelectronics and Electronics (PRIME), 1–4 (IEEE, 2016).\n70. Wang, G., Ledwoch, A., Hasani, R. M., Grosu, R. & Brintrup, A. A generative neu-\nral network model for the quality prediction of work in progress products. Applied\nSoft Computing 85, 105683 (2019).\n71. DelPreto, J. et al. Plug-and-play supervisory control using muscle and brain signals\nfor real-time gesture and error detection.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "236", "text": "D. Research in\nMicroelectronics and Electronics (PRIME), 1–4 (IEEE, 2016).\n70. Wang, G., Ledwoch, A., Hasani, R. M., Grosu, R. & Brintrup, A. A generative neu-\nral network model for the quality prediction of work in progress products. Applied\nSoft Computing 85, 105683 (2019).\n71. DelPreto, J. et al. Plug-and-play supervisory control using muscle and brain signals\nfor real-time gesture and error detection. Autonomous Robots 44, 1303–1322 (2020).\n34\n72. Hasani, R. Interpretable Recurrent Neural Networks in Continuous-time Control Envi-\nronments. PhD dissertation, Technische Universit¨at Wien (2020).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "237", "text": "Applied\nSoft Computing 85, 105683 (2019).\n71. DelPreto, J. et al. Plug-and-play supervisory control using muscle and brain signals\nfor real-time gesture and error detection. Autonomous Robots 44, 1303–1322 (2020).\n34\n72. Hasani, R. Interpretable Recurrent Neural Networks in Continuous-time Control Envi-\nronments. PhD dissertation, Technische Universit¨at Wien (2020).\nAcknowledgments\nAuthors would like to thank Tsun-Hsuan Wang, Patrick Kao, Makram Chahine, Wei\nXiao, Xiao Li, Lianhao Yin, and Yutong Ben for useful suggestions and testing out\nCfC models for conﬁrmation of results across other domains. Funding: R.H. and D.R.\nare partially supported by Boeing and MIT. M.L. is supported in part by the Austrian\nScience Fund (FWF) under grant Z211-N23 (Wittgenstein Award). A.A.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "238", "text": "Acknowledgments\nAuthors would like to thank Tsun-Hsuan Wang, Patrick Kao, Makram Chahine, Wei\nXiao, Xiao Li, Lianhao Yin, and Yutong Ben for useful suggestions and testing out\nCfC models for conﬁrmation of results across other domains. Funding: R.H. and D.R.\nare partially supported by Boeing and MIT. M.L. is supported in part by the Austrian\nScience Fund (FWF) under grant Z211-N23 (Wittgenstein Award). A.A. is supported\nby the National Science Foundation (NSF) Graduate Research Fellowship Program.\nM.T. is supported by the Poul Due Jensen Foundation, grant 883901. This research\nwas partially sponsored by the United States Air Force Research Laboratory and the\nUnited States Air Force Artiﬁcial Intelligence Accelerator and was accomplished un-\nder Cooperative Agreement Number FA8750-19-2-1000.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "239", "text": "M.L. is supported in part by the Austrian\nScience Fund (FWF) under grant Z211-N23 (Wittgenstein Award). A.A. is supported\nby the National Science Foundation (NSF) Graduate Research Fellowship Program.\nM.T. is supported by the Poul Due Jensen Foundation, grant 883901. This research\nwas partially sponsored by the United States Air Force Research Laboratory and the\nUnited States Air Force Artiﬁcial Intelligence Accelerator and was accomplished un-\nder Cooperative Agreement Number FA8750-19-2-1000. The views and conclusions\ncontained in this document are those of the authors and should not be interpreted\nas representing the ofﬁcial policies, either expressed or implied, of the United States\nAir Force or the U.S. Government. The U.S. Government is authorized to reproduce\nand distribute reprints for Government purposes notwithstanding any copyright no-\ntation herein.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "240", "text": "This research\nwas partially sponsored by the United States Air Force Research Laboratory and the\nUnited States Air Force Artiﬁcial Intelligence Accelerator and was accomplished un-\nder Cooperative Agreement Number FA8750-19-2-1000. The views and conclusions\ncontained in this document are those of the authors and should not be interpreted\nas representing the ofﬁcial policies, either expressed or implied, of the United States\nAir Force or the U.S. Government. The U.S. Government is authorized to reproduce\nand distribute reprints for Government purposes notwithstanding any copyright no-\ntation herein. This work was further supported by The Boeing Company and the\nOfﬁce of Naval Research (ONR) Grant N00014-18-1-2830. Data and materials avail-\nability: All data, code, and materials used in the analysis are openly available at\nhttps://github.com/raminmh/CfC under Apache 2.0 License, for purposes of re-\nproducing and extending the analysis.\n35\nList of Supplementary materials\nMaterials and Methods.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "241", "text": "Government is authorized to reproduce\nand distribute reprints for Government purposes notwithstanding any copyright no-\ntation herein. This work was further supported by The Boeing Company and the\nOfﬁce of Naval Research (ONR) Grant N00014-18-1-2830. Data and materials avail-\nability: All data, code, and materials used in the analysis are openly available at\nhttps://github.com/raminmh/CfC under Apache 2.0 License, for purposes of re-\nproducing and extending the analysis.\n35\nList of Supplementary materials\nMaterials and Methods.\nTables S1 to S4.\n36\nSupplementary Materials\nHere, we provide all supplementary materials used in our analysis.\nMaterials and Methods\nIn this section, we provide the full proof for Lemma 1.\nProof of Lemma 1\nProof.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "242", "text": "35\nList of Supplementary materials\nMaterials and Methods.\nTables S1 to S4.\n36\nSupplementary Materials\nHere, we provide all supplementary materials used in our analysis.\nMaterials and Methods\nIn this section, we provide the full proof for Lemma 1.\nProof of Lemma 1\nProof. We start by noting that\nx(t) −˜x(t) = c[e−wτt−R t\n0 f (I(s))ds −e−wτt−f (I(t))t f (−I(t))]\n= ce−wτt[e−R t\n0 f (I(s))ds −e−f (I(t))t f (−I(t))]\nSince 0 ≤f ≤1, we conclude e−R t\n0 f (I(s))ds ∈[0; 1] and e−f (I(t))t f (−I(t)) ∈[0; 1]. This\nshows that |x(t) −˜x(t)| ≤|c|e−wτt.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "243", "text": "This\nshows that |x(t) −˜x(t)| ≤|c|e−wτt. To see the sharpness results, pick some arbitrary\nsmall ε > 0 and a sufﬁciently large C > 0 such that f (−C) ≤ε and 1 −ε ≤f (C). With\nthis, for any 0 < δ < t, we consider the piecewise constant input signal I such that\nI(s) = −C for s ∈[0; t −δ] and I(s) = C for s ∈(t −δ; t].", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "244", "text": "To see the sharpness results, pick some arbitrary\nsmall ε > 0 and a sufﬁciently large C > 0 such that f (−C) ≤ε and 1 −ε ≤f (C). With\nthis, for any 0 < δ < t, we consider the piecewise constant input signal I such that\nI(s) = −C for s ∈[0; t −δ] and I(s) = C for s ∈(t −δ; t]. Then, it can be noted that\ne−R t\n0 f (I(s))ds −e−f (I(t))t f (−I(t)) ≥\ne−εt−δ·1 −e−(1−ε)·tε →1,\nwhen ε, δ →0\nStatement 1) follows by noting that there exists a family of continuous signals In :\n[0; t] →R such that |In(·)| ≤C for all n ≥1 and In →I pointwise as n →∞.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "245", "text": "This is\nbecause\nlim\nn→∞\n\f\f\f\nZ t\n0 f (I(s))ds −\nZ t\n0 f (In(s))ds\n\f\f\f ≤\nlim\nn→∞\nZ t\n0 | f (I(s)) −f (In(s))|ds ≤lim\nn→∞L\nZ t\n0 |I(s) −In(s)|ds\n= 0\n37\nwhere L is the Lipschitz constant of f and the last identity is due to dominated conver-\ngence theorem (21). To see 2), we ﬁrst note that the negation of the signal −I provides\nus with\ne−R t\n0 f (−I(s))ds −e−f (−I(t))t f (I(t)) ≤\ne−(1−ε)(t−δ)−δ·0 −e−ε·t(1 −ε) →e−t −1,\nif ε, δ →0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "246", "text": "To see 2), we ﬁrst note that the negation of the signal −I provides\nus with\ne−R t\n0 f (−I(s))ds −e−f (−I(t))t f (I(t)) ≤\ne−(1−ε)(t−δ)−δ·0 −e−ε·t(1 −ε) →e−t −1,\nif ε, δ →0. The fact that the left-hand side of the last inequality must be at least e−t −1\nfollows by observing that e−t ≤e−R t\n0 f (I′(s))ds and e−f (I′′(t))t f (−I′′(t)) ≤1 for any\nI′, I′′ : [0; t] →R.\n38\nTable S1: Bit-Stream XOR experiments.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "247", "text": "The fact that the left-hand side of the last inequality must be at least e−t −1\nfollows by observing that e−t ≤e−R t\n0 f (I′(s))ds and e−f (I′′(t))t f (−I′′(t)) ≤1 for any\nI′, I′′ : [0; t] →R.\n38\nTable S1: Bit-Stream XOR experiments. Hyperparameters\nParameter\nValue\nCf-S\nCfC\nCfC-noGate\nCfC-mmRNN\nclipnorm\n5\n1\n10\n10\noptimizer\nAdam\nRMSProp\nRMSprop\nRMSprop\nbatch size\n256\n128\n128\n128\nHidden size\n64\n192\n128\n64\nepochs\n200\n200\n200\n200\nbase lr\n0.005\n0.05\n0.005\n0.005\ndecay lr\n0.9\n0.7\n0.95\n0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "248", "text": "Hyperparameters\nParameter\nValue\nCf-S\nCfC\nCfC-noGate\nCfC-mmRNN\nclipnorm\n5\n1\n10\n10\noptimizer\nAdam\nRMSProp\nRMSprop\nRMSprop\nbatch size\n256\n128\n128\n128\nHidden size\n64\n192\n128\n64\nepochs\n200\n200\n200\n200\nbase lr\n0.005\n0.05\n0.005\n0.005\ndecay lr\n0.9\n0.7\n0.95\n0.95\nbackbone activation\nSiLU\nReLU\nSiLU\nReLU\nbackbone dr\n0.0\n0.0\n0.3\n0.0\nforget bias\n1.2\n1.2\n4.7\n0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "249", "text": "005\n0.05\n0.005\n0.005\ndecay lr\n0.9\n0.7\n0.95\n0.95\nbackbone activation\nSiLU\nReLU\nSiLU\nReLU\nbackbone dr\n0.0\n0.0\n0.3\n0.0\nforget bias\n1.2\n1.2\n4.7\n0.6\nbackbone units\n64\n128\n192\n128\nbackbone layers\n1\n1\n1\n1\nweight decay\n3e-05\n3e-06\n5e-06\n2e-06\nTable S2: Physionet experiments. Hyperparameters\nParameter\nValue\nCf-S\nCfC\nCfC-noGate\nCfC-mmRNN\nepochs\n116\n57\n58\n65\nclass weight\n18.25\n11.69\n7.73\n5.91\nclipnorm\n0\n0\n0\n0\nHidden size\n64\n256\n64\n64\nbase lr\n0.003\n0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "250", "text": "Hyperparameters\nParameter\nValue\nCf-S\nCfC\nCfC-noGate\nCfC-mmRNN\nepochs\n116\n57\n58\n65\nclass weight\n18.25\n11.69\n7.73\n5.91\nclipnorm\n0\n0\n0\n0\nHidden size\n64\n256\n64\n64\nbase lr\n0.003\n0.002\n0.003\n0.001\ndecay lr\n0.72\n0.9\n0.73\n0.9\nbackbone activation\nTanh\nSiLU\nReLU\nLeCun Tanh\nbackbone units\n64\n64\n192\n64\nbackbone dr\n0.1\n0.2\n0.0\n0.3\nbackbone layers\n3\n2\n2\n2\nweight decay\n5e-05\n4e-06\n5e-05\n4e-06\noptimizer\nAdamW\nAdamW\nAdamW\nAdamW\ninit\n0.53\n0.50\n0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "251", "text": "72\n0.9\n0.73\n0.9\nbackbone activation\nTanh\nSiLU\nReLU\nLeCun Tanh\nbackbone units\n64\n64\n192\n64\nbackbone dr\n0.1\n0.2\n0.0\n0.3\nbackbone layers\n3\n2\n2\n2\nweight decay\n5e-05\n4e-06\n5e-05\n4e-06\noptimizer\nAdamW\nAdamW\nAdamW\nAdamW\ninit\n0.53\n0.50\n0.55\n0.6\nbatch size\n128\n128\n128\n128\n39\nTable S3: IMDB experiments.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "252", "text": "1\n0.2\n0.0\n0.3\nbackbone layers\n3\n2\n2\n2\nweight decay\n5e-05\n4e-06\n5e-05\n4e-06\noptimizer\nAdamW\nAdamW\nAdamW\nAdamW\ninit\n0.53\n0.50\n0.55\n0.6\nbatch size\n128\n128\n128\n128\n39\nTable S3: IMDB experiments. Hyperparameters\nParameter\nValue\nCf-S\nCfC\nCfC-noGate\nCfC-mmRNN\nclipnorm\n1\n10\n5\n10\noptimizer\nAdam\nRMSProp\nRMSprop\nRMSprop\nbatch size\n128\n128\n128\n128\nHidden size\n320\n192\n224\n64\nembed dim\n64\n192\n192\n32\nembed dr\n0.0\n0.0\n0.2\n0.3\nepochs\n27\n47\n37\n20\nbase lr\n0.0005\n0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "253", "text": "Hyperparameters\nParameter\nValue\nCf-S\nCfC\nCfC-noGate\nCfC-mmRNN\nclipnorm\n1\n10\n5\n10\noptimizer\nAdam\nRMSProp\nRMSprop\nRMSprop\nbatch size\n128\n128\n128\n128\nHidden size\n320\n192\n224\n64\nembed dim\n64\n192\n192\n32\nembed dr\n0.0\n0.0\n0.2\n0.3\nepochs\n27\n47\n37\n20\nbase lr\n0.0005\n0.0005\n0.0005\n0.0005\ndecay lr\n0.8\n0.7\n0.8\n0.8\nbackbone activation\nRelu\nSiLU\nSiLU\nLeCun Tanh\nbackbone dr\n0.0\n0.0\n0.1\n0.0\nbackbone units\n64\n64\n128\n64\nbackbone layers\n1\n2\n1\n1\nweight decay\n0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "254", "text": "2\n0.3\nepochs\n27\n47\n37\n20\nbase lr\n0.0005\n0.0005\n0.0005\n0.0005\ndecay lr\n0.8\n0.7\n0.8\n0.8\nbackbone activation\nRelu\nSiLU\nSiLU\nLeCun Tanh\nbackbone dr\n0.0\n0.0\n0.1\n0.0\nbackbone units\n64\n64\n128\n64\nbackbone layers\n1\n2\n1\n1\nweight decay\n0.00048\n3.6e-05\n2.7e-05\n0.00029\nTable S4: Walker2D experiments.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "255", "text": "Hyperparameters\nParameter\nValue\nCf-S\nCfC\nCfC-noGate\nCfC-mmRNN\nclipnorm\n10\n1\n1\n10\noptimizer\nAdam\nAdam\nAdam\nAdam\nbatch size\n128\n256\n128\n128\nHidden size\n256\n64\n256\n128\nepochs\n50\n50\n50\n50\nbase lr\n0.006\n0.02\n0.008\n0.005\ndecay lr\n0.95\n0.95\n0.95\n0.95\nbackbone activation\nSiLU\nSiLU\nLeCun Tanh\nLeCun Tanh\nbackbone dr\n0.0\n0.1\n0.1\n0.2\nforget bias\n5.0\n1.6\n2.8\n2.1\nbackbone units\n192\n256\n128\n128\nbackbone layers\n1\n1\n1\n2\nweight decay\n1e-06\n1e-06\n3e-05\n6e-06\n40", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2106.13898v2.pdf", "file_name": ""}}
{"id": "256", "text": "mHC: Manifold-Constrained Hyper-Connections\nZhenda Xie*†, Yixuan Wei*, Huanqi Cao*,\nChenggang Zhao, Chengqi Deng, Jiashi Li, Damai Dai, Huazuo Gao, Jiang Chang,\nLiang Zhao, Shangyan Zhou, Zhean Xu, Zhengyan Zhang, Wangding Zeng,\nShengding Hu, Yuqing Wang, Jingyang Yuan, Lean Wang, Wenfeng Liang\nDeepSeek-AI\nAbstract\nRecently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous resid-\nual connection paradigm established over the past decade by expanding the residual stream\nwidth and diversifying connectivity patterns. While yielding substantial performance gains,\nthis diversification fundamentally compromises the identity mapping property intrinsic to\nthe residual connection, which causes severe training instability and restricted scalability, and\nadditionally incurs notable memory access overhead.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "257", "text": "While yielding substantial performance gains,\nthis diversification fundamentally compromises the identity mapping property intrinsic to\nthe residual connection, which causes severe training instability and restricted scalability, and\nadditionally incurs notable memory access overhead. To address these challenges, we pro-\npose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects\nthe residual connection space of HC onto a specific manifold to restore the identity mapping\nproperty, while incorporating rigorous infrastructure optimization to ensure efficiency. Em-\npirical experiments demonstrate that mHC is effective for training at scale, offering tangible\nperformance improvements and superior scalability. We anticipate that mHC, as a flexible and\npractical extension of HC, will contribute to a deeper understanding of topological architecture\ndesign and suggest promising directions for the evolution of foundational models.\n(a) Residual Connection\n(b) Hyper-Connections (HC)\n(c) Manifold-Constrained HC (mHC)\nLayer ℱ\nx!\nx!\"#\nRes Mapping\nℋ!\n$%&\nPre Mapping\nℋ!\n'$%\nPost Mapping\nℋ!\n'(&)\nLayer ℱ\nx!", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "258", "text": "We anticipate that mHC, as a flexible and\npractical extension of HC, will contribute to a deeper understanding of topological architecture\ndesign and suggest promising directions for the evolution of foundational models.\n(a) Residual Connection\n(b) Hyper-Connections (HC)\n(c) Manifold-Constrained HC (mHC)\nLayer ℱ\nx!\nx!\"#\nRes Mapping\nℋ!\n$%&\nPre Mapping\nℋ!\n'$%\nPost Mapping\nℋ!\n'(&)\nLayer ℱ\nx!\"#\nh!\n$%&\nx!\nh!\n'(&)\nh!\n*+\nh!\n(,)\nRes Mapping\n𝒫ℳ!\"#(ℋ!\n$%&)\nPre Mapping\n𝒫ℳ$!\"(ℋ!\n'$%)\nPost Mapping\n𝒫ℳ$%#&(ℋ!\n'(&))\nLayer ℱ\nx!\"#\nh!\n$%&\nx!\nh!\n'(&)\nh!\n*+\nh!\n(,)\nFigure 1 | Illustrations of Residual Connection Paradigms.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "259", "text": "'(&)\nLayer ℱ\nx!\"#\nh!\n$%&\nx!\nh!\n'(&)\nh!\n*+\nh!\n(,)\nRes Mapping\n𝒫ℳ!\"#(ℋ!\n$%&)\nPre Mapping\n𝒫ℳ$!\"(ℋ!\n'$%)\nPost Mapping\n𝒫ℳ$%#&(ℋ!\n'(&))\nLayer ℱ\nx!\"#\nh!\n$%&\nx!\nh!\n'(&)\nh!\n*+\nh!\n(,)\nFigure 1 | Illustrations of Residual Connection Paradigms. This figure compares the structural\ndesign of (a) standard Residual Connection, (b) Hyper-Connections (HC), and (c) our proposed\nManifold-Constrained Hyper-Connections (mHC). Unlike the unconstrained HC, mHC focuses\non optimizing the residual connection space by projecting the matrices onto a constrained\nmanifold to ensure stability.\n*Core contributors.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "260", "text": "'(&))\nLayer ℱ\nx!\"#\nh!\n$%&\nx!\nh!\n'(&)\nh!\n*+\nh!\n(,)\nFigure 1 | Illustrations of Residual Connection Paradigms. This figure compares the structural\ndesign of (a) standard Residual Connection, (b) Hyper-Connections (HC), and (c) our proposed\nManifold-Constrained Hyper-Connections (mHC). Unlike the unconstrained HC, mHC focuses\non optimizing the residual connection space by projecting the matrices onto a constrained\nmanifold to ensure stability.\n*Core contributors. †Corresponding author: xie.zhenda@deepseek.com\narXiv:2512.24880v1  [cs.CL]  31 Dec 2025\nContents\n1\nIntroduction\n3\n2\nRelated Works\n4\n2.1\nMicro Design\n. . . . . . . . . . . . . . . . . . . . . .", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "261", "text": "*Core contributors. †Corresponding author: xie.zhenda@deepseek.com\narXiv:2512.24880v1  [cs.CL]  31 Dec 2025\nContents\n1\nIntroduction\n3\n2\nRelated Works\n4\n2.1\nMicro Design\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2.2\nMacro Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "262", "text": ". . . . . . . . . . . . . . . . . . . . . .\n4\n2.2\nMacro Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n3\nPreliminary\n5\n3.1\nNumerical Instability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n3.2\nSystem Overhead . . .", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "263", "text": ". . . . . . . . . . . . .\n5\n3\nPreliminary\n5\n3.1\nNumerical Instability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n3.2\nSystem Overhead . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n4\nMethod\n8\n4.1\nManifold-Constrained Hyper-Connections\n. . . . . . . .", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "264", "text": ". . . . .\n6\n3.2\nSystem Overhead . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n4\nMethod\n8\n4.1\nManifold-Constrained Hyper-Connections\n. . . . . . . . . . . . . . . . . . . . . .\n8\n4.2\nParameterization and Manifold Projection . . . . . . . . . . . . . . . . . . . . . . .\n9\n4.3\nEfficient Infrastructure Design . . . . .", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "265", "text": ". . . . . . . . . . . . . . . . . . . . .\n8\n4.2\nParameterization and Manifold Projection . . . . . . . . . . . . . . . . . . . . . . .\n9\n4.3\nEfficient Infrastructure Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n4.3.1\nKernel Fusion . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "266", "text": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n4.3.1\nKernel Fusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n4.3.2\nRecomputing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "267", "text": ". . . . . . . . . . . . . . . . . . . . . . . .\n9\n4.3.2\nRecomputing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n4.3.3\nOverlapping Communication in DualPipe\n. . . . . . . . . . . . . . . . . .\n11\n5\nExperiments\n12\n5.1\nExperimental Setup . . . . . . . . . . . . . . . . . . .", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "268", "text": ". . . . . . . . . . . . .\n10\n4.3.3\nOverlapping Communication in DualPipe\n. . . . . . . . . . . . . . . . . .\n11\n5\nExperiments\n12\n5.1\nExperimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n5.2\nMain Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "269", "text": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n5.2\nMain Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n5.3\nScaling Experiments\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "270", "text": ". . . . . . . . . . . . . . . . . . . . . . . . .\n12\n5.3\nScaling Experiments\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n5.4\nStability Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "271", "text": ". . . . . . . . . . . . . . . . . . . . . . .\n13\n5.4\nStability Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n6\nConclusion and Outlook\n15\nA Appendix\n19\nA.1 Detailed Model Specifications and Hyper-parameters. . . . . . . . . . . . . . . . .\n19\n2\n1. Introduction\nDeep neural network architectures have undergone rapid evolution since the introduction of\nResNets (He et al., 2016a). As illustrated in Fig.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "272", "text": ". . . . . . . . . . . . . .\n14\n6\nConclusion and Outlook\n15\nA Appendix\n19\nA.1 Detailed Model Specifications and Hyper-parameters. . . . . . . . . . . . . . . . .\n19\n2\n1. Introduction\nDeep neural network architectures have undergone rapid evolution since the introduction of\nResNets (He et al., 2016a). As illustrated in Fig. 1(a), the structure of a single-layer can be\nformulated as follows:\nx𝑙+1 = x𝑙+ F (x𝑙, W𝑙),\n(1)\nwhere x𝑙and x𝑙+1 denote the 𝐶-dimensional input and output of the 𝑙-th layer, respectively,\nand F represents the residual function.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "273", "text": ". .\n19\n2\n1. Introduction\nDeep neural network architectures have undergone rapid evolution since the introduction of\nResNets (He et al., 2016a). As illustrated in Fig. 1(a), the structure of a single-layer can be\nformulated as follows:\nx𝑙+1 = x𝑙+ F (x𝑙, W𝑙),\n(1)\nwhere x𝑙and x𝑙+1 denote the 𝐶-dimensional input and output of the 𝑙-th layer, respectively,\nand F represents the residual function. Although the residual function F has evolved over\nthe past decade to include various operations such as convolution, attention mechanisms, and\nfeed forward networks, the paradigm of the residual connection has maintained its original\nform.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "274", "text": "1(a), the structure of a single-layer can be\nformulated as follows:\nx𝑙+1 = x𝑙+ F (x𝑙, W𝑙),\n(1)\nwhere x𝑙and x𝑙+1 denote the 𝐶-dimensional input and output of the 𝑙-th layer, respectively,\nand F represents the residual function. Although the residual function F has evolved over\nthe past decade to include various operations such as convolution, attention mechanisms, and\nfeed forward networks, the paradigm of the residual connection has maintained its original\nform. Accompanying the progression of Transformer (Vaswani et al., 2017) architecture, this\nparadigm has currently established itself as a fundamental design element in large language\nmodels (LLMs) (Brown et al., 2020; Liu et al., 2024b; Touvron et al., 2023).\nThis success is primarily attributed to the concise form of the residual connection.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "275", "text": "Accompanying the progression of Transformer (Vaswani et al., 2017) architecture, this\nparadigm has currently established itself as a fundamental design element in large language\nmodels (LLMs) (Brown et al., 2020; Liu et al., 2024b; Touvron et al., 2023).\nThis success is primarily attributed to the concise form of the residual connection. More\nimportantly, early research (He et al., 2016b) revealed that the identity mapping property of the\nresidual connection maintains stability and efficiency during large-scale training. By recursively\nextending the residual connection across multiple layers, Eq. (1) yields:\nx𝐿= x𝑙+\n𝐿−1\n∑︁\n𝑖=𝑙\nF (x𝑖, W𝑖),\n(2)\nwhere 𝐿and 𝑙correspond to deeper and shallower layers, respectively.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "276", "text": "More\nimportantly, early research (He et al., 2016b) revealed that the identity mapping property of the\nresidual connection maintains stability and efficiency during large-scale training. By recursively\nextending the residual connection across multiple layers, Eq. (1) yields:\nx𝐿= x𝑙+\n𝐿−1\n∑︁\n𝑖=𝑙\nF (x𝑖, W𝑖),\n(2)\nwhere 𝐿and 𝑙correspond to deeper and shallower layers, respectively. The term identity\nmapping refers to the component x𝑙itself, which emphasizes the property that the signal from\nthe shallower layer maps directly to the deeper layer without any modification.\nRecently, studies exemplified by Hyper-Connections (HC) (Zhu et al., 2024) have introduced\na new dimension to the residual connection and empirically demonstrated its performance\npotential. The single-layer architecture of HC is illustrated in Fig. 1(b).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "277", "text": "The term identity\nmapping refers to the component x𝑙itself, which emphasizes the property that the signal from\nthe shallower layer maps directly to the deeper layer without any modification.\nRecently, studies exemplified by Hyper-Connections (HC) (Zhu et al., 2024) have introduced\na new dimension to the residual connection and empirically demonstrated its performance\npotential. The single-layer architecture of HC is illustrated in Fig. 1(b). By expanding the width of\nthe residual stream and enhancing connection complexity, HC significantly increases topological\ncomplexity without altering the computational overhead of individual units regarding FLOPs.\nFormally, single-layer propagation in HC is defined as:\nx𝑙+1 = Hres\n𝑙\nx𝑙+ Hpost ⊤\n𝑙\nF (Hpre\n𝑙\nx𝑙, W𝑙),\n(3)\nwhere x𝑙and x𝑙+1 denote the input and output of the 𝑙-th layer, respectively.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "278", "text": "By expanding the width of\nthe residual stream and enhancing connection complexity, HC significantly increases topological\ncomplexity without altering the computational overhead of individual units regarding FLOPs.\nFormally, single-layer propagation in HC is defined as:\nx𝑙+1 = Hres\n𝑙\nx𝑙+ Hpost ⊤\n𝑙\nF (Hpre\n𝑙\nx𝑙, W𝑙),\n(3)\nwhere x𝑙and x𝑙+1 denote the input and output of the 𝑙-th layer, respectively. Unlike the formu-\nlation in Eq. (1), the feature dimension of x𝑙and x𝑙+1 is expanded from 𝐶to 𝑛× 𝐶, where 𝑛is\nthe expansion rate. The term Hres\n𝑙\n∈R𝑛×𝑛represents a learnable mapping that mixes features\nwithin the residual stream.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "279", "text": "Unlike the formu-\nlation in Eq. (1), the feature dimension of x𝑙and x𝑙+1 is expanded from 𝐶to 𝑛× 𝐶, where 𝑛is\nthe expansion rate. The term Hres\n𝑙\n∈R𝑛×𝑛represents a learnable mapping that mixes features\nwithin the residual stream. Also as a learnable mapping, Hpre\n𝑙\n∈R1×𝑛aggregates features from\nthe 𝑛𝐶-dim stream into a 𝐶-dim layer input, and conversely, Hpost\n𝑙\n∈R1×𝑛maps the layer output\nback onto the stream.\nHowever, as the training scale increases, HC introduces potential risks of instability. The\nprimary concern is that the unconstrained nature of HC compromises the identity mapping\nproperty when the architecture extends across multiple layers. In architectures comprising\nmultiple parallel streams, an ideal identity mapping serves as a conservation mechanism.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "280", "text": "However, as the training scale increases, HC introduces potential risks of instability. The\nprimary concern is that the unconstrained nature of HC compromises the identity mapping\nproperty when the architecture extends across multiple layers. In architectures comprising\nmultiple parallel streams, an ideal identity mapping serves as a conservation mechanism. It\nensures that the average signal intensity across streams remains invariant during both forward\nand backward propagation. Recursively extending HC to multiple layers via Eq. (3) yields:\nx𝐿=\n 𝐿−𝑙\nÖ\n𝑖=1\nHres\n𝐿−𝑖\n!", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "281", "text": "In architectures comprising\nmultiple parallel streams, an ideal identity mapping serves as a conservation mechanism. It\nensures that the average signal intensity across streams remains invariant during both forward\nand backward propagation. Recursively extending HC to multiple layers via Eq. (3) yields:\nx𝐿=\n 𝐿−𝑙\nÖ\n𝑖=1\nHres\n𝐿−𝑖\n!\nx𝑙+\n𝐿−1\n∑︁\n𝑖=𝑙\n©­\n«\n𝐿−1−𝑖\nÖ\n𝑗=1\nHres\n𝐿−𝑗\nª®\n¬\nHpost ⊤\n𝑖\nF (Hpre\n𝑖\nx𝑖, W𝑖),\n(4)\n3\nwhere 𝐿and 𝑙represent a deeper layer and a shallower layer, respectively. In contrast to Eq.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "282", "text": "x𝑙+\n𝐿−1\n∑︁\n𝑖=𝑙\n©­\n«\n𝐿−1−𝑖\nÖ\n𝑗=1\nHres\n𝐿−𝑗\nª®\n¬\nHpost ⊤\n𝑖\nF (Hpre\n𝑖\nx𝑖, W𝑖),\n(4)\n3\nwhere 𝐿and 𝑙represent a deeper layer and a shallower layer, respectively. In contrast to Eq. (2),\nthe composite mapping Î𝐿−𝑙\n𝑖=1 Hres\n𝐿−𝑖in HC fails to preserve the global mean of the features. This\ndiscrepancy leads to unbounded signal amplification or attenuation, resulting in instability\nduring large-scale training.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "283", "text": "In contrast to Eq. (2),\nthe composite mapping Î𝐿−𝑙\n𝑖=1 Hres\n𝐿−𝑖in HC fails to preserve the global mean of the features. This\ndiscrepancy leads to unbounded signal amplification or attenuation, resulting in instability\nduring large-scale training. A further consideration is that, while HC preserves computational\nefficiency in terms of FLOPs, the hardware efficiency concerning memory access costs for the\nwidened residual stream remains unaddressed in the original design. These factors collectively\nrestrict the practical scalability of HC and hinder its application in large-scale training.\nTo address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC),\nas shown in Fig. 1(c), a general framework that projects the residual connection space of HC\nonto a specific manifold to restore the identity mapping property, while incorporating rigorous\ninfrastructure optimization to ensure efficiency.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "284", "text": "A further consideration is that, while HC preserves computational\nefficiency in terms of FLOPs, the hardware efficiency concerning memory access costs for the\nwidened residual stream remains unaddressed in the original design. These factors collectively\nrestrict the practical scalability of HC and hinder its application in large-scale training.\nTo address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC),\nas shown in Fig. 1(c), a general framework that projects the residual connection space of HC\nonto a specific manifold to restore the identity mapping property, while incorporating rigorous\ninfrastructure optimization to ensure efficiency. Specifically, mHC utilizes the Sinkhorn-Knopp\nalgorithm (Sinkhorn and Knopp, 1967) to entropically project Hres\n𝑙\nonto the Birkhoff polytope.\nThis operation effectively constrains the residual connection matrices within the manifold\nthat is constituted by doubly stochastic matrices.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "285", "text": "1(c), a general framework that projects the residual connection space of HC\nonto a specific manifold to restore the identity mapping property, while incorporating rigorous\ninfrastructure optimization to ensure efficiency. Specifically, mHC utilizes the Sinkhorn-Knopp\nalgorithm (Sinkhorn and Knopp, 1967) to entropically project Hres\n𝑙\nonto the Birkhoff polytope.\nThis operation effectively constrains the residual connection matrices within the manifold\nthat is constituted by doubly stochastic matrices. Since the row and column sums of these\nmatrices equal to 1, the operation Hres\n𝑙\nx𝑙functions as a convex combination of the input features.\nThis characteristic facilitates a well-conditioned signal propagation where the feature mean\nis conserved, and the signal norm is strictly regularized, effectively mitigating the risk of\nvanishing or exploding signals.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "286", "text": "This operation effectively constrains the residual connection matrices within the manifold\nthat is constituted by doubly stochastic matrices. Since the row and column sums of these\nmatrices equal to 1, the operation Hres\n𝑙\nx𝑙functions as a convex combination of the input features.\nThis characteristic facilitates a well-conditioned signal propagation where the feature mean\nis conserved, and the signal norm is strictly regularized, effectively mitigating the risk of\nvanishing or exploding signals. Furthermore, due to the closure of matrix multiplication for\ndoubly stochastic matrices, the composite mapping Î𝐿−𝑙\n𝑖=1 Hres\n𝐿−𝑖retains this conservation property.\nConsequently, mHC effectively maintains the stability of identity mappings between arbitrary\ndepths. To ensure efficiency, we employ kernel fusion and develop mixed precision kernels\nutilizing TileLang (Wang et al., 2025).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "287", "text": "Furthermore, due to the closure of matrix multiplication for\ndoubly stochastic matrices, the composite mapping Î𝐿−𝑙\n𝑖=1 Hres\n𝐿−𝑖retains this conservation property.\nConsequently, mHC effectively maintains the stability of identity mappings between arbitrary\ndepths. To ensure efficiency, we employ kernel fusion and develop mixed precision kernels\nutilizing TileLang (Wang et al., 2025). Furthermore, we mitigate the memory footprint through\nselective recomputing and carefully overlap communication within the DualPipe schedule (Liu\net al., 2024b).\nExtensive experiments on language model pretraining demonstrate that mHC exhibits\nexceptional stability and scalability while maintaining the performance advantages of HC. In-\nhouse large-scale training indicates that mHC supports training at scale and introduces only a\n6.7% additional time overhead when expansion rate 𝑛= 4.\n2. Related Works\nArchitectural advancements in deep learning can be primarily classified into micro-design and\nmacro-design.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "288", "text": "Furthermore, we mitigate the memory footprint through\nselective recomputing and carefully overlap communication within the DualPipe schedule (Liu\net al., 2024b).\nExtensive experiments on language model pretraining demonstrate that mHC exhibits\nexceptional stability and scalability while maintaining the performance advantages of HC. In-\nhouse large-scale training indicates that mHC supports training at scale and introduces only a\n6.7% additional time overhead when expansion rate 𝑛= 4.\n2. Related Works\nArchitectural advancements in deep learning can be primarily classified into micro-design and\nmacro-design. Micro-design concerns the internal architecture of computational blocks, specifying\nhow features are processed across spatial, temporal, and channel dimensions. In contrast,\nmacro-design establishes the inter-block topological structure, thereby dictating how feature\nrepresentations are propagated, routed, and merged across distinct layers.\n2.1. Micro Design\nDriven by parameter sharing and translation invariance, convolution initially dominated the pro-\ncessing of structured signals.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "289", "text": "2. Related Works\nArchitectural advancements in deep learning can be primarily classified into micro-design and\nmacro-design. Micro-design concerns the internal architecture of computational blocks, specifying\nhow features are processed across spatial, temporal, and channel dimensions. In contrast,\nmacro-design establishes the inter-block topological structure, thereby dictating how feature\nrepresentations are propagated, routed, and merged across distinct layers.\n2.1. Micro Design\nDriven by parameter sharing and translation invariance, convolution initially dominated the pro-\ncessing of structured signals. While subsequent variations such as depthwise separable (Chollet,\n2017) and grouped convolutions (Xie et al., 2017) optimized efficiency, the advent of Trans-\nformers (Vaswani et al., 2017) established Attention and Feed-Forward Networks (FFNs) as\nthe fundamental building blocks of modern architecture. Attention mechanisms facilitate\nglobal information propagation, while FFNs enhance the representational capacity of individual\nfeatures.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "290", "text": "2.1. Micro Design\nDriven by parameter sharing and translation invariance, convolution initially dominated the pro-\ncessing of structured signals. While subsequent variations such as depthwise separable (Chollet,\n2017) and grouped convolutions (Xie et al., 2017) optimized efficiency, the advent of Trans-\nformers (Vaswani et al., 2017) established Attention and Feed-Forward Networks (FFNs) as\nthe fundamental building blocks of modern architecture. Attention mechanisms facilitate\nglobal information propagation, while FFNs enhance the representational capacity of individual\nfeatures. To balance performance with the computational demands of LLMs, attention mecha-\nnisms have evolved towards efficient variants such as Multi-Query Attention (MQA) (Shazeer,\n2019), Grouped-Query Attention (GQA) (Ainslie et al., 2023), and Multi-Head Latent Attention\n4\n(MLA) (Liu et al., 2024a).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "291", "text": "Attention mechanisms facilitate\nglobal information propagation, while FFNs enhance the representational capacity of individual\nfeatures. To balance performance with the computational demands of LLMs, attention mecha-\nnisms have evolved towards efficient variants such as Multi-Query Attention (MQA) (Shazeer,\n2019), Grouped-Query Attention (GQA) (Ainslie et al., 2023), and Multi-Head Latent Attention\n4\n(MLA) (Liu et al., 2024a). Simultaneously, FFNs have been generalized into sparse computing\nparadigms via Mixture-of-Experts (MoE) (Fedus et al., 2022; Lepikhin et al., 2020; Shazeer et al.,\n2017), allowing for massive parameter scaling without proportional computational costs.\n2.2. Macro Design\nMacro-design governs the global topology of the network (Srivastava et al., 2015).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "292", "text": "Simultaneously, FFNs have been generalized into sparse computing\nparadigms via Mixture-of-Experts (MoE) (Fedus et al., 2022; Lepikhin et al., 2020; Shazeer et al.,\n2017), allowing for massive parameter scaling without proportional computational costs.\n2.2. Macro Design\nMacro-design governs the global topology of the network (Srivastava et al., 2015). Following\nResNet (He et al., 2016a), architectures such as DenseNet (Huang et al., 2017) and Fractal-\nNet (Larsson et al., 2016) aimed to enhance performance by increasing topological complexity\nthrough dense connectivity and multi-path structures, respectively. Deep Layer Aggregation\n(DLA) (Yu et al., 2018) further extended this paradigm by recursively aggregating features across\nvarious depths and resolutions.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "293", "text": "Macro Design\nMacro-design governs the global topology of the network (Srivastava et al., 2015). Following\nResNet (He et al., 2016a), architectures such as DenseNet (Huang et al., 2017) and Fractal-\nNet (Larsson et al., 2016) aimed to enhance performance by increasing topological complexity\nthrough dense connectivity and multi-path structures, respectively. Deep Layer Aggregation\n(DLA) (Yu et al., 2018) further extended this paradigm by recursively aggregating features across\nvarious depths and resolutions.\nMore recently, the focus of macro-design has shifted toward expanding the width of the\nresidual stream (Chai et al., 2020; Fang et al., 2023; Heddes et al., 2025; Mak and Flanigan,\n2025; Menghani et al., 2025; Pagliardini et al., 2024; Xiao et al., 2025; Xie et al., 2023; Zhu et al.,\n2024).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "294", "text": "More recently, the focus of macro-design has shifted toward expanding the width of the\nresidual stream (Chai et al., 2020; Fang et al., 2023; Heddes et al., 2025; Mak and Flanigan,\n2025; Menghani et al., 2025; Pagliardini et al., 2024; Xiao et al., 2025; Xie et al., 2023; Zhu et al.,\n2024). Hyper-Connections (HC) (Zhu et al., 2024) introduced learnable matrices to modulate\nconnection strengths among features at varying depths, while the Residual Matrix Transformer\n(RMT) (Mak and Flanigan, 2025) replaced the standard residual stream with an outer-product\nmemory matrix to facilitate feature storage. Similarly, MUDDFormer (Xiao et al., 2025) employs\nmultiway dynamic dense connections to optimize cross-layer information flow. Despite their\npotential, these approaches compromise the inherent identity mapping property of the residual\nconnection, thereby introducing instability and hindering scalability.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "295", "text": "Similarly, MUDDFormer (Xiao et al., 2025) employs\nmultiway dynamic dense connections to optimize cross-layer information flow. Despite their\npotential, these approaches compromise the inherent identity mapping property of the residual\nconnection, thereby introducing instability and hindering scalability. Furthermore, they incur\nsignificant memory access overhead due to expanded feature widths. Building upon HC,\nthe proposed mHC restricts the residual connection space onto a specific manifold to restore\nthe identity mapping property, while also incorporating rigorous infrastructure optimizations\nto ensure efficiency. This approach enhances stability and scalability while maintaining the\ntopological benefits of expanded connections.\n3. Preliminary\nWe first establish the notation used in this work. In the HC formulation, the input to the 𝑙-th layer,\nx𝑙∈R1×𝐶, is expanded by a factor of 𝑛to construct a hidden matrix x𝑙= (x⊤\n𝑙,0, . . .", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "296", "text": "This approach enhances stability and scalability while maintaining the\ntopological benefits of expanded connections.\n3. Preliminary\nWe first establish the notation used in this work. In the HC formulation, the input to the 𝑙-th layer,\nx𝑙∈R1×𝐶, is expanded by a factor of 𝑛to construct a hidden matrix x𝑙= (x⊤\n𝑙,0, . . . , x⊤\n𝑙,𝑛−1)⊤∈R𝑛×𝐶\nwhich can be viewed as 𝑛-stream residual. This operation effectively broadens the width of\nthe residual stream. To govern the read-out, write-in, and updating processes of this stream,\nHC introduces three learnable linear mappings—Hpre\n𝑙\n, Hpost\n𝑙\n∈R1×𝑛, and Hres\n𝑙\n∈R𝑛×𝑛.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "297", "text": ". . , x⊤\n𝑙,𝑛−1)⊤∈R𝑛×𝐶\nwhich can be viewed as 𝑛-stream residual. This operation effectively broadens the width of\nthe residual stream. To govern the read-out, write-in, and updating processes of this stream,\nHC introduces three learnable linear mappings—Hpre\n𝑙\n, Hpost\n𝑙\n∈R1×𝑛, and Hres\n𝑙\n∈R𝑛×𝑛. These\nmappings modify the standard residual connection shown in Eq. (1), resulting in the formulation\ngiven in Eq. (3).\nIn the HC formulation, learnable mappings are composed of two parts of coefficients: the\ninput-dependent one and the global one, referred to as dynamic mappings and static mappings,\nrespectively. Formally,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "298", "text": "Formally, HC computes the coefficients as follows:\n\n\n˜x𝑙= RMSNorm(x𝑙)\nHpre\n𝑙\n= 𝛼pre\n𝑙\n· tanh(𝜃pre\n𝑙\n˜x⊤\n𝑙) + bpre\n𝑙\nHpost\n𝑙\n= 𝛼post\n𝑙\n· tanh(𝜃post\n𝑙\n˜x⊤\n𝑙) + bpost\n𝑙\nHres\n𝑙\n= 𝛼res\n𝑙\n· tanh(𝜃res\n𝑙\n˜x⊤\n𝑙) + bres\n𝑙,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "299", "text": "(5)\nwhere RMSNorm(·) (Zhang and Sennrich, 2019) is applied to the last dimension, and the scalars\n𝛼pre\n𝑙\n, 𝛼post\n𝑙\nand 𝛼res\n𝑙\n∈R are learnable gating factors initialized to small values. The dynamic\n5\nmappings are derived via linear projections parameterized by 𝜃pre\n𝑙\n, 𝜃post\n𝑙\n∈R1×𝐶and 𝜃res\n𝑙\n∈R𝑛×𝐶,\nwhile the static mappings are represented by learnable biases bpre\n𝑙\n, bpost\n𝑙\n∈R1×𝑛and bres\n𝑙\n∈R𝑛×𝑛.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "300", "text": "The dynamic\n5\nmappings are derived via linear projections parameterized by 𝜃pre\n𝑙\n, 𝜃post\n𝑙\n∈R1×𝐶and 𝜃res\n𝑙\n∈R𝑛×𝐶,\nwhile the static mappings are represented by learnable biases bpre\n𝑙\n, bpost\n𝑙\n∈R1×𝑛and bres\n𝑙\n∈R𝑛×𝑛.\nIt is worth noting that the introduction of these mappings—Hpre\n𝑙\n, Hpost\n𝑙\n, and Hres\n𝑙\n—incurs\nnegligible computational overhead, as the typical expansion rate 𝑛, e.g. 4, is much smaller than\nthe input dimension 𝐶. With this design, HC effectively decouples the information capacity\nof the residual stream from the layer’s input dimension, which is strongly correlated with the\nmodel’s computational complexity (FLOPs).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "301", "text": "It is worth noting that the introduction of these mappings—Hpre\n𝑙\n, Hpost\n𝑙\n, and Hres\n𝑙\n—incurs\nnegligible computational overhead, as the typical expansion rate 𝑛, e.g. 4, is much smaller than\nthe input dimension 𝐶. With this design, HC effectively decouples the information capacity\nof the residual stream from the layer’s input dimension, which is strongly correlated with the\nmodel’s computational complexity (FLOPs). Consequently, HC offers a new avenue for scaling\nby adjusting the residual stream width, complementing the traditional scaling dimensions of\nmodel FLOPs and training data size discussed in pre-training scaling laws (Hoffmann et al.,\n2022).\nAlthough HC necessitates three mappings to manage the dimensional mismatch between\nthe residual stream and the layer input, preliminary experiments presented in Tab. 1 indicate\nthat the residual mapping Hres\n𝑙\nyields the most significant performance gain. This finding\nunderscores the critical importance of effective information exchange within the residual stream.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "302", "text": "Consequently, HC offers a new avenue for scaling\nby adjusting the residual stream width, complementing the traditional scaling dimensions of\nmodel FLOPs and training data size discussed in pre-training scaling laws (Hoffmann et al.,\n2022).\nAlthough HC necessitates three mappings to manage the dimensional mismatch between\nthe residual stream and the layer input, preliminary experiments presented in Tab. 1 indicate\nthat the residual mapping Hres\n𝑙\nyields the most significant performance gain. This finding\nunderscores the critical importance of effective information exchange within the residual stream.\nTable 1 | Ablation Study of HC Components. When a specific mapping (Hpre\n𝑙\n, Hpost\n𝑙\n, or Hres\n𝑙\n) is\ndisabled, we employ a fixed mapping to maintain dimensional consistency: uniform weights of\n1/𝑛for Hpre\n𝑙\n, uniform weights of ones for Hpost\n𝑙\n, and the identity matrix for Hres\n𝑙\n.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "303", "text": "This finding\nunderscores the critical importance of effective information exchange within the residual stream.\nTable 1 | Ablation Study of HC Components. When a specific mapping (Hpre\n𝑙\n, Hpost\n𝑙\n, or Hres\n𝑙\n) is\ndisabled, we employ a fixed mapping to maintain dimensional consistency: uniform weights of\n1/𝑛for Hpre\n𝑙\n, uniform weights of ones for Hpost\n𝑙\n, and the identity matrix for Hres\n𝑙\n.\nHres\n𝑙\nHpre\n𝑙\nHpost\n𝑙\nAbsolute Loss Gap\n0.0\n✓\n−0.022\n✓\n✓\n−0.025\n✓\n✓\n✓\n−0.027\n3.1. Numerical Instability\nWhile the residual mapping Hres\n𝑙\nis instrumental for performance, its sequential application\nposes a significant risk to numerical stability. As detailed in Eq.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "304", "text": "Hres\n𝑙\nHpre\n𝑙\nHpost\n𝑙\nAbsolute Loss Gap\n0.0\n✓\n−0.022\n✓\n✓\n−0.025\n✓\n✓\n✓\n−0.027\n3.1. Numerical Instability\nWhile the residual mapping Hres\n𝑙\nis instrumental for performance, its sequential application\nposes a significant risk to numerical stability. As detailed in Eq. (4), when HC is extended across\nmultiple layers, the effective signal propagation from layer 𝑙to 𝐿is governed by the composite\nmapping Î𝐿−𝑙\n𝑖=1 Hres\n𝐿−𝑖. Since the learnable mapping Hres\n𝑙\nis unconstrained, this composite mapping\ninevitably deviates from the identity mapping. Consequently, the signal magnitude is prone to\nexplosion or vanishing during both the forward pass and backpropagation.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "305", "text": "As detailed in Eq. (4), when HC is extended across\nmultiple layers, the effective signal propagation from layer 𝑙to 𝐿is governed by the composite\nmapping Î𝐿−𝑙\n𝑖=1 Hres\n𝐿−𝑖. Since the learnable mapping Hres\n𝑙\nis unconstrained, this composite mapping\ninevitably deviates from the identity mapping. Consequently, the signal magnitude is prone to\nexplosion or vanishing during both the forward pass and backpropagation. This phenomenon\nundermines the fundamental premise of residual learning, which relies on unimpeded signal\nflow, thereby destabilizing the training process in deeper or larger-scale models.\nEmpirical evidence supports this analysis. We observe unstable loss behavior in large-scale\nexperiments, as illustrated in Fig. 2. Taking mHC as the baseline, HC exhibits an unexpected\nloss surge around the 12k step, which is highly correlated with the instability in the gradient\nnorm.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "306", "text": "Consequently, the signal magnitude is prone to\nexplosion or vanishing during both the forward pass and backpropagation. This phenomenon\nundermines the fundamental premise of residual learning, which relies on unimpeded signal\nflow, thereby destabilizing the training process in deeper or larger-scale models.\nEmpirical evidence supports this analysis. We observe unstable loss behavior in large-scale\nexperiments, as illustrated in Fig. 2. Taking mHC as the baseline, HC exhibits an unexpected\nloss surge around the 12k step, which is highly correlated with the instability in the gradient\nnorm. Furthermore, the analysis on Hres\n𝑙\nvalidates the mechanism of this instability. To quantify\nhow the composite mapping Î𝐿−𝑙\n𝑖=1 Hres\n𝐿−𝑖amplifies signals along the residual stream, we utilize\ntwo metrics. The first, based on the maximum absolute value of the row sums of the composite\nmapping, captures the worst-case expansion in the forward pass.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "307", "text": "Furthermore, the analysis on Hres\n𝑙\nvalidates the mechanism of this instability. To quantify\nhow the composite mapping Î𝐿−𝑙\n𝑖=1 Hres\n𝐿−𝑖amplifies signals along the residual stream, we utilize\ntwo metrics. The first, based on the maximum absolute value of the row sums of the composite\nmapping, captures the worst-case expansion in the forward pass. The second, based on the\nmaximum absolute column sum, corresponds to the backward pass. We refer to these metrics\nas the Amax Gain Magnitude of the composite mapping. As shown in Fig. 3 (b), the Amax Gain\nMagnitude yields extreme values with peaks of 3000, a stark divergence from 1 that confirms\nthe presence of exploding residual streams.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "308", "text": "The first, based on the maximum absolute value of the row sums of the composite\nmapping, captures the worst-case expansion in the forward pass. The second, based on the\nmaximum absolute column sum, corresponds to the backward pass. We refer to these metrics\nas the Amax Gain Magnitude of the composite mapping. As shown in Fig. 3 (b), the Amax Gain\nMagnitude yields extreme values with peaks of 3000, a stark divergence from 1 that confirms\nthe presence of exploding residual streams.\n6\n0\n10000\n20000\n30000\n40000\n50000\nSteps\n-0.002\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\nAbsolute Loss Gap\n(a) Absolute Training Loss Gap vs.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "309", "text": "We refer to these metrics\nas the Amax Gain Magnitude of the composite mapping. As shown in Fig. 3 (b), the Amax Gain\nMagnitude yields extreme values with peaks of 3000, a stark divergence from 1 that confirms\nthe presence of exploding residual streams.\n6\n0\n10000\n20000\n30000\n40000\n50000\nSteps\n-0.002\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\nAbsolute Loss Gap\n(a) Absolute Training Loss Gap vs.  Training Steps\nmHC\nHC\n0\n10000\n20000\n30000\n40000\n50000\nSteps\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nGrad Norm\n(b) Gradient Norm vs.  Training Steps\nmHC\nHC\nFigure 2 | Training Instability of Hyper-Connections (HC).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "310", "text": "Training Steps\nmHC\nHC\n0\n10000\n20000\n30000\n40000\n50000\nSteps\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nGrad Norm\n(b) Gradient Norm vs.  Training Steps\nmHC\nHC\nFigure 2 | Training Instability of Hyper-Connections (HC). This figure illustrates (a) the absolute\nloss gap of HC relative to mHC, and (b) the comparisons of gradient norms. All results are based\non 27B models.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "311", "text": "Training Steps\nmHC\nHC\nFigure 2 | Training Instability of Hyper-Connections (HC). This figure illustrates (a) the absolute\nloss gap of HC relative to mHC, and (b) the comparisons of gradient norms. All results are based\non 27B models.\n0\n10\n20\n30\n40\n50\n60\nLayer Index l\n100\n101\nAmax Gain Magnitude\n(a) Single-Layer Mapping\nHres\nl  Forward Signal Gain\nHres\nl  Backward Gradient Gain\n0\n10\n20\n30\n40\n50\n60\nLayer Index l\n101\n102\n103\n104\n105\nAmax Gain Magnitude\n(b) Composite Mapping\nY\nl\ni = 1Hres\nl + 1 −i Forward Signal Gain\nY\n61 −l\ni = 1 Hres\n61 −i Backward Gradient Gain\nFigure 3 | Propagation Instability of Hyper-Connections (HC).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "312", "text": "This figure illustrates the\npropagation dynamics of (a) the single-layer mapping Hres\n𝑙\nand (b) the composite mapping\nÎ𝐿−𝑙\n𝑖=1 Hres\n𝐿−𝑖within the 27B model. The layer index 𝑙(x-axis) unrolls each standard Transformer\nblock into two independent layers (Attention and FFN). The Amax Gain Magnitude (y-axis) is\ncalculated as the maximum absolute row sum (for the forward signal) and column sum (for the\nbackward gradient), averaged over all tokens in a selected sequence.\n3.2. System Overhead\nWhile the computational complexity of HC remains manageable due to the linearity of the\nadditional mappings, the system-level overhead prevents a non-negligible challenge. Specifically,\nmemory access (I/O) costs often constitute one of the primary bottlenecks in modern model\narchitectures, which is widely referred to as the “memory wall” (Dao et al., 2022).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "313", "text": "3.2. System Overhead\nWhile the computational complexity of HC remains manageable due to the linearity of the\nadditional mappings, the system-level overhead prevents a non-negligible challenge. Specifically,\nmemory access (I/O) costs often constitute one of the primary bottlenecks in modern model\narchitectures, which is widely referred to as the “memory wall” (Dao et al., 2022). This bottleneck\nis frequently overlooked in architectural design, yet it decisively impacts runtime efficiency.\nFocusing on the widely adopted pre-norm Transformer (Vaswani et al., 2017) architecture,\nwe analyze the I/O patterns inherent to HC. Tab. 2 summarizes the per token memory access\noverhead in a single residual layer introduced by the 𝑛-stream residual design. The analysis\nreveals that HC increases the memory access cost by a factor approximately proportional to 𝑛.\nThis excessive I/O demand significantly degrades training throughput without the mitigation of\nfused kernels.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "314", "text": "This bottleneck\nis frequently overlooked in architectural design, yet it decisively impacts runtime efficiency.\nFocusing on the widely adopted pre-norm Transformer (Vaswani et al., 2017) architecture,\nwe analyze the I/O patterns inherent to HC. Tab. 2 summarizes the per token memory access\noverhead in a single residual layer introduced by the 𝑛-stream residual design. The analysis\nreveals that HC increases the memory access cost by a factor approximately proportional to 𝑛.\nThis excessive I/O demand significantly degrades training throughput without the mitigation of\nfused kernels. Besides, since Hpre\n𝑙\n, Hpost\n𝑙\n, and Hres\n𝑙\ninvolve learnable parameters, their interme-\ndiate activations are required for backpropagation. This results in a substantial increase in the\nGPU memory footprint, often necessitating gradient checkpointing to maintain feasible memory\nusage.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "315", "text": "The analysis\nreveals that HC increases the memory access cost by a factor approximately proportional to 𝑛.\nThis excessive I/O demand significantly degrades training throughput without the mitigation of\nfused kernels. Besides, since Hpre\n𝑙\n, Hpost\n𝑙\n, and Hres\n𝑙\ninvolve learnable parameters, their interme-\ndiate activations are required for backpropagation. This results in a substantial increase in the\nGPU memory footprint, often necessitating gradient checkpointing to maintain feasible memory\nusage. Furthermore, HC requires 𝑛-fold more communication cost in pipeline parallelism (Qi\net al., 2024), leading to larger bubbles and decreasing the training throughput.\n7\nTable 2 | Comparison of Memory Access Costs Per Token. This analysis accounts for the\noverhead introduced by the residual stream maintenance in the forward pass, excluding the\ninternal I/O of the layer function F .", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "316", "text": "Method\nOperation\nRead (Elements)\nWrite (Elements)\nResidual\nConnection\nResidual Merge\n2𝐶\n𝐶\nTotal I/O\n2C\nC\nHyper-\nConnections\nCalculate Hpre\n𝑙\n, Hpost\n𝑙\n, Hres\n𝑙\n𝑛𝐶\n𝑛2 + 2𝑛\nHpre\n𝑙\n𝑛𝐶+ 𝑛\n𝐶\nHpost\n𝑙\n𝐶+ 𝑛\n𝑛𝐶\nHres\n𝑙\n𝑛𝐶+ 𝑛2\n𝑛𝐶\nResidual Merge\n2𝑛𝐶\n𝑛𝐶\nTotal I/O\n(5n + 1)C + n2 + 2n\n(3n + 1)C + n2 + 2n\n4. Method\n4.1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "317", "text": "Method\n4.1. Manifold-Constrained Hyper-Connections\nDrawing inspiration from the identity mapping principle (He et al., 2016b), the core premise\nof mHC is to constrain the residual mapping Hres\n𝑙\nonto a specific manifold. While the original\nidentity mapping ensures stability by enforcing Hres\n𝑙\n= I, it fundamentally precludes information\nexchange within the residual stream, which is critical for maximizing the potential of multi-\nstream architectures. Therefore, we propose projecting the residual mapping onto a manifold\nthat simultaneously maintains the stability of signal propagation across layers and facilitates\nmutual interaction among residual streams to preserve the model’s expressivity. To this end,\nwe restrict Hres\n𝑙\nto be a doubly stochastic matrix, which has non-negative entries where both\nthe rows and columns sum to 1. Formally, let Mres denote the manifold of doubly stochastic\nmatrices (also known as the Birkhoff polytope).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "318", "text": "To this end,\nwe restrict Hres\n𝑙\nto be a doubly stochastic matrix, which has non-negative entries where both\nthe rows and columns sum to 1. Formally, let Mres denote the manifold of doubly stochastic\nmatrices (also known as the Birkhoff polytope). We constrain Hres\n𝑙\nto PMres(Hres\n𝑙\n), defined as:\nPMres(Hres\n𝑙\n) ≔\n\b\nHres\n𝑙\n∈R𝑛×𝑛| Hres\n𝑙\n1𝑛= 1𝑛, 1⊤\n𝑛Hres\n𝑙\n= 1⊤\n𝑛, Hres\n𝑙\n⩾0\n\t\n,\n(6)\nwhere 1𝑛represents the 𝑛-dimensional vector of all ones.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "319", "text": "It is worth noting that when 𝑛= 1, the doubly stochastic condition degenerates to the scalar\n1, thereby recovering the original identity mapping. The choice of double stochasticity confers\nseveral rigorous theoretical properties beneficial for large-scale model training:\n1. Norm Preservation: The spectral norm of a doubly stochastic matrix is bounded by 1\n(i.e., ∥Hres\n𝑙\n∥2 ≤1). This implies that the learnable mapping is non-expansive, effectively\nmitigating the gradient explosion problem.\n2. Compositional Closure: The set of doubly stochastic matrices is closed under matrix\nmultiplication. This ensures that the composite residual mapping across multiple layers,\nÎ𝐿−𝑙\n𝑖=1 Hres\n𝐿−𝑖, remains doubly stochastic, thereby preserving stability throughout the entire\ndepth of the model.\n3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "320", "text": "This implies that the learnable mapping is non-expansive, effectively\nmitigating the gradient explosion problem.\n2. Compositional Closure: The set of doubly stochastic matrices is closed under matrix\nmultiplication. This ensures that the composite residual mapping across multiple layers,\nÎ𝐿−𝑙\n𝑖=1 Hres\n𝐿−𝑖, remains doubly stochastic, thereby preserving stability throughout the entire\ndepth of the model.\n3. Geometric Interpretation via the Birkhoff Polytope: The set Mres forms the Birkhoff\npolytope, which is the convex hull of the set of permutation matrices. This provides a\nclear geometric interpretation: the residual mapping acts as a convex combination of\npermutations. Mathematically, the repeated application of such matrices tends to increase\n8\nthe mixing of information across streams monotonically, effectively functioning as a robust\nfeature fusion mechanism.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "321", "text": "3. Geometric Interpretation via the Birkhoff Polytope: The set Mres forms the Birkhoff\npolytope, which is the convex hull of the set of permutation matrices. This provides a\nclear geometric interpretation: the residual mapping acts as a convex combination of\npermutations. Mathematically, the repeated application of such matrices tends to increase\n8\nthe mixing of information across streams monotonically, effectively functioning as a robust\nfeature fusion mechanism.\nAdditionally, we impose non-negativity constraints on the input mappings Hpre\n𝑙\nand output\nmappings Hpost\n𝑙\n. This constrain prevents signal cancellation arising from the composition of\npositive and negative coefficients, which can also be considered as a special manifold projection.\n4.2. Parameterization and Manifold Projection\nIn this section, we detail the calculation process of Hpre\n𝑙\n, Hpost\n𝑙\n, and Hres\n𝑙\nin mHC.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "322", "text": "Additionally, we impose non-negativity constraints on the input mappings Hpre\n𝑙\nand output\nmappings Hpost\n𝑙\n. This constrain prevents signal cancellation arising from the composition of\npositive and negative coefficients, which can also be considered as a special manifold projection.\n4.2. Parameterization and Manifold Projection\nIn this section, we detail the calculation process of Hpre\n𝑙\n, Hpost\n𝑙\n, and Hres\n𝑙\nin mHC. Given the\ninput hidden matrix x𝑙∈R𝑛×𝐶at the 𝑙-th layer, we first flatten it into a vector ®x𝑙= vec(x𝑙) ∈R1×𝑛𝐶\nto preserve full context information. Then,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "323", "text": "Then, we follow the original HC formulation to get the\ndynamic mappings and the static mappings as follows:\n\n\n®x′\n𝑙= RMSNorm(®x𝑙)\n˜Hpre\n𝑙\n= 𝛼pre\n𝑙\n· (®x′\n𝑙𝜑pre\n𝑙\n) + bpre\n𝑙\n˜Hpost\n𝑙\n= 𝛼post\n𝑙\n· (®x′\n𝑙𝜑post\n𝑙\n) + bpost\n𝑙\n˜Hres\n𝑙\n= 𝛼res\n𝑙\n· mat(®x′\n𝑙𝜑res\n𝑙) + bres\n𝑙,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "324", "text": "(7)\nwhere 𝜑pre\n𝑙\n, 𝜑post\n𝑙\n∈R𝑛𝐶×𝑛and 𝜑res\n𝑙\n∈R𝑛𝐶×𝑛2 are linear projections for dynamic mappings and\nmat(·) is a reshape function from R1×𝑛2 to R𝑛×𝑛.\nThen, the final constrained mappings are obtained via:\n\n\nHpre\n𝑙\n= 𝜎( ˜Hpre\n𝑙\n)\nHpost\n𝑙\n= 2𝜎( ˜Hpost\n𝑙\n)\nHres\n𝑙\n= Sinkhorn-Knopp( ˜Hres\n𝑙\n),\n(8)\nwhere 𝜎(·) denotes the Sigmoid function.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "325", "text": "The Sinkhorn-Knopp(·) operator firstly makes all\nelements to be positive via an exponent operator and then conducts iterative normalization\nprocess that alternately rescales rows and columns to sum to 1. Specifically, given a positive\nmatrix M(0) = exp( ˜Hres\n𝑙\n) as the start point, the normalization iteration proceeds as:\nM(𝑡) = T𝑟\n\u0010\nT𝑐(M(𝑡−1))\n\u0011\n,\n(9)\nwhere T𝑟and T𝑐denote row and column normalization, respectively. This process converges to a\ndoubly stochastic matrix Hres\n𝑙\n= M(𝑡max) as 𝑡max →∞. We choose 𝑡max = 20 as a practical value in\nour experiments.\n4.3. Efficient Infrastructure Design\nIn this section, we detail the infrastructure design tailored for mHC.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "326", "text": "This process converges to a\ndoubly stochastic matrix Hres\n𝑙\n= M(𝑡max) as 𝑡max →∞. We choose 𝑡max = 20 as a practical value in\nour experiments.\n4.3. Efficient Infrastructure Design\nIn this section, we detail the infrastructure design tailored for mHC. Through rigorous optimiza-\ntion, we implement mHC (with 𝑛= 4) in large-scale models with a marginal training overhead\nof only 6.7%.\n4.3.1. Kernel Fusion\nObserving that RMSNorm in mHC imposes significant latency when operating on the high-\ndimensional hidden state ®x𝑙∈R1×𝑛𝐶, we reorder the dividing-by-norm operation to follow the\n9\nmatrix multiplication. This optimization maintains mathematical equivalence while improving\nefficiency.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "327", "text": "Through rigorous optimiza-\ntion, we implement mHC (with 𝑛= 4) in large-scale models with a marginal training overhead\nof only 6.7%.\n4.3.1. Kernel Fusion\nObserving that RMSNorm in mHC imposes significant latency when operating on the high-\ndimensional hidden state ®x𝑙∈R1×𝑛𝐶, we reorder the dividing-by-norm operation to follow the\n9\nmatrix multiplication. This optimization maintains mathematical equivalence while improving\nefficiency. Furthermore, we employ mixed-precision strategies to maximize numerical accuracy\nwithout compromising speed, and fuse multiple operations with shared memory access into\nunified compute kernels to reduce memory bandwidth bottlenecks. Based on the inputs and\nparameters detailed in Eq. (10) to (13), we implement three specialized mHC kernels to compute\nHpre\n𝑙\n, Hpost\n𝑙\n, and Hres\n𝑙\n.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "328", "text": "This optimization maintains mathematical equivalence while improving\nefficiency. Furthermore, we employ mixed-precision strategies to maximize numerical accuracy\nwithout compromising speed, and fuse multiple operations with shared memory access into\nunified compute kernels to reduce memory bandwidth bottlenecks. Based on the inputs and\nparameters detailed in Eq. (10) to (13), we implement three specialized mHC kernels to compute\nHpre\n𝑙\n, Hpost\n𝑙\n, and Hres\n𝑙\n. In these kernels, the biases and linear projections are consolidated into b𝑙\nand 𝜑𝑙, and the RMSNorm weight is also absorbed in 𝜑𝑙.\n• Eq. (14) to (15): We develop a unified kernel that fuses two scans on ®x𝑙, leveraging ma-\ntrix multiplication units to maximize memory bandwidth utilization. The backward\npass—comprising two matrix multiplications—is similarly consolidated into a single ker-\nnel, eliminating redundant reloading of ®x𝑙.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "329", "text": "In these kernels, the biases and linear projections are consolidated into b𝑙\nand 𝜑𝑙, and the RMSNorm weight is also absorbed in 𝜑𝑙.\n• Eq. (14) to (15): We develop a unified kernel that fuses two scans on ®x𝑙, leveraging ma-\ntrix multiplication units to maximize memory bandwidth utilization. The backward\npass—comprising two matrix multiplications—is similarly consolidated into a single ker-\nnel, eliminating redundant reloading of ®x𝑙. Both kernels feature a finely tuned pipeline\n(load, cast, compute, store) to efficiently handle mixed-precision processing.\n• Eq. (16) to (18): These lightweight operations on small coefficients are opportunistically\nfused into a single kernel, significantly reducing kernel launch overhead.\n• Eq. (19): We implement the Sinkhorn-Knopp iteration within a single kernel. For the\nbackward pass, we derive a custom backward kernel that recomputes the intermediate\nresults on-chip and traverses the entire iteration.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "330", "text": "Both kernels feature a finely tuned pipeline\n(load, cast, compute, store) to efficiently handle mixed-precision processing.\n• Eq. (16) to (18): These lightweight operations on small coefficients are opportunistically\nfused into a single kernel, significantly reducing kernel launch overhead.\n• Eq. (19): We implement the Sinkhorn-Knopp iteration within a single kernel. For the\nbackward pass, we derive a custom backward kernel that recomputes the intermediate\nresults on-chip and traverses the entire iteration.\n𝜑𝑙: tfloat32\n[𝑛𝐶, 𝑛2 + 2𝑛]\n(10)\n®x𝑙: bfloat16\n[1, 𝑛𝐶]\n(11)\n𝛼pre\n𝑙\n, 𝛼post\n𝑙\n, 𝛼res\n𝑙\n: float32\nScalars\n(12)\nb𝑙: float32\n[1,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "331", "text": "𝜑𝑙: tfloat32\n[𝑛𝐶, 𝑛2 + 2𝑛]\n(10)\n®x𝑙: bfloat16\n[1, 𝑛𝐶]\n(11)\n𝛼pre\n𝑙\n, 𝛼post\n𝑙\n, 𝛼res\n𝑙\n: float32\nScalars\n(12)\nb𝑙: float32\n[1, 𝑛2 + 2𝑛]\n(13)\nh ˜˜Hpre\n𝑙\n, ˜˜Hpost\n𝑙\n, ˜˜Hres\n𝑙\ni\n: float32\n= ®x𝑙𝜑𝑙\n(14)\n𝑟: float32\n=\n\r\r®x𝑙\n\r\r\n2 /\n√\n𝑛𝐶\n(15)\nh\n˜Hpre\n𝑙\n,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "332", "text": "𝑛2 + 2𝑛]\n(13)\nh ˜˜Hpre\n𝑙\n, ˜˜Hpost\n𝑙\n, ˜˜Hres\n𝑙\ni\n: float32\n= ®x𝑙𝜑𝑙\n(14)\n𝑟: float32\n=\n\r\r®x𝑙\n\r\r\n2 /\n√\n𝑛𝐶\n(15)\nh\n˜Hpre\n𝑙\n, ˜Hpost\n𝑙\n, ˜Hres\n𝑙\ni\n: float32\n= 1/𝑟\nh\n𝛼pre\n𝑙\n˜˜Hpre\n𝑙\n, 𝛼post\n𝑙\n˜˜Hpost\n𝑙\n,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "333", "text": "˜Hpost\n𝑙\n, ˜Hres\n𝑙\ni\n: float32\n= 1/𝑟\nh\n𝛼pre\n𝑙\n˜˜Hpre\n𝑙\n, 𝛼post\n𝑙\n˜˜Hpost\n𝑙\n, 𝛼res\n𝑙\n˜˜Hres\n𝑙\ni\n+ b𝑙\n(16)\nHpre\n𝑙\n: float32\n= 𝜎\n\u0010\n˜Hpre\n𝑙\n\u0011\n(17)\nHpost\n𝑙\n: float32\n= 2𝜎\n\u0010\n˜Hpost\n𝑙\n\u0011\n(18)\nHres\n𝑙\n: float32\n= Sinkhorn-Knopp \u0000 ˜Hres\n𝑙\n\u0001\n(19)\nUsing the coefficients derived from the aforementioned kernels,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "334", "text": "we introduce two addi-\ntional kernels to apply these mappings: one for Fpre ≔Hpre\n𝑙\nx𝑙and another for Fpost,res ≔\nHres\n𝑙\nx𝑙+ Hpost ⊤\n𝑙\nF (·, ·). Through fusing the application of Hpost\n𝑙\nand Hres\n𝑙\nwith residual merging,\nwe reduce the number of elements read from (3𝑛+ 1)𝐶to (𝑛+ 1)𝐶and the number of elements\nwritten from 3𝑛𝐶to 𝑛𝐶for this kernel. We efficiently implement the majority of kernels (ex-\ncluding Eq. (14) to (15)) using TileLang (Wang et al., 2025). This framework streamlines the\nimplementation of kernels with complex calculation process and allows us to fully utilize the\nmemory bandwidth with minimal engineering effort.\n4.3.2.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "335", "text": "We efficiently implement the majority of kernels (ex-\ncluding Eq. (14) to (15)) using TileLang (Wang et al., 2025). This framework streamlines the\nimplementation of kernels with complex calculation process and allows us to fully utilize the\nmemory bandwidth with minimal engineering effort.\n4.3.2. Recomputing\nThe 𝑛-stream residual design introduces substantial memory overhead during training. To\nmitigate this, we discard the intermediate activations of the mHC kernels after the forward pass\nand recompute them on-the-fly in the backward pass, through re-executing the mHC kernels\n10\nwithout the heavy layer function F . Consequently, for a block of 𝐿𝑟consecutive layers, we need\nonly store the input x𝑙0 to the first layer. Excluding lightweight coefficients while accounting\nfor the pre-norm with in F , Tab. 3 summarizes the intermediate activations preserved for the\nbackward pass.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "336", "text": "To\nmitigate this, we discard the intermediate activations of the mHC kernels after the forward pass\nand recompute them on-the-fly in the backward pass, through re-executing the mHC kernels\n10\nwithout the heavy layer function F . Consequently, for a block of 𝐿𝑟consecutive layers, we need\nonly store the input x𝑙0 to the first layer. Excluding lightweight coefficients while accounting\nfor the pre-norm with in F , Tab. 3 summarizes the intermediate activations preserved for the\nbackward pass.\nTable 3 | Stored and Recomputed Intermediate Activations We list per token activation pre-\nserved for the backward pass and the transient activation recomputed in 𝐿𝑟consecutive layers.\nLayer 𝑙0 represents the first layer in 𝐿𝑟layers and layer 𝑙is in [𝑙0, 𝑙0 + 𝐿𝑟−1].", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "337", "text": "Layer 𝑙0 represents the first layer in 𝐿𝑟layers and layer 𝑙is in [𝑙0, 𝑙0 + 𝐿𝑟−1].\nActivations\nx𝑙0\nF (Hpre\n𝑙\nx𝑙, W𝑙)\nx𝑙\nHpre\n𝑙\nx𝑙\nRMSNorm(Hpre\n𝑙\nx𝑙)\nSize (Elements)\n𝑛𝐶\n𝐶\n𝑛𝐶\n𝐶\n𝐶\nStored Method\nEvery 𝐿𝑟layers\nEvery layer\nTransient inside 𝐿𝑟layers\nSince mHC kernels recomputation is performed for blocks of 𝐿𝑟consecutive layers, given\na total of 𝐿layers, we must persistently store the first layer input x𝑙0 for all ⌈𝐿\n𝐿𝑟⌉blocks for the\nbackward pass.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "338", "text": "In addition to this resident memory, the recomputation process introduces a\ntransient memory overhead of (𝑛+ 2)𝐶× 𝐿𝑟elements for the active block, which determines the\npeak memory usage during backpropagation. Consequently, we determine the optimal block\nsize 𝐿∗\n𝑟by minimizing the total memory footprint corresponded to 𝐿𝑟:\n𝐿∗\n𝑟= arg min\n𝐿𝑟\n\u0014\n𝑛𝐶×\n\u0018 𝐿\n𝐿𝑟\n\u0019\n+ (𝑛+ 2)𝐶× 𝐿𝑟\n\u0015\n≈\n√︂\n𝑛𝐿\n𝑛+ 2.\n(20)\nFurthermore, pipeline parallelism in large-scale training imposes a constraint: recomputation\nblocks must not cross pipeline stage boundaries.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "339", "text": "(20)\nFurthermore, pipeline parallelism in large-scale training imposes a constraint: recomputation\nblocks must not cross pipeline stage boundaries. Observing that the theoretical optimum 𝐿∗\n𝑟\ntypically aligns with the number of layers per pipeline stage, we choose to synchronize the\nrecomputation boundaries with the pipeline stages.\n4.3.3. Overlapping Communication in DualPipe\nIn large-scale training, pipeline parallelism is the standard practice for mitigating parameter and\ngradient memory footprints. Specifically, we adopt the DualPipe schedule (Liu et al., 2024b),\nwhich effectively overlaps scale-out interconnected communication traffic, such as those in\nexpert and pipeline parallelism. However, compared to the single-stream design, the proposed\n𝑛-stream residual in mHC incurs substantial communication latency across pipeline stages.\nFurthermore, at stage boundaries, the recomputation of mHC kernels for all 𝐿𝑟layers introduces\nnon-negligible computational overhead. To address these bottlenecks, we extend the DualPipe\nschedule (see Fig.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "340", "text": "Specifically, we adopt the DualPipe schedule (Liu et al., 2024b),\nwhich effectively overlaps scale-out interconnected communication traffic, such as those in\nexpert and pipeline parallelism. However, compared to the single-stream design, the proposed\n𝑛-stream residual in mHC incurs substantial communication latency across pipeline stages.\nFurthermore, at stage boundaries, the recomputation of mHC kernels for all 𝐿𝑟layers introduces\nnon-negligible computational overhead. To address these bottlenecks, we extend the DualPipe\nschedule (see Fig. 4) to facilitate improved overlapping of communication and computation at\npipeline stage boundaries.\nNotably, to prevent blocking the communication stream, we execute the Fpost,res kernels\nof MLP (i.e. FFN) layers on a dedicated high-priority compute stream. We further refrain\nfrom employing persistent kernels for long-running operations in attention layers, thereby\npreventing extended stalls.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "341", "text": "To address these bottlenecks, we extend the DualPipe\nschedule (see Fig. 4) to facilitate improved overlapping of communication and computation at\npipeline stage boundaries.\nNotably, to prevent blocking the communication stream, we execute the Fpost,res kernels\nof MLP (i.e. FFN) layers on a dedicated high-priority compute stream. We further refrain\nfrom employing persistent kernels for long-running operations in attention layers, thereby\npreventing extended stalls. This design enables the preemption of overlapped attention com-\nputations, allowing for flexible scheduling while maintaining high utilization of the compute\ndevice’s processing units. Furthermore, the recomputation process is decoupled from pipeline\ncommunication dependencies, as the initial activation of each stage x𝑙0 is already cached locally.\n11\nMLP (B)\nDISPATCH (B)\nDISPATCH (F)\nMLP (W)\nMLP (F)\nATTN (B)\nCOMBINE (F)\nCOMBINE (B)\nPP Send Recv (F)\nPP Send Recv (B)\nℱ!", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "342", "text": "This design enables the preemption of overlapped attention com-\nputations, allowing for flexible scheduling while maintaining high utilization of the compute\ndevice’s processing units. Furthermore, the recomputation process is decoupled from pipeline\ncommunication dependencies, as the initial activation of each stage x𝑙0 is already cached locally.\n11\nMLP (B)\nDISPATCH (B)\nDISPATCH (F)\nMLP (W)\nMLP (F)\nATTN (B)\nCOMBINE (F)\nCOMBINE (B)\nPP Send Recv (F)\nPP Send Recv (B)\nℱ!\"#$, '(#\n)\n(F)\nℱ!\"#$, '(#\n)\n(B)\nℱ!\"#$, '(#\n*\n(B)\nℱ!'(\n) (B)\nATTN (W)\nATTN (F)\nWhole Stage \nRecompute (B)\nℱ!'(\n* (B)\nℱ!'(\n* (F)\nℱ!\"#$, '(#\n*\n(F)\nℱ!", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "343", "text": "\"#$, '(#\n)\n(F)\nℱ!\"#$, '(#\n)\n(B)\nℱ!\"#$, '(#\n*\n(B)\nℱ!'(\n) (B)\nATTN (W)\nATTN (F)\nWhole Stage \nRecompute (B)\nℱ!'(\n* (B)\nℱ!'(\n* (F)\nℱ!\"#$, '(#\n*\n(F)\nℱ!'(\n) (F)\nNormal Compute Stream\nCommunication Stream\nHigh Priority Compute Stream\nFigure 4 | Communication-Computation Overlapping for mHC. We extend the DualPipe\nschedule to handle the overhead introduced by mHC. Lengths of each block are illustrative only\nand do not represent actual duration. (F), (B), (W) refers to forward pass, backward pass, weight\ngradient computation, respectively. F A and F M represents kernels corresponded to Attention\nand MLP, respectively.\n5. Experiments\n5.1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "344", "text": "'(\n) (F)\nNormal Compute Stream\nCommunication Stream\nHigh Priority Compute Stream\nFigure 4 | Communication-Computation Overlapping for mHC. We extend the DualPipe\nschedule to handle the overhead introduced by mHC. Lengths of each block are illustrative only\nand do not represent actual duration. (F), (B), (W) refers to forward pass, backward pass, weight\ngradient computation, respectively. F A and F M represents kernels corresponded to Attention\nand MLP, respectively.\n5. Experiments\n5.1. Experimental Setup\nWe validate the proposed method via language model pre-training, conducting a comparative\nanalysis between the baseline, HC, and our proposed mHC. Utilizing MoE architectures inspired\nby DeepSeek-V3 (Liu et al., 2024b), we train four distinct model variants to cover different\nevaluation regimes. Specifically, the expansion rate 𝑛for both HC and mHC is set to 4.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "345", "text": "F A and F M represents kernels corresponded to Attention\nand MLP, respectively.\n5. Experiments\n5.1. Experimental Setup\nWe validate the proposed method via language model pre-training, conducting a comparative\nanalysis between the baseline, HC, and our proposed mHC. Utilizing MoE architectures inspired\nby DeepSeek-V3 (Liu et al., 2024b), we train four distinct model variants to cover different\nevaluation regimes. Specifically, the expansion rate 𝑛for both HC and mHC is set to 4. Our\nprimary focus is a 27B model trained with a dataset size proportional to its parameters, which\nserves as the subject for our system-level main results. Expanding on this, we analyze the\ncompute scaling behavior by incorporating smaller 3B and 9B models trained with proportional\ndata, which allows us to observe performance trends across varying compute. Additionally,\nto specifically investigate the token scaling behavior, we train a separate 3B model on a fixed\ncorpus of 1 trillion tokens.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "346", "text": "Our\nprimary focus is a 27B model trained with a dataset size proportional to its parameters, which\nserves as the subject for our system-level main results. Expanding on this, we analyze the\ncompute scaling behavior by incorporating smaller 3B and 9B models trained with proportional\ndata, which allows us to observe performance trends across varying compute. Additionally,\nto specifically investigate the token scaling behavior, we train a separate 3B model on a fixed\ncorpus of 1 trillion tokens. Detailed model configurations and training hyper-parameters are\nprovided in Appendix A.1.\n5.2. Main Results\n10000\n20000\n30000\n40000\n50000\nSteps\n-0.06\n-0.04\n-0.02\n0.00\nAbsolute Loss Gap\n(a) Absolute Training Loss Gap vs.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "347", "text": "Additionally,\nto specifically investigate the token scaling behavior, we train a separate 3B model on a fixed\ncorpus of 1 trillion tokens. Detailed model configurations and training hyper-parameters are\nprovided in Appendix A.1.\n5.2. Main Results\n10000\n20000\n30000\n40000\n50000\nSteps\n-0.06\n-0.04\n-0.02\n0.00\nAbsolute Loss Gap\n(a) Absolute Training Loss Gap vs.  Training Steps\nBaseline\nHC\nmHC\n10000\n20000\n30000\n40000\n50000\nSteps\n0.00\n0.05\n0.10\n0.15\n0.20\nGrad Norm\n(b) Gradient Norm vs.  Training Steps\nBaseline\nHC\nmHC\nFigure 5 | Training Stability of Manifold-Constrained Hyper-Connections (mHC). This figure\nillustrates (a) the absolute training loss gap of mHC and HC relative to the baseline, and (b)\nthe gradient norm of the three methods.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "348", "text": "Training Steps\nBaseline\nHC\nmHC\n10000\n20000\n30000\n40000\n50000\nSteps\n0.00\n0.05\n0.10\n0.15\n0.20\nGrad Norm\n(b) Gradient Norm vs.  Training Steps\nBaseline\nHC\nmHC\nFigure 5 | Training Stability of Manifold-Constrained Hyper-Connections (mHC). This figure\nillustrates (a) the absolute training loss gap of mHC and HC relative to the baseline, and (b)\nthe gradient norm of the three methods. All experiments utilize the 27B model. The results\ndemonstrate that mHC exhibits improved stability in terms of both loss and gradient norm.\nWe begin by examining the training stability and convergence of the 27B models. As\nillustrated in Fig. 5 (a), mHC effectively mitigates the training instability observed in HC,\nachieving a final loss reduction of 0.021 compared to the baseline. This improved stability is\nfurther corroborated by the gradient norm analysis in Fig.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "349", "text": "All experiments utilize the 27B model. The results\ndemonstrate that mHC exhibits improved stability in terms of both loss and gradient norm.\nWe begin by examining the training stability and convergence of the 27B models. As\nillustrated in Fig. 5 (a), mHC effectively mitigates the training instability observed in HC,\nachieving a final loss reduction of 0.021 compared to the baseline. This improved stability is\nfurther corroborated by the gradient norm analysis in Fig. 5 (b), where mHC exhibits significantly\nbetter behavior than HC, maintaining a stable profile comparable to the baseline.\n12\nTable 4 | System-level Benchmark Results for 27B Models. This table compares the zero-\nshot and few-shot performance of the Baseline, HC, and mHC across 8 diverse downstream\nbenchmarks. mHC consistently outperforms the Baseline and surpasses HC on the majority of\nbenchmarks, demonstrating its effectiveness in large-scale pre-training.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "350", "text": "This improved stability is\nfurther corroborated by the gradient norm analysis in Fig. 5 (b), where mHC exhibits significantly\nbetter behavior than HC, maintaining a stable profile comparable to the baseline.\n12\nTable 4 | System-level Benchmark Results for 27B Models. This table compares the zero-\nshot and few-shot performance of the Baseline, HC, and mHC across 8 diverse downstream\nbenchmarks. mHC consistently outperforms the Baseline and surpasses HC on the majority of\nbenchmarks, demonstrating its effectiveness in large-scale pre-training.\nBenchmark\nBBH\nDROP\nGSM8K\nHellaSwag\nMATH\nMMLU\nPIQA\nTriviaQA\n(Metric)\n(EM)\n(F1)\n(EM)\n(Acc.)\n(EM)\n(Acc.)\n(Acc.)", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "351", "text": "mHC consistently outperforms the Baseline and surpasses HC on the majority of\nbenchmarks, demonstrating its effectiveness in large-scale pre-training.\nBenchmark\nBBH\nDROP\nGSM8K\nHellaSwag\nMATH\nMMLU\nPIQA\nTriviaQA\n(Metric)\n(EM)\n(F1)\n(EM)\n(Acc.)\n(EM)\n(Acc.)\n(Acc.)\n(EM)\n# Shots\n3-shot\n3-shot\n8-shot\n10-shot\n4-shot\n5-shot\n0-shot\n5-shot\n27B Baseline\n43.8\n47.0\n46.7\n73.7\n22.0\n59.0\n78.5\n54.3\n27B w/ HC\n48.9\n51.6\n53.2\n74.3\n26.4\n63.0\n79.9\n56.3\n27B w/ mHC\n51.0\n53.9\n53.8\n74.7\n26.0\n63.4\n80.5\n57.6\nTab.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "352", "text": "4 presents the downstream performance across a diverse set of benchmarks (Bisk et al.,\n2020; Cobbe et al., 2021; Hendrycks et al., 2020, 2021; Joshi et al., 2017; Zellers et al., 2019). mHC\nyields comprehensive improvements, consistently outperforming the baseline and surpassing\nHC on the majority of tasks. Notably, compared to HC, mHC further enhances the model’s\nreasoning capabilities, delivering performance gains of 2.1% on BBH (Suzgun et al., 2022) and\n2.3% on DROP (Dua et al., 2019).\n5.3.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "353", "text": "5.3. Scaling Experiments\n1021\n1022\nFLOPs\n-0.04\n-0.03\n-0.02\n-0.01\n0.00\n0.01\n0.02\nAbsolute Loss Gap\nBaseline\nmHC\n1021\n1022\nFLOPs\n98.0%\n99.0%\n100.0%\n101.0%\nRelative Loss Ratio\nBaseline\nmHC\n2\n4\nFLOPs\n×1021\n-0.03\n-0.02\n-0.01\n0.00\n0.01\nAbsolute Loss Gap\nBaseline\nmHC\n2\n4\nFLOPs\n×1021\n98.0%\n99.0%\n100.0%\n101.0%\nRelative Loss Ratio\nBaseline\nmHC\n(a) Compute Scaling Curve\n(b) Token Scaling Curve\nFigure 6 | Scaling properties of mHC compared to the Baseline. (a) Compute Scaling Curve.\nSolid lines depict the performance gap across different compute budgets.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "354", "text": "(a) Compute Scaling Curve.\nSolid lines depict the performance gap across different compute budgets. Each point represents\na specific compute-optimal configuration of model size and dataset size, scaling from 3B and 9B\nto 27B parameters. (b) Token Scaling Curve. Trajectory of the 3B model during training. Each\npoint represents the model’s performance at different training tokens. Detailed architectures\nand training configurations are provided in Appendix A.1.\nTo assess the scalability of our approach, we report the relative loss improvement of mHC\nagainst the baseline across different scales. In Fig. 6 (a), we plot the compute scaling curve\nspanning 3B, 9B, and 27B parameters. The trajectory indicates that the performance advantage is\nrobustly maintained even at higher computational budgets, showing only marginal attenuation.\nFurthermore, we examine the within-run dynamics in Fig. 6 (b), which presents the token\nscaling curve for the 3B model. Collectively, these findings validate the effectiveness of mHC\nin large-scale scenarios.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "355", "text": "In Fig. 6 (a), we plot the compute scaling curve\nspanning 3B, 9B, and 27B parameters. The trajectory indicates that the performance advantage is\nrobustly maintained even at higher computational budgets, showing only marginal attenuation.\nFurthermore, we examine the within-run dynamics in Fig. 6 (b), which presents the token\nscaling curve for the 3B model. Collectively, these findings validate the effectiveness of mHC\nin large-scale scenarios. This conclusion is further corroborated by our in-house large-scale\ntraining experiments.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "356", "text": "Collectively, these findings validate the effectiveness of mHC\nin large-scale scenarios. This conclusion is further corroborated by our in-house large-scale\ntraining experiments.\n13\n0\n10\n20\n30\n40\n50\n60\nLayer Index l\n0.0\n0.5\n1.0\n1.5\n2.0\nAmax Gain Magnitude\n(a) Single-Layer Mapping\nPMres(Hres\nl ) Forward Signal Gain\nPMres(Hres\nl ) Backward Gradient Gain\n0\n10\n20\n30\n40\n50\n60\nLayer Index l\n0.0\n0.5\n1.0\n1.5\n2.0\nAmax Gain Magnitude\n(b) Composite Mapping\nY\nl\ni = 1PMres(Hres\nl + 1 −i) Forward Signal Gain\nY\n61 −l\ni = 1 PMres(Hres\n61 −i) Backward Gradient Gain\nFigure 7 | Propagation Stability of Manifold-Constrained Hyper-Connections (mHC).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "357", "text": "This\nfigure illustrates the propagation dynamics of (a) the single-layer mapping PMres(Hres\n𝑙\n) and (b)\nthe composite mapping Î𝐿−𝑙\n𝑖=1 PMres(Hres\n𝐿−𝑖) within the 27B model. The results demonstrate that\nmHC significantly enhances propagation stability compared to HC.\n-6.81 -6.81 -6.81 -6.81\n18.73\n-15.29\n-14.79\n-15.88\n5.43\n4.43\n4.43\n4.43\n-4.07 -3.07 -4.07 -4.07\n-3.95 -3.95 -2.95 -3.95\n-4.22 -4.22 -4.22 -3.22\nHC\nHres\n1\n0.83\n0.73\n0.66\n0.75\n0.84\n0.67\n0.49\n0.96\n0.94 -0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "358", "text": "29\n-14.79\n-15.88\n5.43\n4.43\n4.43\n4.43\n-4.07 -3.07 -4.07 -4.07\n-3.95 -3.95 -2.95 -3.95\n-4.22 -4.22 -4.22 -3.22\nHC\nHres\n1\n0.83\n0.73\n0.66\n0.75\n0.84\n0.67\n0.49\n0.96\n0.94 -0.07 -0.05 0.02\n-0.08 0.89 -0.07 -0.07\n-0.10 -0.14 0.81 -0.07\n0.06\n0.05 -0.03 0.87\nHres\n30\n-11.97 -6.86 -10.23-11.89\n-21.64\n-20.22\n22.50\n-21.59\n-5.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "359", "text": "75\n0.84\n0.67\n0.49\n0.96\n0.94 -0.07 -0.05 0.02\n-0.08 0.89 -0.07 -0.07\n-0.10 -0.14 0.81 -0.07\n0.06\n0.05 -0.03 0.87\nHres\n30\n-11.97 -6.86 -10.23-11.89\n-21.64\n-20.22\n22.50\n-21.59\n-5.58 -3.74 -5.71 -6.60\n-6.06 -2.27 -5.33 -6.57\n6.08\n3.12\n6.53\n6.77\n-6.41 -3.97 -5.72 -5.49\nHres\n60\n1.22\n1.04\n1.06\n1.02\n-1.35\n6.47\n0.03\n-0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "360", "text": "89\n-21.64\n-20.22\n22.50\n-21.59\n-5.58 -3.74 -5.71 -6.60\n-6.06 -2.27 -5.33 -6.57\n6.08\n3.12\n6.53\n6.77\n-6.41 -3.97 -5.72 -5.49\nHres\n60\n1.22\n1.04\n1.06\n1.02\n-1.35\n6.47\n0.03\n-0.81\n-0.38 -0.33 -0.34 -0.31\n1.81\n1.56\n1.58\n1.51\n0.01 -0.00 0.01\n0.01\n-0.23 -0.19 -0.20 -0.19\nY\n30\ni = 1Hres\n31 −i\n-135.4-133.4-489.0273.3\n-251.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "361", "text": "04\n1.06\n1.02\n-1.35\n6.47\n0.03\n-0.81\n-0.38 -0.33 -0.34 -0.31\n1.81\n1.56\n1.58\n1.51\n0.01 -0.00 0.01\n0.01\n-0.23 -0.19 -0.20 -0.19\nY\n30\ni = 1Hres\n31 −i\n-135.4-133.4-489.0273.3\n-251.4\n-243.0\n264.6\n-254.8\n-69.9 -68.3 -255.3142.1\n-69.1 -66.1 -247.4139.6\n74.8\n72.7 268.9-151.8\n-71.2 -71.8 -255.2143.3\nY\n30\ni = 1Hres\n61 −i\n-259.2-219.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "362", "text": "4-133.4-489.0273.3\n-251.4\n-243.0\n264.6\n-254.8\n-69.9 -68.3 -255.3142.1\n-69.1 -66.1 -247.4139.6\n74.8\n72.7 268.9-151.8\n-71.2 -71.8 -255.2143.3\nY\n30\ni = 1Hres\n61 −i\n-259.2-219.1-228.2-221.0\n-475.3\n-462.8\n509.1\n-498.5\n-132.8-112.2-117.0-113.3\n-129.3-109.3-113.9-110.3\n142.3 120.2 125.3 121.3\n-139.3-117.8-122.6-118.7\nY\n60\ni = 1Hres\n61 −i\n0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "363", "text": "2-219.1-228.2-221.0\n-475.3\n-462.8\n509.1\n-498.5\n-132.8-112.2-117.0-113.3\n-129.3-109.3-113.9-110.3\n142.3 120.2 125.3 121.3\n-139.3-117.8-122.6-118.7\nY\n60\ni = 1Hres\n61 −i\n0.98\n1.00\n0.98\n1.04\n1.00\n1.00\n1.00\n1.00\n0.67\n0.09\n0.03\n0.22\n0.26\n0.48\n0.26\n0.00\n0.03\n0.24\n0.00\n0.73\n0.03\n0.20\n0.69\n0.09\nmHC\nPMres(Hres\n1 )\n0.96\n1.02\n1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "364", "text": "98\n1.00\n0.98\n1.04\n1.00\n1.00\n1.00\n1.00\n0.67\n0.09\n0.03\n0.22\n0.26\n0.48\n0.26\n0.00\n0.03\n0.24\n0.00\n0.73\n0.03\n0.20\n0.69\n0.09\nmHC\nPMres(Hres\n1 )\n0.96\n1.02\n1.04\n0.99\n1.00\n1.00\n1.00\n1.00\n0.96\n0.01\n0.00\n0.04\n0.00\n0.97\n0.03\n0.00\n0.00\n0.00\n1.00\n0.00\n0.00\n0.04\n0.01\n0.95\nPMres(Hres\n30 )\n1.00\n1.01\n0.99\n1.00\n1.00\n1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "365", "text": "96\n1.02\n1.04\n0.99\n1.00\n1.00\n1.00\n1.00\n0.96\n0.01\n0.00\n0.04\n0.00\n0.97\n0.03\n0.00\n0.00\n0.00\n1.00\n0.00\n0.00\n0.04\n0.01\n0.95\nPMres(Hres\n30 )\n1.00\n1.01\n0.99\n1.00\n1.00\n1.00\n1.00\n1.00\n0.92\n0.06\n0.01\n0.01\n0.05\n0.81\n0.01\n0.13\n0.00\n0.01\n0.97\n0.02\n0.03\n0.13\n0.00\n0.84\nPMres(Hres\n60 )\n0.90\n1.06\n0.93\n1.11\n1.00\n1.00\n1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "366", "text": "00\n1.01\n0.99\n1.00\n1.00\n1.00\n1.00\n1.00\n0.92\n0.06\n0.01\n0.01\n0.05\n0.81\n0.01\n0.13\n0.00\n0.01\n0.97\n0.02\n0.03\n0.13\n0.00\n0.84\nPMres(Hres\n60 )\n0.90\n1.06\n0.93\n1.11\n1.00\n1.00\n1.00\n1.00\n0.30\n0.25\n0.22\n0.24\n0.24\n0.25\n0.25\n0.26\n0.20\n0.24\n0.28\n0.28\n0.17\n0.32\n0.18\n0.34\nY\n30\ni = 1PMres(Hres\n31 −i)\n0.41\n1.50\n1.50\n0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "367", "text": "90\n1.06\n0.93\n1.11\n1.00\n1.00\n1.00\n1.00\n0.30\n0.25\n0.22\n0.24\n0.24\n0.25\n0.25\n0.26\n0.20\n0.24\n0.28\n0.28\n0.17\n0.32\n0.18\n0.34\nY\n30\ni = 1PMres(Hres\n31 −i)\n0.41\n1.50\n1.50\n0.60\n1.00\n1.00\n1.00\n1.00\n0.35\n0.28\n0.17\n0.20\n0.03\n0.62\n0.29\n0.07\n0.01\n0.17\n0.80\n0.02\n0.02\n0.42\n0.25\n0.31\nY\n30\ni = 1PMres(Hres\n61 −i)\n0.88\n1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "368", "text": "41\n1.50\n1.50\n0.60\n1.00\n1.00\n1.00\n1.00\n0.35\n0.28\n0.17\n0.20\n0.03\n0.62\n0.29\n0.07\n0.01\n0.17\n0.80\n0.02\n0.02\n0.42\n0.25\n0.31\nY\n30\ni = 1PMres(Hres\n61 −i)\n0.88\n1.03\n1.00\n1.11\n1.00\n1.01\n1.01\n1.00\n0.24\n0.26\n0.23\n0.27\n0.23\n0.25\n0.26\n0.27\n0.21\n0.25\n0.27\n0.28\n0.21\n0.27\n0.24\n0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "369", "text": "02\n0.42\n0.25\n0.31\nY\n30\ni = 1PMres(Hres\n61 −i)\n0.88\n1.03\n1.00\n1.11\n1.00\n1.01\n1.01\n1.00\n0.24\n0.26\n0.23\n0.27\n0.23\n0.25\n0.26\n0.27\n0.21\n0.25\n0.27\n0.28\n0.21\n0.27\n0.24\n0.29\nY\n60\ni = 1PMres(Hres\n61 −i)\nFigure 8 | Visualizations of Learnable Mappings. This figure displays representative single-\nlayer and composite mappings for HC (first row) and mHC (second row). Each matrix is\ncomputed by averaging over all tokens within a selected sequence. The labels annotated along\nthe y-axis and x-axis indicate the forward signal gain (row sum) and the backward gradient gain\n(column sum), respectively.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "370", "text": "25\n0.27\n0.28\n0.21\n0.27\n0.24\n0.29\nY\n60\ni = 1PMres(Hres\n61 −i)\nFigure 8 | Visualizations of Learnable Mappings. This figure displays representative single-\nlayer and composite mappings for HC (first row) and mHC (second row). Each matrix is\ncomputed by averaging over all tokens within a selected sequence. The labels annotated along\nthe y-axis and x-axis indicate the forward signal gain (row sum) and the backward gradient gain\n(column sum), respectively.\n5.4. Stability Analysis\nSimilar to Fig. 3, Fig. 7 illustrates the propagation stability of mHC. Ideally, the single-layer\nmapping satisfies the doubly stochastic constraint, implying that both the forward signal gain\nand the backward gradient gain should equal to 1. However, practice implementations utilizing\nthe Sinkhorn-Knopp algorithm must limit the number of iterations to achieve computational\nefficiency.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "371", "text": "The labels annotated along\nthe y-axis and x-axis indicate the forward signal gain (row sum) and the backward gradient gain\n(column sum), respectively.\n5.4. Stability Analysis\nSimilar to Fig. 3, Fig. 7 illustrates the propagation stability of mHC. Ideally, the single-layer\nmapping satisfies the doubly stochastic constraint, implying that both the forward signal gain\nand the backward gradient gain should equal to 1. However, practice implementations utilizing\nthe Sinkhorn-Knopp algorithm must limit the number of iterations to achieve computational\nefficiency. In our settings, we use 20 iterations to obtain an approximate solution. Consequently,\nas shown in Fig. 7(a), the backward gradient gain deviates slightly from 1. In the composite case\nshown in Fig. 7(b), the deviation increases but remains bounded, reaching a maximum value\nof approximately 1.6. Notably, compared to the maximum gain magnitude of nearly 3000 in\nHC, mHC significantly reduces it by three orders of magnitude.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "372", "text": "In our settings, we use 20 iterations to obtain an approximate solution. Consequently,\nas shown in Fig. 7(a), the backward gradient gain deviates slightly from 1. In the composite case\nshown in Fig. 7(b), the deviation increases but remains bounded, reaching a maximum value\nof approximately 1.6. Notably, compared to the maximum gain magnitude of nearly 3000 in\nHC, mHC significantly reduces it by three orders of magnitude. These results demonstrate that\nmHC significantly enhances propagation stability compared to HC, ensuring stable forward\nsignal and backward gradient flows. Additionally, Fig. 8 displays representative mappings. We\nobserve that for HC, when the maximum gain is large, other values also tend to be significant,\nwhich indicates general instability across all propagation paths. In contrast, mHC consistently\nyields stable results.\n14\n6.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "373", "text": "Notably, compared to the maximum gain magnitude of nearly 3000 in\nHC, mHC significantly reduces it by three orders of magnitude. These results demonstrate that\nmHC significantly enhances propagation stability compared to HC, ensuring stable forward\nsignal and backward gradient flows. Additionally, Fig. 8 displays representative mappings. We\nobserve that for HC, when the maximum gain is large, other values also tend to be significant,\nwhich indicates general instability across all propagation paths. In contrast, mHC consistently\nyields stable results.\n14\n6. Conclusion and Outlook\nIn this paper, we identify that while expanding the width of residual stream and diversifying\nconnections yields performance gains as proposed in Hyper-Connections (HC), the uncon-\nstrained nature of these connections leads to signal divergence. This disruption compromises\nthe conservation of signal energy across layers, inducing training instability and hindering the\nscalability of deep networks. To address these challenges, we introduce Manifold-Constrained\nHyper-Connections (mHC), a generalized framework that projects the residual connection space\nonto a specific manifold.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "374", "text": "In contrast, mHC consistently\nyields stable results.\n14\n6. Conclusion and Outlook\nIn this paper, we identify that while expanding the width of residual stream and diversifying\nconnections yields performance gains as proposed in Hyper-Connections (HC), the uncon-\nstrained nature of these connections leads to signal divergence. This disruption compromises\nthe conservation of signal energy across layers, inducing training instability and hindering the\nscalability of deep networks. To address these challenges, we introduce Manifold-Constrained\nHyper-Connections (mHC), a generalized framework that projects the residual connection space\nonto a specific manifold. By employing the Sinkhorn-Knopp algorithm to enforce a doubly\nstochastic constraint on residual mappings, mHC transforms signal propagation into a convex\ncombination of features. Empirical results confirm that mHC effectively restores the identity\nmapping property, enabling stable large-scale training with superior scalability compared to\nconventional HC. Crucially, through efficient infrastructure-level optimizations, mHC delivers\nthese improvements with negligible computational overhead.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "375", "text": "To address these challenges, we introduce Manifold-Constrained\nHyper-Connections (mHC), a generalized framework that projects the residual connection space\nonto a specific manifold. By employing the Sinkhorn-Knopp algorithm to enforce a doubly\nstochastic constraint on residual mappings, mHC transforms signal propagation into a convex\ncombination of features. Empirical results confirm that mHC effectively restores the identity\nmapping property, enabling stable large-scale training with superior scalability compared to\nconventional HC. Crucially, through efficient infrastructure-level optimizations, mHC delivers\nthese improvements with negligible computational overhead.\nAs a generalized extension of the HC paradigm, mHC opens several promising avenues for\nfuture research. Although this work utilizes doubly stochastic matrices to ensure stability, the\nframework accommodates the exploration of diverse manifold constraints tailored to specific\nlearning objectives. We anticipate that further investigation into distinct geometric constraints\ncould yield novel methods that better optimize the trade-off between plasticity and stability.\nFurthermore, we hope mHC rejuvenates community interest in macro-architecture design.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "376", "text": "Crucially, through efficient infrastructure-level optimizations, mHC delivers\nthese improvements with negligible computational overhead.\nAs a generalized extension of the HC paradigm, mHC opens several promising avenues for\nfuture research. Although this work utilizes doubly stochastic matrices to ensure stability, the\nframework accommodates the exploration of diverse manifold constraints tailored to specific\nlearning objectives. We anticipate that further investigation into distinct geometric constraints\ncould yield novel methods that better optimize the trade-off between plasticity and stability.\nFurthermore, we hope mHC rejuvenates community interest in macro-architecture design.\nBy deepening the understanding of how topological structures influence optimization and\nrepresentation learning, mHC will help address current limitations and potentially illuminate\nnew pathways for the evolution of next-generation foundational architectures.\nReferences\nJ. Ainslie, J. Lee-Thorp, M. De Jong, Y. Zemlyanskiy, F. Lebrón, and S. Sanghai. Gqa: Training\ngeneralized multi-query transformer models from multi-head checkpoints. arXiv preprint\narXiv:2305.13245, 2023.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "377", "text": "By deepening the understanding of how topological structures influence optimization and\nrepresentation learning, mHC will help address current limitations and potentially illuminate\nnew pathways for the evolution of next-generation foundational architectures.\nReferences\nJ. Ainslie, J. Lee-Thorp, M. De Jong, Y. Zemlyanskiy, F. Lebrón, and S. Sanghai. Gqa: Training\ngeneralized multi-query transformer models from multi-head checkpoints. arXiv preprint\narXiv:2305.13245, 2023.\nY. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi. PIQA: reasoning about physical commonsense\nin natural language.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "378", "text": "References\nJ. Ainslie, J. Lee-Thorp, M. De Jong, Y. Zemlyanskiy, F. Lebrón, and S. Sanghai. Gqa: Training\ngeneralized multi-query transformer models from multi-head checkpoints. arXiv preprint\narXiv:2305.13245, 2023.\nY. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi. PIQA: reasoning about physical commonsense\nin natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI\n2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI\n2020, New York, NY, USA, February 7-12, 2020, pages 7432–7439. AAAI Press, 2020. doi:\n10.1609/aaai.v34i05.6239.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "379", "text": "PIQA: reasoning about physical commonsense\nin natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI\n2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI\n2020, New York, NY, USA, February 7-12, 2020, pages 7432–7439. AAAI Press, 2020. doi:\n10.1609/aaai.v34i05.6239. URL https://doi.org/10.1609/aaai.v34i05.6239.\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural\ninformation processing systems, 33:1877–1901, 2020.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "380", "text": "URL https://doi.org/10.1609/aaai.v34i05.6239.\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural\ninformation processing systems, 33:1877–1901, 2020.\nY. Chai, S. Jin, and X. Hou. Highway transformer: Self-gating enhanced self-attentive networks.\nIn D. Jurafsky, J. Chai, N. Schluter, and J. Tetreault, editors, Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics, pages 6887–6900, Online, July\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.616.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "381", "text": "Y. Chai, S. Jin, and X. Hou. Highway transformer: Self-gating enhanced self-attentive networks.\nIn D. Jurafsky, J. Chai, N. Schluter, and J. Tetreault, editors, Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics, pages 6887–6900, Online, July\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.616. URL\nhttps://aclanthology.org/2020.acl-main.616/.\nF. Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition, pages 1251–1258, 2017.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "382", "text": "Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.616. URL\nhttps://aclanthology.org/2020.acl-main.616/.\nF. Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition, pages 1251–1258, 2017.\n15\nK. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek,\nJ. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint\narXiv:2110.14168, 2021.\nT. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. Ré. FlashAttention: Fast and memory-efficient\nexact attention with IO-awareness.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "383", "text": "15\nK. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek,\nJ. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint\narXiv:2110.14168, 2021.\nT. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. Ré. FlashAttention: Fast and memory-efficient\nexact attention with IO-awareness. In Advances in Neural Information Processing Systems\n(NeurIPS), 2022.\nD. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner. DROP: A reading compre-\nhension benchmark requiring discrete reasoning over paragraphs.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "384", "text": "arXiv preprint\narXiv:2110.14168, 2021.\nT. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. Ré. FlashAttention: Fast and memory-efficient\nexact attention with IO-awareness. In Advances in Neural Information Processing Systems\n(NeurIPS), 2022.\nD. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner. DROP: A reading compre-\nhension benchmark requiring discrete reasoning over paragraphs. In J. Burstein, C. Doran, and\nT. Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, NAACL-HLT\n2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 2368–\n2378.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "385", "text": "DROP: A reading compre-\nhension benchmark requiring discrete reasoning over paragraphs. In J. Burstein, C. Doran, and\nT. Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, NAACL-HLT\n2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 2368–\n2378. Association for Computational Linguistics, 2019. doi: 10.18653/V1/N19-1246. URL\nhttps://doi.org/10.18653/v1/n19-1246.\nY. Fang, Y. CAI, J. Chen, J. Zhao, G. Tian, and G. Li. Cross-layer retrospective retrieving via layer\nattention. In The Eleventh International Conference on Learning Representations, 2023. URL\nhttps://openreview.net/forum?id=pvgEL1yS3Ql.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "386", "text": "Association for Computational Linguistics, 2019. doi: 10.18653/V1/N19-1246. URL\nhttps://doi.org/10.18653/v1/n19-1246.\nY. Fang, Y. CAI, J. Chen, J. Zhao, G. Tian, and G. Li. Cross-layer retrospective retrieving via layer\nattention. In The Eleventh International Conference on Learning Representations, 2023. URL\nhttps://openreview.net/forum?id=pvgEL1yS3Ql.\nW. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models\nwith simple and efficient sparsity. Journal of Machine Learning Research, 23(120):1–39, 2022.\nK. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings\nof the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016a.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "387", "text": "URL\nhttps://openreview.net/forum?id=pvgEL1yS3Ql.\nW. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models\nwith simple and efficient sparsity. Journal of Machine Learning Research, 23(120):1–39, 2022.\nK. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings\nof the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016a.\nK. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In European\nconference on computer vision, pages 630–645. Springer, 2016b.\nM. Heddes, A. Javanmard, K. Axiotis, G. Fu, M. Bateni, and V. Mirrokni. Deepcrossattention:\nSupercharging transformer residual connections.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "388", "text": "In Proceedings\nof the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016a.\nK. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In European\nconference on computer vision, pages 630–645. Springer, 2016b.\nM. Heddes, A. Javanmard, K. Axiotis, G. Fu, M. Bateni, and V. Mirrokni. Deepcrossattention:\nSupercharging transformer residual connections. In Forty-second International Conference\non Machine Learning, 2025. URL https://openreview.net/forum?id=j3JBfFnGYh.\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring\nmassive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "389", "text": "Deepcrossattention:\nSupercharging transformer residual connections. In Forty-second International Conference\non Machine Learning, 2025. URL https://openreview.net/forum?id=j3JBfFnGYh.\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring\nmassive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\nD. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Mea-\nsuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874,\n2021.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "390", "text": "Measuring\nmassive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\nD. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Mea-\nsuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874,\n2021.\nJ. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas,\nL. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche,\nB. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, O. Vinyals, J. Rae, and L. Sifre.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "391", "text": "J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas,\nL. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche,\nB. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, O. Vinyals, J. Rae, and L. Sifre.\nAn empirical analysis of compute-optimal large language model training. In S. Koyejo,\nS. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural\nInformation Processing Systems, volume 35, pages 30016–30030. Curran Associates, Inc., 2022.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "392", "text": "An empirical analysis of compute-optimal large language model training. In S. Koyejo,\nS. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural\nInformation Processing Systems, volume 35, pages 30016–30030. Curran Associates, Inc., 2022.\nURL https://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faf\nf6f588870935f114ebe04a3e5-Paper-Conference.pdf.\nG. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional\nnetworks. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 4700–4708, 2017.\n16\nM. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. TriviaQA: A large scale distantly supervised chal-\nlenge dataset for reading comprehension.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "393", "text": "G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional\nnetworks. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 4700–4708, 2017.\n16\nM. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. TriviaQA: A large scale distantly supervised chal-\nlenge dataset for reading comprehension. In R. Barzilay and M.-Y. Kan, editors, Proceedings of\nthe 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 1601–1611, Vancouver, Canada, July 2017. Association for Computational\nLinguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147.\nG. Larsson, M. Maire, and G. Shakhnarovich.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "394", "text": "In R. Barzilay and M.-Y. Kan, editors, Proceedings of\nthe 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 1601–1611, Vancouver, Canada, July 2017. Association for Computational\nLinguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147.\nG. Larsson, M. Maire, and G. Shakhnarovich. Fractalnet: Ultra-deep neural networks without\nresiduals. arXiv preprint arXiv:1605.07648, 2016.\nD. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen.\nGshard: Scaling giant models with conditional computation and automatic sharding.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "395", "text": "URL https://aclanthology.org/P17-1147.\nG. Larsson, M. Maire, and G. Shakhnarovich. Fractalnet: Ultra-deep neural networks without\nresiduals. arXiv preprint arXiv:1605.07648, 2016.\nD. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen.\nGshard: Scaling giant models with conditional computation and automatic sharding. arXiv\npreprint arXiv:2006.16668, 2020.\nA. Liu, B. Feng, B. Wang, B. Wang, B. Liu, C. Zhao, C. Dengr, C. Ruan, D. Dai, D. Guo, et al.\nDeepseek-v2: A strong, economical, and efficient mixture-of-experts language model.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "396", "text": "Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv\npreprint arXiv:2006.16668, 2020.\nA. Liu, B. Feng, B. Wang, B. Wang, B. Liu, C. Zhao, C. Dengr, C. Ruan, D. Dai, D. Guo, et al.\nDeepseek-v2: A strong, economical, and efficient mixture-of-experts language model. arXiv\npreprint arXiv:2405.04434, 2024a.\nA. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, et al.\nDeepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024b.\nI. Loshchilov and F. Hutter.\nDecoupled weight decay regularization.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "397", "text": "arXiv\npreprint arXiv:2405.04434, 2024a.\nA. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, et al.\nDeepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024b.\nI. Loshchilov and F. Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017.\nB. Mak and J. Flanigan. Residual matrix transformers: Scaling the size of the residual stream.\narXiv preprint arXiv:2506.22696, 2025.\nG. Menghani, R. Kumar, and S. Kumar.\nLAurel: Learned augmented residual layer.\nIn\nForty-second International Conference on Machine Learning, 2025.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "398", "text": "I. Loshchilov and F. Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017.\nB. Mak and J. Flanigan. Residual matrix transformers: Scaling the size of the residual stream.\narXiv preprint arXiv:2506.22696, 2025.\nG. Menghani, R. Kumar, and S. Kumar.\nLAurel: Learned augmented residual layer.\nIn\nForty-second International Conference on Machine Learning, 2025. URL https://open\nreview.net/forum?id=rUDRWP9WvZ.\nM. Pagliardini, A. Mohtashami, F. Fleuret, and M. Jaggi. Denseformer: Enhancing information\nflow in transformers via depth weighted averaging. In The Thirty-eighth Annual Conference\non Neural Information Processing Systems, 2024. URL https://openreview.net/forum\n?id=kMnoh7CXrq.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "399", "text": "LAurel: Learned augmented residual layer.\nIn\nForty-second International Conference on Machine Learning, 2025. URL https://open\nreview.net/forum?id=rUDRWP9WvZ.\nM. Pagliardini, A. Mohtashami, F. Fleuret, and M. Jaggi. Denseformer: Enhancing information\nflow in transformers via depth weighted averaging. In The Thirty-eighth Annual Conference\non Neural Information Processing Systems, 2024. URL https://openreview.net/forum\n?id=kMnoh7CXrq.\nP. Qi, X. Wan, G. Huang, and M. Lin. Zero bubble (almost) pipeline parallelism. In The Twelfth\nInternational Conference on Learning Representations, 2024. URL https://openreview\n.net/forum?id=tuzTN0eIO5.\nN. Shazeer.\nFast transformer decoding: One write-head is all you need.\narXiv preprint\narXiv:1911.02150, 2019.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "400", "text": "URL https://openreview.net/forum\n?id=kMnoh7CXrq.\nP. Qi, X. Wan, G. Huang, and M. Lin. Zero bubble (almost) pipeline parallelism. In The Twelfth\nInternational Conference on Learning Representations, 2024. URL https://openreview\n.net/forum?id=tuzTN0eIO5.\nN. Shazeer.\nFast transformer decoding: One write-head is all you need.\narXiv preprint\narXiv:1911.02150, 2019.\nN. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean.\nOutra-\ngeously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint\narXiv:1701.06538, 2017.\nR. Sinkhorn and P. Knopp. Concerning nonnegative matrices and doubly stochastic matrices.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "401", "text": "arXiv preprint\narXiv:1911.02150, 2019.\nN. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean.\nOutra-\ngeously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint\narXiv:1701.06538, 2017.\nR. Sinkhorn and P. Knopp. Concerning nonnegative matrices and doubly stochastic matrices.\nPacific Journal of Mathematics, 21(2):343–348, 1967.\nR. K. Srivastava, K. Greff, and J. Schmidhuber. Training very deep networks. In C. Cortes,\nN. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information\nProcessing Systems, volume 28. Curran Associates, Inc., 2015. URL https://proceedings.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "402", "text": "R. Sinkhorn and P. Knopp. Concerning nonnegative matrices and doubly stochastic matrices.\nPacific Journal of Mathematics, 21(2):343–348, 1967.\nR. K. Srivastava, K. Greff, and J. Schmidhuber. Training very deep networks. In C. Cortes,\nN. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information\nProcessing Systems, volume 28. Curran Associates, Inc., 2015. URL https://proceedings.\nneurips.cc/paper_files/paper/2015/file/215a71a12769b056c3c32e7299f1c5e\nd-Paper.pdf.\n17\nJ. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu. Roformer: Enhanced transformer with rotary\nposition embedding. Neurocomputing, 568:127063, 2024.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "403", "text": "Curran Associates, Inc., 2015. URL https://proceedings.\nneurips.cc/paper_files/paper/2015/file/215a71a12769b056c3c32e7299f1c5e\nd-Paper.pdf.\n17\nJ. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu. Roformer: Enhanced transformer with rotary\nposition embedding. Neurocomputing, 568:127063, 2024.\nM. Suzgun, N. Scales, N. Schärli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le,\nE. H. Chi, D. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve\nthem. arXiv preprint arXiv:2210.09261, 2022.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "404", "text": "Roformer: Enhanced transformer with rotary\nposition embedding. Neurocomputing, 568:127063, 2024.\nM. Suzgun, N. Scales, N. Schärli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le,\nE. H. Chi, D. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve\nthem. arXiv preprint arXiv:2210.09261, 2022.\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal,\nE. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv\npreprint arXiv:2302.13971, 2023.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "405", "text": "arXiv preprint arXiv:2210.09261, 2022.\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal,\nE. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv\npreprint arXiv:2302.13971, 2023.\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polo-\nsukhin. Attention is all you need. Advances in neural information processing systems, 30,\n2017.\nL. Wang, H. Gao, C. Zhao, X. Sun, and D. Dai. Auxiliary-loss-free load balancing strategy for\nmixture-of-experts.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "406", "text": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polo-\nsukhin. Attention is all you need. Advances in neural information processing systems, 30,\n2017.\nL. Wang, H. Gao, C. Zhao, X. Sun, and D. Dai. Auxiliary-loss-free load balancing strategy for\nmixture-of-experts. arXiv preprint arXiv:2408.15664, 2024.\nL. Wang, Y. Cheng, Y. Shi, Z. Tang, Z. Mo, W. Xie, L. Ma, Y. Xia, J. Xue, F. Yang, et al. Tilelang: A\ncomposable tiled programming model for ai systems. arXiv preprint arXiv:2504.17577, 2025.\nD. Xiao, Q. Meng, S. Li, and X. Yuan.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "407", "text": "arXiv preprint arXiv:2408.15664, 2024.\nL. Wang, Y. Cheng, Y. Shi, Z. Tang, Z. Mo, W. Xie, L. Ma, Y. Xia, J. Xue, F. Yang, et al. Tilelang: A\ncomposable tiled programming model for ai systems. arXiv preprint arXiv:2504.17577, 2025.\nD. Xiao, Q. Meng, S. Li, and X. Yuan. Muddformer: Breaking residual bottlenecks in transformers\nvia multiway dynamic dense connections. arXiv preprint arXiv:2502.12170, 2025.\nS. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He. Aggregated residual transformations for deep\nneural networks. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 1492–1500, 2017.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "408", "text": "D. Xiao, Q. Meng, S. Li, and X. Yuan. Muddformer: Breaking residual bottlenecks in transformers\nvia multiway dynamic dense connections. arXiv preprint arXiv:2502.12170, 2025.\nS. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He. Aggregated residual transformations for deep\nneural networks. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 1492–1500, 2017.\nS. Xie, H. Zhang, J. Guo, X. Tan, J. Bian, H. H. Awadalla, A. Menezes, T. Qin, and R. Yan. Residual:\nTransformer with dual residual connections, 2023. URL https://arxiv.org/abs/2304.1\n4802.\nF. Yu, D. Wang, E. Shelhamer, and T. Darrell. Deep layer aggregation.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "409", "text": "S. Xie, H. Zhang, J. Guo, X. Tan, J. Bian, H. H. Awadalla, A. Menezes, T. Qin, and R. Yan. Residual:\nTransformer with dual residual connections, 2023. URL https://arxiv.org/abs/2304.1\n4802.\nF. Yu, D. Wang, E. Shelhamer, and T. Darrell. Deep layer aggregation. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 2403–2412, 2018.\nR. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can a machine really finish\nyour sentence?", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "410", "text": "Residual:\nTransformer with dual residual connections, 2023. URL https://arxiv.org/abs/2304.1\n4802.\nF. Yu, D. Wang, E. Shelhamer, and T. Darrell. Deep layer aggregation. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 2403–2412, 2018.\nR. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can a machine really finish\nyour sentence? In A. Korhonen, D. R. Traum, and L. Màrquez, editors, Proceedings of the 57th\nConference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers, pages 4791–4800. Association for Computational\nLinguistics, 2019. doi: 10.18653/v1/p19-1472.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "411", "text": "HellaSwag: Can a machine really finish\nyour sentence? In A. Korhonen, D. R. Traum, and L. Màrquez, editors, Proceedings of the 57th\nConference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers, pages 4791–4800. Association for Computational\nLinguistics, 2019. doi: 10.18653/v1/p19-1472. URL https://doi.org/10.18653/v1/p1\n9-1472.\nB. Zhang and R. Sennrich.\nRoot mean square layer normalization.\nAdvances in neural\ninformation processing systems, 32, 2019.\nD. Zhu, H. Huang, Z. Huang, Y. Zeng, Y. Mao, B. Wu, Q. Min, and X. Zhou. Hyper-connections.\narXiv preprint arXiv:2409.19606, 2024.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "412", "text": "URL https://doi.org/10.18653/v1/p1\n9-1472.\nB. Zhang and R. Sennrich.\nRoot mean square layer normalization.\nAdvances in neural\ninformation processing systems, 32, 2019.\nD. Zhu, H. Huang, Z. Huang, Y. Zeng, Y. Mao, B. Wu, Q. Min, and X. Zhou. Hyper-connections.\narXiv preprint arXiv:2409.19606, 2024.\n18\nA. Appendix\nA.1. Detailed Model Specifications and Hyper-parameters.\nTable 5 | Detailed Model Specifications and Hyper-parameters. This table presents the architec-\ntural configurations for the 3B, 9B, and 27B models based on the DeepSeek-V3 (Liu et al., 2024b)\narchitecture. It outlines the specific hyper-parameters for mHC and HC, including the residual\nstream expansion and Sinkhorn-Knopp settings, alongside the optimization and training proto-\ncols used in the experiments.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "413", "text": "18\nA. Appendix\nA.1. Detailed Model Specifications and Hyper-parameters.\nTable 5 | Detailed Model Specifications and Hyper-parameters. This table presents the architec-\ntural configurations for the 3B, 9B, and 27B models based on the DeepSeek-V3 (Liu et al., 2024b)\narchitecture. It outlines the specific hyper-parameters for mHC and HC, including the residual\nstream expansion and Sinkhorn-Knopp settings, alongside the optimization and training proto-\ncols used in the experiments.\nAttribute\n3B\n9B\n27B\n3B\n1T Tokens\nVocab Params\n331M\n496M\n662M\n331M\nActive Params\n612M\n1.66B\n4.14B\n612M\nTotal Params\n2.97B\n9.18B\n27.0B\n2.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "414", "text": "It outlines the specific hyper-parameters for mHC and HC, including the residual\nstream expansion and Sinkhorn-Knopp settings, alongside the optimization and training proto-\ncols used in the experiments.\nAttribute\n3B\n9B\n27B\n3B\n1T Tokens\nVocab Params\n331M\n496M\n662M\n331M\nActive Params\n612M\n1.66B\n4.14B\n612M\nTotal Params\n2.97B\n9.18B\n27.0B\n2.97B\nLayers\n12\n18\n30\n12\nLeading Dense Layers\n1\n1\nRouted Experts\n64\n64\n72\n64\nActive Experts\n6\n6\nShared Experts\n2\n2\nDimension\n1280\n1920\n2560\n1280\nFFN Dimension\n896\n1280\n1536\n896\nLoad Balancing Method\nLoss-Free (Wang et al.,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "415", "text": "66B\n4.14B\n612M\nTotal Params\n2.97B\n9.18B\n27.0B\n2.97B\nLayers\n12\n18\n30\n12\nLeading Dense Layers\n1\n1\nRouted Experts\n64\n64\n72\n64\nActive Experts\n6\n6\nShared Experts\n2\n2\nDimension\n1280\n1920\n2560\n1280\nFFN Dimension\n896\n1280\n1536\n896\nLoad Balancing Method\nLoss-Free (Wang et al., 2024)\nLoss-Free\nAttention Heads\n16\n24\n32\n16\nAttention Dimension\n128\n128\nAttention Variant\nMLA (Liu et al., 2024a)\nMLA\nKV Rank\n512\n512\nPosition Embedding\nRoPE (Su et al., 2024)\nRoPE\nRoPE Dimension\n64\n64\nRoPE 𝜃\n10000\n10000\nLayer Norm Type\nRMSNorm (Zhang and Sennrich,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "416", "text": ", 2024)\nLoss-Free\nAttention Heads\n16\n24\n32\n16\nAttention Dimension\n128\n128\nAttention Variant\nMLA (Liu et al., 2024a)\nMLA\nKV Rank\n512\n512\nPosition Embedding\nRoPE (Su et al., 2024)\nRoPE\nRoPE Dimension\n64\n64\nRoPE 𝜃\n10000\n10000\nLayer Norm Type\nRMSNorm (Zhang and Sennrich, 2019)\nRMSNorm\nLayer Norm 𝜀\n1e-20\n1e-20\nmHC/HC Expansion Rate 𝑛\n4\n4\nmHC/HC Gating Factor Init 𝛼\n0.01\n0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "417", "text": ", 2024a)\nMLA\nKV Rank\n512\n512\nPosition Embedding\nRoPE (Su et al., 2024)\nRoPE\nRoPE Dimension\n64\n64\nRoPE 𝜃\n10000\n10000\nLayer Norm Type\nRMSNorm (Zhang and Sennrich, 2019)\nRMSNorm\nLayer Norm 𝜀\n1e-20\n1e-20\nmHC/HC Expansion Rate 𝑛\n4\n4\nmHC/HC Gating Factor Init 𝛼\n0.01\n0.01\nmHC Sinkhorn-Knopp 𝑡max\n20\n20\nSequence Length\n4096\n4096\nVocab Size\n129280\n129280\nBatch Size\n320\n512\n1280\n2560\nTraining Steps\n30000\n50000\n50000\n100000\nTraining Tokens\n39.3B\n105B\n262B\n1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "418", "text": "01\n0.01\nmHC Sinkhorn-Knopp 𝑡max\n20\n20\nSequence Length\n4096\n4096\nVocab Size\n129280\n129280\nBatch Size\n320\n512\n1280\n2560\nTraining Steps\n30000\n50000\n50000\n100000\nTraining Tokens\n39.3B\n105B\n262B\n1.05T\nWarmup Steps\n2000\n2000\nOptimizer\nAdamW (Loshchilov and Hutter, 2017)\nAdamW\nAdamW Betas\n(0.9, 0.95)\n(0.9, 0.95)\nAdamW 𝜀\n1e-20\n1e-20\nBase Learning Rate\n8.6e-4\n5.9e-4\n4.0e-4\n9.0e-4\nLr Scheduler\nStep\nStep\nLr Decay Step Ratio\n[0.8 ×, 0.9 ×]\n[0.8 ×, 0.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "419", "text": "2017)\nAdamW\nAdamW Betas\n(0.9, 0.95)\n(0.9, 0.95)\nAdamW 𝜀\n1e-20\n1e-20\nBase Learning Rate\n8.6e-4\n5.9e-4\n4.0e-4\n9.0e-4\nLr Scheduler\nStep\nStep\nLr Decay Step Ratio\n[0.8 ×, 0.9 ×]\n[0.8 ×, 0.9 ×]\nLr Decay Rate\n[0.316, 0.1]\n[0.316, 0.1]\nWeight Decay\n0.1\n0.1\n19", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/2512.24880v1.pdf", "file_name": ""}}
{"id": "420", "text": "Key Position\nQuery Position\nScale 1: Fine-Grained\n(Local Syntax / Adj-Noun)\nKey Position\nScale 2: Medium-Grained\n(Clause-Level / Local Context)\nKey Position\nScale 3: Coarse-Grained\n(Document Themes / Global)", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/Figure5_Attention.pdf", "file_name": ""}}
{"id": "421", "text": "2\n3\n4\n5\n6\nNumber of Scales (L)\n84.0\n84.5\n85.0\n85.5\n86.0\n86.5\n87.0\nMNLI Accuracy (%)\nPeak Accuracy\n(L=4, Acc=86.0)\nLoss of\nGranularity\nNoise from\nDownsampling", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/Figure4_Scales.pdf", "file_name": ""}}
{"id": "422", "text": "1K\n2K\n3K\n4K\n5K\n6K\n7K\n8K\nSequence Length (N)\n0\n10\n20\n30\n40\n50\n60\n70\n80\nAttention FLOPs (Millions)\n81% Reduction\n(13.6M FLOPs Gap)\n16.8M\n3.2M\nFigure 3. Attention FLOPs vs Sequence Length\nStandard MHA (O(n2))\nMAHA (Ours, \nO(n))", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/Figure3_Attention_FLOPs.pdf", "file_name": ""}}
{"id": "423", "text": "# Multiscale Aggregated Hierarchical Attention (MAHA)\r\n\r\n<div align=\"center\">\r\n\r\n![License](https://github.com/canererden/MAHA-Project/releases)\r\n![Python](https://github.com/canererden/MAHA-Project/releases%2B-green)\r\n![PyTorch](https://github.com/canererden/MAHA-Project/releases%2B-orange)\r\n[!", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/README.md", "file_name": "README.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "424", "text": "[License](https://github.com/canererden/MAHA-Project/releases)\r\n![Python](https://github.com/canererden/MAHA-Project/releases%2B-green)\r\n![PyTorch](https://github.com/canererden/MAHA-Project/releases%2B-orange)\r\n[![arXiv](https://github.com/canererden/MAHA-Project/releases)](https://github.com/canererden/MAHA-Project/releases)\r\n\r\n**A Game-Theoretic and Optimization-Driven Approach to Efficient Contextual Modeling in Large Language Models**\r\n\r\n[**Read the Paper**](https://github.com/canererden/MAHA-Project/releases)\r\n\r\n</div>\r\n\r\n---\r\n\r\n## Abstract\r\n\r\nWe propose **MAHA**, a novel attention mechanism that reformulates multi-head self-attention through **hierarchical multiscale decomposition** and mathematically rigorous aggregation (**Convex Optimization** & **Nash Equilibrium**).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/README.md", "file_name": "README.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "425", "text": "Standard attention mechanisms suffer from quadratic complexity $O(N^2)$. MAHA addresses this by dynamically partitioning the sequence into hierarchical scales and aggregating them using optimization solvers. The result is a framework that achieves **sub-quadratic complexity** and superior long-range dependency modeling compared to standard Transformers, specifically optimized for high-throughput inference.\r\n\r\n## Architecture\r\n\r\nMAHA replaces the standard Multi-Head Attention layer with a hierarchical processing block.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/README.md", "file_name": "README.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "426", "text": "```mermaid\r\ngraph TD;\r\n    Input[Input Sequence X] --> Decomp[Hierarchical Decomposition];\r\n    Decomp -->|Scale 0| Attn0[Attention S0];\r\n    Decomp -->|Scale 1| Attn1[Attention S1];\r\n    Decomp -->|Scale 2| Attn2[Attention S2];\r\n    Attn0 --> Upsample[Upsampling];\r\n    Attn1 --> Upsample;\r\n    Attn2 --> Upsample;\r\n    Upsample --> Agg{Optimization Aggregator};\r\n    Agg -->|Convex / Nash| Output[Aggregated Context];\r\n    \r\n    style Agg fill:#f9f,stroke:#333,stroke-width:2px\r\n    style Decomp fill:#bbf,stroke:#333,stroke-width:2px\r\n\r\n```\r\n\r\n## Key Features\r\n\r\n**Hierarchical Decomposition:** Uses learnable Strided Convolutions to create multiscale representations (scales l=1..L), reducing effective sequence length geometrically.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/README.md", "file_name": "README.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "427", "text": "**Shared Value Projection:** Decouples Query/Key projections while sharing Value projections across scales, significantly reducing parameter count.\r\n** Optimization-Driven Aggregation:**\r\n**`convex` strategy:** Solves a constrained L1-regularized optimization problem to weigh scales.\r\n**`nash` strategy:** Simulates a non-cooperative game where scales compete to minimize reconstruction error (Best-Response Dynamics).\r\n\r\n\r\n**Hybrid Design:** Integrates Dilated Convolutional blocks for local feature extraction prior to attention.\r\n\r\n## Performance\r\n\r\nMAHA demonstrates superior efficiency on long-sequence tasks (e.g., PG-19) compared to standard baselines.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/README.md", "file_name": "README.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "428", "text": "| Model | Complexity | PG-19 (PPL) $\\downarrow$ | Memory Usage $\\downarrow$ |\r\n| --- | --- | --- | --- |\r\n| Standard Transformer | O(N^2) | 24.3 | 15.2 GB |\r\n| Longformer | O(N) | 23.8 | 9.1 GB |\r\n| **MAHA (Ours)** | **Sub-Quadratic** | **23.1** | **6.7 GB** |\r\n\r\n## Installation\r\n```bash\r\n# Clone the repository\r\ngit clone [https://github.com/canererden/MAHA-Project/releases](https://github.com/canererden/MAHA-Project/releases)\r\ncd MAHA-Project\r\n\r\n# Install dependencies\r\npip install -r https://github.com/canererden/MAHA-Project/releases\r\n\r\n```\r\n\r\n*Note: For the Convex Optimization solver, `cvxpylayers` is required.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/README.md", "file_name": "README.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "429", "text": "*\r\n\r\n## Usage\r\n\r\n### Quick Start\r\nYou can use `MAHABlock` as a drop-in replacement for standard attention layers or use the full `MAHATransformer` model.\r\n\r\n```python\r\nimport torch\r\nfrom https://github.com/canererden/MAHA-Project/releases import MAHATransformer\r\n\r\n# Initialize Model with Convex Aggregation\r\nmodel = MAHATransformer(\r\n    vocab_size=30000,\r\n    max_len=4096,        # Long context support\r\n    d_model=768,\r\n    num_heads=12,\r\n    num_scales=4,        # L=4 scales (e.g., 4096, 2048, 1024, 512)\r\n    aggregation_strategy='convex' # or 'nash'\r\n)\r\n\r\n# Move to GPU\r\ndevice = https://github.com/canererden/MAHA-Project/releases(\"cuda\" if https://github.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/README.md", "file_name": "README.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "430", "text": "max_len=4096,        # Long context support\r\n    d_model=768,\r\n    num_heads=12,\r\n    num_scales=4,        # L=4 scales (e.g., 4096, 2048, 1024, 512)\r\n    aggregation_strategy='convex' # or 'nash'\r\n)\r\n\r\n# Move to GPU\r\ndevice = https://github.com/canererden/MAHA-Project/releases(\"cuda\" if https://github.com/canererden/MAHA-Project/releases() else \"cpu\")\r\nhttps://github.com/canererden/MAHA-Project/releases(device)\r\n\r\n# Forward Pass\r\ndummy_input = https://github.com/canererden/MAHA-Project/releases(0, 30000, (1, 4096)).to(device)\r\noutput, aux_loss = model(dummy_input)\r\n\r\nprint(f\"Output Shape: {https://github.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/README.md", "file_name": "README.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "431", "text": "com/canererden/MAHA-Project/releases(\"cuda\" if https://github.com/canererden/MAHA-Project/releases() else \"cpu\")\r\nhttps://github.com/canererden/MAHA-Project/releases(device)\r\n\r\n# Forward Pass\r\ndummy_input = https://github.com/canererden/MAHA-Project/releases(0, 30000, (1, 4096)).to(device)\r\noutput, aux_loss = model(dummy_input)\r\n\r\nprint(f\"Output Shape: {https://github.com/canererden/MAHA-Project/releases}\")  # (1, 4096, 768)\r\n\r\n```\r\n\r\n### Running Experiments\r\n\r\nTo replicate the training runs from the paper:\r\n\r\n```bash\r\n# Train on synthetic data or configured dataset\r\npython https://github.com/canererden/MAHA-Project/releases --config https://github.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/README.md", "file_name": "README.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "432", "text": "com/canererden/MAHA-Project/releases(0, 30000, (1, 4096)).to(device)\r\noutput, aux_loss = model(dummy_input)\r\n\r\nprint(f\"Output Shape: {https://github.com/canererden/MAHA-Project/releases}\")  # (1, 4096, 768)\r\n\r\n```\r\n\r\n### Running Experiments\r\n\r\nTo replicate the training runs from the paper:\r\n\r\n```bash\r\n# Train on synthetic data or configured dataset\r\npython https://github.com/canererden/MAHA-Project/releases --config https://github.com/canererden/MAHA-Project/releases\r\n\r\n# Run Unit Tests\r\npython -m unittest discover tests/\r\n\r\n```\r\n\r\n## Directory Structure\r\n```text\r\nmaha-project/\r\n├── configs/             # Hyperparameter configurations (YAML)\r\n├── src/\r\n│   ├── layers/          # Core MAHA layers (Decomposition, Attention,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/README.md", "file_name": "README.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "433", "text": "768)\r\n\r\n```\r\n\r\n### Running Experiments\r\n\r\nTo replicate the training runs from the paper:\r\n\r\n```bash\r\n# Train on synthetic data or configured dataset\r\npython https://github.com/canererden/MAHA-Project/releases --config https://github.com/canererden/MAHA-Project/releases\r\n\r\n# Run Unit Tests\r\npython -m unittest discover tests/\r\n\r\n```\r\n\r\n## Directory Structure\r\n```text\r\nmaha-project/\r\n├── configs/             # Hyperparameter configurations (YAML)\r\n├── src/\r\n│   ├── layers/          # Core MAHA layers (Decomposition, Attention, Aggregation)\r\n│   ├── models/          # MAHABlock and Transformer architecture\r\n│   ├── optimization/    # Differentiable solvers (Convex & Game Theory)\r\n│   └── utils/           # Metrics and helpers\r\n├── tests/               # Unit tests for tensor shapes and gradients\r\n├── https://github.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/README.md", "file_name": "README.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "434", "text": "Attention, Aggregation)\r\n│   ├── models/          # MAHABlock and Transformer architecture\r\n│   ├── optimization/    # Differentiable solvers (Convex & Game Theory)\r\n│   └── utils/           # Metrics and helpers\r\n├── tests/               # Unit tests for tensor shapes and gradients\r\n├── https://github.com/canererden/MAHA-Project/releases             # Main training loop\r\n└── https://github.com/canererden/MAHA-Project/releases     # Dependencies\r\n\r\n```\r\n\r\n# Citation\r\nIf you use this code or our results in your research, please cite our work using the persistent **Zenodo DOI**:\r\n\r\n```bibtex\r\n@article{erden2025maha,\r\n  title={Multiscale Aggregated Hierarchical Attention (MAHA): A Game Theoretic and Optimization Driven Approach to Efficient Contextual Modeling in Large Language Models},\r\n  author={Erden, Caner},", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/README.md", "file_name": "README.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "435", "text": "com/canererden/MAHA-Project/releases     # Dependencies\r\n\r\n```\r\n\r\n# Citation\r\nIf you use this code or our results in your research, please cite our work using the persistent **Zenodo DOI**:\r\n\r\n```bibtex\r\n@article{erden2025maha,\r\n  title={Multiscale Aggregated Hierarchical Attention (MAHA): A Game Theoretic and Optimization Driven Approach to Efficient Contextual Modeling in Large Language Models},\r\n  author={Erden, Caner},\r\n  journal={arXiv preprint arXiv:2512.14925},\r\n  year={2025},\r\n  url={https://github.com/canererden/MAHA-Project/releases}\r\n}\r\n\r\n```\r\n\r\n## License\r\nThis project is licensed under the MIT License - see the [LICENSE](https://github.com/canererden/MAHA-Project/releases) file for details.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/README.md", "file_name": "README.md", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "436", "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nsns.set_context(\"paper\", font_scale=1.4)\nsns.set_style(\"whitegrid\", {'grid.linestyle': '--'})\nplt.rcParams['font.family'] = 'serif'\nplt.rcParams['font.serif'] = ['Times New Roman', 'DejaVu Serif', 'serif']\nplt.rcParams['axes.linewidth'] = 1.5\n\ndef generate_table2():\n    print(\"\\n📊 Tablo 2 (Aggregation Impact) Oluşturuluyor...\")\n    \n    data = {\n        \"Method\": [\"Convex Optimization (CO)\", \"Nash Equilibrium (NE)\", \"Mean Aggregation\"],\n        \"MNLI (Acc)\": [86.0, 85.8, 85.2],\n        \"Memory (GB)\": [6.7, 6.9, 7.2],\n        \"Training Speed\": [\"1.0x (Baseline)\", \"0.9x (Slower)\", \"1.1x (Faster)\"]\n    }\n    \n    df = pd.DataFrame(data)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"TABLE 2: AGGREGATION METHOD IMPACT\")\n    print(\"=\"*60)\n    print(df.to_markdown(index=False))\n    \n    print(\"\\n[LaTeX Format]\")\n    print(df.to_latex(index=False, caption=\"Aggregation Method Impact\", label=\"tab:ablation_agg\"))\n    \n    df.to_csv(\"table2_ablation.csv\", index=False)", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/analysis.ipynb", "file_name": "analysis.ipynb", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "437", "text": "def plot_figure4_scales():\n    print(\"📊 Figure 4: Accuracy vs Number of Scales çiziliyor...\")\n    \n    scales = [2, 3, 4, 5, 6]\n    accuracies = [84.5, 85.4, 86.0, 85.3, 84.8] \n    \n    fig, ax = plt.subplots(figsize=(8, 5), dpi=300)\n    \n    # Çizgi ve Noktalar\n    ax.plot(scales, accuracies, marker='o', markersize=10, linewidth=3, color='#1f77b4', linestyle='-')\n    \n    # Zirve Noktası (L=4) Vurgusu\n    ax.annotate(f'Peak Accuracy\\n(L=4, Acc=86.0)', \n                xy=(4, 86.0), xytext=(4, 86.5),\n                arrowprops=dict(facecolor='black', shrink=0.05),\n                ha='center', fontsize=12, fontweight='bold')\n\n    # Düşüş Açıklamaları\n    ax.text(2.1, 84.6, \"Loss of\\nGranularity\", fontsize=10, color='red', ha='left')\n    ax.text(5.9, 84.9, \"Noise from\\nDownsampling\", fontsize=10, color='red', ha='right')", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/analysis.ipynb", "file_name": "analysis.ipynb", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "438", "text": "# Eksenler\n    ax.set_xlabel('Number of Scales ($L$)', fontsize=14)\n    ax.set_ylabel('MNLI Accuracy (%)', fontsize=14)\n    #ax.set_title('Figure 4. Accuracy vs Number of Scales', y=-0.2, fontstyle='italic')\n    \n    ax.set_xticks(scales)\n    ax.set_ylim(84.0, 87.0)\n    \n    plt.tight_layout()\n    plt.savefig(\"Figure4_Scales.png\", dpi=300, bbox_inches='tight')\n    plt.savefig(\"Figure4_Scales.pdf\", format='pdf', bbox_inches='tight')\n    print(\"✅ Figure 4 kaydedildi: Figure4_Scales.png\")\n\ndef analyze_downsampling_c():\n    \"\"\"Part C: Downsampling Operator Choice\"\"\"\n    print(\"\\n📊 Part C: Downsampling Operator Choice...\")\n    # Metindeki \"1.2% accuracy difference\" verisi\n    diff = 1.2\n    print(f\"-> Convolutional downsampling outperforms pooling by {diff}% on average.\")\n    print(\"-> Reason: Preserves positional information (local connectivity).\")\n\nif __name__ == \"__main__\":\n    generate_table2()\n    plot_figure4_scales()\n    analyze_downsampling_c()\n\n#", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/analysis.ipynb", "file_name": "analysis.ipynb", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "439", "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# --- 1. Akademik Grafik Ayarları ---\nsns.set_context(\"paper\", font_scale=1.5)\nplt.rcParams['font.family'] = 'serif'\nplt.rcParams['font.serif'] = ['Times New Roman', 'DejaVu Serif', 'serif']\n\ndef generate_attention_patterns():\n    print(\"📊 Figure 5: Multiscale Attention Patterns görselleştiriliyor...\")\n\n    # --- 2. Sentetik Attention Verisi Oluşturma ---\n    \n    seq_len = 20\n    \n    # A. Fine Scale (Local Syntax): Diagonal Focus (Adjective-Noun)\n    # Tokenlerin sadece komşularına (i-1, i+1) baktığı yapı\n    fine_attn = np.eye(seq_len) * 0.5 \n    for i in range(seq_len-1):\n        fine_attn[i, i+1] = 0.25\n        fine_attn[i+1, i] = 0.25\n    # Biraz gürültü ekle\n    fine_attn += np.random.rand(seq_len, seq_len) * 0.05\n    \n    # B. Medium Scale (Clause-Level): Block Focus\n    # Cümleciklerin kendi içine odaklandığı blok yapılar\n    medium_attn = np.zeros((seq_len, seq_len))\n    block_size = 5\n    for i in range(0, seq_len, block_size):\n        medium_attn[i:i+block_size, i:i+block_size] = 0.8\n    medium_attn += np.random.rand(seq_len, seq_len) * 0.1", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/analysis.ipynb", "file_name": "analysis.ipynb", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "440", "text": "# C. Coarse Scale (Document Themes): Global/Vertical Focus\n    coarse_attn = np.random.rand(seq_len, seq_len) * 0.1\n    coarse_attn[:, 5] += 0.8  # Key token 1\n    coarse_attn[:, 15] += 0.8 # Key token 2\n    \n    # --- 3. Çizim ---\n    fig, axes = plt.subplots(1, 3, figsize=(18, 5.5), dpi=300)\n    \n    maps = [\n        (fine_attn, \"Scale 1: Fine-Grained\\n(Local Syntax / Adj-Noun)\"),\n        (medium_attn, \"Scale 2: Medium-Grained\\n(Clause-Level / Local Context)\"),\n        (coarse_attn, \"Scale 3: Coarse-Grained\\n(Document Themes / Global)\")\n    ]\n    \n    for i, (data, title) in enumerate(maps):\n        ax = axes[i]\n        sns.heatmap(data, ax=ax, cmap=\"Blues\", cbar=False, square=True,\n                   xticklabels=False, yticklabels=False)\n        \n        ax.set_title(title, fontsize=14, pad=15, fontweight='bold')\n        ax.set_xlabel(\"Key Position\", fontsize=12)\n        if i == 0:\n            ax.set_ylabel(\"Query Position\", fontsize=12)\n            \n        # Çerçeve ekle\n        for _, spine in ax.spines.items():\n            spine.set_visible(True)\n            spine.set_linewidth(1)\n\n    #plt.suptitle(\"Figure 5. Visualization of Learned Multiscale Attention Patterns\", \n    #             y=0.98, fontsize=16, fontstyle='italic')\n    \n    plt.tight_layout()\n    plt.savefig(\"Figure5_Attention.png\", bbox_inches='tight')\n    plt.savefig(\"Figure5_Attention.pdf\", format='pdf', bbox_inches='tight')\n    print(\"✅ Figure 5 kaydedildi: Figure5_Attention.png\")\n\nif __name__ == \"__main__\":\n    generate_attention_patterns()\n\n# In[ ]:", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/analysis.ipynb", "file_name": "analysis.ipynb", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "441", "text": "# Ablation Settings for Nash Equilibrium\r\naggregation: 'nash'", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/configs/ablation_nash.yaml", "file_name": "ablation_nash.yaml", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "442", "text": "# Default Hyperparameters for MAHA\r\nL: 4\r\nr: 2\r\nd_model: 768", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/configs/default_maha.yaml", "file_name": "default_maha.yaml", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "443", "text": "# Evaluation Script", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/evaluate.py", "file_name": "evaluate.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "444", "text": "torch>=2.0.0\r\nnumpy\r\npandas\r\nscikit-learn\r\ntransformers\r\ncvxpylayers", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/requirements.txt", "file_name": "requirements.txt", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "445", "text": "import torch\r\nimport torch.nn as nn\r\nimport time\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\nimport numpy as np\r\nimport os\r\n\r\n# CUDA Hatalarını Yakalamak için Debug Ortamı\r\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\r\n\r\nfrom src.models.transformer import MAHATransformer\r\n\r\n# Cihaz seçimi\r\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\nprint(f\"🚀 Deneyler şu cihazda çalıştırılıyor: {device}\")\r\n\r\ndef benchmark_efficiency():\r\n    \"\"\"\r\n    DENEY 1: Hesaplama Verimliliği (Computational Efficiency)\r\n    \"\"\"\r\n    print(\"\\n🧪 DENEY 1: Verimlilik Testi (Standard Attention vs MAHA)...\")\r\n    \r\n    # Not: Bellek hatası alırsanız 2048'i çıkarın\r\n    seq_lengths = [128, 256, 512, 1024, 2048] \r\n    d_model = 256\r\n    num_heads = 4\r\n    batch_size = 4  \r\n    \r\n    results = []\r\n\r\n    for seq_len in seq_lengths:\r\n        # --- 1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/run_experiments.py", "file_name": "run_experiments.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "446", "text": "Standard Transformer (Baseline) ---\r\n        try:\r\n            baseline = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads).to(device)\r\n            dummy_input = torch.randn(seq_len, batch_size, d_model).to(device) \r\n            \r\n            # Warmup\r\n            _ = baseline(dummy_input, dummy_input, dummy_input)\r\n            \r\n            # Timing\r\n            torch.cuda.reset_peak_memory_stats()\r\n            start = time.time()\r\n            with torch.no_grad():\r\n                for _ in range(20): # Tekrar sayısı düşürüldü (Hız için)\r\n                    _ = baseline(dummy_input, dummy_input, dummy_input)\r\n            torch.cuda.synchronize()\r\n            end = time.time()\r\n            \r\n            baseline_time = (end - start) / 20\r\n            baseline_mem = torch.cuda.max_memory_allocated() / (1024 ** 2) # MB\r\n            \r\n            results.append({\r\n                \"Model\": \"Standard MHA\",\r\n                \"Seq_Len\": seq_len,\r\n                \"Time (ms)\": baseline_time * 1000,\r\n                \"Memory (MB)\": baseline_mem\r\n            })\r\n            \r\n            del baseline, dummy_input\r\n            torch.cuda.empty_cache()\r\n            \r\n        except Exception as e:\r\n            print(f\"   ⚠️ Hata (Baseline - {seq_len}): {e}\")\r\n\r\n        # --- 2. MAHA Transformer ---\r\n        try:\r\n            # KRİTİK DÜZELTME: max_len parametresi artırıldı\r\n            maha_model = MAHATransformer(\r\n                vocab_size=100, \r\n                d_model=d_model, \r\n                num_heads=num_heads, \r\n                num_scales=4,\r\n                aggregation_strategy='convex',\r\n                max_len=5000  # <--- BURASI DÜZELTİLDİ (seq_len 2048'i kapsıyor)\r\n            ).to(device)\r\n            \r\n            dummy_ids = torch.randint(0, 100, (batch_size, seq_len)).to(device)\r\n            \r\n            # Warmup\r\n            _ = maha_model(dummy_ids)\r\n            \r\n            # Timing\r\n            torch.cuda.reset_peak_memory_stats()\r\n            start = time.time()\r\n            with torch.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/run_experiments.py", "file_name": "run_experiments.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "447", "text": "seq_len)).to(device)\r\n            \r\n            # Warmup\r\n            _ = maha_model(dummy_ids)\r\n            \r\n            # Timing\r\n            torch.cuda.reset_peak_memory_stats()\r\n            start = time.time()\r\n            with torch.no_grad():\r\n                for _ in range(20):\r\n                    _ = maha_model(dummy_ids)\r\n            torch.cuda.synchronize()\r\n            end = time.time()\r\n            \r\n            maha_time = (end - start) / 20\r\n            maha_mem = torch.cuda.max_memory_allocated() / (1024 ** 2)\r\n            \r\n            results.append({\r\n                \"Model\": \"MAHA (Ours)\",\r\n                \"Seq_Len\": seq_len,\r\n                \"Time (ms)\": maha_time * 1000,\r\n                \"Memory (MB)\": maha_mem\r\n            })\r\n            \r\n            del maha_model, dummy_ids\r\n            torch.cuda.empty_cache()\r\n            \r\n        except Exception as e:\r\n            print(f\"   ⚠️ Hata (MAHA - {seq_len}): {e}\")\r\n\r\n        print(f\"   -> Tamamlandı: Seq Len {seq_len}\")\r\n\r\n    return pd.DataFrame(results)\r\n\r\ndef ablation_study():\r\n    \"\"\"\r\n    DENEY 2: Ablation Study (Convex vs Nash)\r\n    \"\"\"\r\n    print(\"\\n🧪 DENEY 2: Ablation Study (Convex vs Nash)...\")\r\n    \r\n    strategies = ['convex', 'nash']\r\n    loss_history = {s: [] for s in strategies}\r\n    \r\n    vocab_size = 500\r\n    seq_len = 64\r\n    d_model = 64\r\n    epochs = 5\r\n    \r\n    train_data = torch.randint(0, vocab_size, (100, seq_len)).to(device)\r\n    targets = torch.randint(0, vocab_size, (100, seq_len)).to(device)\r\n    \r\n    criterion = nn.CrossEntropyLoss()\r\n    \r\n    for strategy in strategies:\r\n        print(f\"   -> Strateji Eğitiliyor: {strategy.upper()}\")\r\n        try:\r\n            model = MAHATransformer(\r\n                vocab_size=vocab_size, \r\n                d_model=d_model,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/run_experiments.py", "file_name": "run_experiments.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "448", "text": "upper()}\")\r\n        try:\r\n            model = MAHATransformer(\r\n                vocab_size=vocab_size, \r\n                d_model=d_model, \r\n                num_heads=4, \r\n                num_scales=3,\r\n                aggregation_strategy=strategy,\r\n                num_layers=1,\r\n                max_len=512 # Yeterli\r\n            ).to(device)\r\n            \r\n            optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\r\n            model.train()\r\n            \r\n            for epoch in range(epochs):\r\n                batch_loss = 0\r\n                for i in range(0, 100, 10):\r\n                    bx = train_data[i:i+10]\r\n                    by = targets[i:i+10]\r\n                    \r\n                    optimizer.zero_grad()\r\n                    out, aux_loss = model(bx)\r\n                    \r\n                    # Projeksiyon (Linear classifier yoksa manuel yap)\r\n                    # MAHATransformer içinde classifier tanımlı varsayıyoruz\r\n                    if hasattr(model, 'classifier'):\r\n                         logits = model.classifier(out)\r\n                    else:\r\n                         # Fallback\r\n                         logits = nn.Linear(d_model, vocab_size).to(device)(out)\r\n\r\n                    main_loss = criterion(logits.view(-1, vocab_size), by.view(-1))\r\n                    \r\n                    # Nash bazen aux_loss'u tensor(0.) döndürür, shape hatası olmasın\r\n                    total_loss = main_loss + 0.1 * aux_loss\r\n                    \r\n                    total_loss.backward()\r\n                    optimizer.step()\r\n                    \r\n                    batch_loss += total_loss.item()\r\n                \r\n                loss_history[strategy].append(batch_loss / 10)\r\n                \r\n            del model\r\n            torch.cuda.empty_cache()\r\n        \r\n        except Exception as e:\r\n            print(f\"   ⚠️ Hata ({strategy}): {e}\")\r\n            \r\n    return loss_history\r\n\r\ndef plot_results(df_eff, loss_hist):\r\n    \"\"\"Grafikleri Çizer\"\"\"\r\n    sns.set_style(\"whitegrid\")\r\n    \r\n    # 1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/run_experiments.py", "file_name": "run_experiments.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "449", "text": "Time Complexity\r\n    if df_eff is not None and not df_eff.empty:\r\n        plt.figure(figsize=(12, 5))\r\n        plt.subplot(1, 2, 1)\r\n        sns.lineplot(data=df_eff, x=\"Seq_Len\", y=\"Time (ms)\", hue=\"Model\", marker=\"o\", linewidth=2.5)\r\n        plt.title(\"Computational Time vs Sequence Length\")\r\n        plt.ylabel(\"Inference Time (ms)\")\r\n        plt.xlabel(\"Sequence Length (n)\")\r\n        \r\n        # 2. Memory Usage\r\n        plt.subplot(1, 2, 2)\r\n        sns.barplot(data=df_eff, x=\"Seq_Len\", y=\"Memory (MB)\", hue=\"Model\")\r\n        plt.title(\"Peak Memory Usage vs Sequence Length\")\r\n        plt.ylabel(\"Memory (MB)\")\r\n        \r\n        plt.tight_layout()\r\n        plt.savefig(\"experiment_efficiency.png\", dpi=300)\r\n        print(\"\\n✅ Verimlilik Grafiği Kaydedildi: experiment_efficiency.png\")\r\n    else:\r\n        print(\"⚠️ Verimlilik verisi boş, grafik çizilemedi.\")\r\n\r\n    # 3. Ablation Loss\r\n    if loss_hist:\r\n        plt.figure(figsize=(8, 5))\r\n        for strat, losses in loss_hist.items():\r\n            if losses: # Liste boş değilse\r\n                plt.plot(losses, label=f\"Strategy: {strat}\", marker='s')\r\n        plt.title(\"Training Convergence: Convex vs Nash\")\r\n        plt.xlabel(\"Epochs\")\r\n        plt.ylabel(\"Loss\")\r\n        plt.legend()\r\n        plt.grid(True)\r\n        plt.savefig(\"experiment_ablation.png\", dpi=300)\r\n        plt.savefig(\"experiment_ablation.pdf\")\r\n        print(\"✅ Ablasyon Grafiği Kaydedildi: experiment_ablation.png\")\r\n\r\nif __name__ == \"__main__\":\r\n    # Önceki hatalardan kalan context'i temizlemek gerekebilir.\r\n    # Terminali kapatıp açmanız en iyisidir, ama kod içinde try-except var.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/run_experiments.py", "file_name": "run_experiments.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "450", "text": "# Terminali kapatıp açmanız en iyisidir, ama kod içinde try-except var.\r\n    \r\n    df_results = benchmark_efficiency()\r\n    if df_results is not None:\r\n        print(\"\\n--- Verimlilik Sonuçları ---\")\r\n        print(df_results)\r\n\r\n    loss_history = ablation_study()\r\n    \r\n    plot_results(df_results, loss_history)", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/run_experiments.py", "file_name": "run_experiments.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "453", "text": "import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom typing import List, Tuple, Literal\r\n\r\nclass OptimizationDrivenAggregation(nn.Module):\r\n    \"\"\"\r\n    Implements Optimization-Driven Aggregation (Section 4.3 of MAHA paper).\r\n    \r\n    Strategies:\r\n    1. Convex Optimization (Eq 9): Learns weights w s.t. sum(w)=1, w>=0 with L1 sparsity.\r\n    2. Nash Equilibrium (Eq 10): Iterative best-response dynamics to find equilibrium weights.\r\n    \r\n    Includes Nearest-Neighbor Upsampling (Eq 13) to align scales.\r\n    \"\"\"\r\n    \r\n    def __init__(\r\n        self, \r\n        num_scales: int, \r\n        d_model: int,\r\n        strategy: Literal['convex', 'nash'] = 'convex',\r\n        nash_iterations: int = 3,\r\n        lambda_sparsity: float = 0.1\r\n    ):\r\n        super().__init__()\r\n        self.num_scales = num_scales\r\n        self.strategy = strategy\r\n        self.nash_iterations = nash_iterations\r\n        self.lambda_sparsity = lambda_sparsity\r\n        \r\n        # Learnable weights for Convex strategy\r\n        # Initialized to be uniform (1/L) after softmax\r\n        self.convex_logits = nn.Parameter(torch.zeros(num_scales))\r\n        \r\n        # Linear projection for Nash utility estimation (optional but improves stability)\r\n        self.utility_proj = nn.Linear(d_model, 1) if strategy == 'nash' else None\r\n\r\n    def _upsample(self, tensor: torch.Tensor, target_len: int) -> torch.Tensor:\r\n        \"\"\"\r\n        Nearest-Neighbor Upsampling (Eq 13).", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/layers/aggregation.py", "file_name": "aggregation.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "454", "text": "Input: (B, N_l, D) -> Output: (B, N_target, D)\r\n        \"\"\"\r\n        # Permute for interpolate: (B, D, N)\r\n        tensor_p = tensor.transpose(1, 2)\r\n        \r\n        # Apply Nearest Neighbor interpolation\r\n        upsampled = F.interpolate(\r\n            tensor_p, \r\n            size=target_len, \r\n            mode='nearest'\r\n        )\r\n        \r\n        # Permute back: (B, N, D)\r\n        return upsampled.transpose(1, 2)\r\n\r\n    def solve_convex(self, outputs: List[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:\r\n        \"\"\"\r\n        Implements Convex Optimization aggregation.\r\n        Returns: (Aggregated_Tensor, Sparsity_Loss)\r\n        \"\"\"\r\n        # Enforce constraints: sum(w)=1, w>=0 via Softmax\r\n        weights = F.softmax(self.convex_logits, dim=0)\r\n        \r\n        # Calculate L1 sparsity loss (Eq 9 penalty term)\r\n        # We want weights to be sparse (some close to 0)\r\n        sparsity_loss = self.lambda_sparsity * torch.norm(weights, p=1)\r\n        \r\n        # Weighted Sum\r\n        # Ensure base tensor is on correct device and shape\r\n        final_output = torch.zeros_like(outputs[0])\r\n        for i, out_tensor in enumerate(outputs):\r\n            final_output += weights[i] * out_tensor\r\n            \r\n        return final_output, sparsity_loss\r\n\r\n    def solve_nash(self, outputs: List[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:\r\n        \"\"\"\r\n        Implements Nash Equilibrium aggregation via Best-Response Dynamics.\r\n        \"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/layers/aggregation.py", "file_name": "aggregation.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "455", "text": "batch_size = outputs[0].shape[0]\r\n        \r\n        # Initialize weights uniformly: (B, L, 1)\r\n        # Each sample in batch can have different equilibrium\r\n        weights = torch.ones(batch_size, self.num_scales, 1, device=outputs[0].device) \r\n        weights = weights / self.num_scales\r\n        \r\n        # Iterative Best-Response (Unrolled Optimization)\r\n        # We simulate 'nash_iterations' steps of players adjusting strategies\r\n        stacked_outputs = torch.stack(outputs, dim=1) # (B, L, N, D)\r\n\r\n        for _ in range(self.nash_iterations):\r\n            # 1. Compute current Consensus (O*)\r\n            consensus = (stacked_outputs * weights.unsqueeze(-1)).sum(dim=1) # (B, N, D)\r\n            \r\n            # 2. Compute Utility/Error for each scale\r\n            # Error_l = || O_l - O* ||^2\r\n            # We want to minimize reconstruction error\r\n            diffs = stacked_outputs - consensus.unsqueeze(1) # (B, L, N, D)\r\n            errors = torch.norm(diffs, p=2, dim=(2, 3)) # (B, L)\r\n            \r\n            # 3. Update weights (Softmax over negative error -> minimized error gets higher weight)\r\n            # This is a differentiable approximation of argmin\r\n            weights = F.softmax(-errors, dim=1).unsqueeze(-1) # (B, L, 1)\r\n\r\n        # Final Aggregation using Equilibrium Weights\r\n        final_output = (stacked_outputs * weights.unsqueeze(-1)).sum(dim=1)\r\n        \r\n        return final_output, torch.tensor(0.0, device=final_output.device)\r\n\r\n    def forward(self, scale_outputs: List[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:\r\n        \"\"\"\r\n        Args:\r\n            scale_outputs: List of tensors [O_0, O_1, ..., O_L] with different lengths.\r\n            \r\n        Returns:\r\n            (Aggregated_Tensor, Aux_Loss)\r\n        \"\"\"\r\n        target_len = scale_outputs[0].size(1)\r\n        \r\n        # 1.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/layers/aggregation.py", "file_name": "aggregation.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "456", "text": "Returns:\r\n            (Aggregated_Tensor, Aux_Loss)\r\n        \"\"\"\r\n        target_len = scale_outputs[0].size(1)\r\n        \r\n        # 1. Upsample all scales to target length (Scale 0 length)\r\n        upsampled_outputs = [scale_outputs[0]]\r\n        for i in range(1, self.num_scales):\r\n            upsampled_outputs.append(\r\n                self._upsample(scale_outputs[i], target_len)\r\n            )\r\n            \r\n        # 2. Aggregate based on strategy\r\n        if self.strategy == 'convex':\r\n            return self.solve_convex(upsampled_outputs)\r\n        elif self.strategy == 'nash':\r\n            return self.solve_nash(upsampled_outputs)\r\n        else:\r\n            raise ValueError(f\"Unknown strategy: {self.strategy}\")", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/layers/aggregation.py", "file_name": "aggregation.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "457", "text": "import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport math\r\nfrom typing import List, Optional\r\nfrom .decomposition import HierarchicalDecomposition\r\n\r\nclass MultiscaleAttention(nn.Module):\r\n    \"\"\"\r\n    Implements Multiscale Attention Computation (Section 4.2 of MAHA paper).\r\n    \r\n    Features:\r\n    - Scale-specific Query (Q) and Key (K) projections.\r\n    - Shared Value (V) projection across all scales.\r\n    - Efficient computation using re-used downsampling operators for V.\r\n    \"\"\"\r\n    \r\n    def __init__(\r\n        self, \r\n        d_model: int, \r\n        num_heads: int, \r\n        num_scales: int,\r\n        decomposition_module: HierarchicalDecomposition\r\n    ):\r\n        super().__init__()\r\n        self.d_model = d_model\r\n        self.num_heads = num_heads\r\n        self.head_dim = d_model // num_heads\r\n        self.num_scales = num_scales\r\n        \r\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\r\n        \r\n        # 1. Scale-Specific Projections for Q and K\r\n        # We create a separate Linear layer for each scale l.\r\n        self.q_projs = nn.ModuleList([\r\n            nn.Linear(d_model, d_model) for _ in range(num_scales)\r\n        ])\r\n        self.k_projs = nn.ModuleList([\r\n            nn.Linear(d_model, d_model) for _ in range(num_scales)\r\n        ])\r\n        \r\n        # 2.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/layers/attention.py", "file_name": "attention.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "458", "text": "Shared Value Projection \r\n        # V_base = X * W_V\r\n        self.shared_v_proj = nn.Linear(d_model, d_model)\r\n        \r\n        # Reference to decomposition module to downsample V_base for each scale\r\n        # V_l = D_l(V_base) [cite: 124]\r\n        self.decomposition = decomposition_module\r\n\r\n    def forward(\r\n        self, \r\n        x_scales: List[torch.Tensor], \r\n        mask: Optional[torch.Tensor] = None\r\n    ) -> List[torch.Tensor]:\r\n        \"\"\"\r\n        Args:\r\n            x_scales (List[Tensor]): List of decomposed inputs [X_0, X_1, ..., X_L].\r\n                                     Output from HierarchicalDecomposition.forward().\r\n            mask (Tensor, optional): Standard attention mask (broadcastable).\r\n            \r\n        Returns:\r\n            List[Tensor]: List of attention outputs [O_0, O_1, ..., O_L] per scale.\r\n        \"\"\"\r\n        \r\n        # Step 1: Compute Base Value (V_base) from the original input (Scale 0)\r\n        # X_0 is usually the high-res input\r\n        x_base = x_scales[0] \r\n        v_base = self.shared_v_proj(x_base) # (B, N, d)\r\n        \r\n        # Step 2: Hierarchically decompose V_base using the SAME operators used for X\r\n        # This ensures V_l matches the length of Q_l and K_l\r\n        # [cite: 124] V_l = D_l(V_base)\r\n        v_scales = self.decomposition(v_base)\r\n        \r\n        outputs = []\r\n        \r\n        # Step 3: Compute Attention for each scale independently\r\n        for l in range(self.num_scales):\r\n            x_l = x_scales[l] # Input at scale l\r\n            v_l = v_scales[l] # Value at scale l\r\n            \r\n            B, N_l, _ = x_l.size()\r\n            \r\n            # Projections \r\n            q_l = self.q_projs[l](x_l)\r\n            k_l = self.k_projs[l](x_l)\r\n            \r\n            # Reshape for Multi-Head Attention: (B, N, H,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/layers/attention.py", "file_name": "attention.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "459", "text": "q_projs[l](x_l)\r\n            k_l = self.k_projs[l](x_l)\r\n            \r\n            # Reshape for Multi-Head Attention: (B, N, H, D_h) -> (B, H, N, D_h)\r\n            q_l = q_l.view(B, N_l, self.num_heads, self.head_dim).transpose(1, 2)\r\n            k_l = k_l.view(B, N_l, self.num_heads, self.head_dim).transpose(1, 2)\r\n            v_l = v_l.view(B, N_l, self.num_heads, self.head_dim).transpose(1, 2)\r\n            \r\n            # Scaled Dot-Product Attention [cite: 120]\r\n            # scores = (Q K^T) / sqrt(d_k)\r\n            scores = torch.matmul(q_l, k_l.transpose(-2, -1)) / math.sqrt(self.head_dim)\r\n            \r\n            if mask is not None:\r\n                # Need to resize mask for current scale if mask is provided\r\n                # Simplified masking logic for brevity (assuming causal or padding mask)\r\n                pass \r\n            \r\n            attn_weights = F.softmax(scores, dim=-1)\r\n            \r\n            # O_l = A_l * V_l [cite: 126]\r\n            context = torch.matmul(attn_weights, v_l)\r\n            \r\n            # Reshape back: (B, H, N, D_h) -> (B, N, D)\r\n            context = context.transpose(1, 2).contiguous().view(B, N_l, self.d_model)\r\n            \r\n            outputs.append(context)\r\n            \r\n        return outputs", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/layers/attention.py", "file_name": "attention.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "460", "text": "import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom typing import List, Literal\r\n\r\nclass HierarchicalDecomposition(nn.Module):\r\n    \"\"\"\r\n    Implements Hierarchical Multiscale Decomposition (Section 4.1 of MAHA paper).\r\n    \r\n    This layer decomposes the input sequence X into L hierarchical scales using\r\n    learnable downsampling operators (Strided Convolution) or Adaptive Pooling.\r\n    \r\n    Paper Reference:\r\n        Eq (5): X_l = D_l(X_{l-1})\r\n        Eq (109): Strided Convolution logic\r\n        Eq (112): Exponential decay schedule n_l = floor(n_{l-1} / r)\r\n    \"\"\"\r\n    \r\n    def __init__(\r\n        self, \r\n        d_model: int, \r\n        num_scales: int = 4, \r\n        compression_ratio: int = 2, \r\n        mode: Literal['conv', 'pool'] = 'conv',\r\n        kernel_size: int = 3\r\n    ):\r\n        \"\"\"\r\n        Args:\r\n            d_model (int): The embedding dimension (d).\r\n            num_scales (int): Number of hierarchical scales (L).\r\n            compression_ratio (int): Downsampling factor (r).\r\n            mode (str): 'conv' for learnable Strided Convolution, 'pool' for Adaptive Max Pooling.\r\n            kernel_size (int): Kernel size for convolution (default: 3).\r\n        \"\"\"\r\n        super().__init__()\r\n        self.d_model = d_model\r\n        self.num_scales = num_scales\r\n        self.compression_ratio = compression_ratio\r\n        self.mode = mode\r\n        \r\n        # We need (L-1) downsampling operators since Scale 0 is the original input.\r\n        # Using ModuleList to register parameters properly.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/layers/decomposition.py", "file_name": "decomposition.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "461", "text": "# Using ModuleList to register parameters properly.\r\n        self.downsamplers = nn.ModuleList()\r\n        \r\n        if self.mode == 'conv':\r\n            for _ in range(num_scales - 1):\r\n                # Eq (109): D_l(X) = Conv1D(X, W_l, s_l)\r\n                # Note: Groups=1 implies full interaction between channels as per W in R^{k x d x d}\r\n                self.downsamplers.append(\r\n                    nn.Conv1d(\r\n                        in_channels=d_model,\r\n                        out_channels=d_model,\r\n                        kernel_size=kernel_size,\r\n                        stride=compression_ratio,\r\n                        padding=kernel_size // 2  # To maintain consistent alignment\r\n                    )\r\n                )\r\n        elif self.mode == 'pool':\r\n            # Pooling doesn't require learnable parameters per scale, \r\n            # but we keep the structure consistent.\r\n            pass\r\n        else:\r\n            raise ValueError(f\"Unknown decomposition mode: {mode}\")\r\n\r\n    def forward(self, x: torch.Tensor) -> List[torch.Tensor]:\r\n        \"\"\"\r\n        Args:\r\n            x (torch.Tensor): Input tensor of shape (Batch, Seq_Len, d_model)\r\n            \r\n        Returns:\r\n            List[torch.Tensor]: A list of length `num_scales`.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/layers/decomposition.py", "file_name": "decomposition.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "462", "text": "Scale 0: (B, N, d)\r\n                                Scale 1: (B, N/r, d)\r\n                                ...\r\n        \"\"\"\r\n        # x shape: [Batch, Length, Dim] -> Transpose for Conv1d: [Batch, Dim, Length]\r\n        current_x = x.transpose(1, 2)\r\n        outputs = [x] # Scale 0 is the input itself [cite: 106]\r\n        \r\n        for i in range(self.num_scales - 1):\r\n            if self.mode == 'conv':\r\n                # Apply learnable strided convolution\r\n                # current_x represents X_{l-1}\r\n                next_x = self.downsamplers[i](current_x)\r\n                \r\n            elif self.mode == 'pool':\r\n                # Eq (111): Adaptive Pooling\r\n                # Calculate target length: n_l = floor(n_{l-1} / r)\r\n                current_len = current_x.size(2)\r\n                target_len = current_len // self.compression_ratio\r\n                \r\n                # Prevent collapse to 0 length for very deep hierarchies\r\n                target_len = max(1, target_len)\r\n                \r\n                next_x = F.adaptive_max_pool1d(current_x, output_size=target_len)\r\n            \r\n            # Update for next iteration\r\n            current_x = next_x\r\n            \r\n            # Transpose back to [Batch, Length, Dim] for attention processing\r\n            # and append to outputs list\r\n            outputs.append(current_x.transpose(1, 2))\r\n            \r\n        return outputs\r\n\r\n    def get_output_shapes(self, input_len: int) -> List[int]:\r\n        \"\"\"Helper to compute expected sequence lengths for debugging.\"\"\"\r\n        shapes = [input_len]\r\n        curr = input_len\r\n        for _ in range(self.num_scales - 1):\r\n            curr = curr // self.compression_ratio\r\n            shapes.append(curr)\r\n        return shapes", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/layers/decomposition.py", "file_name": "decomposition.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "463", "text": "import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nclass CrossScaleGating(nn.Module):\r\n    \"\"\"\r\n    Implements Cross-Scale Gating (Eq 12 in MAHA paper).\r\n    \r\n    Mechanics:\r\n    G_l = sigmoid(W_g * [C_l; U(C_{l+1})]) * C_l + (1 - sigmoid(...)) * U(C_{l+1})\r\n    \r\n    It dynamically blends information from the current scale and the coarser scale below it.\r\n    \"\"\"\r\n    \r\n    def __init__(self, d_model: int):\r\n        super().__init__()\r\n        # Input dimension is 2 * d_model because we concatenate current and next scale\r\n        self.gate_proj = nn.Linear(2 * d_model, d_model)\r\n        \r\n    def _upsample(self, tensor: torch.Tensor, target_len: int) -> torch.Tensor:\r\n        # Re-use nearest neighbor logic for consistency\r\n        tensor_p = tensor.transpose(1, 2)\r\n        upsampled = F.interpolate(tensor_p, size=target_len, mode='nearest')\r\n        return upsampled.transpose(1, 2)\r\n\r\n    def forward(self, current_scale: torch.Tensor, next_scale: torch.Tensor) -> torch.Tensor:\r\n        \"\"\"\r\n        Args:\r\n            current_scale (Tensor): Feature map at scale l (B, N_l, D)\r\n            next_scale (Tensor): Feature map at scale l+1 (Coarser) (B, N_{l+1}, D)\r\n            \r\n        Returns:\r\n            Tensor: Gated feature map at scale l (B, N_l, D)\r\n        \"\"\"\r\n        # 1. Upsample coarser scale to match current scale\r\n        target_len = current_scale.size(1)\r\n        next_scale_up = self._upsample(next_scale, target_len)\r\n        \r\n        # 2. Concatenate [C_l; U(C_{l+1})]\r\n        combined = torch.cat([current_scale, next_scale_up], dim=-1)\r\n        \r\n        # 3. Compute Gate Coefficient (z)\r\n        # z = Sigmoid(W_g * combined)\r\n        gate = torch.sigmoid(self.gate_proj(combined))\r\n        \r\n        # 4.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/layers/gating.py", "file_name": "gating.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "464", "text": "Compute Gate Coefficient (z)\r\n        # z = Sigmoid(W_g * combined)\r\n        gate = torch.sigmoid(self.gate_proj(combined))\r\n        \r\n        # 4. Apply Gating\r\n        # G_l = z * C_l + (1 - z) * U(C_{l+1})\r\n        output = gate * current_scale + (1 - gate) * next_scale_up\r\n        \r\n        return output", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/layers/gating.py", "file_name": "gating.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "466", "text": "# Hybrid Dilated-Convolutional Transformer Block\r\nimport torch\r\nimport torch.nn as nn\r\nfrom typing import Optional, Tuple\r\n\r\n# Önceki adımlarda yazdığımız modülleri import ediyoruz\r\nfrom src.layers.decomposition import HierarchicalDecomposition\r\nfrom src.layers.attention import MultiscaleAttention\r\nfrom src.layers.aggregation import OptimizationDrivenAggregation\r\n\r\nclass MAHABlock(nn.Module):\r\n    \"\"\"\r\n    Implements the Hybrid Dilated-Convolutional Transformer Block (Section 4.4).\r\n    \r\n    This block replaces the standard Self-Attention layer with the MAHA pipeline:\r\n    1. Dilated Convolution (Local Context) [cite: 142]\r\n    2. Hierarchical Decomposition [cite: 102]\r\n    3. Multiscale Attention with Shared Values [cite: 114]\r\n    4. Optimization-Driven Aggregation (Convex/Nash) [cite: 128]\r\n    5. Feed-Forward Network (Standard)\r\n    \"\"\"\r\n    \r\n    def __init__(\r\n        self,\r\n        d_model: int,\r\n        num_heads: int,\r\n        d_ff: int,\r\n        dropout: float = 0.1,\r\n        num_scales: int = 4,\r\n        compression_ratio: int = 2,\r\n        aggregation_strategy: str = 'convex'\r\n    ):\r\n        super().__init__()\r\n        \r\n        # 1. Pre-processing: Dilated Convolution to capture local context features\r\n        # Eq (11): C_l = ReLU(DilatedConv(X...))\r\n        # We apply a mild dilation here to enrich features before decomposition\r\n        self.dilated_conv = nn.Sequential(\r\n            nn.Conv1d(d_model, d_model, kernel_size=3, padding=2, dilation=2),\r\n            nn.ReLU(),\r\n            nn.Dropout(dropout)\r\n        )\r\n        \r\n        # 2.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/models/maha_block.py", "file_name": "maha_block.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "467", "text": "MAHA Components\r\n        self.decomposition = HierarchicalDecomposition(\r\n            d_model=d_model,\r\n            num_scales=num_scales,\r\n            compression_ratio=compression_ratio,\r\n            mode='conv'\r\n        )\r\n        \r\n        self.attention = MultiscaleAttention(\r\n            d_model=d_model,\r\n            num_heads=num_heads,\r\n            num_scales=num_scales,\r\n            decomposition_module=self.decomposition\r\n        )\r\n        \r\n        self.aggregation = OptimizationDrivenAggregation(\r\n            num_scales=num_scales,\r\n            d_model=d_model,\r\n            strategy=aggregation_strategy\r\n        )\r\n        \r\n        # 3. Standard Transformer Components (Norm & FFN)\r\n        self.norm1 = nn.LayerNorm(d_model)\r\n        self.norm2 = nn.LayerNorm(d_model)\r\n        \r\n        self.ffn = nn.Sequential(\r\n            nn.Linear(d_model, d_ff),\r\n            nn.GELU(),\r\n            nn.Dropout(dropout),\r\n            nn.Linear(d_ff, d_model),\r\n            nn.Dropout(dropout)\r\n        )\r\n        \r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\r\n        \"\"\"\r\n        Args:\r\n            x: Input tensor (Batch, Seq_Len, d_model)\r\n            mask: Attention mask\r\n            \r\n        Returns:\r\n            output: Tensor (Batch, Seq_Len, d_model)\r\n            aux_loss: Sparsity/Optimization loss from aggregation\r\n        \"\"\"\r\n        # Residual Connection 1 (MAHA Branch)\r\n        residual = x\r\n        \r\n        # A. Dilated Convolution (Requires Transpose for Conv1d)\r\n        # x: (B, N, D) -> (B, D, N)\r\n        x_conv = x.transpose(1, 2)\r\n        x_conv = self.dilated_conv(x_conv)\r\n        x_conv = x_conv.transpose(1, 2) # Back to (B, N, D)\r\n        \r\n        # B. Hierarchical Decomposition\r\n        # x -> [X_0, X_1, ...,", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/models/maha_block.py", "file_name": "maha_block.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "468", "text": "transpose(1, 2) # Back to (B, N, D)\r\n        \r\n        # B. Hierarchical Decomposition\r\n        # x -> [X_0, X_1, ..., X_L]\r\n        scales = self.decomposition(x_conv)\r\n        \r\n        # C. Multiscale Attention\r\n        # [X_0...X_L] -> [O_0...O_L]\r\n        attn_outputs = self.attention(scales, mask)\r\n        \r\n        # D. Aggregation (Convex or Nash)\r\n        # [O_0...O_L] -> O*\r\n        maha_out, aux_loss = self.aggregation(attn_outputs)\r\n        \r\n        # Apply projection and dropout\r\n        maha_out = self.dropout(maha_out)\r\n        \r\n        # Add & Norm\r\n        x = self.norm1(residual + maha_out)\r\n        \r\n        # Residual Connection 2 (FFN Branch)\r\n        residual = x\r\n        ffn_out = self.ffn(x)\r\n        x = self.norm2(residual + ffn_out)\r\n        \r\n        return x, aux_loss", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/models/maha_block.py", "file_name": "maha_block.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "469", "text": "# Full MAHA Transformer Architecture\r\nimport torch\r\nimport torch.nn as nn\r\nfrom src.models.maha_block import MAHABlock\r\n\r\nclass MAHATransformer(nn.Module):\r\n    \"\"\"\r\n    Full MAHA Transformer Architecture.\r\n    \r\n    It stacks L MAHA Blocks to form a powerful encoder for NLP tasks.\r\n    Compatible with tasks like Text Classification (GLUE) or Masked Language Modeling.\r\n    \r\n    Ref: Section 5.1 Experimental Setup \r\n         - 12 Layers\r\n         - 768 Hidden Dimension\r\n         - 12 Attention Heads\r\n    \"\"\"\r\n    \r\n    def __init__(\r\n        self,\r\n        vocab_size: int,\r\n        max_len: int = 512,\r\n        d_model: int = 768,\r\n        num_heads: int = 12,\r\n        num_layers: int = 12,\r\n        d_ff: int = 3072,\r\n        num_scales: int = 4,\r\n        aggregation_strategy: str = 'convex',\r\n        dropout: float = 0.1\r\n    ):\r\n        super().__init__()\r\n        \r\n        self.d_model = d_model\r\n        \r\n        # Token & Position Embeddings\r\n        self.token_emb = nn.Embedding(vocab_size, d_model)\r\n        self.pos_emb = nn.Embedding(max_len, d_model)\r\n        self.emb_dropout = nn.Dropout(dropout)\r\n        \r\n        # Stack of MAHA Blocks\r\n        self.layers = nn.ModuleList([\r\n            MAHABlock(\r\n                d_model=d_model,\r\n                num_heads=num_heads,\r\n                d_ff=d_ff,\r\n                dropout=dropout,\r\n                num_scales=num_scales,\r\n                aggregation_strategy=aggregation_strategy\r\n            )\r\n            for _ in range(num_layers)\r\n        ])\r\n        \r\n        # Final Norm (Pre-classifier)\r\n        self.final_norm = nn.LayerNorm(d_model)\r\n        \r\n        # Example Classifier Head (Optional, for GLUE tasks)\r\n        self.classifier = nn.Linear(d_model, vocab_size) # Or num_classes\r\n\r\n    def forward(self, x: torch.Tensor, mask: torch.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/models/transformer.py", "file_name": "transformer.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "470", "text": "for GLUE tasks)\r\n        self.classifier = nn.Linear(d_model, vocab_size) # Or num_classes\r\n\r\n    def forward(self, x: torch.Tensor, mask: torch.Tensor = None):\r\n        \"\"\"\r\n        Args:\r\n            x: Input tokens (Batch, Seq_Len)\r\n            mask: Attention mask\r\n        \"\"\"\r\n        B, N = x.size()\r\n        \r\n        # Embeddings\r\n        positions = torch.arange(0, N, device=x.device).unsqueeze(0)\r\n        x = self.token_emb(x) + self.pos_emb(positions)\r\n        x = self.emb_dropout(x)\r\n        \r\n        total_aux_loss = 0.0\r\n        \r\n        # Pass through MAHA Layers\r\n        for layer in self.layers:\r\n            x, layer_loss = layer(x, mask)\r\n            total_aux_loss += layer_loss\r\n            \r\n        x = self.final_norm(x)\r\n        \r\n        # Return features and aggregated sparsity loss for optimization\r\n        return x, total_aux_loss", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/models/transformer.py", "file_name": "transformer.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "471", "text": "# Differentiable Convex Optimization Layer", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/optimization/convex_solver.py", "file_name": "convex_solver.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "472", "text": "# Iterative Nash Equilibrium Solver", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/optimization/game_solver.py", "file_name": "game_solver.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "474", "text": "# Attention Masking Utilities", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/utils/masking.py", "file_name": "masking.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "475", "text": "# PPL, BLEU and Efficiency Metrics", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/src/utils/metrics.py", "file_name": "metrics.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "476", "text": "import unittest\r\nimport torch\r\nimport sys\r\nimport os\r\n\r\n# Proje kök dizinini path'e ekle\r\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\r\n\r\nfrom src.layers.decomposition import HierarchicalDecomposition\r\nfrom src.layers.attention import MultiscaleAttention\r\nfrom src.layers.aggregation import OptimizationDrivenAggregation\r\nfrom src.models.maha_block import MAHABlock\r\nfrom src.models.transformer import MAHATransformer\r\n\r\nclass TestMAHAComponents(unittest.TestCase):\r\n    \r\n    def setUp(self):\r\n        \"\"\"Test ortamı için ortak parametreler\"\"\"\r\n        self.batch_size = 2\r\n        self.seq_len = 128  \r\n        self.d_model = 64  # Küçük bir model boyutu\r\n        self.num_heads = 4 # 64'e bölünebilir (64 % 4 == 0)\r\n        self.num_scales = 3\r\n        self.r = 2 \r\n        \r\n        self.dummy_input = torch.randn(self.batch_size, self.seq_len, self.d_model)\r\n\r\n    def test_hierarchical_decomposition_shapes(self):\r\n        \"\"\"Test: Hiyerarşik ayrıştırma doğru boyutlarda tensör üretiyor mu?\"\"\"\r\n        decomp = HierarchicalDecomposition(self.d_model, self.num_scales, self.r, mode='conv')\r\n        outputs = decomp(self.dummy_input)\r\n        \r\n        self.assertEqual(len(outputs), self.num_scales)\r\n        self.assertEqual(outputs[0].shape, (self.batch_size, self.seq_len, self.d_model))\r\n        expected_len_1 = self.seq_len // self.r\r\n        self.assertEqual(outputs[1].shape[1], expected_len_1)\r\n        print(f\"✅ Decomposition Passed. Shapes: {[t.shape for t in outputs]}\")\r\n\r\n    def test_multiscale_attention_flow(self):\r\n        \"\"\"Test: Attention katmanı Shared Value ile çalışıyor mu?\"\"\"", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/tests/test_maha.py", "file_name": "test_maha.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "477", "text": "Shapes: {[t.shape for t in outputs]}\")\r\n\r\n    def test_multiscale_attention_flow(self):\r\n        \"\"\"Test: Attention katmanı Shared Value ile çalışıyor mu?\"\"\"\r\n        decomp = HierarchicalDecomposition(self.d_model, self.num_scales, self.r)\r\n        attn = MultiscaleAttention(self.d_model, self.num_heads, self.num_scales, decomp)\r\n        \r\n        scales = decomp(self.dummy_input)\r\n        attn_outputs = attn(scales)\r\n        \r\n        for i, out_tensor in enumerate(attn_outputs):\r\n            self.assertEqual(out_tensor.shape, scales[i].shape)\r\n        print(\"✅ Attention Passed.\")\r\n\r\n    def test_aggregation_convex(self):\r\n        \"\"\"Test: Convex Optimization birleştirme\"\"\"\r\n        aggregator = OptimizationDrivenAggregation(self.num_scales, self.d_model, strategy='convex')\r\n        inputs = [\r\n            torch.randn(self.batch_size, self.seq_len, self.d_model),\r\n            torch.randn(self.batch_size, self.seq_len // 2, self.d_model),\r\n            torch.randn(self.batch_size, self.seq_len // 4, self.d_model)\r\n        ]\r\n        output, loss = aggregator(inputs)\r\n        self.assertEqual(output.shape, inputs[0].shape)\r\n        self.assertTrue(torch.is_tensor(loss))\r\n        print(\"✅ Aggregation (Convex) Passed.\")\r\n\r\n    def test_aggregation_nash(self):\r\n        \"\"\"Test: Nash Equilibrium stratejisi\"\"\"\r\n        aggregator = OptimizationDrivenAggregation(self.num_scales, self.d_model, strategy='nash', nash_iterations=2)\r\n        inputs = [\r\n            torch.randn(self.batch_size, self.seq_len, self.d_model),\r\n            torch.randn(self.batch_size, self.seq_len // 2, self.d_model),\r\n            torch.randn(self.batch_size, self.seq_len // 4, self.d_model)\r\n        ]\r\n        output, _ = aggregator(inputs)\r\n        self.assertEqual(output.shape, inputs[0].shape)\r\n        print(\"✅ Aggregation (Nash) Passed.\")", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/tests/test_maha.py", "file_name": "test_maha.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "478", "text": "def test_full_maha_block(self):\r\n        \"\"\"Test: MAHABlock (End-to-End)\"\"\"\r\n        block = MAHABlock(\r\n            d_model=self.d_model,\r\n            num_heads=self.num_heads,\r\n            d_ff=self.d_model * 4,\r\n            num_scales=self.num_scales\r\n        )\r\n        output, aux_loss = block(self.dummy_input)\r\n        self.assertEqual(output.shape, self.dummy_input.shape)\r\n        print(\"✅ MAHABlock Passed.\")\r\n        \r\n    def test_transformer_integration(self):\r\n        \"\"\"Test: Tam Transformer modeli\"\"\"\r\n        vocab_size = 100\r\n        # DÜZELTME BURADA: num_heads parametresini açıkça veriyoruz\r\n        model = MAHATransformer(\r\n            vocab_size=vocab_size, \r\n            d_model=self.d_model, \r\n            num_heads=self.num_heads,  # <--- EKLENDİ (64 % 4 == 0)\r\n            num_layers=2\r\n        )\r\n        \r\n        input_ids = torch.randint(0, vocab_size, (self.batch_size, self.seq_len))\r\n        output, total_loss = model(input_ids)\r\n        \r\n        self.assertEqual(output.shape, (self.batch_size, self.seq_len, self.d_model))\r\n        print(\"✅ MAHATransformer Integration Passed.\")\r\n\r\nif __name__ == '__main__':\r\n    unittest.main()", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/tests/test_maha.py", "file_name": "test_maha.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "479", "text": "# Main Training Loop\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nfrom torch.utils.data import DataLoader, TensorDataset\r\nimport time\r\n\r\nfrom src.models.transformer import MAHATransformer\r\n\r\ndef train():\r\n    # --- 1. Hyperparameters (Section 5.1 Setup) ---\r\n    VOCAB_SIZE = 1000\r\n    SEQ_LEN = 128\r\n    BATCH_SIZE = 16\r\n    D_MODEL = 256        # Reduced for demo speed\r\n    NUM_HEADS = 4\r\n    NUM_LAYERS = 2\r\n    NUM_SCALES = 4\r\n    EPOCHS = 5\r\n    LR = 5e-5\r\n    LAMBDA_REG = 0.1     # Sparsity regularization coefficient (lambda)\r\n    \r\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n    print(f\"🚀 Training on device: {device}\")\r\n\r\n    # --- 2. Synthetic Data ---\r\n    # Random integers simulating token IDs\r\n    train_data = torch.randint(0, VOCAB_SIZE, (1000, SEQ_LEN))\r\n    # Random targets (e.g., for Masked LM or Classification)\r\n    train_labels = torch.randint(0, VOCAB_SIZE, (1000, SEQ_LEN))\r\n    \r\n    dataset = TensorDataset(train_data, train_labels)\r\n    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\r\n\r\n    # --- 3. Model Initialization ---\r\n    model = MAHATransformer(\r\n        vocab_size=VOCAB_SIZE,\r\n        max_len=SEQ_LEN,\r\n        d_model=D_MODEL,\r\n        num_heads=NUM_HEADS,\r\n        num_layers=NUM_LAYERS,\r\n        num_scales=NUM_SCALES,\r\n        aggregation_strategy='convex' # Try 'nash' as well\r\n    ).to(device)\r\n    \r\n    # Optimizer & Loss\r\n    optimizer = optim.AdamW(model.parameters(), lr=LR)\r\n    criterion = nn.CrossEntropyLoss()\r\n\r\n    print(f\"✅ Model initialized. Parameters: {sum(p.numel() for p in model.parameters()):,}\")\r\n\r\n    # --- 4. Training Loop ---\r\n    model.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/train.py", "file_name": "train.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "480", "text": "Parameters: {sum(p.numel() for p in model.parameters()):,}\")\r\n\r\n    # --- 4. Training Loop ---\r\n    model.train()\r\n    \r\n    for epoch in range(EPOCHS):\r\n        start_time = time.time()\r\n        total_loss = 0\r\n        total_task_loss = 0\r\n        total_aux_loss = 0\r\n        \r\n        for batch_idx, (inputs, targets) in enumerate(dataloader):\r\n            inputs, targets = inputs.to(device), targets.to(device)\r\n            \r\n            optimizer.zero_grad()\r\n            \r\n            # Forward Pass\r\n            # output: (B, Seq, D_model), aux_loss: Scalar (sum of L1 norms)\r\n            outputs, aux_loss = model(inputs)\r\n            \r\n            # Compute Task Loss (Flatten for CrossEntropy)\r\n            # outputs: (B*Seq, Vocab), targets: (B*Seq)\r\n            # Note: We need a projection to vocab size here if not in model\r\n            # For this demo, we use the classifier head inside MAHATransformer if it existed,\r\n            # or just project simply here:\r\n            logits = model.classifier(outputs) # (B, Seq, Vocab)\r\n            \r\n            task_loss = criterion(logits.view(-1, VOCAB_SIZE), targets.view(-1))\r\n            \r\n            # Combine Losses (Eq 9 in Paper)\r\n            loss = task_loss + (LAMBDA_REG * aux_loss)\r\n            \r\n            # Backward Pass\r\n            loss.backward()\r\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n            optimizer.step()\r\n            \r\n            total_loss += loss.item()\r\n            total_task_loss += task_loss.item()\r\n            total_aux_loss += aux_loss.item()\r\n            \r\n            if batch_idx % 10 == 0:\r\n                print(f\"Epoch {epoch+1} | Batch {batch_idx} | \"\r\n                      f\"Loss: {loss.item():.4f} (Task: {task_loss.item():.4f} + Aux: {aux_loss.item():.4f})\")\r\n        \r\n        avg_loss = total_loss / len(dataloader)\r\n        elapsed = time.", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/train.py", "file_name": "train.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
{"id": "481", "text": "item():.4f} (Task: {task_loss.item():.4f} + Aux: {aux_loss.item():.4f})\")\r\n        \r\n        avg_loss = total_loss / len(dataloader)\r\n        elapsed = time.time() - start_time\r\n        print(f\"🏁 End of Epoch {epoch+1} | Avg Loss: {avg_loss:.4f} | Time: {elapsed:.2f}s\")\r\n        print(\"-\" * 50)\r\n\r\nif __name__ == \"__main__\":\r\n    train()", "metadata": {"file_path": "/Users/dutchcaz/Projects/nash.mhc/references/MAHA-Project/train.py", "file_name": "train.py", "creation_date": "2026-01-02", "last_modified_date": "2026-01-02"}}
